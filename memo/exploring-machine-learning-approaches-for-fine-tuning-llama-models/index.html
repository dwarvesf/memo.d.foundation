<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content><meta name=keywords content><meta name=title content="Exploring Machine Learning Approaches For Fine Tuning Llama Models"><meta name=description content><meta property="og:title" content="Exploring Machine Learning Approaches For Fine Tuning Llama Models"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://note.d.foundation/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/"><meta property="og:image" content="https://note.d.foundation/img/LOGO.png"><meta property="twitter:title" content="Exploring Machine Learning Approaches For Fine Tuning Llama Models"><meta property="twitter:description" content><meta property="twitter:type" content="article"><meta property="twitter:url" content="https://note.d.foundation/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/"><meta property="twitter:image" content="https://note.d.foundation/img/LOGO.png"><link rel=icon type=image/x-icon href=https://note.d.foundation/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://note.d.foundation/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://note.d.foundation/favicon-32x32.png><link rel=apple-touch-icon href=https://note.d.foundation/apple-touch-icon.png><title>Exploring Machine Learning Approaches For Fine Tuning Llama Models</title>
<link rel=canonical href=https://note.d.foundation/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-vs.min.css integrity="sha512-Jn4HzkCnzA7Bc+lbSQHAMeen0EhSTy71o9yJbXZtQx9VvozKVBV/2zfR3VyuDFIxGvHgbOMMNvb80l+jxFBC1Q==" crossorigin=anonymous referrerpolicy=no-referrer media="(prefers-color-scheme: dark)"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-vsc-dark-plus.min.css integrity="sha512-ML8rkwYTFNcblPFx+VLgFIT2boa6f8DDP6p6go4+FT0/mJ8DCbCgi6S0UdjtzB3hKCr1zhU+YVB0AHhIloZP8Q==" crossorigin=anonymous referrerpolicy=no-referrer media="(prefers-color-scheme: light)"><script defer data-domain=note.d.foundation src=https://plausible.io/js/script.js></script><script defer src=https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js></script><link rel=preconnect href=https://B0BWKXLVM9-dsn.algolia.net crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@docsearch/css@3><script src=https://cdn.jsdelivr.net/npm/@docsearch/js@3></script><style>body{font-family:ibm plex sans,sans-serif;font-size:1rem;color:#000;text-rendering:optimizeLegibility;padding:2rem;max-width:100%;margin:0;padding:0;background-color:var(--primary-background-color-light);line-height:1.5;min-height:100vh;display:flex;flex-direction:column;hyphens:auto}h1,h2,h3,h4{color:var(--primary-font-color);line-height:1.24}h1{font-size:32pt}h2{font-size:24pt}h3{font-size:16pt}h4{font-size:12pt}a[href^="#"]{text-decoration:none}time{padding-right:1rem}blockquote{margin:1em 0;padding:0 2em;border-left:3px solid #eee}img{max-width:100%}video{max-width:100%}a,a:visited{color:#000}q{font-size:20px;max-width:500px;display:block;margin-left:30px}pre{font-family:monospace!important;background:#000!important;color:#dadada;padding:4px!important;margin:0!important;border-radius:2px;overflow-x:auto;width:100%;max-width:100vw!important;box-sizing:border-box;background-clip:padding-box}pre>code{text-indent:0;white-space:inherit;font-weight:400;padding:15px!important;box-sizing:border-box;background-clip:padding-box}code{line-height:1.5!important;font-family:monospace!important;font-size:12px!important;background:#333!important;color:#dadada;padding:2px 5px;white-space:wrap;border-radius:2px;font-weight:700;display:inline-block;margin-bottom:-4px;box-sizing:border-box;background-clip:padding-box}code comment{color:#777}table code{margin:0;background:var(--secondary-background-color-light)!important;color:var(--secondary-font-color-light);border:.5px solid var(--secondary-font-color-light)}span a,span a:visited{color:inherit}[style*="color:yellow"]{color:gold!important}[style*="color:blue"]{color:#6495ed!important}[style*="color:red"]{color:crimson!important}@media(prefers-color-scheme:dark){a,a:visited{color:#dadada}pre{font-family:monospace!important;background:#fff!important;color:var(--primary-background-color)}code{font-family:monospace!important;background:#ddd!important;color:var(--primary-background-color)}table code{background:var(--secondary-background-color)!important;color:var(--secondary-font-color)}}iframe{box-sizing:border-box;max-width:100%}@media(prefers-color-scheme:light){h1,h2,h3,h4{color:var(--primary-font-color-light)}}@media screen and (max-width:1100px){h1{font-size:calc(1.45rem + 2vw)}h2{font-size:calc(1rem + 1.5vw)}h3{font-size:calc(1rem + 1vw)}h4{font-size:calc(1rem + .5vw)}}:root{--primary-color:#e13F5e;--docsearch-primary-color:#e13F5e;--docsearch-primary-color:#e13F5e;--primary-font-color:#ddd;--primary-font-color-light:#333;--secondary-font-color:#e8e8e8;--secondary-font-color-light:#717171;--primary-background-color:#1e1e1e;--primary-background-color-light:#fff;--secondary-background-color:#282828;--secondary-background-color-light:#f6f6f6;--border-color:#3d3d3d;--border-color-light:#e7e7e7;--container-max-width:1240px;--main-padding:36px;--column-gap:24px;--nav-sidebar-width:200px;--nav-sidebar-offset:0px}.grid .title{grid-area:title;margin:32px 0 0!important;color:var(--primary-color)}.grid .subtitle{grid-area:subtitle;font-size:18px;padding-top:16px;padding-bottom:32px}.grid #docsearch{grid-area:docsearch}.grid #docsearch>button{width:90%;height:75%;border-radius:0}.grid #docsearch>button>span{padding-left:12px;font-family:ibm plex sans,sans-serif}.DocSearch-Footer{box-shadow:none}.active{color:var(--primary-color);line-height:1;border-bottom:2px solid var(--primary-color);padding-top:4px!important}nav.tab-menu{grid-area:tabmenu;border-bottom:1px solid var(--border-color-light);padding-bottom:0;overflow:auto}nav.tab-menu ul{list-style-type:none;padding:0;margin-bottom:0;display:inline-grid;grid-auto-flow:column;grid-template-rows:1fr;grid-template-columns:repeat(auto-fit,minmax(min-content,1fr));column-gap:48px}nav.tab-menu ul li{font-size:16px;font-weight:500;display:inline-block;padding:0 0 16px;width:min-content}nav.tab-menu ul li a{text-decoration:none;color:inherit}nav.tab-menu ul li a:hover{background-color:var(--primary-background-color);padding:0}nav.tab-menu ul li:last-of-type{margin-right:30px!important}@media(prefers-color-scheme:dark){nav.tab-menu{border-bottom:1px solid var(--border-color)}}@media only screen and (max-width:1100px){.DocSearch{width:100%!important;height:50px!important;padding:0;margin:0}}nav.menu{padding:calc(var(--main-padding) - 12px)var(--main-padding);line-height:22px;white-space:nowrap;grid-area:nav}details{margin-left:-7px;overflow:hidden;font-size:14px}summary{display:block;padding:5px 5px 12px 20px;position:relative;cursor:pointer;font-size:20px;font-weight:600;text-transform:capitalize;user-select:none}nav section details:not(:last-of-type) ul{margin-bottom:24px}summary:before{content:'\25B8';position:absolute;top:7px;left:0;transform:rotate(0);transform-origin:50% 50%;transition:.25s transform ease;font-size:16px;height:16px;width:16px;font-weight:200;display:flex;justify-content:center;align-items:center}summary:before>*{height:4px;width:4px}details[open]>summary:before{transform:rotate(90deg)}details summary::-webkit-details-marker{display:none}details>ul{margin-bottom:0}hr{margin:20px 0!important;width:100%;max-width:inherit;border-color:var(--secondary-background-color-light)}nav.menu{background-color:var(--secondary-background-color-light)}nav .site-nav{display:inline-block;width:100%}nav .site-nav section{margin-top:-2px}nav .site-nav section h2{margin:20px 0;font-size:24px;text-transform:lowercase}nav.menu ul{display:inline-grid;grid-template-rows:repeat(auto-fill,minmax(0,1fr));grid-template-columns:repeat(auto-fill,minmax(var(--nav-sidebar-width),1fr));padding:0 10px 10px 0;margin:0;list-style-type:none;line-height:22px;box-sizing:border-box}nav .site-nav ul li{list-style-type:none;display:flex;font-variant-numeric:tabular-nums;font-size:14px;padding-left:20px;max-width:fit-content}nav .site-nav ul li a{padding:0 4px}.load-more{font-style:italic;padding-top:8px;color:var(--primary-color)!important;cursor:pointer}@media(prefers-color-scheme:dark){nav.menu{background-color:var(--secondary-background-color)}hr{border-color:var(--secondary-background-color)}}@media only screen and (max-width:1100px){nav.menu{padding:4.5%}nav.menu ul{width:100%;row-gap:4px;column-gap:4px}nav .site-nav section{margin:0 10px 10px 0}nav.menu{max-width:100%;box-sizing:border-box;display:inline-block}}footer{padding:var(--main-padding);clear:both;border-top:1px solid var(--border-color-light)}footer span{color:var(--secondary-font-color-light)}@media(prefers-color-scheme:dark){footer{border-top:1px solid var(--border-color)}footer span{color:var(--secondary-font-color)}}.note-title{display:inline-grid;width:100%;row-gap:12px;margin:0;grid-template-areas:"pagetitle pagetitle" "date tags" "authors tags";grid-template-rows:repeat(auto-fill,minmax(0,1fr));grid-template-columns:repeat(2,1fr)}.note-title .pagetitle{grid-area:pagetitle;margin:0}.note-title .date{grid-area:date;color:#636466}.note-title .tags{display:inline-flex;flex-wrap:wrap;gap:6px;grid-area:tags;justify-self:right;justify-content:right}.note-title .tags .tag{padding:1px 6px;border-radius:4px;background-color:var(--secondary-background-color-light);height:fit-content;text-decoration:none;color:var(--secondary-font-color-light)}.note-title .authors{grid-area:authors}.clear-title{grid-template-areas:"pagetitle pagetitle";gap:0}.clear-title .date{display:none}.clear-title .tags{display:none}.clear-title .authors{display:none}code.button{background:var(--primary-background-color-light);color:#000;font-size:smaller;display:inline-block;padding:0 6px;font-weight:700;border-radius:2px;line-height:22px}.grid{display:inline-grid;grid-template-areas:"title title title title" "subtitle subtitle docsearch docsearch" "tabmenu tabmenu tabmenu tabmenu" "nav content content pagenav" "nav content content pagenav";grid-template-rows:repeat(auto-fill,minmax(0,1fr));grid-template-columns:325px 2fr 2fr 1fr;column-gap:var(--column-gap)}.grid>*{padding-right:calc(var(--main-padding) - 12px);padding-left:calc(var(--main-padding) + 12px)}#docsearch{grid-area:docsearch}main{grid-area:content;display:inline-block;padding:var(--main-padding);width:calc(100vw - 325px - 72px);max-width:var(--container-max-width);box-sizing:border-box;overflow-wrap:break-word;hyphens:none;justify-self:left}main ul{line-height:25px;margin:0;padding:0;padding-inline-start:40px}main ul>li>a{text-decoration:underline}.scrollable{overflow:auto}.dual-list>ul{columns:2}.single-list>ul{columns:2}.profile img{width:100px;height:100px;object-fit:cover;aspect-ratio:1/1}.profile object{width:100px;height:100px;object-fit:cover;aspect-ratio:1/1}table,th,td{border:1px solid;border-collapse:collapse;border-spacing:0;border-color:var(--border-color-light)}thead *{padding:6px 12px;background-color:var(--secondary-background-color-light)}th,td{padding:6px 12px;overflow:hidden;text-overflow:ellipsis;vertical-align:top;text-align:left}td:has(img){padding:12px;line-height:0}table{table-layout:fixed}table:has(th:empty){border:none}table:has(th:empty) th{border:none;padding:0;line-height:0}table:has(th:empty) td{border:none}table:has(img){width:100%;height:100%}table:has(img) td{min-width:200px}table:has(img) img{width:100%!important;height:100%!important;object-fit:cover}#TableOfContents>ul>li>a{text-decoration:underline}#TableOfContents>ul{display:grid;column-gap:var(--column-gap);box-sizing:border-box;grid-template-columns:repeat(auto-fill,minmax(0,calc(50% - var(--column-gap))));line-height:20px}#TableOfContents li{white-space:nowrap;list-style-type:decimal-leading-zero;list-style-position:inside;overflow:hidden;text-overflow:ellipsis}.notice{padding:20px;border:2px solid #000;margin-bottom:16px}main>blockquote{padding:20px;border:2px solid #000}main>*{max-width:inherit}li:has(input[type=checkbox]){display:block;list-style-type:none}ul:has(li input[type=checkbox]){padding:0}ul li a{overflow:hidden;text-overflow:ellipsis;white-space:wrap;text-decoration:none;transition:all .25s ease}ul li a:hover{background-color:var(--primary-color);color:#fff;padding-left:4px;padding-right:4px;border-radius:4px;width:100%;z-index:10}ul li p{margin-block-end:0;margin-block-start:0}@media(prefers-color-scheme:dark){html[data-theme=dark]{--docsearch-searchbox-background:var(--secondary-background-color);--docsearch-modal-background:var(--secondary-background-color);--docsearch-footer-background:var(--secondary-background-color);--docsearch-key-gradient:linear-gradient(-26.5deg, #555, var(--secondary-background-color));--docsearch-muted-color:#777;--docsearch-key-shadow:inset 0 -2px 0 0 var(--secondary-background-color),inset 0 0 1px 1px #555,0 2px 2px 0 rgba(3,4,9,0.3)}body{background:var(--primary-background-color);color:#dadada}body select{color:#000;background-color:#fff}.project{border-left:2px solid #fff}.notice{border-color:#fff}.mono{filter:invert(1)}.progress>div{background-color:#fff}.progress>div>span{color:#000}main>blockquote{border-color:#fff}ul li a{background-color:inherit}section>ul li a{background-color:inherit}.note-title .tags .tag{background-color:var(--secondary-background-color);color:var(--secondary-font-color)}table,th,td{border-color:var(--border-color)}thead *{background-color:var(--secondary-background-color)}}@media only screen and (max-width:1100px){.grid{display:inline-grid;grid-template-areas:"title" "subtitle" "docsearch" "tabmenu" "content" "nav" "pagenav";grid-template-rows:repeat(auto-fill,minmax(0,1fr));grid-template-columns:1fr}.grid>*{padding-right:4.5%;padding-left:4.5%}.note-title{width:100%;grid-template-areas:"pagetitle" "date" "tags" "authors";grid-template-rows:repeat(auto-fill,minmax(0,1fr));grid-template-columns:1fr}.note-title .tags{max-width:max-content;justify-self:left;justify-content:flex-start}main{justify-self:center;width:100%;max-width:700px}}.col-2{display:inline-grid;column-gap:36px;padding-bottom:36px;grid-template-columns:repeat(auto-fit,minmax(20%,1fr))}.col-2>*{overflow-wrap:break-word;overflow:auto}</style><script defer src=/js/process-comments.js></script><script defer src=/js/tab-menu.js></script><script defer src=/js/markdown-entities.js></script><script defer src=/js/algolia.js></script><script defer src=/js/dark-light-check.js></script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"memo","name":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","headline":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","alternativeHeadline":"","description":"At Dwarves, we\u0026rsquo;ve been increasingly exposed to more state-of-the-art news coming from AI than ever before, of course, related to Large Language Models (LLM). We\u0026rsquo;ve had a taste of what AI has to offer with Stable Diffusion and more commercial apps, and have been eager to learn and hone our skillsets in applying these new AI breakthroughs in our everyday lives and our apps.\nIntroduction If 2021 was the year of blockchain, it\u0026rsquo;s probably safe to say that 2023 is the year of generative AI.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/note.d.foundation\/memo\/exploring-machine-learning-approaches-for-fine-tuning-llama-models\/"},"author":{"@type":"Person","name":"Han Ngo"},"creator":{"@type":"Person","name":"Han Ngo"},"accountablePerson":{"@type":"Person","name":"Han Ngo"},"copyrightHolder":"Dwarves Foundation","copyrightYear":"2023","date":"2023-05-04T00:00:00.00Z","dateCreated":"2023-05-04T00:00:00.00Z","datePublished":"2023-05-04T00:00:00.00Z","dateModified":"2023-05-04T00:00:00.00Z","publisher":{"@type":"Organization","name":"Dwarves Foundation","url":"https://note.d.foundation/","logo":{"@type":"ImageObject","url":"https:\/\/note.d.foundation\/","width":"32","height":"32"}},"image":"https://note.d.foundation/","url":"https:\/\/note.d.foundation\/memo\/exploring-machine-learning-approaches-for-fine-tuning-llama-models\/","wordCount":"1851","genre":["machine learning","LLM","engineering"],"keywords":["machine learning","LLM","engineering"]}</script><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><script type=text/javascript>window.addEventListener("DOMContentLoaded",function(){document.body.style.visibility="visible",document.body.style.opacity=1})</script></head><body><div class=grid><h1 class=title><a style=color:inherit;text-decoration:none href=/site-index>Dwarves Notes</a></h1><div class=subtitle>A collection of notes for everything we do and operate at Dwarves.</div><div id=docsearch></div><nav class=tab-menu x-data="{
    currentUrl: `/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/`,
    links: [
      { title: 'Home', url: '/home/' },
      { title: 'Consulting', url: '/memo/consulting/' },
      { title: 'Labs', url: '/memo/labs/' },
      { title: 'Earn', url: '/earn/' },
      { title: 'Hiring', url: '/hiring/' },
      { title: 'Newsletter', url: '/newsletter/' },
      { title: 'Events', url: '/events/' },
    ],
    activeUrl: '',
    isActiveUrl(url) {
      if (!this.activeUrl) {
        this.activeUrl = this.currentUrl.startsWith(url);
        return this.activeUrl;
      }
    },
  }"><ul><template x-for="item in links"><li x-bind:class="{ 'active': isActiveUrl(item.url) }"><a x-bind:href=item.url x-text=item.title></a></li></template></ul></nav><nav class=menu><section class=site-nav x-data='{
      currentUrl: `/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/`,
      get data() {
        const dataString = `[{"date":"2023-11-30T00:00:00Z","tags":["dwarves","forward-engineering","labs","duckdb","AI","rust","ui-practices","passwordless"],"title":"November Forward Engineering 2023","url":"/memo/forward-engineering-november-2023/"},{"date":"2023-11-23T00:00:00Z","tags":["dwarves","work","duckdb","internal","workshop","discussion","demo","event","labs"],"title":"DuckDB demo and showcase","url":"/memo/labs/events/duckdb-demo-and-showcase/"}]`;
        const data = JSON.parse(dataString);
        data.sort((a, b) => Date.parse(b.date) - Date.parse(a.date));
        return data;
      },
    }'><details open><summary>Pinned Notes</summary><ul><template x-for="item in data" x-bind:key=item.url><li><a x-bind:href=item.url x-text=item.title></a></li></template></ul></details></section></nav><main x-data='{
      currentUrl: `/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/`,
      title: `Exploring Machine Learning Approaches For Fine Tuning Llama Models`,
      get tagsData() {
        const dataString = `[{"tags":["machine learning","LLM","engineering"],"url":"/memo/exploring-machine-learning-approaches-for-fine-tuning-llama-models/"}]`;
        const data = JSON.parse(dataString);
        return data;
      },
      get hideTitle() {
        const hideTitle = (/true/i).test(``);
        return hideTitle;
      },
      isClearPage() {
        const hideFrontmatter = (/true/i).test(`false`);
        return hideFrontmatter || this.currentUrl.includes(&#39;home&#39;) || this.currentUrl.includes(&#39;tags&#39;);
      },
      isTagPage() {
        return this.currentUrl.includes(&#39;tags&#39;);
      }
    }'><template x-if=!hideTitle><div x-bind:class="{ 'note-title': true, 'clear-title': isClearPage() }"><div class=title-index style=display:none>Exploring Machine Learning Approaches For Fine Tuning Llama Models</div><div class=tags-index style=display:none>tags: machine learning, LLM, engineering</div><template x-if=isTagPage()><h1 class=pagetitle x-text="`#${title}`"></h1></template><template x-if=!isTagPage()><h1 class=pagetitle x-text=title>Exploring Machine Learning Approaches For Fine Tuning Llama Models</h1></template><div class=date>May 4, 2023</div><template x-for="data in tagsData"><template x-if="data.tags && data.tags.length > 0"><div class=tags>Tags:
<template x-for="tag in data.tags"><a class=tag x-bind:href="`/tags/${tag.toLowerCase()}`" x-text=tag></a></template></div></template></template></div></template><template x-if=!hideTitle><hr></template><p><img x-data="{
    altText: `b6f4054793fd3b5ef748c5dcf072cc09_MD5.webp`,
    get altWidthHeight() {
      const widthHeightRegex = /^(\d+)(?!x)(\d*)$/g;
      return this.altText.match(widthHeightRegex) ?? [];
    },
  }" src=/memo/assets/exploring-machine-learning-approaches-for-fine-tuning-llama-models/b6f4054793fd3b5ef748c5dcf072cc09_MD5.webp alt=b6f4054793fd3b5ef748c5dcf072cc09_MD5.webp x-bind:style="{ width: altWidthHeight[0] + 'px', height: altWidthHeight[1] + 'px' }" onerror="this.onerror=null"></p><p><em>At Dwarves, we&rsquo;ve been increasingly exposed to more state-of-the-art news coming from AI than ever before, of course, related to Large Language Models (LLM). We&rsquo;ve had a taste of what AI has to offer with Stable Diffusion and more commercial apps, and have been eager to learn and hone our skillsets in applying these new AI breakthroughs in our everyday lives and our apps.</em></p><h2 id=introduction><a href=#introduction>Introduction <a href=#introduction></a></a></h2><p>If 2021 was the year of blockchain, it&rsquo;s probably safe to say that 2023 is the year of generative AI. The pace and progress of AI, and by extension AGI, is becoming very hard to keep up. Apps using OpenAI ChatGPT are just saturating the market, but there are already fears that ChatGPT plugins could take over a good majority of their use cases.</p><p>There has also been an increasing amount of interest in custom <a href=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/>LLaMA</a> models, almost a similar trend to what we saw with Stable Diffusion against DALL-E. The landscape for LLMs has been progressing at a neck-breaking pace, with the mean time for outdated AI news becoming closer to within a single day.</p><p>We&rsquo;re at a point where everything is moving fast and <a href="https://www.linkedin.com/posts/chiphuyen_now-is-the-time-to-get-into-ai-in-2016-activity-7054911279328555008-xLc8/?utm_source=share&amp;utm_medium=member_desktop">no one is yet an expert</a> in the field of AI. We felt that we would get left behind if we at least didn&rsquo;t take a look at the technical side of AI, which eventually motivated our research in LLMs.</p><h2 id=prior-research><a href=#prior-research>Prior Research <a href=#prior-research></a></a></h2><p>For AI, a lot of us at Dwarves use available tools to help us to do extensive learning, get over writer&rsquo;s block, experiment, and generally make our lives a little bit easier. A handful of us, including myself, have dabbled a bit in Stable Diffusion, mostly to create fun pictures, but also to help us get an idea of the current landscape of generative art.</p><p>For research on LLMs, we&rsquo;ve investigated vector databases and how to apply a basic form of indexing on them for use with OpenAI. You can check out our basic example at <a href=https://df-doc-search.vercel.app/>https://df-doc-search.vercel.app/</a> and ask it some questions about our company, although don&rsquo;t expect too much ðŸ˜¶.</p><p>Likewise, we&rsquo;ve created a few Jupyter notebooks working on <a href=https://python.langchain.com/en/latest/index.html>Langchain</a> and what strategies and utilities we use from it to generate more directed results. You can view some of what we&rsquo;ve worked on and noted here:</p><ul><li><a href=https://brain.d.foundation/Engineering/AI/Workaround+with+OpenAI%27s+token+limit+with+Langchain>Workaround with OpenAI&rsquo;s token limit with Langchain - The Dwarves Brainery</a></li><li><a href=https://brain.d.foundation/Engineering/AI/Working+with+langchain+document+loaders>Working with langchain document loaders - The Dwarves Brainery</a></li><li><a href="https://colab.research.google.com/drive/1FMbBYPLz01lLma4jK36-fABjQC2vhyRJ?usp=sharing">https://colab.research.google.com/drive/1FMbBYPLz01lLma4jK36-fABjQC2vhyRJ?usp=sharing</a></li><li><a href=https://github.com/dudaka/automating-pdf-interaction-with-langchain-and-chatgpt>https://github.com/dudaka/automating-pdf-interaction-with-langchain-and-chatgpt</a></li></ul><h2 id=problem><a href=#problem>Problem <a href=#problem></a></a></h2><p>Using OpenAI is great, but we will eventually find ourselves needing to use more private LLM models. Unlike Microsoft&rsquo;s Azure, a lot of companies don&rsquo;t have the opportunity or financial resources to make deals with OpenAI for data security and fine-tuning privacy for their foundational AIs. Along with efforts on engineering prompts with Langchain, we want to eventually fine-tune our own LLMs to suit more specialized needs to then pipeline them together for more complex use cases in the future.</p><p>While we want to fine-tune more private (and of course personal) LLMs, we want to do it in a way that doesn&rsquo;t reinvent the wheel and break the bank. We don&rsquo;t want to spend thousands of dollars just to recreate something that ChatGPT already does. There already has been huge progress in the open-source community with <a href=https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm>Dolly 2.0</a> and <a href=https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models>StableLM</a> and we&rsquo;re not going to win the race on base models even if we joined.</p><h2 id=adapter-fine-tuning-with-peft-lora><a href=#adapter-fine-tuning-with-peft-lora>Adapter fine-tuning with PEFT LoRA <a href=#adapter-fine-tuning-with-peft-lora></a></a></h2><p>One novel approach to enhancing the performance of LLMs involves the fine-tuning of LLaMA models using a technique called <a href=https://github.com/huggingface/peft>PEFT LoRA</a> (Parameter-Efficient Fine-Tuning with Layer Rotation Attention). PEFT LoRA offers a cost-effective and efficient way to adapt models with very little data, given a strong instruction model. It is very similar to Dreambooth LoRA for Stable Diffusion, but with much less hassle.</p><h3 id=how-does-it-work><a href=#how-does-it-work>How does it work? <a href=#how-does-it-work></a></a></h3><p>PEFT LoRA works on top of pre-trained language models by adding LoRA weights to the feed-forward layer of the transformer. It does this in a way without needing to fine-tune all of the model&rsquo;s parameters. This is particularly great if the majority of the AI workload is in vector indexing and we just want a chatbot specialized for a particular dialogue path without sounding too stupid.</p><p><img x-data="{
    altText: `20f6cea84bc8d93af997167eb7b3f224_MD5.webp`,
    get altWidthHeight() {
      const widthHeightRegex = /^(\d+)(?!x)(\d*)$/g;
      return this.altText.match(widthHeightRegex) ?? [];
    },
  }" src=/memo/assets/exploring-machine-learning-approaches-for-fine-tuning-llama-models/20f6cea84bc8d93af997167eb7b3f224_MD5.webp alt=20f6cea84bc8d93af997167eb7b3f224_MD5.webp x-bind:style="{ width: altWidthHeight[0] + 'px', height: altWidthHeight[1] + 'px' }" onerror="this.onerror=null"></p><p>The trained weights from PEFT LoRA are significantly much smaller (within a few MBs depending on your data) and don&rsquo;t require as much CPU/GPU power to fine-tune existing frozen models.</p><h2 id=proof-of-concept><a href=#proof-of-concept>Proof of Concept <a href=#proof-of-concept></a></a></h2><p>For the moment, we just want to get our first foot out the door, since not many of us have experience in creating machine learning pipelines. The proof of concept here will be to:</p><ul><li>Train with base model to output PEFT LoRA adapters</li><li>Use the PEFT LoRA adapters with the base model at runtime and ask it questions</li><li>Save the adapter files for later use</li><li>Merge the PEFT LoRA adapters to the frozen model and save those files for later use</li></ul><p>As such, the dataset we will use here will be sparse and probably won&rsquo;t do much to change the pattern behavior of the model. Things like feature engineering or labeling data appropriate for instruction or prompt tuning we can hold off for later.</p><h3 id=preparing-data-for-instruct-tuning><a href=#preparing-data-for-instruct-tuning>Preparing data for instruct-tuning <a href=#preparing-data-for-instruct-tuning></a></a></h3><p>We will do some basic instruct-tuning using <a href=https://huggingface.co/wxjiao/alpaca-7b>wxjiao/alpaca-7b</a> as the base model. We set up our prompts for training, similar to <a href=https://github.com/gururise/AlpacaDataCleaned>https://github.com/gururise/AlpacaDataCleaned</a>, which will have an instruction query, an input for contextual reference, and an expected output dialogue:</p><pre><code class=language-json>{
    &quot;instruction&quot;: &quot;What is Dwarves Foundation's community all hands?&quot;,
    &quot;input&quot;: &quot;&quot;,
    &quot;output&quot;: &quot;An event hosted every end of the month on Friday, at a Discord stage where we talk about our company progress along with notable news and wins across the month. After every community all hands, we host a company dinner where everyone working at Dwarves are invited.&quot;
}
</code></pre><h3 id=loading-the-dataset><a href=#loading-the-dataset>Loading the dataset <a href=#loading-the-dataset></a></a></h3><p>We then load our very small sample dataset to a data map, that will have our list of data points tokenized and formatted to our <code>generate_prompt</code> function.</p><pre><code class=language-python>from datasets import load_dataset

data = load_dataset(&quot;json&quot;,
                    data_files=&quot;./dwarves-dataset/dwarves_sample_dataset.json&quot;)

def generate_prompt(data_point):
    # taken from https://github.com/tloen/alpaca-lora
    if data_point[&quot;instruction&quot;]:
        return f&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{data_point[&quot;instruction&quot;]}

### Input:
{data_point[&quot;input&quot;]}

### Response:
{data_point[&quot;output&quot;]}&quot;&quot;&quot;
    else:
        return f&quot;&quot;&quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{data_point[&quot;instruction&quot;]}

### Response:
{data_point[&quot;output&quot;]}&quot;&quot;&quot;

data = data.map(lambda data_point: {&quot;prompt&quot;: tokenizer(generate_prompt(data_point))})
</code></pre><h3 id=fine-tuning-the-model><a href=#fine-tuning-the-model>Fine-tuning the model <a href=#fine-tuning-the-model></a></a></h3><p>Training the model is surprisingly simple and transparent. Most of the work for the fine-tuning is really in how we configure it, and of course how we label our data.</p><p>We first need to set our config for the <a href=https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py>LoraConfig</a> class. We prepare them as environment variables to help manage them easier. There are also some environment variables set specifically to our Google Colab resources, which we used to run our fine-tuning.</p><pre><code class=language-python># Settings for A100 - For 3090
MICRO_BATCH_SIZE = 4  # change to 4 for 3090
BATCH_SIZE = 128
GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE
EPOCHS = 2  # paper uses 3
LEARNING_RATE = 2e-5
CUTOFF_LEN = 256
LORA_R = 4
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
</code></pre><p>We then prepare our model for training and apply our environment variables to our <code>LoraConfig</code> and bind it to our <code>model</code>:</p><pre><code class=language-python>model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)

config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)
model = get_peft_model(model, config)
tokenizer.pad_token_id = 0

data = load_dataset(&quot;json&quot;, data_files=&quot;./dwarves-dataset/dwarves_sample_dataset.json&quot;)

data = data.shuffle().map(
    lambda data_point: tokenizer(
        generate_prompt(data_point),
        truncation=True,
        max_length=CUTOFF_LEN,
        padding=&quot;max_length&quot;,
    )
)
</code></pre><p>With the updated <code>model</code>, we then use it to set up our model <code>trainer</code> with the rest of the training variables we set earlier.</p><pre><code class=language-python>trainer = transformers.Trainer(
    model=model,
    train_dataset=data[&quot;train&quot;],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=MICRO_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=100,
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=True,
        logging_steps=1,
        output_dir=&quot;lora-alpaca&quot;,
        save_total_limit=3,
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False
</code></pre><p>All that is left to do is run the <code>trainer</code> to train our model. Thankfully that method is a simple one-liner. The trainer will then output how many steps it has progressed through and output what the <a href=https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss>training loss</a> is for each step.</p><pre><code class=language-python>trainer.train(resume_from_checkpoint=False)
</code></pre><p><img x-data="{
    altText: `21b3ea87423ab6377aed99987dbad148_MD5.webp`,
    get altWidthHeight() {
      const widthHeightRegex = /^(\d+)(?!x)(\d*)$/g;
      return this.altText.match(widthHeightRegex) ?? [];
    },
  }" src=/memo/assets/exploring-machine-learning-approaches-for-fine-tuning-llama-models/21b3ea87423ab6377aed99987dbad148_MD5.webp alt=21b3ea87423ab6377aed99987dbad148_MD5.webp x-bind:style="{ width: altWidthHeight[0] + 'px', height: altWidthHeight[1] + 'px' }" onerror="this.onerror=null"></p><p>Then if we want to save the file to our disk, we can use the <code>save_pretrained</code> method on our model to save it to a named folder.</p><pre><code class=language-python>model.save_pretrained(&quot;alpaca-lora-dwarves&quot;)
</code></pre><h3 id=talking-to-the-model><a href=#talking-to-the-model>Talking to the model <a href=#talking-to-the-model></a></a></h3><p>We will use <a href=https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py.>Dolly 2.0&rsquo;s instruction pipeline</a> through <code>InstructionTextGenerationPipeline</code> to help us communicate and produce responses with the model:</p><pre><code class=language-python>generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)

generate_text(&quot;What is the cold start problem in serverless architecture?&quot;)

# The cold start problem in serverless architecture is the phenomenon of a serverless function being triggered multiple times, which can significantly increase its execution time and costs. This is because each time the function is triggered, the entire code has to be executed, even if the previous execution didn't finish. To mitigate this, serverless functions can be designed to leverage environment variables, which allow certain variables to be set and shared between executions of the function.&lt;/s&gt;
</code></pre><h3 id=saving-the-models-to-huggingface><a href=#saving-the-models-to-huggingface>Saving the models to HuggingFace <a href=#saving-the-models-to-huggingface></a></a></h3><p>On Google Colab or in Jupyter notebooks, we can directly login to HuggingFace with the use of an API token:</p><pre><code class=language-python>from huggingface_hub import notebook_login

notebook_login()
</code></pre><p>Then we can push our PEFT LoRA adapter files to a directed repository like so (<code>monotykamary/alpaca-7b-lora-dwarves-poc</code>):</p><pre><code class=language-python>model.push_to_hub(&quot;monotykamary/alpaca-7b-lora-dwarves-poc&quot;, use_auth_token=True)
</code></pre><p>In case we want to merge the adapter and base model to create a new model, we can use the <code>merge_and_unload</code> method and save it to our disk:</p><pre><code class=language-python>model = model.merge_and_unload()
model.save_pretrained(&quot;monotykamary/alpaca-7b-lora-merged-dwarves-poc&quot;)
</code></pre><p>If you want to then push that model to HuggingFace, since we&rsquo;ve transformed our model, the function to push will be the same as above, but we&rsquo;ll just direct it to a different repository (<code>monotykamary/alpaca-7b-lora-merged-dwarves-poc</code>):</p><pre><code class=language-python>model.push_to_hub(&quot;monotykamary/alpaca-7b-lora-merged-dwarves-poc&quot;, use_auth_token=True)
</code></pre><h3 id=full-google-colab-example--huggingface><a href=#full-google-colab-example--huggingface>Full Google Colab Example + HuggingFace <a href=#full-google-colab-example--huggingface></a></a></h3><p>All of our examples, findings, and work are available on our Google Colab. You can view it there to get a full picture of our training pipeline.</p><ul><li><a href="https://colab.research.google.com/drive/1c1t_QG62r9gxQ5ygKGvMbEZYep94ViAD?usp=sharing">https://colab.research.google.com/drive/1c1t_QG62r9gxQ5ygKGvMbEZYep94ViAD?usp=sharing</a></li></ul><p>You can also view our dataset and our output models on HuggingFace:</p><ul><li><a href=https://huggingface.co/datasets/monotykamary/dwarves-dataset>monotykamary/dwarves-dataset Â· Datasets at Hugging Face</a></li><li><a href=https://huggingface.co/monotykamary/alpaca-7b-lora-dwarves-poc>monotykamary/alpaca-7b-lora-dwarves-poc Â· Hugging Face</a></li><li><a href=https://huggingface.co/monotykamary/alpaca-7b-lora-merged-dwarves-poc>monotykamary/alpaca-7b-lora-merged-dwarves-poc Â· Hugging Face</a></li></ul><h2 id=further-work><a href=#further-work>Further work <a href=#further-work></a></a></h2><p>Our next step will likely be experimenting with different tuning methods and with much larger datasets to help our models better output certain dialogue patterns. We are interested in testing out a few ideas on applying Langchain with our fine-tuned models as well.</p><p>In addition, we also aim to make our pipeline more cloud agnostic by using <a href=https://skypilot.readthedocs.io/en/latest/index.html>SkyPilot</a>. Google Colab and other tools have their own hardware limitations, not to mention high overhead costs for hosting portable Jupyter notebooks. Tools like SkyPilot is the novel equivalent of an easier-to-use Terraform for AI workloads. <a href=https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna>Vicuna also used SkyPilot to fine-tune their models</a>, so we&rsquo;re excited to try it out.</p><h2 id=conclusion><a href=#conclusion>Conclusion <a href=#conclusion></a></a></h2><p>In conclusion, after looking through a ton of resources, we were able to walk our first baby steps into the world of LLMs. Fine-tuning with PEFT LoRA has been a very insightful experience and has kind of opened our eyes to the fact of how low the barrier to entry AI has gotten. There&rsquo;s still of course a bit of nuance and &ldquo;maneuvering&rdquo; left to do with these models before they are app-ready, but we&rsquo;re looking forward to how we can apply these in the future.</p><h2 id=references><a href=#references>References <a href=#references></a></a></h2><ul><li><a href=https://renaissancerachel.com/prompting/>https://renaissancerachel.com/prompting/</a></li><li><a href=https://xiaosean5408.medium.com/fine-tuning-llms-made-easy-with-lora-and-generative-ai-stable-diffusion-lora-39ff27480fda>https://xiaosean5408.medium.com/fine-tuning-llms-made-easy-with-lora-and-generative-ai-stable-diffusion-lora-39ff27480fda</a></li><li><a href=https://arxiv.org/abs/2206.15312>https://arxiv.org/abs/2206.15312</a></li></ul></main></div><footer><span><b>Dwarves, LLC</b> Â© 2015 - 2023 All rights reserved.</span></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js></script></body></html>