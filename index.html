<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Dwarves Memo - Home</title><meta property="title" content="Dwarves Memo - Home" data-next-head=""/><meta property="og:title" content="Dwarves Memo - Home" data-next-head=""/><meta name="description" content="Knowledge sharing platform for Dwarves Foundation" data-next-head=""/><meta property="og:description" content="Knowledge sharing platform for Dwarves Foundation" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:site_name" content="Dwarves Memo" data-next-head=""/><link rel="icon" type="image/x-icon" href="{{ $favicon.Permalink }}" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><link rel="alternate" type="application/rss+xml" title="Dwarves Memo - Home - RSS Feed" href="/feed.xml" data-next-head=""/><link rel="alternate" type="application/atom+xml" title="Dwarves Memo - Home - Atom Feed" href="/atom.xml" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com" data-next-head=""/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/0cc4cb6eb0ac7c1f.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=Public+Sans:ital,wght@0,100..900;1,100..900&amp;family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&amp;display=swap" rel="stylesheet" data-next-head=""/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><link rel="stylesheet" href="/_next/static/css/0cc4cb6eb0ac7c1f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8caccc7e2f67a840.js" defer=""></script><script src="/_next/static/chunks/framework-6e85e635ddcbf499.js" defer=""></script><script src="/_next/static/chunks/main-d8ff59cf99127f56.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c18a2b88d5866408.js" defer=""></script><script src="/_next/static/chunks/09244f9f-698dc360f6b3d7a1.js" defer=""></script><script src="/_next/static/chunks/4779-2bb41cf208b57e7c.js" defer=""></script><script src="/_next/static/chunks/205-c772e70bea947412.js" defer=""></script><script src="/_next/static/chunks/pages/index-6d5f3b1edfff250f.js" defer=""></script><script src="/_next/static/uBOsB0SlboeWG4_OQtqeb/_buildManifest.js" defer=""></script><script src="/_next/static/uBOsB0SlboeWG4_OQtqeb/_ssgManifest.js" defer=""></script><style id="__jsx-a10783270b45f6c2">[data-rk] .ju367v1g{font-weight:500}[data-rk] .ju367v1h{font-weight:500}[data-rk] .ju367v1i{font-weight:600}</style></head><body class="min-h-screen antialiased"><script>
              (function() {
                // Get saved theme or default to system preference
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const savedTheme = localStorage.getItem('theme');
                
                // Default to system preference if no saved preference
                const theme = (savedTheme === 'light' || savedTheme === 'dark') 
                  ? savedTheme 
                  : (prefersDark ? 'dark' : 'light');
                
                // Apply theme
                if (theme === 'dark') {
                  document.documentElement.classList.add('dark');
                  document.documentElement.setAttribute('data-theme', 'dark');
                } else {
                  document.documentElement.classList.remove('dark');
                  document.documentElement.setAttribute('data-theme', 'light');
                }
              })();
            </script><link rel="preload" as="image" href="/assets/home_cover.webp"/><div id="__next"><div data-rk=""><style>[data-rk]{--rk-blurs-modalOverlay:small;--rk-fonts-body:var(--font-sans);--rk-radii-actionButton:var(--radius);--rk-radii-connectButton:var(--radius);--rk-radii-menuButton:var(--radius);--rk-radii-modal:var(--radius);--rk-radii-modalMobile:var(--radius);--rk-colors-accentColor:var(--primary);--rk-colors-accentColorForeground:var(--primary-foreground);--rk-colors-actionButtonBorder:transparent;--rk-colors-actionButtonBorderMobile:transparent;--rk-colors-actionButtonSecondaryBackground:var(--muted);--rk-colors-closeButton:var(--muted-foreground);--rk-colors-closeButtonBackground:var(--muted);--rk-colors-connectButtonBackground:var(--muted);--rk-colors-connectButtonBackgroundError:var(--primary);--rk-colors-connectButtonInnerBackground:var(--background);--rk-colors-connectButtonText:var(--muted-foreground);--rk-colors-connectButtonTextError:var(--primary-foreground);--rk-colors-connectionIndicator:var(--primary);--rk-colors-downloadBottomCardBackground:linear-gradient(126deg, rgba(255, 255, 255, 0) 9.49%, rgba(171, 171, 171, 0.04) 71.04%), #FFFFFF;--rk-colors-downloadTopCardBackground:linear-gradient(126deg, rgba(171, 171, 171, 0.2) 9.49%, rgba(255, 255, 255, 0) 71.04%), #FFFFFF;--rk-colors-error:var(--primary);--rk-colors-generalBorder:var(--border);--rk-colors-generalBorderDim:var(--border);--rk-colors-menuItemBackground:var(--muted);--rk-colors-modalBackdrop:color-mix(in oklab, var(--color-black) 50%, transparent);--rk-colors-modalBackground:var(--background);--rk-colors-modalBorder:var(--border);--rk-colors-modalText:var(--foreground);--rk-colors-modalTextDim:var(--muted-foreground);--rk-colors-modalTextSecondary:var(--muted-foreground);--rk-colors-profileAction:var(--muted);--rk-colors-profileActionHover:var(--accent);--rk-colors-profileForeground:var(--foreground);--rk-colors-selectedOptionBorder:var(--primary);--rk-colors-standby:var(--primary);--rk-shadows-connectButton:0px 4px 12px rgba(0, 0, 0, 0.1);--rk-shadows-dialog:0px 8px 32px rgba(0, 0, 0, 0.32);--rk-shadows-profileDetailsAction:0px 2px 6px rgba(37, 41, 46, 0.04);--rk-shadows-selectedOption:0px 2px 6px rgba(0, 0, 0, 0.24);--rk-shadows-selectedWallet:0px 2px 6px rgba(0, 0, 0, 0.12);--rk-shadows-walletLogo:0px 2px 16px rgba(0, 0, 0, 0.16);}</style><div class="bg-background border-border w-sidebar-mobile xl:w-sidebar fixed top-0 left-0 z-40 flex h-full flex-col border-r pt-4 pb-12 font-sans transition-transform duration-300 ease-in-out translate-x-[-100%] xl:translate-x-0 "><a class="mx-4 flex h-10 items-center gap-2 px-2 xl:mx-0 xl:justify-center" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6.25 w-6 min-w-6"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="inline-block font-sans text-xs leading-tight font-bold tracking-tight uppercase xl:hidden">Dwarves<br/>Memo</span></a><nav class="flex flex-1 flex-col p-4 xl:items-center xl:px-2"><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start text-primary" id="sidebar-item-0" data-state="closed" data-slot="tooltip-trigger" href="/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/home.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/home.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Home</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-1" data-state="closed" data-slot="tooltip-trigger" href="/consulting/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/consulting.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/consulting.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Consulting</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-2" data-state="closed" data-slot="tooltip-trigger" href="/earn/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/earn.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/earn.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Earn</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-3" data-state="closed" data-slot="tooltip-trigger" href="/careers/hiring/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/hiring.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/hiring.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Hiring</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-4" data-state="closed" data-slot="tooltip-trigger" href="/updates/digest/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/digest.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/digest.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Digest</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-5" data-state="closed" data-slot="tooltip-trigger" href="/updates/ogif/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/ogifs.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/ogifs.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">OGIFs</span></a></nav><div class="mx-4 border-t pt-1 xl:mx-2"><div class="flex items-center justify-between gap-3 p-2 xl:justify-center"><button class="flex cursor-pointer items-center justify-center hover:opacity-80"><svg viewBox="0 0 20 20" width="24" height="24"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></button><span class="inline-block flex-1 shrink-0 text-sm leading-6 font-medium xl:hidden">Night mode</span><button class="bg-border flex h-5 w-9 cursor-pointer items-center justify-center rounded-full py-0.5 pr-4.5 pl-0.5 hover:opacity-95 xl:hidden"><div class="text-foreground-light rounded-full bg-white p-0.5"><svg viewBox="0 0 20 20" width="12" height="12"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></div></button></div></div></div><div class="bg-background text-foreground relative flex h-screen font-sans transition-colors "><div id="sidebar" class="bg-background-secondary xl:w-directory-width h-[calc(100svh-32px)] w-0 flex-col pt-10 pb-2 text-sm leading-normal xl:pr-3 xl:ml-sidebar translate-0 transition duration-100 ease-in-out z-2 overflow-y-auto reading:opacity-0 reading:translate-x-[-10%] reading:pr-0 reading:w-0 reading:ml-0"><div class=""><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/pinned/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Pinned Notes</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium text-muted-foreground pl-2" href="/playbook/operations/ogif/"><span>OGIF - Oh God It&#x27;s Friday</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Home</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/careers/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Careers</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/consulting/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Consulting</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/earn/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Earn</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/fund/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Fund</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/handbook/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Handbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/opensource/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Opensource</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playbook/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playground/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playground</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/rfc/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>RFC</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/updates/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Updates</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Popular Tags</span></a></div></div></div><div class="relative flex flex-1 flex-col overflow-y-auto"><header class="bg-background/95 supports-[backdrop-filter]:bg-background/60 top-0 w-full shrink-0 font-sans backdrop-blur"><div class="mx-auto flex h-full items-center justify-between border-b p-2 xl:border-none xl:px-5"><div class="flex items-center gap-2.5"><button id="sidebar-toggle" aria-label="Toggle sidebar" class="flex h-10 w-10 cursor-pointer items-center justify-center focus:outline-none xl:hidden"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="text-current"><path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><a class="flex items-center gap-2 xl:hidden" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-[32px] w-[29px] min-w-6 shrink-0"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="font-ibm-sans text-xs text-[11px] leading-[14.849px] font-bold -tracking-[0.157px] uppercase">Dwarves<br/>Memo</span></a></div><div class="ml-auto flex items-center gap-3.5"><div class="command-palette relative z-50"><button class="hover:border-border hidden w-50 cursor-pointer justify-between rounded-lg border border-transparent bg-transparent px-3 py-1.5 transition-all duration-100 ease-in-out md:flex" aria-label="Open command palette"><div class="flex items-center gap-0.5"><span class="text-muted-foreground text-sm filter-[opacity(50%)]">🔍 Search note</span></div><div class="text-muted-foreground flex items-center gap-0.5 text-xs"><kbd class="text-black-secondary dark:bg-border dark:text-foreground rounded-[2px] bg-[#F7F7F7] px-1.5 py-0.5 font-sans shadow-[0px_2px_0px_0px_#D4D3D0] dark:shadow-[0px_2px_0px_0px_#2D2D2D]">⌘</kbd><kbd class="text-black-secondary dark:bg-border dark:text-foreground rounded-[2px] bg-[#F7F7F7] px-1.5 py-0.5 font-sans shadow-[0px_2px_0px_0px_#D4D3D0] dark:shadow-[0px_2px_0px_0px_#2D2D2D]">K</kbd></div></button><button class="text-foreground flex h-10 w-10 items-center justify-center border-none bg-transparent p-0 md:hidden" aria-label="Open search"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 16 16" fill="none" class="text-foreground" aria-hidden="true"><circle cx="6.88881" cy="6.8889" r="5.55556" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></circle><path d="M11.3333 11.3333L14.6666 14.6667" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><button class="hidden cursor-pointer items-center justify-center border-0 bg-transparent outline-none hover:opacity-95 active:opacity-100 xl:flex" aria-label="Toggle reading mode" data-reading-mode="false" data-state="closed" data-slot="tooltip-trigger"><svg width="48" height="28" viewBox="0 0 62 34" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-7 w-12 xl:w-14"><g><rect width="62" height="34" rx="17" class="fill-border dark:fill-border"></rect><g class="transition-transform duration-150 ease-in-out translate-x-0"><circle cx="17" cy="17" r="14" class="fill-white"></circle><path d="M17 23.898V18.3265C17 17.9747 17.1398 17.6373 17.3885 17.3885C17.6373 17.1398 17.9747 17 18.3265 17C18.6783 17 19.0158 17.1398 19.2645 17.3885C19.5133 17.6373 19.6531 17.9747 19.6531 18.3265V21.2449H21.7755C22.3384 21.2449 22.8782 21.4685 23.2763 21.8666C23.6744 22.2646 23.898 22.8045 23.898 23.3673V23.898" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2119 12.8561C14.8891 11.4004 13.114 10.4334 11.1736 10.1113C11.0416 10.0926 10.9071 10.1022 10.7791 10.1395C10.6511 10.1768 10.5324 10.2409 10.4311 10.3275C10.3279 10.4158 10.245 10.5253 10.1883 10.6487C10.1315 10.772 10.1021 10.9062 10.1021 11.0419V18.6088C10.1007 18.8411 10.1854 19.0658 10.3399 19.2394C10.4944 19.413 10.7077 19.5232 10.9386 19.5487C12.4542 19.7543 13.8794 20.354 15.0774 21.276" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2124 15.7885V12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M21.4852 19.5487C21.7161 19.5232 21.9295 19.413 22.084 19.2394C22.2385 19.0658 22.3232 18.8411 22.3218 18.6088V11.0419C22.3218 10.9062 22.2924 10.772 22.2356 10.6487C22.1788 10.5253 22.096 10.4158 21.9928 10.3275C21.8915 10.2409 21.7728 10.1768 21.6447 10.1395C21.5167 10.1022 21.3823 10.0926 21.2502 10.1113C19.3098 10.4334 17.5347 11.4004 16.2119 12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path></g></g></svg></button><div class="h-9 w-9"></div></div></div></header><div class="main-grid relative w-full flex-1 flex-col"><div class="right-sidebar leading-[140% xl:w-right-sidebar-width hidden font-sans text-sm font-medium xl:flex transition-[transform,opacity,visibility] duration-100 ease-in-out visible w-0 translate-x-0 transform opacity-100 reading:opacity-0 reading:xl:translate-x-[10px] reading:invisible reading:fixed reading:right-[calc((100vw-var(--container-max-width)-var(--nav-sidebar-width))/2-var(--right-sidebar-width)-var(--column-gap))]"><div class="sticky top-[60px] right-0 flex w-full flex-col gap-y-8 pt-4 pb-10 transition-[top] duration-200 ease-in-out"></div></div><main class="main-content mx-auto max-w-[var(--container-max-width)] min-w-0 flex-1 p-[var(--main-padding-mobile)] font-serif xl:p-[var(--main-padding)]"><img alt="" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" class="yggdrasil-tree no-zoom pointer-events-none object-contain opacity-[0.03] md:w-[20vw] xl:w-[20vw] dark:opacity-100 absolute bottom-8 left-1/2 w-[50vw] max-w-xs -translate-x-1/2 xl:translate-x-[80%]" style="color:transparent" src="/assets/img/footer-bg.svg"/><div class="memo-content pb-8"><div class="font-serif"><img src="/assets/home_cover.webp" class="no-zoom max-h-[500px] rounded-sm"/><p class="mt-[var(--element-margin)]">Welcome to the Dwarves Memo.</p><p class="mt-[var(--element-margin)]">This site is a part of our continuous learning engine, where we want to build up the 1% improvement habit, learning in public.</p><p class="mt-[var(--element-margin)]">Written by Dwarves for product craftsmen.</p><p class="mt-[var(--element-margin)]">Learned by engineers. Experimented by engineers.</p><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">💡 OGIFs</h2><div id="changelog" class="link-v-list mt-2.5 flex flex-col gap-1.5" data-placement="vertical"><div id="memo-1" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/41-20250314/">#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing</a></div><div id="memo-2" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/39-20250207/">#39 Frontend report, DB Scaling, AI Workflow</a></div><div id="memo-3" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/38-20250117/">#38 Erlang automata, AI Trends, Year-End Awards</a></div><div id="memo-4" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/37-20241227/">#37 AI Fine-tuning, Data archiving, Datalakes</a></div><div id="memo-5" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/28-20241018/">#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis</a></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">✨ New memos</h2><div class="v-list" data-placement="vertical"><div id="memo-1" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Productivity" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/000-productivity/">Productivity</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao/">tieubao</a></div></div></div><div id="memo-2" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Software Quality" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/001-quality/">Software Quality</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao/">tieubao</a></div></div></div><div id="memo-3" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Open Source" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/002-open-source/">Open Source</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao/">tieubao</a></div></div></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">🧑‍💻 Life at Dwarves</h2><div class="v-list" data-placement="vertical"><div id="memo-1" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/playground/01_literature/record-reward-sharing-culture/">Record and reward sharing at Dwarves</a><div class="text-foreground line-clamp-2 text-sm font-normal">Discover how the Dwarves community fosters a culture of continuous learning and knowledge sharing. With a monthly reward pool of up to 2500 ICY, contributors are recognized for sharing valuable insights, especially in areas like AI/LLM, Golang, and more. Get involved and grow with us.</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">September 05, 2024</div></div></div><div id="memo-2" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/careers/life/2023-11-27-22-cat-nguyen/">Cat Nguyen</a><div class="text-foreground line-clamp-2 text-sm font-normal">Cat Nguyen shares her experience as a Junior Backend Engineer at Dwarves, highlighting the supportive team culture and how asking for help accelerated her growth</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">November 27, 2023</div></div></div><div id="memo-3" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/playbook/operations/red-flags/">Red Flags</a><div class="text-foreground line-clamp-2 text-sm font-normal">These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">February 16, 2023</div></div></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">📝 Changelog</h2><div id="changelog" class="link-v-list mt-2.5 flex flex-col gap-1.5" data-placement="vertical"><div id="memo-1" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/15-new-year-gathering/">#15 New year gathering</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->February 04, 2025</span></div><div id="memo-2" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/14-back-to-the-office/">#14 Hybrid work harmony</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->September 25, 2024</span></div><div id="memo-3" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/13-more-than-lines-of-code/">#13 More than lines of code</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->July 20, 2024</span></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">🤝 Open positions</h2><div class="v-list" data-placement="vertical"></div><div class="font-sans"><h2 class="font-ibm-sans mt-6 text-[10px] font-medium uppercase">Love what we are doing?</h2><ul class="xs:grid-cols-2 mt-2.5 grid list-none gap-2.5 pl-0"><li><a href="https://discord.gg/dfoundation" class="text-primary text-sm">🩷 Join our Discord Network →</a></li><li><a href="https://github.com/dwarvesf/playground" class="text-primary text-sm">🔥 Contribute to our Memo →</a></li><li><a href="https://careers.d.foundation/" class="text-primary text-sm">🤝 Join us, we are hiring →</a></li><li><a href="http://memo.d.foundation/earn/" class="text-primary text-sm">🙋 Give us a helping hand →</a></li></ul></div></div></div></main><div class="toc-space"></div></div></div><footer class="border-t-border bg-background fixed right-0 bottom-0 left-0 z-40 flex h-8 items-stretch overflow-hidden border-t px-3 py-0 text-[0.875rem] leading-[140%] font-normal tracking-[-0.0125rem]"><div class="socials flex items-center gap-x-[10px] pr-3"><a href="https://github.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Plus/Github"><path id="Vector" d="M9 1.5C8.01509 1.5 7.03982 1.69399 6.12987 2.0709C5.21993 2.44781 4.39314 3.00026 3.6967 3.6967C2.29018 5.10322 1.5 7.01088 1.5 9C1.5 12.315 3.6525 15.1275 6.63 16.125C7.005 16.185 7.125 15.9525 7.125 15.75V14.4825C5.0475 14.9325 4.605 13.4775 4.605 13.4775C4.26 12.6075 3.7725 12.375 3.7725 12.375C3.09 11.91 3.825 11.925 3.825 11.925C4.575 11.9775 4.9725 12.6975 4.9725 12.6975C5.625 13.8375 6.7275 13.5 7.155 13.32C7.2225 12.8325 7.4175 12.5025 7.6275 12.315C5.9625 12.1275 4.215 11.4825 4.215 8.625C4.215 7.7925 4.5 7.125 4.9875 6.5925C4.9125 6.405 4.65 5.625 5.0625 4.6125C5.0625 4.6125 5.6925 4.41 7.125 5.3775C7.7175 5.2125 8.3625 5.13 9 5.13C9.6375 5.13 10.2825 5.2125 10.875 5.3775C12.3075 4.41 12.9375 4.6125 12.9375 4.6125C13.35 5.625 13.0875 6.405 13.0125 6.5925C13.5 7.125 13.785 7.7925 13.785 8.625C13.785 11.49 12.03 12.12 10.3575 12.3075C10.6275 12.54 10.875 12.9975 10.875 13.695V15.75C10.875 15.9525 10.995 16.1925 11.3775 16.125C14.355 15.12 16.5 12.315 16.5 9C16.5 8.01509 16.306 7.03982 15.9291 6.12987C15.5522 5.21993 14.9997 4.39314 14.3033 3.6967C13.6069 3.00026 12.7801 2.44781 11.8701 2.0709C10.9602 1.69399 9.98491 1.5 9 1.5Z" fill="#9B9B9B"></path></g></svg></a><a href="https://discord.gg/dfoundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Discord"><path id="Union" d="M14.1919 3.95003C13.2419 3.50716 12.2133 3.18572 11.1418 3C11.1324 2.9997 11.1231 3.00146 11.1144 3.00517C11.1058 3.00887 11.0981 3.01442 11.0918 3.02143C10.9632 3.25715 10.8132 3.5643 10.7132 3.80002C9.57677 3.62859 8.42102 3.62859 7.28456 3.80002C7.18456 3.55716 7.03455 3.25715 6.89884 3.02143C6.89169 3.00715 6.87026 3 6.84883 3C5.77738 3.18572 4.75592 3.50716 3.79875 3.95003C3.79161 3.95003 3.78447 3.95717 3.77732 3.96431C1.83441 6.87154 1.29868 9.70019 1.56298 12.5003C1.56298 12.5145 1.57012 12.5288 1.58441 12.536C2.87015 13.4789 4.1059 14.0503 5.32737 14.4289C5.34879 14.436 5.37022 14.4289 5.37737 14.4146C5.66309 14.0217 5.92024 13.6074 6.14167 13.1717C6.15596 13.1431 6.14167 13.1146 6.1131 13.1074C5.70595 12.9503 5.32022 12.7646 4.94164 12.5503C4.91307 12.536 4.91307 12.4931 4.9345 12.4717C5.01307 12.4145 5.09164 12.3503 5.17022 12.2931C5.1845 12.2788 5.20593 12.2788 5.22022 12.286C7.67743 13.4074 10.3275 13.4074 12.7561 12.286C12.7704 12.2788 12.7919 12.2788 12.8061 12.2931C12.8847 12.3574 12.9633 12.4145 13.0419 12.4788C13.0704 12.5003 13.0704 12.5431 13.0347 12.5574C12.6633 12.7788 12.2704 12.9574 11.8633 13.1146C11.8347 13.1217 11.8275 13.1574 11.8347 13.1789C12.0633 13.6146 12.3204 14.0289 12.599 14.4217C12.6204 14.4289 12.6419 14.436 12.6633 14.4289C13.8919 14.0503 15.1276 13.4789 16.4134 12.536C16.4277 12.5288 16.4348 12.5145 16.4348 12.5003C16.7491 9.26446 15.9134 6.45724 14.2205 3.96431C14.2133 3.95717 14.2062 3.95003 14.1919 3.95003ZM6.51311 10.7931C5.77738 10.7931 5.16307 10.1145 5.16307 9.27875C5.16307 8.44301 5.76309 7.76442 6.51311 7.76442C7.27028 7.76442 7.87029 8.45015 7.86315 9.27875C7.86315 10.1145 7.26313 10.7931 6.51311 10.7931ZM11.4918 10.7931C10.7561 10.7931 10.1418 10.1145 10.1418 9.27875C10.1418 8.44301 10.7418 7.76442 11.4918 7.76442C12.249 7.76442 12.849 8.45015 12.8419 9.27875C12.8419 10.1145 12.249 10.7931 11.4918 10.7931Z" fill="#9B9B9B"></path></g></svg></a><a href="https://www.facebook.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55 0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95"></path></svg></a><a href="https://dwarves.foundation/" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M17.9 17.39c-.26-.8-1.01-1.39-1.9-1.39h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41a7.984 7.984 0 0 1 2.9 12.8M11 19.93c-3.95-.49-7-3.85-7-7.93c0-.62.08-1.22.21-1.79L9 15v1a2 2 0 0 0 2 2m1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10a10 10 0 0 0 10-10A10 10 0 0 0 12 2"></path></svg></a><a href="mailto:team@dwarves.foundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 36 36"><path fill="#9b9b9b" d="M32.33 6a2 2 0 0 0-.41 0h-28a2 2 0 0 0-.53.08l14.45 14.39Z" class="clr-i-solid clr-i-solid-path-1"></path><path fill="#9b9b9b" d="m33.81 7.39l-14.56 14.5a2 2 0 0 1-2.82 0L2 7.5a2 2 0 0 0-.07.5v20a2 2 0 0 0 2 2h28a2 2 0 0 0 2-2V8a2 2 0 0 0-.12-.61M5.3 28H3.91v-1.43l7.27-7.21l1.41 1.41Zm26.61 0h-1.4l-7.29-7.23l1.41-1.41l7.27 7.21Z" class="clr-i-solid clr-i-solid-path-2"></path><path fill="none" d="M0 0h36v36H0z"></path></svg></a></div><div class="authors !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Dwarves Foundation</span></div><div class="filename !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Memo</span></div><div class="last-updated hidden items-center px-3 text-[#9b9b9b]"><span class="text-[var(--secondary-font-color-light-2)]">© 2025</span></div></footer></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"directoryTree":{"/pinned":{"label":"Pinned Notes","children":{"/playbook/operations/ogif":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/":{"label":"Home","children":{"/earn":{"label":"Earn","children":{"/earn/000-productivity":{"label":"Productivity","children":{}},"/earn/001-quality":{"label":"Software Quality","children":{}},"/earn/002-open-source":{"label":"Open Source","children":{}},"/earn/003-liquidity":{"label":"Liquidity","children":{}},"/earn/readme":{"label":"👾 Open bounties","children":{}}}},"/consulting":{"label":"Consulting","children":{"/consulting/case-study":{"label":"Case Study","children":{"/consulting/case-study/screenz-ai":{"label":"Screenz.ai","children":{}},"/consulting/case-study/kafi":{"label":"Kafi","children":{}},"/consulting/case-study/droppii":{"label":"Droppii","children":{}},"/consulting/case-study/konvoy":{"label":"Konvoy","children":{}},"/consulting/case-study/cimb":{"label":"CIMB","children":{}},"/consulting/case-study/swift":{"label":"Swift","children":{}},"/consulting/case-study/startupvn":{"label":"StartupVN","children":{}},"/consulting/case-study/open-fabric":{"label":"Open Fabric","children":{}},"/consulting/case-study/icrosschain":{"label":"iCrosschain","children":{}},"/consulting/case-study/hedge-foundation":{"label":"Hedge Foundation","children":{}},"/consulting/case-study/searchio":{"label":"Search.io","children":{}},"/consulting/case-study/tokenomy":{"label":"Tokenomy","children":{}},"/consulting/case-study/basehq":{"label":"BaseHQ","children":{}},"/consulting/case-study/momos":{"label":"Momos","children":{}},"/consulting/case-study/attrace":{"label":"Attrace","children":{}},"/consulting/case-study/setel":{"label":"Setel","children":{}},"/consulting/case-study/joinpara":{"label":"JoinPara","children":{}},"/consulting/case-study/relay":{"label":"Relay","children":{}},"/consulting/case-study/naru":{"label":"Naru","children":{}},"/consulting/case-study/mudah":{"label":"Mudah","children":{}},"/consulting/case-study/reapit":{"label":"Reapit","children":{}},"/consulting/case-study/aharooms":{"label":"Aharooms","children":{}},"/consulting/case-study/begroup":{"label":"beGroup","children":{}},"/consulting/case-study/airwatt":{"label":"AirWatt","children":{}},"/consulting/case-study/voconic":{"label":"Voconic","children":{}},"/consulting/case-study/sol":{"label":"Sol","children":{}},"/consulting/case-study/dental-marketplace":{"label":"Dental Marketplace","children":{}},"/consulting/case-study/bhd":{"label":"BHD Cinema","children":{}}}},"/consulting/partners-network":{"label":"Partners Network","children":{}},"/consulting/readme":{"label":"💼 Consulting team","children":{}}}},"/handbook":{"label":"Handbook","children":{"/handbook/navigate-changes":{"label":"Navigate changes","children":{}},"/handbook/community":{"label":"Community","children":{"/handbook/community/icy-worth":{"label":"How much is your ICY worth","children":{}},"/handbook/community/icy-swap":{"label":"How to swap ICY to BTC","children":{}},"/handbook/community/icy":{"label":"ICY","children":{}},"/handbook/community/discord":{"label":"Discord","children":{}},"/handbook/community/earn":{"label":"Earn","children":{}},"/handbook/community/radar":{"label":"Radar","children":{}},"/handbook/community/sharing":{"label":"Sharing knowledge","children":{}},"/handbook/community/showcase":{"label":"Showcase","children":{}},"/handbook/community/memo":{"label":"Memo","children":{}}}},"/handbook/guides":{"label":"Guides","children":{"/handbook/guides/check-in-at-office":{"label":"Office check-in process for earning ICY","children":{}},"/handbook/guides/leave-request":{"label":"Leave request","children":{}},"/handbook/guides/nda":{"label":"NDA \u0026 Agreements","children":{}},"/handbook/guides/configure-the-company-email":{"label":"Configure your company email","children":{}},"/handbook/guides/one-on-one-meeting":{"label":"1-on-1 meetings","children":{}},"/handbook/guides/continuing-education-allowance":{"label":"Continuing education allowance","children":{}},"/handbook/guides/reimbursement":{"label":"Reimbursement","children":{}},"/handbook/guides/email-communication-and-use":{"label":"Email use","children":{}},"/handbook/guides/password-sharing":{"label":"Password Sharing","children":{}},"/handbook/guides/asset-request":{"label":"Request an asset","children":{}},"/handbook/guides/effective-meeting":{"label":"Effective meetings","children":{}},"/handbook/guides/conduct-a-meeting":{"label":"How to conduct a meeting","children":{}}}},"/handbook/making-a-career":{"label":"Making a career","children":{}},"/handbook/as-a-community":{"label":"As a community","children":{}},"/handbook/knowledge-base":{"label":"Knowledge base","children":{}},"/handbook/stock-option-plan":{"label":"Stock option plan","children":{}},"/handbook/readme":{"label":"📔 Handbook","children":{}},"/handbook/compliance":{"label":"Compliance","children":{}},"/handbook/mma":{"label":"MMA","children":{}},"/handbook/hybrid-working":{"label":"Hybrid Working","children":{}},"/handbook/routine":{"label":"Work routine","children":{}},"/handbook/ventures":{"label":"Ventures arm","children":{}},"/handbook/purpose":{"label":"Purpose","children":{}},"/handbook/benefits-and-perks":{"label":"Benefits \u0026 perks","children":{}},"/handbook/dwarves-foundation-is-you":{"label":"You are Dwarves Foundation","children":{}},"/handbook/getting-started":{"label":"💎 Getting started","children":{}},"/handbook/how-we-hire":{"label":"How we hire","children":{}},"/handbook/how-we-spend-money":{"label":"How we spend money","children":{}},"/handbook/misc":{"label":"Misc","children":{"/handbook/misc/marketing-assets":{"label":"Marketing assets","children":{}}}},"/handbook/moonlighting":{"label":"Moonlighting","children":{}},"/handbook/places-to-work":{"label":"Places to work","children":{}},"/handbook/security-rules":{"label":"Security rules","children":{}},"/handbook/tools-and-systems":{"label":"Tools and systems","children":{}},"/handbook/what-we-stand-for":{"label":"What we stand for","children":{}},"/handbook/what-we-value":{"label":"What we value","children":{}},"/handbook/where-we-work":{"label":"Where we work","children":{}},"/handbook/who-does-what":{"label":"Who does what","children":{}},"/handbook/faq":{"label":"FAQ","children":{}},"/handbook/how-we-work":{"label":"How we work","children":{}}}},"/playground":{"label":"Playground","children":{"/playground/01_literature":{"label":"01_literature","children":{"/playground/01_literature/evolutionary-database-design":{"label":"Evolutionary Database Design: Managing Change and Scaling with the System","children":{}},"/playground/01_literature/design":{"label":"Design","children":{"/playground/01_literature/design/product-design-commentary-20241122":{"label":"Product Design Commentary #7: Hyper-personalization - How AI improves user experience personalization","children":{}},"/playground/01_literature/design/product-design-commentary-20241115":{"label":"Product Design Commentary #6: AI in Design - Cool ideas and how to make them happen","children":{}},"/playground/01_literature/design/product-design-commentary-20241101":{"label":"Product Design Commentary #5: Figma to SwiftUI (functional code) with Claude AI","children":{}},"/playground/01_literature/design/product-design-commentary-20241018":{"label":"Product Design Commentary #4: Generative AI UX design patterns","children":{}},"/playground/01_literature/design/product-design-commentary-20241011":{"label":"Product Design Commentary #3: The art of prompting in AI-human interaction","children":{}},"/playground/01_literature/design/product-design-commentary-20241004":{"label":"Product Design Commentary #2: Unpacking the sparkles icon and AI onboarding challenges","children":{}},"/playground/01_literature/design/product-design-commentary-20240927":{"label":"Product Design Commentary #1: New technologies changing UX/UI and product design","children":{}}}},"/playground/01_literature/giving-a-talk-checklist":{"label":"Giving a talk","children":{}},"/playground/01_literature/database-design-circular":{"label":"Database design Circular","children":{}},"/playground/01_literature/a-lens-to-modern-data-engineering":{"label":"A Lens to Modern Data Engineering","children":{}},"/playground/01_literature/security":{"label":"Security","children":{"/playground/01_literature/security/a-holistic-guide-to-security":{"label":"A Holistic Guide to Security","children":{}},"/playground/01_literature/security/how-i-came-up-with-our-security-standard":{"label":"How I came up with our Security Standard","children":{}}}},"/playground/01_literature/record-reward-sharing-culture":{"label":"Record and reward sharing at Dwarves","children":{}},"/playground/01_literature/designing-for-forgiveness":{"label":"Designing for Forgiveness: Creating Error-Tolerant Interfaces","children":{}},"/playground/01_literature/design-file-sharing-system-part-2-permission-and-password":{"label":"Design file-sharing system - Part 2: Permission \u0026 Password","children":{}},"/playground/01_literature/designing-a-model-with-dynamic-properties":{"label":"Designing a model with dynamic properties","children":{}},"/playground/01_literature/hybrid-search":{"label":"Evaluating search engine in RAG systems","children":{}},"/playground/01_literature/design-file-sharing-system-part-1-directory-structure":{"label":"Design file-sharing system - Part 1: Directory Structure","children":{}},"/playground/01_literature/using-foundry-for-evm-smart-contract-developement":{"label":"Using Foundry for EVM smart contract development","children":{}},"/playground/01_literature/creating-a-fully-local-search-engine-on-memo":{"label":"Building a Local Search Engine for Our Memo Website","children":{}},"/playground/01_literature/observer-pattern":{"label":"Introduce the Observer pattern and its use cases","children":{}},"/playground/01_literature/visitor-design-pattern":{"label":"Visitor design pattern, the concept, problem solution and use cases","children":{}},"/playground/01_literature/strategy-design-pattern":{"label":"Strategy design pattern, the concept, use cases and difference with the state design pattern","children":{}},"/playground/01_literature/vietnam-tech-ecosystem-report":{"label":"Vietnam Tech Ecosystem 2024 Report","children":{}},"/playground/01_literature/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing":{"label":"How we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","children":{}},"/playground/01_literature/feedback-mechanism":{"label":"Design feedback mechanism for LLM applications","children":{}},"/playground/01_literature/local-first-software":{"label":"Local-first Software","children":{}},"/playground/01_literature/error-handling-in-rust":{"label":"Error handling on Rust","children":{}},"/playground/01_literature/engineering":{"label":"Engineering","children":{"/playground/01_literature/engineering/backend":{"label":"Backend","children":{"/playground/01_literature/engineering/backend/bloom-filter":{"label":"Bloom Filter","children":{}},"/playground/01_literature/engineering/backend/introduction-to-crdt":{"label":"Introduction to CRDT","children":{}},"/playground/01_literature/engineering/backend/sql-sargable-queries-and-their-impact-on-database-performance":{"label":"SQL Saragable Queries and Their Impact on Database Performance","children":{}},"/playground/01_literature/engineering/backend/the-removal-of-apache-kafkas-dependency-on-zookeeper":{"label":"The removal of Apache Kafka's dependency on Zookeeper","children":{}},"/playground/01_literature/engineering/backend/sql-and-how-it-relates-to-disk-reads-and-writes":{"label":"SQL and how it relates to Disk Reads and Writes","children":{}}}},"/playground/01_literature/engineering/data":{"label":"Data","children":{"/playground/01_literature/engineering/data/data-pipeline-design-framework":{"label":"Data Pipeline Design Framework","children":{}},"/playground/01_literature/engineering/data/quick-learning-vector-database":{"label":"Quick Learning Vector Database","children":{}},"/playground/01_literature/engineering/data/mapreduce":{"label":"MapReduce","children":{}}}},"/playground/01_literature/engineering/google-data-fusion":{"label":"Google Data Fusion","children":{}},"/playground/01_literature/engineering/google-dataproc":{"label":"Google Dataproc","children":{}},"/playground/01_literature/engineering/introducing-htmx-navigating-the-advantages-and-concerns":{"label":"Introducing HTMX - Navigating the Advantages and Concerns","children":{}},"/playground/01_literature/engineering/typesafe-client-server":{"label":"Typesafe Client Server","children":{}},"/playground/01_literature/engineering/url-redirect-vs-rewrite":{"label":"URL Redirect vs. Rewrite; What’s the difference?","children":{}}}},"/playground/01_literature/template-method-design-pattern":{"label":"A Tour of Template method pattern with Golang","children":{}},"/playground/01_literature/command-pattern":{"label":"Command Pattern","children":{}},"/playground/01_literature/radix-sort":{"label":"Radix Sort","children":{}},"/playground/01_literature/state-pattern":{"label":"State Pattern","children":{}},"/playground/01_literature/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana":{"label":"Dynamic Liquidity Market Maker - a new form of concentrated liquidity AMM on Solana","children":{}},"/playground/01_literature/memo-knowledge-base-meeting":{"label":"Memo Knowledge Base Meeting","children":{}},"/playground/01_literature/peep-nft":{"label":"Claim your Peeps NFT","children":{}},"/playground/01_literature/recording-flow":{"label":"How We Set Up a Recording Workflow for Dwarves Office Hours","children":{}},"/playground/01_literature/memo-publication-workflow":{"label":"Memo Publication Workflow","children":{}},"/playground/01_literature/history-of-structured-output-for-llms":{"label":"History of Structured Outputs for LLMs","children":{}},"/playground/01_literature/builder-design-pattern":{"label":"Introduce the Builder pattern and its use cases","children":{}},"/playground/01_literature/how-to-make-a-moc":{"label":"How to make a MOC","children":{}},"/playground/01_literature/prototype-design-pattern":{"label":"Going Through use cases of the prototype design pattern and it place among the creational patterns","children":{}},"/playground/01_literature/singleton-design-pattern":{"label":"A tour of Singleton design pattern with Golang","children":{}},"/playground/01_literature/echelon-x-singapore-2024-where-innovations-meet-inspiration":{"label":"Echelon X Singapore 2024: Where Innovations Meet Inspiration","children":{}},"/playground/01_literature/c4-modelling":{"label":"Breaking Down Complexity: The Role of Abstractions and UML in C4 Modelling","children":{}},"/playground/01_literature/dollar-cost-averaging":{"label":"Dollar Cost Averaging (DCA)","children":{}},"/playground/01_literature/how-i-create-content-for-multiple-platforms-at-dwarves":{"label":"How I Create Content for Multiple Platforms at Dwarves","children":{}},"/playground/01_literature/understanding-saving-investing-and-speculating-key-differences-and-strategies":{"label":"Understanding Saving, Investing, and Speculating: Key Differences and Strategies","children":{}},"/playground/01_literature/writing-content-for-multimedia-guidelines":{"label":"Writing Content for Multimedia Guidelines","children":{}},"/playground/01_literature/how-to-earn-reward-from-staking-dfg":{"label":"How to earn reward from staking DFG","children":{}},"/playground/01_literature/how-to-transfer-dfg-from-eth-to-base-for-staking":{"label":"How to bridge $DFG from Ethereum Mainnet to Base Network for staking","children":{}},"/playground/01_literature/design-less-present-more-with-deckset":{"label":"Design less, present more with Deckset","children":{}},"/playground/01_literature/level-up-your-markdown-memos":{"label":"Level Up Your Markdown Memos: Avoiding Common Pitfalls","children":{}},"/playground/01_literature/tech-canvas":{"label":"Tech Canvas","children":{}},"/playground/01_literature/how-to-recap-a-publication":{"label":"Recapping A publication","children":{}},"/playground/01_literature/lifecycle-of-a-publication":{"label":"Life cycle of a publication","children":{}},"/playground/01_literature/how-to-set-up-environment-for-editing-memo":{"label":"How to set up environment to edit memo","children":{}},"/playground/01_literature/_how-to-setup-crypto-wallet-to-withdraw-icy":{"label":"How to set up crypto wallet to withdraw ICY","children":{}},"/playground/01_literature/_how-to-withdraw-icy":{"label":"How to withdraw ICY","children":{}},"/playground/01_literature/how-to-take-better-screenshots-on-mac":{"label":"How To Take Better Screenshots On Mac","children":{}},"/playground/01_literature/how-to-push-content-on-note-d":{"label":"How to push content on memo.d.foundation","children":{}},"/playground/01_literature/labs-weekly-catchup-5":{"label":"Labs Weekly Catchup #5","children":{}},"/playground/01_literature/labs-weekly-catchup-4":{"label":"Labs Weekly Catchup #4","children":{}},"/playground/01_literature/labs-weekly-catchup-3":{"label":"Labs Weekly Catchup #3","children":{}},"/playground/01_literature/labs-weekly-catchup-2":{"label":"Labs Weekly Catchup #2","children":{}},"/playground/01_literature/labs-weekly-catchup-1":{"label":"Labs Weekly Catchup #1","children":{}},"/playground/01_literature/labs-who-we-are":{"label":"Labs - Who we are","children":{}},"/playground/01_literature/readme":{"label":"Dwarves Memo","children":{}},"/playground/01_literature/duckdb-demo-and-showcase":{"label":"DuckDB demo and showcase","children":{}},"/playground/01_literature/salary-advance":{"label":"$icy Salary Advance","children":{}},"/playground/01_literature/how-rd-contributes-to-performance-review":{"label":"How R\u0026D contributes to Performance Review","children":{}},"/playground/01_literature/knowledge-journey":{"label":"Knowledge Journey","children":{}},"/playground/01_literature/labs-new-member-onboarding":{"label":"Labs - New Member Onboarding","children":{}},"/playground/01_literature/labs-roadmap-nov-23-update":{"label":"Labs Roadmap (Nov 23 update)","children":{}},"/playground/01_literature/labs-topic-proposal-progress-tracking":{"label":"Labs - Topic proposal \u0026 progress tracking","children":{}},"/playground/01_literature/labs-x-consulting-workflow":{"label":"Labs x Consulting Workflow","children":{}},"/playground/01_literature/reward-model-nomination":{"label":"Reward Model \u0026 Nomination","children":{}},"/playground/01_literature/our-view-on-fullstack-engineering":{"label":"Our View On Fullstack Engineering","children":{}},"/playground/01_literature/adoption-of-pnpm":{"label":"Adoption Of Pnpm","children":{}},"/playground/01_literature/working-on-a-project-interview-assessment-at-dwarves":{"label":"Working On A Project Interview Assessment At Dwarves","children":{}},"/playground/01_literature/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt":{"label":"How We Created An AI Powered Interview System Using Openais Chatgpt","children":{}},"/playground/01_literature/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{}},"/playground/01_literature/exploring-machine-learning-approaches-for-fine-tuning-llama-models":{"label":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","children":{}},"/playground/01_literature/managing-dataflow-and-sql-database-with-concurrency-control":{"label":"Managing Dataflow And Sql Database With Concurrency Control","children":{}},"/playground/01_literature/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue":{"label":"Choosing The Right Javascript Framework A Deep Dive Into React Vs Angular Vs Vue","children":{}},"/playground/01_literature/design-system-for-layer-2-using-zk-rollup":{"label":"Design System For Layer 2 Using Zk Rollup","children":{}},"/playground/01_literature/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation":{"label":"Lessons Learned From Being A Part Of Corporate Micro Frontend Implementation","children":{}},"/playground/01_literature/cost-of-react-native":{"label":"Cost Of React Native","children":{}},"/playground/01_literature/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{}},"/playground/01_literature/database-designs-for-multilingual-apps":{"label":"Database Designs For Multilingual Apps","children":{}},"/playground/01_literature/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{}},"/playground/01_literature/how-blue-green-deployment-helped-mochi":{"label":"How Blue Green Deployment Helped Mochi","children":{}},"/playground/01_literature/i18n-frontend-guideline":{"label":"I18n Frontend Guideline","children":{}},"/playground/01_literature/radio-talk-61-monorepo":{"label":"Radio Talk 61 Monorepo","children":{}},"/playground/01_literature/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan Turbo Monorepo","children":{}},"/playground/01_literature/radio-talk-60-blue-green-deployment":{"label":"Radio Talk 60 Blue Green Deployment","children":{}},"/playground/01_literature/growth-is-our-universal-language":{"label":"Growth Is Our Universal Language","children":{}},"/playground/01_literature/the-key-of-security-mechanisms-in-tackling-cyber-threats":{"label":"The Key Of Security Mechanisms In Tackling Cyber Threats","children":{}},"/playground/01_literature/responsibility":{"label":"Responsibility","children":{}},"/playground/01_literature/configure-the-company-email":{"label":"Configure The Company Email","children":{}},"/playground/01_literature/tech-event-in-the-latest-transforming-healthcare-with-technology":{"label":"Tech Event In The Latest Transforming Healthcare With Technology","children":{}},"/playground/01_literature/data-analyst-in-retail-trading":{"label":"Data Analyst In Retail Trading","children":{}},"/playground/01_literature/passing-the-probation-get-3-upvotes":{"label":"Passing The Probation Get 3 Upvotes","children":{}},"/playground/01_literature/react-native-new-architecture":{"label":"React Native New Architecture","children":{}},"/playground/01_literature/writing":{"label":"Writing","children":{"/playground/01_literature/writing/state-explain-link":{"label":"State, Explain, Link - An all-purpose writing technique","children":{}}}},"/playground/01_literature/dwarves-radio-talk-17-conduct-a-1-1-session":{"label":"Dwarves Radio Talk 17 Conduct A 1 1 Session","children":{}},"/playground/01_literature/dwarves-radio-talk-16-run-an-effective-performance-review":{"label":"Dwarves Radio Talk 16 Run An Effective Performance Review","children":{}},"/playground/01_literature/understanding-an-application-design":{"label":"Understanding An Application Design","children":{}},"/playground/01_literature/sql-practices-orm-vs-plain-sql":{"label":"Sql Practices Orm Vs Plain Sql","children":{}},"/playground/01_literature/what-i-learned-on-design-thinking-and-software-development":{"label":"What I Learned On Design Thinking And Software Development","children":{}},"/playground/01_literature/six-things-i-extracted-from-design-thinking":{"label":"Six Things I Extracted From Design Thinking","children":{}},"/playground/01_literature/gitflow-pull-request":{"label":"Gitflow Pull Request","children":{}},"/playground/01_literature/git-commit-message-convention":{"label":"Git Commit Message Convention","children":{}},"/playground/01_literature/are-we-really-engineers":{"label":"Are We Really Engineers","children":{}},"/playground/01_literature/how-we-setup-cicd":{"label":"How We Setup Cicd","children":{}},"/playground/01_literature/getting-started-with-webflow":{"label":"Getting Started With Webflow","children":{}},"/playground/01_literature/ui-design-best-practices-dwarves":{"label":"UI Design Best Practices Dwarves","children":{}},"/playground/01_literature/xpc-services-on-macos-app-using-swift":{"label":"Xpc Services On Macos App Using Swift","children":{}},"/playground/01_literature/the-correct-way-to-build-kpi":{"label":"The Correct Way To Build Kpi","children":{}},"/playground/01_literature/domain-insight-research-framework":{"label":"Domain Insight Research Framework","children":{}},"/playground/01_literature/asking-as-a-junior":{"label":"Asking As A Junior","children":{}},"/playground/01_literature/infinite-image-gallery-with-r3f-an-approach":{"label":"Infinite Image Gallery With R3f An Approach","children":{}},"/playground/01_literature/market":{"label":"Market","children":{"/playground/01_literature/market/an-overview-of-micro-investment-in-real-estate":{"label":"An Overview Of Micro Investment In Real Estate","children":{}}}},"/playground/01_literature/grid-and-layout":{"label":"Grid And Layout","children":{}},"/playground/01_literature/startups-vs-junior-designers":{"label":"Startups Vs Junior Designers","children":{}},"/playground/01_literature/gestalt-principles-in-ui-design":{"label":"Gestalt Principles In UI Design","children":{}},"/playground/01_literature/aarrr-framework-in-a-nutshell":{"label":"AARRR Framework In A Nutshell","children":{}},"/playground/01_literature/a-quick-intro-to-webassembly":{"label":"A Quick Intro To Webassembly","children":{}},"/playground/01_literature/sdk-event-sourcing":{"label":"Sdk Event Sourcing","children":{}},"/playground/01_literature/software-development-life-cycle-101":{"label":"Software Development Life Cycle 101","children":{}},"/playground/01_literature/introduce-to-dwarves-memo":{"label":"Introduce To Dwarves Memo","children":{}},"/playground/01_literature/daemons-and-services-programming-guide":{"label":"Daemons And Services Programming Guide","children":{}},"/playground/01_literature/remote-moderated-usability-testing":{"label":"Remote Moderated Usability Testing","children":{}},"/playground/01_literature/an-alternative-to-tm":{"label":"An Alternative To Tm","children":{}},"/playground/01_literature/how-a-design-system-work":{"label":"How A Design System Work","children":{}},"/playground/01_literature/software-modeling":{"label":"Software Modeling","children":{}},"/playground/01_literature/reusability-in-software-development":{"label":"Reusability In Software Development","children":{}},"/playground/01_literature/blockchain-for-designers":{"label":"Blockchain For Designers","children":{}},"/playground/01_literature/design-better-mobile-application":{"label":"Design Better Mobile Application","children":{}},"/playground/01_literature/introduction-to-software-craftsmanship":{"label":"Introduction To Software Craftsmanship","children":{}},"/playground/01_literature/domain-glossary":{"label":"Domain Glossary","children":{}},"/playground/01_literature/architecture-decision-record":{"label":"Architecture Decision Record","children":{}},"/playground/01_literature/build-an-assistant-on-the-terminal":{"label":"Build An Assistant On The Terminal","children":{}},"/playground/01_literature/create-circular-text-using-swiftui":{"label":"Create Circular Text Using Swiftui","children":{}},"/playground/01_literature/draw-watch-face-using-swiftui":{"label":"Draw Watch Face Using Swiftui","children":{}},"/playground/01_literature/applied-security-basis":{"label":"Applied Security Basis","children":{}},"/playground/01_literature/swiftui":{"label":"Swiftui","children":{}},"/playground/01_literature/bunk-license-check":{"label":"Bunk License Check","children":{}},"/playground/01_literature/well-crafted-software":{"label":"Well Crafted Software","children":{}},"/playground/01_literature/objective":{"label":"Objective","children":{}},"/playground/01_literature/project-management":{"label":"Project Management","children":{}},"/playground/01_literature/kubernetes-helm-101":{"label":"Kubernetes Helm 101","children":{}},"/playground/01_literature/what-is-kubernetes":{"label":"What Is Kubernetes","children":{}},"/playground/01_literature/traits-to-assess-during-an-interview":{"label":"Traits To Assess During An Interview","children":{}},"/playground/01_literature/recursively-export-file-pattern-in-javascript-es6-application":{"label":"Recursively Export File Pattern In Javascript Es6 Application","children":{}},"/playground/01_literature/playaround-with-clojure":{"label":"Playaround With Clojure","children":{}},"/playground/01_literature/playaround-with-rust":{"label":"Playaround With Rust","children":{}},"/playground/01_literature/overview-on-broker-pattern-in-distributed-system":{"label":"Overview On Broker Pattern In Distributed System","children":{}},"/playground/01_literature/fundamental-end-to-end-frontend-testing-with-cypress":{"label":"Fundamental End To End Frontend Testing With Cypress","children":{}},"/playground/01_literature/uidynamicanimator":{"label":"Uidynamicanimator","children":{}},"/playground/01_literature/reproduce-apple-find-me-bottom-menu-view":{"label":"Reproduce Apple Find Me Bottom Menu View","children":{}},"/playground/01_literature/build-a-passcode-view-with-swift":{"label":"Build A Passcode View With Swift","children":{}},"/playground/01_literature/istio":{"label":"Istio","children":{}},"/playground/01_literature/different-ways-to-test-react-application":{"label":"Different Ways To Test React Application","children":{}},"/playground/01_literature/federated-byzantine":{"label":"Federated Byzantine","children":{}},"/playground/01_literature/fabric-hyperledger-architecture-explanation":{"label":"Fabric Hyperledger Architecture Explanation","children":{}},"/playground/01_literature/setup-react-project-with-webpack-and-babel":{"label":"Setup React Project With Webpack And Babel","children":{}},"/playground/01_literature/split-and-reuse-code-in-react-application":{"label":"Split And Reuse Code In React Application","children":{}},"/playground/01_literature/hoc-renderprops-and-hook-in-reactjs":{"label":"Hoc Renderprops And Hook In Reactjs","children":{}},"/playground/01_literature/resource-assignment":{"label":"Resource Assignment","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-2":{"label":"The Principle Of Spacing In UI Design Part 2","children":{}},"/playground/01_literature/finite-state-machine":{"label":"Finite State Machine","children":{}},"/playground/01_literature/card-sorting-and-a-glimpse-at-experimental-sorting-session":{"label":"Card Sorting And A Glimpse At Experimental Sorting Session","children":{}},"/playground/01_literature/about-devops":{"label":"About Devops","children":{}},"/playground/01_literature/our-daily-standup-format":{"label":"Our Daily Standup Format","children":{}},"/playground/01_literature/good-design-understanding":{"label":"Good Design Understanding","children":{}},"/playground/01_literature/competency-mapping":{"label":"Competency Mapping","children":{}},"/playground/01_literature/design-resourcestools":{"label":"Design Resourcestools","children":{}},"/playground/01_literature/design-tips-tricks":{"label":"Design Tips Tricks","children":{}},"/playground/01_literature/design-system":{"label":"Design System","children":{}},"/playground/01_literature/design-workflow":{"label":"Design Workflow","children":{}},"/playground/01_literature/three-levels-of-design":{"label":"Three Levels Of Design","children":{}},"/playground/01_literature/ui-design-fundamental":{"label":"UI Design Fundamental","children":{}},"/playground/01_literature/ux-model":{"label":"UX Model","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-1":{"label":"The Principle Of Spacing In UI Design Part 1","children":{}},"/playground/01_literature/be-careful-with-your-code-splitting-setup":{"label":"Be Careful With Your Code Splitting Setup","children":{}},"/playground/01_literature/qc-onboarding":{"label":"Qc Onboarding","children":{}},"/playground/01_literature/dcos-series-part-5-gitlab":{"label":"Dcos Series Part 5 Gitlab","children":{}},"/playground/01_literature/dcos-series-part-4-deploy-simple-application-with-backend-database":{"label":"Dcos Series Part 4 Deploy Simple Application With Backend Database","children":{}},"/playground/01_literature/dcos-series-part-3-service-discovery-and-load-balancing":{"label":"Dcos Series Part 3 Service Discovery And Load Balancing","children":{}},"/playground/01_literature/dcos-series-part-2-deploy-simple-applications":{"label":"Dcos Series Part 2 Deploy Simple Applications","children":{}},"/playground/01_literature/dcos-series-part-1-quick-look-installation":{"label":"Dcos Series Part 1 Quick Look Installation","children":{}},"/playground/01_literature/skill-of-software-engineer":{"label":"Skill Of Software Engineer","children":{}},"/playground/01_literature/docker-registry":{"label":"Docker Registry","children":{}},"/playground/01_literature/agile-using-clickup-as-agile-management-tool":{"label":"Agile Using Clickup As Agile Management Tool","children":{}},"/playground/01_literature/agile-how-to-create-clickup-tickets":{"label":"Agile How To Create Clickup Tickets","children":{}},"/playground/01_literature/considering-factors-for-performance-evaluating":{"label":"Considering Factors For Performance Evaluating","children":{}},"/playground/01_literature/how-we-contribute-to-homebrew":{"label":"How We Contribute To Homebrew","children":{}},"/playground/01_literature/the-10x-engineer":{"label":"The 10x Engineer","children":{}},"/playground/01_literature/definition-of-done":{"label":"Definition Of Done","children":{}},"/playground/01_literature/estimation-in-agile":{"label":"Estimation In Agile","children":{}},"/playground/01_literature/sprint-lifecycle":{"label":"Sprint Lifecycle","children":{}},"/playground/01_literature/remote-prepare-and-get-going":{"label":"Remote Prepare And Get Going","children":{}},"/playground/01_literature/docker-microcontainers":{"label":"Docker Microcontainers","children":{}}}},"/playground/00_fleeting":{"label":"00_fleeting","children":{"/playground/00_fleeting/automata":{"label":"Automata","children":{}},"/playground/00_fleeting/error-handling-patterns":{"label":"Error Handling Patterns","children":{}},"/playground/00_fleeting/founder-liquidity":{"label":"Founder Liquidity","children":{}},"/playground/00_fleeting/why-hollywood-and-gaming-struggle-with-ai":{"label":"Why Hollywood and gaming struggle with AI","children":{}},"/playground/00_fleeting/subscription-pricing-models":{"label":"Subscription Pricing Models","children":{}},"/playground/00_fleeting/erlang-fsm":{"label":"Erlang Finite State Machine","children":{}},"/playground/00_fleeting/rust-trait":{"label":"Rust Trait","children":{}},"/playground/00_fleeting/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy":{"label":"Explaining Gradient Descent in Machine Learning with a simple analogy","children":{}},"/playground/00_fleeting/organize-team-know-how-with-zettelkasten-method":{"label":"Organize team know-how with Zettelkasten Method","children":{}},"/playground/00_fleeting/how-to-talk-to-chatgpt-effectively":{"label":"How to talk to ChatGPT effectively","children":{}},"/playground/00_fleeting/icy-in-2024":{"label":"$icy in 2024","children":{}},"/playground/00_fleeting/icy-dfg":{"label":"💠 df protocol, $icy and $dfg","children":{}},"/playground/00_fleeting/202302281019-case-study-write-heavy-scalable-and-reliable-inventory-platform":{"label":"Case study: Write-heavy scalable and reliable inventory platform","children":{}},"/playground/00_fleeting/202301191192-multi-column-index-in-db":{"label":"Multi-column index in DB","children":{}},"/playground/00_fleeting/202301091379-invoking-component-functions-in-react":{"label":"Invoking component functions in React","children":{}},"/playground/00_fleeting/202212131609-how-to-deal-with-technical-debt-in-scrum":{"label":"How to deal with technical debt in Scrum","children":{}},"/playground/00_fleeting/202211141287-go-json-parsing":{"label":"Go JSON parser: number \u003c-\u003e interface","children":{}},"/playground/00_fleeting/202211141513-materialized-view-pattern":{"label":"Materialized View Pattern","children":{}},"/playground/00_fleeting/202211081111-error-messaging":{"label":"Error Messaging","children":{}},"/playground/00_fleeting/202210172128-sign-in-form-best-practices":{"label":"Sign-in Form Best Practices","children":{}},"/playground/00_fleeting/202210162154-the-best-of-css-tldr":{"label":"The Best of CSS TLDR","children":{}},"/playground/00_fleeting/202210150019-migration-planning":{"label":"Migration Planning","children":{}},"/playground/00_fleeting/202210131000-behavior-driven-development":{"label":"Behavior Driven Development","children":{}},"/playground/00_fleeting/202210131516-react-fiber":{"label":"React Fiber","children":{}},"/playground/00_fleeting/202210122014-forward-proxy":{"label":"Forward Proxy","children":{}}}},"/playground/_radar":{"label":"_radar","children":{"/playground/_radar/readme":{"label":"Tech Radar","children":{}},"/playground/_radar/codecept":{"label":"Codecept","children":{}},"/playground/_radar/apache-spark":{"label":"Apache Spark","children":{}},"/playground/_radar/ant-design":{"label":"Ant Design","children":{}},"/playground/_radar/apache-kafka":{"label":"Apache Kafka","children":{}},"/playground/_radar/argocd":{"label":"Argocd","children":{}},"/playground/_radar/astro":{"label":"Astro","children":{}},"/playground/_radar/backstage":{"label":"Backstage","children":{}},"/playground/_radar/blue-green-deployment":{"label":"Blue Green Deployment","children":{}},"/playground/_radar/browserstack":{"label":"Browserstack","children":{}},"/playground/_radar/carbon":{"label":"Carbon","children":{}},"/playground/_radar/chatgpt-assistance":{"label":"Chatgpt Assistance","children":{}},"/playground/_radar/chromatic":{"label":"Chromatic","children":{}},"/playground/_radar/clickhouse":{"label":"Clickhouse","children":{}},"/playground/_radar/cloudflare-workers":{"label":"Cloudflare Workers","children":{}},"/playground/_radar/commitlint":{"label":"Commitlint","children":{}},"/playground/_radar/copilot":{"label":"Copilot","children":{}},"/playground/_radar/cucumber":{"label":"Cucumber","children":{}},"/playground/_radar/cypress":{"label":"Cypress","children":{}},"/playground/_radar/dapr":{"label":"Dapr","children":{}},"/playground/_radar/deno":{"label":"Deno","children":{}},"/playground/_radar/detox":{"label":"Detox","children":{}},"/playground/_radar/devcontainers":{"label":"Devcontainers","children":{}},"/playground/_radar/devpod":{"label":"Devpod","children":{}},"/playground/_radar/dora-metrics":{"label":"Dora Metrics","children":{}},"/playground/_radar/duckdb":{"label":"Duckdb","children":{}},"/playground/_radar/earthly":{"label":"Earthly","children":{}},"/playground/_radar/elixir-umbrella-project":{"label":"Elixir Umbrella Project","children":{}},"/playground/_radar/elixir":{"label":"Elixir","children":{}},"/playground/_radar/erlang":{"label":"Erlang","children":{}},"/playground/_radar/error-logging-convention":{"label":"Error Logging Convention","children":{}},"/playground/_radar/eslint":{"label":"Eslint","children":{}},"/playground/_radar/event-sourcing":{"label":"Event Sourcing","children":{}},"/playground/_radar/excalidraw":{"label":"Excalidraw","children":{}},"/playground/_radar/expo":{"label":"Expo","children":{}},"/playground/_radar/figma":{"label":"Figma","children":{}},"/playground/_radar/formal-verification":{"label":"Formal Verification","children":{}},"/playground/_radar/fullstack-tracing":{"label":"Fullstack Tracing","children":{}},"/playground/_radar/gestalt-principle":{"label":"Gestalt Principle","children":{}},"/playground/_radar/github-actions":{"label":"Github Actions","children":{}},"/playground/_radar/golang":{"label":"Golang","children":{}},"/playground/_radar/grafana":{"label":"Grafana","children":{}},"/playground/_radar/graylog":{"label":"Graylog","children":{}},"/playground/_radar/headless-ui":{"label":"Headless UI","children":{}},"/playground/_radar/hoppscotch":{"label":"Hoppscotch","children":{}},"/playground/_radar/ipfs":{"label":"Ipfs","children":{}},"/playground/_radar/jotai":{"label":"Jotai","children":{}},"/playground/_radar/k6":{"label":"K6","children":{}},"/playground/_radar/k9s":{"label":"K9s","children":{}},"/playground/_radar/kaniko":{"label":"Kaniko","children":{}},"/playground/_radar/kotlin":{"label":"Kotlin","children":{}},"/playground/_radar/kubeseal-sops":{"label":"Kubeseal Sops","children":{}},"/playground/_radar/ladle":{"label":"Ladle","children":{}},"/playground/_radar/langchain":{"label":"Langchain","children":{}},"/playground/_radar/large-language-model-llm":{"label":"Large Language Model LLM","children":{}},"/playground/_radar/loki":{"label":"Loki","children":{}},"/playground/_radar/makefile":{"label":"Makefile","children":{}},"/playground/_radar/micro-frontend":{"label":"Micro Frontend","children":{}},"/playground/_radar/monorepo":{"label":"Monorepo","children":{}},"/playground/_radar/msw":{"label":"Msw","children":{}},"/playground/_radar/n6n":{"label":"N6n","children":{}},"/playground/_radar/nestjs":{"label":"Nestjs","children":{}},"/playground/_radar/netlify":{"label":"Netlify","children":{}},"/playground/_radar/newrelic":{"label":"Newrelic","children":{}},"/playground/_radar/nextjs":{"label":"Nextjs","children":{}},"/playground/_radar/nodejs":{"label":"Nodejs","children":{}},"/playground/_radar/nostrum":{"label":"Nostrum","children":{}},"/playground/_radar/nx":{"label":"Nx","children":{}},"/playground/_radar/orval":{"label":"Orval","children":{}},"/playground/_radar/page-object-model":{"label":"Page Object Model","children":{}},"/playground/_radar/partytown":{"label":"Partytown","children":{}},"/playground/_radar/phaser":{"label":"Phaser","children":{}},"/playground/_radar/phoenix":{"label":"Phoenix","children":{}},"/playground/_radar/playwright":{"label":"Playwright","children":{}},"/playground/_radar/pnpm":{"label":"Pnpm","children":{}},"/playground/_radar/progressive-delivery":{"label":"Progressive Delivery","children":{}},"/playground/_radar/prometheus":{"label":"Prometheus","children":{}},"/playground/_radar/prompt-engineering":{"label":"Prompt Engineering","children":{}},"/playground/_radar/qwik":{"label":"Qwik","children":{}},"/playground/_radar/radix-ui":{"label":"Radix UI","children":{}},"/playground/_radar/react-hook-form":{"label":"React Hook Form","children":{}},"/playground/_radar/react-llm":{"label":"React LLM","children":{}},"/playground/_radar/react-native":{"label":"React Native","children":{}},"/playground/_radar/react-query":{"label":"React Query","children":{}},"/playground/_radar/react-server-component":{"label":"React Server Component","children":{}},"/playground/_radar/react-testing-library":{"label":"React Testing Library","children":{}},"/playground/_radar/react":{"label":"React","children":{}},"/playground/_radar/reinforcement-learning-from-human-feedback":{"label":"Reinforcement Learning From Human Feedback","children":{}},"/playground/_radar/remix":{"label":"Remix","children":{}},"/playground/_radar/replayio":{"label":"Replayio","children":{}},"/playground/_radar/reverse-engineering":{"label":"Reverse Engineering","children":{}},"/playground/_radar/rust":{"label":"Rust","children":{}},"/playground/_radar/selenium":{"label":"Selenium","children":{}},"/playground/_radar/semantic-release-auto-release":{"label":"Semantic Release Auto Release","children":{}},"/playground/_radar/sentry":{"label":"Sentry","children":{}},"/playground/_radar/serverlessq":{"label":"Serverlessq","children":{}},"/playground/_radar/solidity":{"label":"Solidity","children":{}},"/playground/_radar/solidjs":{"label":"Solidjs","children":{}},"/playground/_radar/stern":{"label":"Stern","children":{}},"/playground/_radar/svelte":{"label":"Svelte","children":{}},"/playground/_radar/swagger":{"label":"Swagger","children":{}},"/playground/_radar/swift-ui":{"label":"Swift UI","children":{}},"/playground/_radar/swift":{"label":"Swift","children":{}},"/playground/_radar/swr":{"label":"Swr","children":{}},"/playground/_radar/tailwindcss":{"label":"Tailwindcss","children":{}},"/playground/_radar/tauri":{"label":"Tauri","children":{}},"/playground/_radar/team-topologies":{"label":"Team Topologies","children":{}},"/playground/_radar/timeline":{"label":"Timeline","children":{"/playground/_radar/timeline/create-working-devcontainer-for-nextjs-boilerplate":{"label":"Create Working Devcontainer For Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/open-source-devpod-paperspace-provider":{"label":"Open Source Devpod Paperspace Provider","children":{}},"/playground/_radar/timeline/create-working-devcontainer-for-go-api":{"label":"Create Working Devcontainer For Go Api","children":{}},"/playground/_radar/timeline/fe-23-training-type-safe-client-server":{"label":"Fe 23 Training Type Safe Client Server","children":{}},"/playground/_radar/timeline/first-introduced-use-of-duckdb-in-consolelabs-logconsoleso":{"label":"First Introduced Use Of Duckdb In Consolelabs Logconsoleso","children":{}},"/playground/_radar/timeline/add-type-safe-client-server-support-for-next-boilerplate":{"label":"Add Type Safe Client Server Support For Next Boilerplate","children":{}},"/playground/_radar/timeline/building-reliable-apps-sentry-and-distributed-tracing-for-effective-monitoring":{"label":"Building Reliable Apps Sentry And Distributed Tracing For Effective Monitoring","children":{}},"/playground/_radar/timeline/an-engineering-story-map-for-llms":{"label":"An Engineering Story Map For Llms","children":{}},"/playground/_radar/timeline/exploring-resumable-server-side-rendering-with-qwik":{"label":"Exploring Resumable Server Side Rendering With Qwik","children":{}},"/playground/_radar/timeline/challenge-faced-when-researching-rlhf-with-open-assistant":{"label":"Challenge Faced When Researching Rlhf With Open Assistant","children":{}},"/playground/_radar/timeline/embracing-go-1210s-slog-a-unified-logging-interface-with-benchmarks-against-zerolog-and-zap":{"label":"Embracing Go 1210s Slog A Unified Logging Interface With Benchmarks Against Zerolog And Zap","children":{}},"/playground/_radar/timeline/adoption-of-pnpm":{"label":"Adoption Of Pnpm","children":{}},"/playground/_radar/timeline/diagnosing-and-resolving-performance-issues-with-pprof-and-trace-in-go":{"label":"Diagnosing And Resolving Performance Issues With Pprof And Trace In Go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-fortress":{"label":"Migrate Yarn To Pnpm In Fortress","children":{}},"/playground/_radar/timeline/level-up-your-testing-game-harnessing-gomock-for-unbeatable-unit-testing-in-go":{"label":"Level Up Your Testing Game Harnessing Gomock For Unbeatable Unit Testing In Go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nghe-nhan-droppii":{"label":"Migrate Yarn To Pnpm In Nghe Nhan Droppii","children":{}},"/playground/_radar/timeline/common-design-patterns-in-golang-part-1":{"label":"Common Design Patterns In Golang Part 1","children":{}},"/playground/_radar/timeline/go-training-2023-from-basic-to-advanced":{"label":"Go Training 2023 From Basic To Advanced","children":{}},"/playground/_radar/timeline/llms-accuracy-self-refinement":{"label":"Llms Accuracy Self Refinement","children":{}},"/playground/_radar/timeline/adversarial-prompting":{"label":"Adversarial Prompting","children":{}},"/playground/_radar/timeline/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"Chunking Strategies To Overcome Context Limitation In LLM","children":{}},"/playground/_radar/timeline/dealing-with-long-term-memory-of-chatbot":{"label":"Dealing With Long Term Memory Of Chatbot","children":{}},"/playground/_radar/timeline/error-handling-and-failure-management-in-a-go-system":{"label":"Error Handling And Failure Management In A Go System","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nextjs-boilerplate":{"label":"Migrate Yarn To Pnpm In Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/lessons-learned-building-an-llm-chatbot-a-case-study":{"label":"Lessons Learned Building An LLM Chatbot A Case Study","children":{}},"/playground/_radar/timeline/q-learning":{"label":"Q Learning","children":{}},"/playground/_radar/timeline/foundation-model":{"label":"Foundation Model","children":{}},"/playground/_radar/timeline/integrate-zod-to-nextjs-boilerplate":{"label":"Integrate Zod To Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/llm-query-caching":{"label":"LLM Query Caching","children":{}},"/playground/_radar/timeline/build-your-chatbot-with-open-source-large-language-models":{"label":"Build Your Chatbot With Open Source Large Language Models","children":{}},"/playground/_radar/timeline/integrate-playwright-x-codecept-with-discord":{"label":"Integrate Playwright X Codecept With Discord","children":{}},"/playground/_radar/timeline/overcoming-distributed-system-challenges-using-golang":{"label":"Overcoming Distributed System Challenges Using Golang","children":{}},"/playground/_radar/timeline/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{}},"/playground/_radar/timeline/migrate-headlessui-to-radixui":{"label":"Migrate Headlessui To Radixui","children":{}},"/playground/_radar/timeline/llm-101-enhance-developer-productivity":{"label":"LLM 101 Enhance Developer Productivity","children":{}},"/playground/_radar/timeline/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{}},"/playground/_radar/timeline/lessons-learned-from-being-a-part-of-corporate-microfrontend-implementation":{"label":"Lessons Learned From Being A Part Of Corporate Microfrontend Implementation","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-react-toolkit":{"label":"Migrate Yarn To Pnpm In React Toolkit","children":{}},"/playground/_radar/timeline/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{}},"/playground/_radar/timeline/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"Applying Mock Service Worker Msw For Seamless Web Development","children":{}},"/playground/_radar/timeline/integrate-playwright-to-run-e2e-test-with-fortress":{"label":"Integrate Playwright To Run E2e Test With Fortress","children":{}},"/playground/_radar/timeline/from-multi-repo-to-monorepo-a-case-study-with-nghenhan":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan","children":{}},"/playground/_radar/timeline/case-study-how-blue-green-deployment-help-mochi":{"label":"Case Study How Blue Green Deployment Help Mochi","children":{}},"/playground/_radar/timeline/develop-codecept-to-integrate-with-fortress":{"label":"Develop Codecept To Integrate With Fortress","children":{}},"/playground/_radar/timeline/case-study-from-multiple-repo-to-monorepo-at-nghe-nhan":{"label":"Case Study From Multiple Repo To Monorepo At Nghe Nhan","children":{}},"/playground/_radar/timeline/apply-blue-green-deployment-to-mochi":{"label":"Apply Blue Green Deployment To Mochi","children":{}},"/playground/_radar/timeline/memo-blue-green-deployment":{"label":"Memo Blue Green Deployment","children":{}},"/playground/_radar/timeline/brainery-blue-green-deployment":{"label":"Brainery Blue Green Deployment","children":{}},"/playground/_radar/timeline/brainery-validation-with-zod":{"label":"Brainery Validation With Zod","children":{}},"/playground/_radar/timeline/brainery-progressive-delivery":{"label":"Brainery Progressive Delivery","children":{}},"/playground/_radar/timeline/memo-react-native-new-architecture":{"label":"Memo React Native New Architecture","children":{}},"/playground/_radar/timeline/backend-for-call-requests-to-binance-and-get-data-from-multiple-platforms":{"label":"Backend For Call Requests To Binance And Get Data From Multiple Platforms","children":{}},"/playground/_radar/timeline/create-backend-monorepo-to-share-code-and-manage-multiple-services-in-one-repo":{"label":"Create Backend Monorepo To Share Code And Manage Multiple Services In One Repo","children":{}},"/playground/_radar/timeline/nextjs-boilerplate":{"label":"Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-wego":{"label":"Apply Page Object Model Structure To Wego","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-aharooms":{"label":"Apply Page Object Model Structure To Aharooms","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-artzy":{"label":"Apply Page Object Model Structure To Artzy","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-sci":{"label":"Apply Page Object Model Structure To Sci","children":{}},"/playground/_radar/timeline/build-automation-for-sci":{"label":"Build Automation For Sci","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-basehq":{"label":"Apply Page Object Model Structure To Basehq","children":{}},"/playground/_radar/timeline/practice-and-using-selenium-in-setel-project":{"label":"Practice And Using Selenium In Setel Project","children":{}},"/playground/_radar/timeline/mdx-document-for":{"label":"Mdx Document For","children":{}},"/playground/_radar/timeline/develop":{"label":"Develop","children":{}},"/playground/_radar/timeline/apply-monorepos-to-repit-to-resolve-the-problem-of-consistency":{"label":"Apply Monorepos To Repit To Resolve The Problem Of Consistency","children":{}},"/playground/_radar/timeline/learn-typescript-as-a-mandatory-to-develop-reapit-foundation":{"label":"Learn Typescript As A Mandatory To Develop Reapit Foundation","children":{}},"/playground/_radar/timeline/develop-sdk-integration-demo-for-sajari":{"label":"Develop Sdk Integration Demo For Sajari","children":{}},"/playground/_radar/timeline/live-view":{"label":"Live View","children":{}},"/playground/_radar/timeline/migrate-aharooms-pms-to-typescript":{"label":"Migrate Aharooms Pms To Typescript","children":{}},"/playground/_radar/timeline/create-api-service-for-urbox-to-sync-orders-from-3rd-parties-and-manage-shipment":{"label":"Create Api Service For Urbox To Sync Orders From 3rd Parties And Manage Shipment","children":{}},"/playground/_radar/timeline/nghenhan-microservices":{"label":"Nghenhan Microservices","children":{}},"/playground/_radar/timeline/radio-talk-65-fullstack-type-safe-with-trpc":{"label":"Radio Talk 65 Fullstack Type Safe With Trpc","children":{}},"/playground/_radar/timeline/understanding-test-doubles-an-in-depth-look":{"label":"Understanding Test Doubles An In Depth Look","children":{}},"/playground/_radar/timeline/radio-talk-64-coding-best-practice-that-optimizing-go-compiler":{"label":"Radio Talk 64 Coding Best Practice That Optimizing Go Compiler","children":{}},"/playground/_radar/timeline/reward-model":{"label":"Reward Model","children":{}},"/playground/_radar/timeline/sum-command":{"label":"Sum Command","children":{}},"/playground/_radar/timeline/reinforcement-learning":{"label":"Reinforcement Learning","children":{}},"/playground/_radar/timeline/react-server-component":{"label":"React Server Component","children":{}},"/playground/_radar/timeline/select-vector-database-for-llm":{"label":"Select Vector Database For LLM","children":{}},"/playground/_radar/timeline/workaround-with-openais-token-limit-with-langchain":{"label":"Workaround With Openais Token Limit With Langchain","children":{}},"/playground/_radar/timeline/working-with-langchain-document-loaders":{"label":"Working With Langchain Document Loaders","children":{}},"/playground/_radar/timeline/the-cost-of-react-native":{"label":"The Cost Of React Native","children":{}},"/playground/_radar/timeline/state-of-frontend-2023-react-vs-angular-vs-vue":{"label":"State Of Frontend 2023 React Vs Angular Vs Vue","children":{}},"/playground/_radar/timeline/unit-testing-best-practices-in-golang":{"label":"Unit Testing Best Practices In Golang","children":{}},"/playground/_radar/timeline/what-is-pnpm":{"label":"What Is Pnpm","children":{}},"/playground/_radar/timeline/tackling-server-state-complexity-in-frontend-development":{"label":"Tackling Server State Complexity In Frontend Development","children":{}},"/playground/_radar/timeline/why-we-chose-our-tech-stack":{"label":"Why We Chose Our Tech Stack","children":{}},"/playground/_radar/timeline/why-micro-frontend":{"label":"Why Micro Frontend","children":{}},"/playground/_radar/timeline/radio-talk-monorepo":{"label":"Radio Talk Monorepo","children":{}},"/playground/_radar/timeline/radio-talk-blue-green-deployment":{"label":"Radio Talk Blue Green Deployment","children":{}},"/playground/_radar/timeline/radio-talk-a-demo-of-query-engine-postgresql-vs-apache-spark":{"label":"Radio Talk A Demo Of Query Engine Postgresql Vs Apache Spark","children":{}},"/playground/_radar/timeline/rnd-team-mentioned-apache-spark-as-a-solution-to-handle-query-big-data":{"label":"Rnd Team Mentioned Apache Spark As A Solution To Handle Query Big Data","children":{}},"/playground/_radar/timeline/radio-talk-engineering-health-metrics":{"label":"Radio Talk Engineering Health Metrics","children":{}},"/playground/_radar/timeline/radio-talk-nextjs-13":{"label":"Radio Talk Nextjs 13","children":{}},"/playground/_radar/timeline/radio-talk-using-nextjs-as-a-fullstack-framework":{"label":"Radio Talk Using Nextjs As A Fullstack Framework","children":{}},"/playground/_radar/timeline/use-yup-to-validate-form-values-in-droppii":{"label":"Use Yup To Validate Form Values In Droppii","children":{}},"/playground/_radar/timeline/vitejs-native-modules":{"label":"Vitejs Native Modules","children":{}},"/playground/_radar/timeline/radio-talk-introduction-to-apache-spark":{"label":"Radio Talk Introduction To Apache Spark","children":{}},"/playground/_radar/timeline/vercel-switching-their-packages-from-yarn-to-pnpm-caught-our-attention":{"label":"Vercel Switching Their Packages From Yarn To Pnpm Caught Our Attention","children":{}},"/playground/_radar/timeline/radio-talk-remix-vs-nextjs":{"label":"Radio Talk Remix Vs Nextjs","children":{}},"/playground/_radar/timeline/radio-talk-turborepo":{"label":"Radio Talk Turborepo","children":{}},"/playground/_radar/timeline/react-toolkit-migrate-from-lerna-to-turporepo":{"label":"React Toolkit Migrate From Lerna To Turporepo","children":{}},"/playground/_radar/timeline/react-toolkit":{"label":"React Toolkit","children":{}},"/playground/_radar/timeline/urbox-backend-api":{"label":"Urbox Backend Api","children":{}},"/playground/_radar/timeline/use-monorepos-to-build-v3-of-react-sdk-for-searchio":{"label":"Use Monorepos To Build V3 Of React Sdk For Searchio","children":{}},"/playground/_radar/timeline/use-nx-for-managing-basehq-frontend-monorepos":{"label":"Use Nx For Managing Basehq Frontend Monorepos","children":{}},"/playground/_radar/timeline/using-k6-in-setel":{"label":"Using K6 In Setel","children":{}},"/playground/_radar/timeline/use-monorepos-to-resolve-the-problem-of-sharing-ui-components-in-aharoom":{"label":"Use Monorepos To Resolve The Problem Of Sharing UI Components In Aharoom","children":{}},"/playground/_radar/timeline/a-case-study-interview-into-micro-frontends-building-design-system-for-e-commerce-platform":{"label":"A Case Study Interview Into Micro Frontends Building Design System For E Commerce Platform","children":{}},"/playground/_radar/timeline/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{}},"/playground/_radar/timeline/adapt-cucumber-as-a-bdd-for-wego":{"label":"Adapt Cucumber As A Bdd For Wego","children":{}}}},"/playground/_radar/timescaledb":{"label":"Timescaledb","children":{}},"/playground/_radar/tla":{"label":"Tla","children":{}},"/playground/_radar/trunk-based-development":{"label":"Trunk Based Development","children":{}},"/playground/_radar/turborepo":{"label":"Turborepo","children":{}},"/playground/_radar/type-safe-client-server":{"label":"Type Safe Client Server","children":{}},"/playground/_radar/typescript":{"label":"Typescript","children":{}},"/playground/_radar/ui-documentation":{"label":"UI Documentation","children":{}},"/playground/_radar/uno-css":{"label":"Uno Css","children":{}},"/playground/_radar/upptime":{"label":"Upptime","children":{}},"/playground/_radar/v-model":{"label":"V Model","children":{}},"/playground/_radar/vector-database":{"label":"Vector Database","children":{}},"/playground/_radar/vercel":{"label":"Vercel","children":{}},"/playground/_radar/vitejs":{"label":"Vitejs","children":{}},"/playground/_radar/volta":{"label":"Volta","children":{}},"/playground/_radar/wasm":{"label":"Wasm","children":{}},"/playground/_radar/webdriverio":{"label":"Webdriverio","children":{}},"/playground/_radar/webflow":{"label":"Webflow","children":{}},"/playground/_radar/yup":{"label":"Yup","children":{}},"/playground/_radar/zod":{"label":"Zod","children":{}},"/playground/_radar/zustand":{"label":"Zustand","children":{}}}},"/playground/blockchain":{"label":"Blockchain","children":{"/playground/blockchain/build-custom-ai-agent-with-elizaos":{"label":"Build custom AI Agent with ElizaOS","children":{}},"/playground/blockchain/web3-development-with-foundry":{"label":"Web3 Development with Foundry","children":{}},"/playground/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin":{"label":"Implement a Token Swap from the Base chain to Bitcoin for cross-chain transactions","children":{}},"/playground/blockchain/ton_core_concept":{"label":"Ton's base concepts","children":{}},"/playground/blockchain/ton_blockchain_of_blockchains":{"label":"Ton: Blockchain of blockchains","children":{}},"/playground/blockchain/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana":{"label":"Introduce to Solana Token 2022 - new standard to create a token in solana","children":{}},"/playground/blockchain/solana-core-concept":{"label":"Solana core concepts","children":{}},"/playground/blockchain/metaplex-nft-compression":{"label":"Metaplex NFT Compression","children":{}},"/playground/blockchain/plonky2":{"label":"Plonky2","children":{}},"/playground/blockchain/polygon-zkevm-architecture":{"label":"Polygon zkEVM architecture","children":{}},"/playground/blockchain/starknet-architecture":{"label":"StarkNet architecture","children":{}},"/playground/blockchain/zk-snarks":{"label":"zk-SNARKs","children":{}},"/playground/blockchain/layer-2":{"label":"Layer 2: Scaling Solutions for Ethereum","children":{}},"/playground/blockchain/solana-account":{"label":"Solana Account","children":{}},"/playground/blockchain/foundational-topics":{"label":"Foundational Topics","children":{"/playground/blockchain/foundational-topics/zero-knowledge-proofs":{"label":"Zero-knowledge Proofs","children":{}},"/playground/blockchain/foundational-topics/blocks":{"label":"Blocks","children":{}},"/playground/blockchain/foundational-topics/distributed-systems":{"label":"Distributed systems","children":{}},"/playground/blockchain/foundational-topics/pos":{"label":"PoS","children":{}},"/playground/blockchain/foundational-topics/smart-contract":{"label":"Smart Contract","children":{}},"/playground/blockchain/foundational-topics/topics":{"label":"Topics","children":{}}}},"/playground/blockchain/multisign-wallet":{"label":"Multisign wallet","children":{}},"/playground/blockchain/anchor-framework":{"label":"Anchor framework","children":{}},"/playground/blockchain/blockchain-bridge":{"label":"Blockchain Bridge","children":{}},"/playground/blockchain/nft-fractionalization":{"label":"NFT Fractionalization","children":{}},"/playground/blockchain/how-tokens-work-on-solana":{"label":"How Tokens Work on Solana","children":{}},"/playground/blockchain/liquidity-pool":{"label":"Liquidity pool","children":{}}}},"/playground/frontend":{"label":"Frontend","children":{"/playground/frontend/report":{"label":"Report","children":{"/playground/frontend/report/frontend-report-march-2025":{"label":"March 2025","children":{}},"/playground/frontend/report/frontend-report-february-2025":{"label":"February 2025","children":{}},"/playground/frontend/report/frontend-report-january-2025":{"label":"January 2025","children":{}},"/playground/frontend/report/frontend-report-second-half-of-november-2024":{"label":"Nov 2024 (Second Half)","children":{}},"/playground/frontend/report/frontend-report-first-half-of-november-2024":{"label":"Nov 2024 (First Half)","children":{}},"/playground/frontend/report/frontend-report-october-2024":{"label":"October 2024","children":{}},"/playground/frontend/report/frontend-report-september-2024":{"label":"September 2024","children":{}},"/playground/frontend/report/frontend-report-august-2024":{"label":"August 2024","children":{}},"/playground/frontend/report/frontend-report-july-2024":{"label":"July 2024","children":{}}}},"/playground/frontend/react":{"label":"React","children":{"/playground/frontend/react/code-splitting":{"label":"Code splitting","children":{}},"/playground/frontend/react/component-composition-patterns":{"label":"Component composition patterns","children":{}},"/playground/frontend/react/design-system-integration":{"label":"Design system integration","children":{}},"/playground/frontend/react/hook-architecture":{"label":"Hook architecture","children":{}},"/playground/frontend/react/rendering-strategies":{"label":"Rendering strategies","children":{}},"/playground/frontend/react/state-management-strategy":{"label":"State management strategy","children":{}},"/playground/frontend/react/testing-strategies":{"label":"Testing strategies","children":{}}}},"/playground/frontend/websockets":{"label":"WebSockets","children":{}},"/playground/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree":{"label":"From Markup to Pixels - A look inside the DOM, CSSOM, and Render Tree","children":{}},"/playground/frontend/window-and-iframe-communication":{"label":"Window and iframe communication","children":{}},"/playground/frontend/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"Applying Mock Service Worker (MSW) for Seamless Web Development","children":{}},"/playground/frontend/render-optimization-in-data-fetching-libraries":{"label":"Render optimization in data-fetching libraries","children":{}},"/playground/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql":{"label":"A Fragment Colocation Pattern with React \u0026 Apollo GraphQL","children":{}},"/playground/frontend/scroll-driven-animations":{"label":"Scroll-driven animations","children":{}},"/playground/frontend/react-server-component":{"label":"React Server Components, NextJs Route and Data Fetching","children":{}},"/playground/frontend/url-formats-for-sharing-via-social-networks":{"label":"URL formats for sharing via social networks","children":{}},"/playground/frontend/shadow-dom":{"label":"Shadow DOM","children":{}},"/playground/frontend/retain-scroll-position-in-infinite-scroll":{"label":"Retain scroll position in infinite scroll","children":{}},"/playground/frontend/continuous-translation":{"label":"Continuous Translation","children":{}},"/playground/frontend/what-is-pnpm-compare-to-npmyarn":{"label":"What is PNPM Compare To NPM/Yarn","children":{}},"/playground/frontend/why-micro-frontend":{"label":"Why Micro Frontend","children":{}},"/playground/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution":{"label":"Why We Chose Our Tech Stack Accelerating Development With A Robust Frontend Solution","children":{}},"/playground/frontend/tackling-server-state-complexity-in-frontend-development":{"label":"Tackling Server State complexity in Frontend Development","children":{}},"/playground/frontend/variable-fonts":{"label":"Variable Fonts","children":{}},"/playground/frontend/when-should-we-use-usereducer-instead-of-usestate":{"label":"When should we use useReducer instead of useState?","children":{}},"/playground/frontend/preserving-and-resetting-state-in-react":{"label":"Preserving and Resetting state in React","children":{}},"/playground/frontend/mixpanel":{"label":"Mixpanel","children":{}},"/playground/frontend/validation-with-zod":{"label":"Validation with Zod","children":{}},"/playground/frontend/parse-dont-validate-in-typescript":{"label":"Parse, don't validate in TypeScript","children":{}},"/playground/frontend/webassembly":{"label":"Webassembly","children":{}},"/playground/frontend/singleton-design-pattern-in-javascript":{"label":"Singleton Design Pattern in Javascript","children":{}},"/playground/frontend/an-introduction-to-atomic-css":{"label":"An Introduction to Atomic CSS","children":{}},"/playground/frontend/intro-to-indexeddb":{"label":"Intro to IndexedDB","children":{}},"/playground/frontend/the-fundamental-of-web-performance":{"label":"The fundamental of web performance","children":{}},"/playground/frontend/wai-aria":{"label":"WAI-ARIA","children":{}},"/playground/frontend/build-polymorphic-react-components-with-typescript":{"label":"Build polymorphic React components with Typescript","children":{}},"/playground/frontend/threejs":{"label":"Threejs","children":{"/playground/frontend/threejs/cameras-in-threejs":{"label":"Cameras in ThreeJS","children":{}}}},"/playground/frontend/prevent-layout-thrashing":{"label":"Prevent Layout Thrashing","children":{}},"/playground/frontend/pure-css-parallax":{"label":"Pure CSS Parallax","children":{}},"/playground/frontend/css-container-queries":{"label":"CSS Container Queries","children":{}},"/playground/frontend/hsl-color":{"label":"HSL Color","children":{}},"/playground/frontend/mitigate-blocking-the-main-thread":{"label":"Mitigate blocking the main thread","children":{}},"/playground/frontend/css-in-js":{"label":"CSS in JS","children":{}},"/playground/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second":{"label":"Dark mode flickers a white background for a fraction of a second","children":{}},"/playground/frontend/why-dom-manipulation-is-slow":{"label":"Why DOM manipulation is slow?","children":{}},"/playground/frontend/why-virtual-dom-is-fast":{"label":"Why Virtual DOM is fast?","children":{}},"/playground/frontend/vitejs-native-modules":{"label":"ViteJS native modules","children":{}},"/playground/frontend/javascript-modules":{"label":"JavaScript modules","children":{}},"/playground/frontend/atomic-design-pattern":{"label":"Atomic Design Pattern","children":{}},"/playground/frontend/focus-trap":{"label":"Focus trap","children":{}},"/playground/frontend/html-inert":{"label":"HTML inert","children":{}},"/playground/frontend/useeffect-double-calls-in-react-18":{"label":"useEffect double calls in React 18","children":{}},"/playground/frontend/react-18":{"label":"React 18","children":{}},"/playground/frontend/remix-versus-nextjs":{"label":"Remix Versus Nextjs","children":{}},"/playground/frontend/zaplib-post-mortem":{"label":"Zaplib post-mortem","children":{}},"/playground/frontend/parallelism-in-javascript":{"label":"Parallelism in JavaScript","children":{}},"/playground/frontend/mpa-spa-and-partial-hydration":{"label":"MPA, SPA and Partial Hydration","children":{}},"/playground/frontend/micro-frontends-microservices-for-frontend-development":{"label":"Micro Frontends Microservices For Frontend Development","children":{}},"/playground/frontend/using-correct-html-element-to-increase-website-accessibility":{"label":"Using Correct Html Element To Increase Website Accessibility","children":{}},"/playground/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss":{"label":"Remove Unused CSS Styles From Bootstrap Using Purgecss","children":{}}}},"/playground/use-cases":{"label":"Use Cases","children":{"/playground/use-cases/service_monitoring_with_upptime":{"label":"Secure and transparent uptime monitoring with Upptime and GitHub secrets","children":{}},"/playground/use-cases/create-slides-with-overleaf":{"label":"Create slides with Overleaf and ChatGPT","children":{}},"/playground/use-cases/optimize-init-load-time-for-trading-platform":{"label":"Optimizing initial load time for a Trading Platform","children":{}},"/playground/use-cases/ai-interview-platform-mvp":{"label":"Building MVP for AI-driven interview platform","children":{}},"/playground/use-cases/optimizing-ui-for-effective-investment-experience":{"label":"Hedge Foundation - Optimizing UI for effective investment experience","children":{}},"/playground/use-cases/implement-binance-future-pnl-analysis-page":{"label":"Implement Binance Futures PNL analysis page by Phoenix LiveView","children":{}},"/playground/use-cases/migrate-normal-table-to-timescale-table":{"label":"Migrate regular tables into TimescaleDB hypertables to improve query performance","children":{}},"/playground/use-cases/bitcoin-alt-performance-tracking":{"label":"Tracking Bitcoin-Altcoin Performance Indicators in BTC Hedging Strategy","children":{}},"/playground/use-cases/database-hardening-for-trading-platform":{"label":"Database hardening for a trading platform","children":{}},"/playground/use-cases/data-archive-and-recovery":{"label":"Building a data archive and recovery strategy for high-volume trading system","children":{}},"/playground/use-cases/persist-history-using-data-snapshot-pattern":{"label":"Implementing data snapshot pattern to persist historical data","children":{}},"/playground/use-cases/ai-ruby-travel-assistant-chatbot":{"label":"AI-powered Ruby travel assistant","children":{}},"/playground/use-cases/building-chatbot-agent-for-project-management-tool":{"label":"Building chatbot agent to streamline project management","children":{}},"/playground/use-cases/building-data-pipeline-ogif-transcriber":{"label":"Building data pipeline for OGIF transcriber","children":{}},"/playground/use-cases/centralized-monitoring-setup-for-trading-platform":{"label":"Setup centralized monitoring system for Hedge Foundation trading platform","children":{}},"/playground/use-cases/binance-transfer-matching":{"label":"Building better Binance transfer tracking","children":{}},"/playground/use-cases/crypto-market-outperform-chart-rendering":{"label":"Visualizing crypto market performance: BTC-Alt dynamic indicators in Golang","children":{}},"/playground/use-cases/enhancing-cryptocurrency-transfer-logger":{"label":"Transfer mapping: enhancing loggers for better transparency","children":{}},"/playground/use-cases/reconstructing_trading_pnl_data_pipeline_approach":{"label":"Reconstructing historical trading PnL: a data pipeline approach","children":{}},"/playground/use-cases/ai-powered-monthly-project-reports":{"label":"Project reports system: a case study","children":{}}}},"/playground/ai":{"label":"AI","children":{"/playground/ai/securing-your-remote-mcp-servers":{"label":"Securing your remote MCP servers","children":{}},"/playground/ai/tool-level-security-for-remote-mcp-servers":{"label":"Tool-Level Security for Remote MCP Servers","children":{}},"/playground/ai/model-context-protocol":{"label":"Intro to Model Context Protocol","children":{}},"/playground/ai/building-llm-system":{"label":"Building LLM System","children":{"/playground/ai/building-llm-system/quantization-in-llm":{"label":"Quantization for large language models","children":{}},"/playground/ai/building-llm-system/graphrag":{"label":"GraphRAG - Building a knowledge graph for RAG system","children":{}},"/playground/ai/building-llm-system/guardrails-in-llm":{"label":"Guardrails in LLM","children":{}},"/playground/ai/building-llm-system/react-in-llm":{"label":"ReAct(Reason + Act) in LLM","children":{}},"/playground/ai/building-llm-system/rewoo-in-llm":{"label":"ReWOO: Reasoning without observation - A deeper look","children":{}},"/playground/ai/building-llm-system/model-selection":{"label":"Model selection","children":{}},"/playground/ai/building-llm-system/logs-pillar":{"label":"Logging","children":{}},"/playground/ai/building-llm-system/metric-pillar":{"label":"Metrics","children":{}},"/playground/ai/building-llm-system/observability-in-ai-platforms":{"label":"Observability in AI platforms","children":{}},"/playground/ai/building-llm-system/trace-pillar":{"label":"Tracing","children":{}},"/playground/ai/building-llm-system/intent-classification-by-llm":{"label":"Intent classification by LLM","children":{}},"/playground/ai/building-llm-system/llm-as-a-judge":{"label":"LLM as a judge","children":{}},"/playground/ai/building-llm-system/use-cases-for-llm-applications":{"label":"Use cases for LLM applications","children":{}},"/playground/ai/building-llm-system/the-rise-of-ai-applications-with-llm":{"label":"The rise of AI applications with LLM","children":{}},"/playground/ai/building-llm-system/evaluation-guideline-for-llm-application":{"label":"Evaluation guidelines for LLM applications","children":{}},"/playground/ai/building-llm-system/prevent-prompt-injection":{"label":"Prevent prompt injection","children":{}},"/playground/ai/building-llm-system/building-llm-system":{"label":"§ Building LLM system","children":{}},"/playground/ai/building-llm-system/multi-agent-collaboration-for-task-completion":{"label":"Multi-agent collaboration for task completion","children":{}},"/playground/ai/building-llm-system/multimodal-in-rag":{"label":"Multimodal in RAG","children":{}}}},"/playground/ai/digest":{"label":"Digest","children":{"/playground/ai/digest/ai-digest-02":{"label":"AI digest #2 New command Aider, OpenHands, Qwen2.5 Coder 32B, Predicted Output","children":{}},"/playground/ai/digest/ai-digest-01":{"label":"AI digest #1 Aider reasoning, OpenAI Realtime API, Cline - pre Claude-dev ","children":{}}}},"/playground/ai/copilots":{"label":"Copilots","children":{"/playground/ai/copilots/projects-operations":{"label":"Project Operations Copilots","children":{}},"/playground/ai/copilots/team-copilots":{"label":"Team Copilots","children":{}}}},"/playground/ai/text-to-mongodb":{"label":"Natural Language to Database Queries: Text-to-MongoDB","children":{}},"/playground/ai/use-cases":{"label":"Use Cases","children":{"/playground/ai/use-cases/salesforce":{"label":"Salesforce use cases","children":{}},"/playground/ai/use-cases/yelp":{"label":"Yelp use cases","children":{}}}},"/playground/ai/evaluate-chatbot-agent-by-simulated-user":{"label":"Evaluate Chatbot Agent by User Simulation","children":{}},"/playground/ai/journey-of-thought-prompting":{"label":"Journey of Thought Prompting: Harnessing AI to Craft Better Prompts","children":{}},"/playground/ai/llm-tracing-in-ai-system":{"label":"LLM tracing in AI system","children":{}},"/playground/ai/caching-with-rag-system":{"label":"Evaluating caching in RAG systems","children":{}},"/playground/ai/generative-ui":{"label":"What is Generative UI?","children":{}},"/playground/ai/re-ranking-in-rag":{"label":"Re-ranking in RAG","children":{}},"/playground/ai/function-calling":{"label":"Function calling in AI agents","children":{}},"/playground/ai/building-llm-powered-tools-with-dify":{"label":"Streamlining Internal Tool Development with Managed LLMOps: A Dify Case Study","children":{}},"/playground/ai/thumbs-up-and-thumbs-down-pattern":{"label":"Thumbs up and Thumbs down pattern","children":{}},"/playground/ai/supervisor-ai-agents":{"label":"Building Agent Supervisors to Generate Insights","children":{}},"/playground/ai/raptor-llm-retrieval":{"label":"RAPTOR: Tree-based Retrieval for Language Models","children":{}},"/playground/ai/proximal-policy-optimization":{"label":"Proximal Policy Optimization","children":{}},"/playground/ai/a-grand-unified-theory-of-the-ai-hype-cycle":{"label":"A Grand Unified Theory of the AI Hype Cycle","children":{}},"/playground/ai/developing-rapidly-with-generative-ai":{"label":"Developing rapidly with Generative AI","children":{}},"/playground/ai/rlhf-with-open-assistant":{"label":"RLHF with Open Assistant","children":{}},"/playground/ai/story-map-for-llms":{"label":"Story map for LLMs","children":{}},"/playground/ai/adversarial-prompting":{"label":"Adversarial Prompting in Prompt Engineering","children":{}},"/playground/ai/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"Chunking strategies to overcome context limitation in LLM","children":{}},"/playground/ai/llms-accuracy-self-refinement":{"label":"LLM's Accuracy - Self Refinement","children":{}},"/playground/ai/llm-query-caching":{"label":"Query Caching for Large Language Models","children":{}},"/playground/ai/reinforcement-learning":{"label":"Introduction to Reinforcement Learning and Its Application with LLMs","children":{}},"/playground/ai/foundation-model":{"label":"Foundation Models: The Latest Advancement in AI","children":{}},"/playground/ai/select-vector-database-for-llm":{"label":"Select Vector Database for LLM","children":{}},"/playground/ai/build-your-chatbot-with-open-source-large-language-models":{"label":"Build your chatbot with open source Large Language Models","children":{}},"/playground/ai/workaround-with-openais-token-limit-with-langchain":{"label":"Workaround with OpenAI's token limit with Langchain","children":{}},"/playground/ai/working-with-langchain-document-loaders":{"label":"Working with langchain document loaders","children":{}}}},"/playground/market-commentary":{"label":"Market Commentary","children":{"/playground/market-commentary/event-takeaways-2nd":{"label":"2nd Talks and Takeaways","children":{}},"/playground/market-commentary/event-takeaways-1st":{"label":"1st Talks and Takeaways","children":{}},"/playground/market-commentary/2025-28th-feb":{"label":"#9: Bybit Loses $1.5B in Hack, Claude 3.7 Sonnet Drops, and OpenArt Designs Characters","children":{}},"/playground/market-commentary/2025-21th-feb":{"label":"#8: R1 1776 Goes Open-Source, Cardex Gets Hacked, and Grok-3 Debuts","children":{}},"/playground/market-commentary/2025-14th-feb":{"label":"#7: 10x AI Cost Reduction, Lyft’s 2026 Robotaxi Milestone, and Solana ETF Buzz","children":{}},"/playground/market-commentary/2025-7th-feb":{"label":"#6 Trending Products, DeepSeek Wave, and Ethereum Predictions","children":{}},"/playground/market-commentary/2025-17th-jan":{"label":"#5 VC Trends, Blockchain Breakthroughs, and AI Innovations","children":{}},"/playground/market-commentary/2025-10th-jan":{"label":"#4 AI Supercomputers, Mini AI PCs, SEA VC","children":{}},"/playground/market-commentary/2025-3rd-jan":{"label":"#3 AI at CES, Wall Street Boom, Blockchain Trends","children":{}},"/playground/market-commentary/2024-27th-dec":{"label":"#2 AI Talent Wars, OpenAI’s New Models, Hyperliquid","children":{}},"/playground/market-commentary/2024-13th-dec":{"label":"#1 Gemini 2.0, OpenAI’s Sora,  a16z’s Predictions","children":{}}}},"/playground/forward-engineering":{"label":"Forward Engineering","children":{"/playground/forward-engineering/2025-02":{"label":"20242025","children":{}},"/playground/forward-engineering/2024-09":{"label":"September 2024","children":{}},"/playground/forward-engineering/2023-11":{"label":"November 2023","children":{}},"/playground/forward-engineering/2023-10":{"label":"October 2023","children":{}},"/playground/forward-engineering/2023-08":{"label":"August 2023","children":{}},"/playground/forward-engineering/2023-06":{"label":"June 2023","children":{}},"/playground/forward-engineering/2023-05":{"label":"May 2023","children":{}},"/playground/forward-engineering/2023-03":{"label":"March 2023","children":{}},"/playground/forward-engineering/2023-12":{"label":"December 2023","children":{}},"/playground/forward-engineering/2022":{"label":"2022","children":{}},"/playground/forward-engineering/volume-03":{"label":"Tech Radar Volume 03","children":{}},"/playground/forward-engineering/volume-02":{"label":"Tech Radar Volume 02","children":{}},"/playground/forward-engineering/volume-01":{"label":"Tech Radar Volume 01","children":{}},"/playground/forward-engineering/readme":{"label":"📡 Tech Radar","children":{}}}},"/playground/go":{"label":"Go","children":{"/playground/go/weekly":{"label":"Weekly","children":{"/playground/go/weekly/dec-13":{"label":"#24 Go 1.24 testing/synctest experiment for time and concurrency testing","children":{}},"/playground/go/weekly/dec-06":{"label":"#23 Draft Release Notes for Go 1.24 and weak pointers in Go","children":{}},"/playground/go/weekly/nov-29":{"label":"#22 GoMLX: ML in Go without Python","children":{}},"/playground/go/weekly/nov-22":{"label":"#21 Go sync.Once is Simple","children":{}},"/playground/go/weekly/nov-15":{"label":"#20 Go Turns 15","children":{}},"/playground/go/weekly/nov-08":{"label":"#19 Writing secure Go code","children":{}},"/playground/go/weekly/nov-01":{"label":"#18 Fuzz Testing Go HTTP Services","children":{}},"/playground/go/weekly/oct-25":{"label":"#17 Leveraging benchstat Projects in Go benchmark and Go Plan9 memo on 450% speeding up calculations","children":{}},"/playground/go/weekly/oct-18":{"label":"#16 Understand sync.Map","children":{}},"/playground/go/weekly/oct-11":{"label":"#15 Go embed and Reflect","children":{}},"/playground/go/weekly/oct-04":{"label":"#14 Compile-time eval \u0026 SQLite with wazero","children":{}},"/playground/go/weekly/sep-27":{"label":"#13 Compiler Quests and Vector Vexations","children":{}},"/playground/go/weekly/sep-20":{"label":"#12 CLI Tools for K8s, REST, and Terminals","children":{}},"/playground/go/weekly/sep-13":{"label":"#11 Actors, Frameworks, and the Future of Go","children":{}},"/playground/go/weekly/sep-06":{"label":"#10 Script, Telemetry","children":{}},"/playground/go/weekly/aug-30":{"label":"#9 TinyGo, SQLite vector search, and Permify","children":{}},"/playground/go/weekly/aug-23":{"label":"#8 GoNB, kubetrim, and GopherCon UK 2024","children":{}},"/playground/go/weekly/aug-16":{"label":"#7 Go 1.23, Websockets, and Structs","children":{}},"/playground/go/weekly/aug-09":{"label":"#6 Cogent Core, Russ Cox stepping down","children":{}},"/playground/go/weekly/aug-02":{"label":"#5 Go 1.23 features, Memory, Minecraft, and More","children":{}},"/playground/go/weekly/jul-26":{"label":"#4 Ethical Hacking, HTTP Requests, Mac App Development","children":{}},"/playground/go/weekly/jul-12":{"label":"#3 Generic Collections, Generics Constraints, AI Bot","children":{}},"/playground/go/weekly/jul-05":{"label":"#2 Go 1.23 Iterators","children":{}},"/playground/go/weekly/june-27":{"label":"#1 eBPF and PGO Optimization Techniques","children":{}}}},"/playground/go/extension-interface-pattern":{"label":"Go extension interface pattern","children":{}},"/playground/go/go-import":{"label":"Go import design: using git repo path","children":{}},"/playground/go/go-package":{"label":"Package first design","children":{}},"/playground/go/go-generics-type-safety":{"label":"How does Go achieve type safety when it enables generics?","children":{}},"/playground/go/go-for-enterprise":{"label":"Go For Enterprise","children":{"/playground/go/go-for-enterprise/who-using-golang-in-enterprise":{"label":"Who is using Go in enterprise?","children":{}},"/playground/go/go-for-enterprise/enterprise-standard-language":{"label":"Go as an Enterprise Standard Language","children":{}},"/playground/go/go-for-enterprise/how-to-use-go-in-enterprise":{"label":"How to use Go in the Enterprise","children":{}},"/playground/go/go-for-enterprise/when-to-use-golang-in-enterprise":{"label":"When to use Go in the Enterprise","children":{}},"/playground/go/go-for-enterprise/why-enterprise-chose-java":{"label":"Why Enterprise Chose Java","children":{}},"/playground/go/go-for-enterprise/why-go":{"label":"Why Go?","children":{}}}},"/playground/go/compute-union-2-finite-automata":{"label":"Efficient Union of Finite Automata in Golang: A Practical Approach","children":{}},"/playground/go/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{}},"/playground/go/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq":{"label":"Message Queues And Streaming Platforms Eg Kafka Nats Rabbitmq","children":{}},"/playground/go/unit-testing-best-practices-in-golang":{"label":"Unit Testing Best Practices In Golang","children":{}},"/playground/go/profiling-in-go":{"label":"Profiling in Go","children":{}},"/playground/go/go-in-software-engineering":{"label":"Go In Software Engineering","children":{}},"/playground/go/go-concurrency":{"label":"Go Concurrency","children":{}},"/playground/go/slice-and-array-in-golang":{"label":"Slice And Array In Golang","children":{}},"/playground/go/use-go-selenium-to-crawl-data":{"label":"Use Go Selenium To Crawl Data","children":{}},"/playground/go/connecting-vim-with-golang":{"label":"Connecting Vim With Golang","children":{}}}},"/playground/market-report":{"label":"Market Report","children":{"/playground/market-report/2024-october":{"label":"October 2024","children":{}},"/playground/market-report/2024-september":{"label":"September 2024","children":{}},"/playground/market-report/2024-august":{"label":"August 2024","children":{}},"/playground/market-report/2024-july":{"label":"July 2024","children":{}},"/playground/market-report/2024-may":{"label":"May 2024","children":{}},"/playground/market-report/2024-april":{"label":"April 2024","children":{}},"/playground/market-report/2024-march":{"label":"March 2024","children":{}},"/playground/market-report/2024-february":{"label":"February 2024","children":{}},"/playground/market-report/2024-january":{"label":"January 2024","children":{}},"/playground/market-report/2023-december":{"label":"December 2023","children":{}}}},"/playground/devbox":{"label":"Devbox","children":{"/playground/devbox/devbox":{"label":"§ Devbox","children":{}},"/playground/devbox/story":{"label":"Story","children":{"/playground/devbox/story/devbox-production-success-story":{"label":"Devbox in Production: Our Success Story","children":{}},"/playground/devbox/story/devbox-local-development-env":{"label":"Using Devbox to setup local development environment","children":{}},"/playground/devbox/story/devbox-nix-and-our-devbox-adoption":{"label":"The overview into Nix \u0026 how we use Devbox @ Dwarves","children":{}},"/playground/devbox/story/devbox-docker-adoption-and-challenges":{"label":"Our Docker adoption and its challenges","children":{}},"/playground/devbox/story/devbox-a-world-before-docker":{"label":"The world before Docker","children":{}}}},"/playground/devbox/guide":{"label":"Guide","children":{"/playground/devbox/guide/containerless":{"label":"Ditch the Containers: Go Containerless with Devbox","children":{}},"/playground/devbox/guide/devboxjson":{"label":"Devbox.json: Your Project's DNA","children":{}},"/playground/devbox/guide/run-your-own-shell":{"label":"Devbox Shell: Your Dev Environment, Your Rules","children":{}}}},"/playground/devbox/introduction":{"label":"Introduction","children":{"/playground/devbox/introduction/the-reason-for-being":{"label":"The reason for being","children":{}},"/playground/devbox/introduction/why-devbox-but-not-nix":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}}}},"/playground/devbox/research":{"label":"Research","children":{"/playground/devbox/research/content-addressable-storage-in-docker":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}},"/playground/devbox/research/fixed-output-derivation":{"label":"Fixed-output Derivation in Nix","children":{}},"/playground/devbox/research/nix-is-faster-than-docker-build":{"label":"Nix is Faster Than Docker Build","children":{}},"/playground/devbox/research/pinning-nixpkgs":{"label":"Pinning nixpkgs in Nix","children":{}},"/playground/devbox/research/shadow-copies":{"label":"Shadow Copies in Docker Builds","children":{}},"/playground/devbox/research/unstable-package-installation":{"label":"Unstable Package Installation in Docker","children":{}}}}}}}},"/careers":{"label":"Careers","children":{"/careers/archived":{"label":"Archived","children":{"/careers/archived/full-stack-engineer":{"label":"Full-Stack Engineer","children":{}},"/careers/archived/executive-assistant":{"label":"Executive Assistant","children":{}},"/careers/archived/technical-recruiter":{"label":"Technical Recruiter","children":{}},"/careers/archived/backend-engineer-go-elixir-rust":{"label":"Backend Engineer, Go/Elixir/Rust","children":{}},"/careers/archived/react-native-developer":{"label":"React Native Developer","children":{}},"/careers/archived/android-developer":{"label":"Mobile Engineer, Android","children":{}},"/careers/archived/community-executive":{"label":"Community Executive","children":{}},"/careers/archived/data-engineering":{"label":"Energy - Data Engineering","children":{}},"/careers/archived/devops":{"label":"DevOps Engineer - FinTech","children":{}},"/careers/archived/frontend-developer-junior":{"label":"Junior Frontend Developer","children":{}},"/careers/archived/frontend":{"label":"Frontend","children":{}},"/careers/archived/ios-developer":{"label":"iOS Developer - EnergyTech","children":{}},"/careers/archived/macos-developer":{"label":"Software Engineer, macOS","children":{}},"/careers/archived/product-designer-new-grad":{"label":"Product Designer, New Grad","children":{}},"/careers/archived/product-designer":{"label":"Product Designer","children":{}},"/careers/archived/qc-automation":{"label":"QC Engineer, Automation - Logistics","children":{}},"/careers/archived/qc-manual":{"label":"Fintech - QC Engineer, Manual","children":{}},"/careers/archived/reactjs-web-engineer":{"label":"Web Engineer, React.js","children":{}},"/careers/archived/visual-designer":{"label":"Visual Designer","children":{}},"/careers/archived/android":{"label":"Android","children":{}},"/careers/archived/golang":{"label":"Golang","children":{}},"/careers/archived/intern":{"label":"Intern","children":{}},"/careers/archived/ios":{"label":"iOS Developer","children":{}},"/careers/archived/qa":{"label":"QA Engineer","children":{}}}},"/careers/open-positions":{"label":"Open Positions","children":{"/careers/open-positions/business-manager":{"label":"Business Development Manager","children":{}},"/careers/open-positions/growth-lead":{"label":"Growth Lead","children":{}}}},"/careers/life":{"label":"Life","children":{"/careers/life/2024-09-26-29-dat-nguyen":{"label":"Dat Nguyen","children":{}},"/careers/life/2024-02-19-28-duyen-tran":{"label":"Duyen Tran","children":{}},"/careers/life/2024-01-22-27-tri-tran":{"label":"Tri Tran","children":{}},"/careers/life/2024-01-03-25-khoi-nguyen":{"label":"Khoi Nguyen","children":{}},"/careers/life/2023-12-13-24-tai-pham":{"label":"Tai Pham","children":{}},"/careers/life/2023-12-12-23-hieu-nghia":{"label":"Hieu Nghia","children":{}},"/careers/life/2023-11-27-22-cat-nguyen":{"label":"Cat Nguyen","children":{}},"/careers/life/2023-11-20-21-minh-cloud":{"label":"Minh Cloud","children":{}},"/careers/life/2023-11-13-20-hoai-khang":{"label":"Hoai Khang","children":{}},"/careers/life/2023-11-03-19-vi-tran":{"label":"Vi Tran","children":{}},"/careers/life/2023-10-30-18-tuan-tran":{"label":"Tuan Tran","children":{}},"/careers/life/2023-10-16-16-kim-ngan":{"label":"Kim Ngan","children":{}},"/careers/life/2023-10-13-17-hoang-nguyen":{"label":"Hoang Nguyen","children":{}},"/careers/life/2023-10-09-15-khoi-ngo":{"label":"Khoi Ngo","children":{}},"/careers/life/2023-10-02-14-dat-pham":{"label":"Dat Pham","children":{}},"/careers/life/2023-09-29-13-bien-vo":{"label":"Bien Vo","children":{}},"/careers/life/2023-09-18-12-toan-ho":{"label":"Toan Ho","children":{}},"/careers/life/2023-09-05-11-dinh-nam":{"label":"Dinh Nam","children":{}},"/careers/life/2023-08-17-10-cuong-mai":{"label":"Cuong Mai","children":{}},"/careers/life/2023-08-07-9-hoang-anh":{"label":"Hoang Anh","children":{}},"/careers/life/2023-06-30-7-khac-vy":{"label":"Khac Vy","children":{}},"/careers/life/group":{"label":"Group","children":{"/careers/life/group/2023-06-01-software-design-group":{"label":"Software Design Group","children":{}}}},"/careers/life/2022-09-21-7-my-anh":{"label":"My Anh","children":{}},"/careers/life/2022-08-11-6-hieu-vu":{"label":"Hieu Vu","children":{}},"/careers/life/2022-08-04-6-duy-nguyen":{"label":"Duy Nguyen","children":{}},"/careers/life/2022-08-03-5-nam-nguyen":{"label":"Nam Nguyen","children":{}},"/careers/life/2022-07-22-4-an-tran":{"label":"An Tran","children":{}},"/careers/life/2022-03-17-3-tom-nguyen":{"label":"Tom Nguyen","children":{}},"/careers/life/2022-02-25-2-anh-tran":{"label":"Anh Tran","children":{}},"/careers/life/2022-02-14-1-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/life/2021-03-31-0-tuan-dao":{"label":"Tuan Dao","children":{}},"/careers/life/2021-03-11-0-phat-nguyen-career":{"label":"Phat Nguyen","children":{}},"/careers/life/2020-05-08-0-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/life/2020-04-10-0-huy-nguyen":{"label":"Huy Nguyen","children":{}}}},"/careers/culture":{"label":"Culture","children":{}},"/careers/manifesto":{"label":"Manifesto","children":{}},"/careers/internship":{"label":"Internship","children":{"/careers/internship/2019":{"label":"2019","children":{"/careers/internship/2019/2019":{"label":"Spring Internship 2019","children":{}}}}}},"/careers/apprentice":{"label":"Apprentice","children":{"/careers/apprentice/2022":{"label":"2022","children":{"/careers/apprentice/2022/batch-of-2022":{"label":"Batch of 2022","children":{}},"/careers/apprentice/2022/2022-meet-ngoc-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/apprentice/2022/2022-meet-tuan-dao":{"label":"Tuan Dao","children":{}}}},"/careers/apprentice/apprentice":{"label":"Apprentice program","children":{}}}},"/careers/readme":{"label":"👋 Join the Dwarves","children":{}}}},"/opensource":{"label":"Opensource","children":{"/opensource/readme":{"label":"☀️ Open source","children":{}}}},"/playbook":{"label":"Playbook","children":{"/playbook/operations":{"label":"Operations","children":{"/playbook/operations/culture-test":{"label":"Culture Test","children":{}},"/playbook/operations/checklists":{"label":"Checklists","children":{"/playbook/operations/checklists/leave-and-request-checklist":{"label":"Leave Request","children":{}},"/playbook/operations/checklists/offboarding-checklist":{"label":"Offboarding","children":{}},"/playbook/operations/checklists/artifact-checklist":{"label":"Back up Artifact","children":{}},"/playbook/operations/checklists/project-archive":{"label":"Project Archive","children":{}},"/playbook/operations/checklists/project-case-study":{"label":"Project Case Study","children":{}},"/playbook/operations/checklists/project-communication":{"label":"Project Communication","children":{}},"/playbook/operations/checklists/project-handover":{"label":"Project Handover","children":{}},"/playbook/operations/checklists/project-initialization":{"label":"Project Initialization","children":{}},"/playbook/operations/checklists/assets-checklist":{"label":"Assets","children":{}},"/playbook/operations/checklists/billing-checklist":{"label":"Billing","children":{}},"/playbook/operations/checklists/candidate-checklist":{"label":"Candidate","children":{}},"/playbook/operations/checklists/consulting-contract-checklist":{"label":"Consulting Contract","children":{}},"/playbook/operations/checklists/hiring-checklist":{"label":"Hiring","children":{}},"/playbook/operations/checklists/onboarding-checklist":{"label":"Onboarding","children":{}},"/playbook/operations/checklists/unemployment-social-health-insurance":{"label":"Unemployment, Social, Health Insurance","children":{}},"/playbook/operations/checklists/vietnam-invoice-checklist":{"label":"Vietnam Invoice","children":{}}}},"/playbook/operations/how-to-conduct-delivery-reports":{"label":"How to conduct delivery reports","children":{}},"/playbook/operations/how-we-do-effective-planning-and-reporting":{"label":"How we do effective planning and reporting","children":{}},"/playbook/operations/project-schedule-delivery-guidelines":{"label":"Project Delivery Schedule and Guidelines","children":{}},"/playbook/operations/ogif":{"label":"OGIF - Oh God It's Friday","children":{}},"/playbook/operations/red-flags":{"label":"Red Flags","children":{}},"/playbook/operations/focus-on-software-delivery":{"label":"Focus On Software Delivery","children":{}},"/playbook/operations/are-you-helping":{"label":"Are You Helping","children":{}},"/playbook/operations/the-inner-circle":{"label":"The Inner Circle","children":{}},"/playbook/operations/mbti-type-intj":{"label":"MBTI Type INTJ","children":{}},"/playbook/operations/mbti-type-istp":{"label":"MBTI Type ISTP","children":{}},"/playbook/operations/mbti-type-estj":{"label":"MBTI Type ESTJ","children":{}},"/playbook/operations/mbti-type-istj":{"label":"MBTI Type ISTJ","children":{}},"/playbook/operations/applying-myersbriggs-type-indicator-in-hr":{"label":"Applying Myersbriggs Type Indicator In Hiring","children":{}},"/playbook/operations/the-four-preferences":{"label":"The Four Preferences","children":{}},"/playbook/operations/making-decision-as-a-team-member":{"label":"Making Decision As A Team Member","children":{}},"/playbook/operations/adjust-the-way-we-work-in-basecamp-style":{"label":"Adjust The Way We Work In Basecamp Style","children":{}},"/playbook/operations/beyond-the-title":{"label":"Beyond The Title","children":{}},"/playbook/operations/go-the-extra-mile":{"label":"Go The Extra Mile","children":{}},"/playbook/operations/the-dwarves-runs-by-ideas":{"label":"The Dwarves Runs By Ideas","children":{}},"/playbook/operations/a-tips-of-hiring-dont":{"label":"A Tips Of Hiring - Do \u0026 Don't","children":{}},"/playbook/operations/the-dwarves-culture-handbook":{"label":"The Dwarves Culture Handbook","children":{}},"/playbook/operations/how-people-matter-should-work":{"label":"How People Matter Should Work","children":{}},"/playbook/operations/delegation-and-believe-it-will-work":{"label":"Delegation And Believe It Will Work","children":{}},"/playbook/operations/constructive-feedback":{"label":"Constructive Feedback","children":{}},"/playbook/operations/transparency":{"label":"Transparency","children":{}},"/playbook/operations/bric-a-brac":{"label":"Bric A Brac","children":{}},"/playbook/operations/account":{"label":"Account","children":{}},"/playbook/operations/avoid-burn-out":{"label":"Avoid Burn Out","children":{}},"/playbook/operations/writing-management-objectives-in-smart":{"label":"Writing Management Objectives In Smart","children":{}},"/playbook/operations/building-a-solid-high-performing-team":{"label":"Building A Solid High Performing Team","children":{}},"/playbook/operations/hiring-for-operations-team":{"label":"Hiring For Operations Team","children":{}},"/playbook/operations/annual-bonus-for-sales":{"label":"Annual bonus for sales","children":{}},"/playbook/operations/bunk-license-check":{"label":"Bunk license check","children":{}},"/playbook/operations/collaboration-guidelines":{"label":"Collaboration Guidelines","children":{}},"/playbook/operations/compliance-check-process":{"label":"Compliance Check Process","children":{}},"/playbook/operations/email-template":{"label":"Email Template","children":{"/playbook/operations/email-template/assignment-invitation-2":{"label":"Assignment Inviation (Skip pre-assessment)","children":{}},"/playbook/operations/email-template/assignment-invitation":{"label":"Assignment Inviation","children":{}},"/playbook/operations/email-template/confirm-resume-date":{"label":"Confirm Employee's Resume Date Day","children":{}},"/playbook/operations/email-template/farewell":{"label":"Farewell Letter","children":{}},"/playbook/operations/email-template/follow-up-onboarding-items":{"label":"Follow-up Onboarding Items","children":{}},"/playbook/operations/email-template/hung-king-commemoration-day":{"label":"Hung King Commemoration Day","children":{}},"/playbook/operations/email-template/information-about-resource-change":{"label":"Inform about resource change","children":{}},"/playbook/operations/email-template/international-labour-day":{"label":"International Labour Day","children":{}},"/playbook/operations/email-template/interview-invitation":{"label":"Interview Invitation","children":{}},"/playbook/operations/email-template/milestone-sign-off":{"label":"Milestone sign-off","children":{}},"/playbook/operations/email-template/national-day":{"label":"National Day","children":{}},"/playbook/operations/email-template/new-year-day":{"label":"New Year Day","children":{}},"/playbook/operations/email-template/offer-letter":{"label":"Offer Letter","children":{}},"/playbook/operations/email-template/referral-bonus-confirmation-note":{"label":"Referral Bonus Confirmation Note","children":{}},"/playbook/operations/email-template/rejection-email":{"label":"Rejection","children":{}},"/playbook/operations/email-template/salary-increment":{"label":"Salary Increment Announcement","children":{}},"/playbook/operations/email-template/tet-holiday":{"label":"Tet Holiday","children":{}},"/playbook/operations/email-template/thank-you-letter":{"label":"Thank you letter","children":{}},"/playbook/operations/email-template/welcome-onboard":{"label":"Welcome Onboard","children":{}},"/playbook/operations/email-template/welcome-to-dwarves-update":{"label":"Welcome to Dwarves Updates","children":{}}}},"/playbook/operations/naming-convention":{"label":"Naming convention","children":{}},"/playbook/operations/setup-email-template":{"label":"Setup email template in Gmail","children":{}},"/playbook/operations/delegate-work-not-responsibility":{"label":"Delegate Work Not Responsibility","children":{}},"/playbook/operations/types-of-employees":{"label":"Types Of Employees","children":{}},"/playbook/operations/hiring-approach":{"label":"Hiring Approach","children":{}},"/playbook/operations/the-okr":{"label":"The OKR","children":{}},"/playbook/operations/our-metrics-for-performance-review":{"label":"Our Metrics For Performance Review","children":{}},"/playbook/operations/make-remote-working-works":{"label":"Make Remote Working Works","children":{}},"/playbook/operations/blocking-distraction":{"label":"Blocking Distraction","children":{}},"/playbook/operations/effective-meeting":{"label":"Effective Meeting","children":{}},"/playbook/operations/our-policy-for-remote-working":{"label":"Our Policy For Remote Working","children":{}}}},"/playbook/business":{"label":"Business","children":{"/playbook/business/pricing-model-bill-by-hours":{"label":"Pricing model: Bill by hours","children":{}},"/playbook/business/invoice":{"label":"Invoice","children":{}},"/playbook/business/nda":{"label":"NDA","children":{}},"/playbook/business/collaboration-guideline":{"label":"Collaboration Guideline","children":{}},"/playbook/business/df-workflow":{"label":"Dwarves Workflow","children":{}},"/playbook/business/fbsc":{"label":"FBSC","children":{}},"/playbook/business/how-to-work-with-clients":{"label":"How to work with clients","children":{}},"/playbook/business/service-feedbacks":{"label":"Service Feedbacks","children":{}},"/playbook/business/setting-the-budget":{"label":"Setting The Budget","children":{}},"/playbook/business/fixed-budget-scope-controlled":{"label":"Fixed Budget Scope Controlled","children":{}},"/playbook/business/the-adjacent-possible":{"label":"The Adjacent Possible","children":{}}}},"/playbook/engineering":{"label":"Engineering","children":{"/playbook/engineering/estimation-guidelines":{"label":"Estimation Guidelines","children":{}},"/playbook/engineering/presentation":{"label":"monitoring","children":{}},"/playbook/engineering/repo-icon":{"label":"release","children":{}}}},"/playbook/design":{"label":"Design","children":{"/playbook/design/design-system":{"label":"lean-canvas","children":{}},"/playbook/design/ia":{"label":"NDA","children":{}},"/playbook/design/ix":{"label":"IA","children":{}},"/playbook/design/aarrr":{"label":"AARRR","children":{}},"/playbook/design/design-sprint":{"label":"Design Sprint","children":{}},"/playbook/design/lean-canvas":{"label":"Lean Canvas","children":{}},"/playbook/design/prototype":{"label":"Low-fidelity prototype: UI Design","children":{}},"/playbook/design/ui":{"label":"UI","children":{}},"/playbook/design/ux":{"label":"UX","children":{}},"/playbook/design/wireframe":{"label":"wireframe","children":{}}}},"/playbook/readme":{"label":"Playbook","children":{}}}},"/fund":{"label":"Fund","children":{"/fund/ventures-fund-1":{"label":"Dwarves Ventures Fund 1","children":{}},"/fund/ventures-fund-0":{"label":"Dwarves Ventures Fund 0","children":{}}}},"/rfc":{"label":"RFC","children":{"/rfc/readme":{"label":"RFCs","children":{}},"/rfc/000-template":{"label":"000 RFC template","children":{}}}},"/updates":{"label":"Updates","children":{"/updates/digest":{"label":"Digest","children":{"/updates/digest/174-2025-whats-new-march":{"label":"What's New in March 2025","children":{}},"/updates/digest/173-2025-whats-new-february":{"label":"What's New in February 2025","children":{}},"/updates/digest/15-new-year-gathering":{"label":"#15 New year gathering","children":{}},"/updates/digest/2024-in-review":{"label":"2024 In Review","children":{}},"/updates/digest/172-2024-whats-new-december":{"label":"What's New in December 2024","children":{}},"/updates/digest/2024-summit-building-bonds-our-way":{"label":"Summit 2024: Building bonds our way","children":{}},"/updates/digest/171-2024-whats-new-november":{"label":"What's New in November 2024","children":{}},"/updates/digest/170-2024-whats-new-oct":{"label":"What's New in October 2024","children":{}},"/updates/digest/169-2024-whats-new-september":{"label":"What's New in September 2024","children":{}},"/updates/digest/2024-navigating-changes":{"label":"Navigating changes","children":{}},"/updates/digest/14-back-to-the-office":{"label":"#14 Hybrid work harmony","children":{}},"/updates/digest/168-2024-whats-new-august":{"label":"What's New in August 2024","children":{}},"/updates/digest/167-2024-whats-new-july":{"label":"What's New in July 2024","children":{}},"/updates/digest/13-more-than-lines-of-code":{"label":"#13 More than lines of code","children":{}},"/updates/digest/12-summer-moments":{"label":"#12 Summer moments","children":{}},"/updates/digest/166-2024-whats-new-june":{"label":"What's New in June 2024","children":{}},"/updates/digest/2024-semi-annual-review":{"label":"State of Dwarves: 2024 Semi-annual Review","children":{}},"/updates/digest/11-come-grow-with-us":{"label":"#11 Come grow with us","children":{}},"/updates/digest/10-from-lean-to-learner":{"label":"#10 From lean to learner","children":{}},"/updates/digest/165-2024-whats-new-may":{"label":"What's New in May 2024","children":{}},"/updates/digest/2024-community-meet-up":{"label":"Dwarves' 2nd community offline meet-up","children":{}},"/updates/digest/9-a-little-more-speed-for-summer":{"label":"#9 A little more speed for summer","children":{}},"/updates/digest/8-then-came-the-last-days-of-may":{"label":"#8 Then came the last days of May","children":{}},"/updates/digest/7-a-journey-through-time":{"label":"#7 A journey through time","children":{}},"/updates/digest/164-2024-whats-new-april":{"label":"What's New in April 2024","children":{}},"/updates/digest/6-stay-for-the-culture":{"label":"#6 Come for the conversation, stay for the culture","children":{}},"/updates/digest/5-delay-the-gratification":{"label":"#5 Endure the hardship, delay the gratification","children":{}},"/updates/digest/4-finding-your-authentic-tribe":{"label":"#4 Finding your authentic tribe","children":{}},"/updates/digest/3-we-all-start-somewhere":{"label":"#3 We all start somewhere","children":{}},"/updates/digest/2-walk-around-learn-around":{"label":"#2 Walk around learn around","children":{}},"/updates/digest/1-what-do-you-stand-for":{"label":"#1 What do you stand for?","children":{}},"/updates/digest/163-2024-whats-new-march":{"label":"What's New in March 2024","children":{}},"/updates/digest/162-2024-whats-new-february":{"label":"What's New in February 2024","children":{}},"/updates/digest/161-2024-whats-new-january":{"label":"What's New in January 2024","children":{}},"/updates/digest/160-2023-whats-new-december":{"label":"What's New in December 2023","children":{}},"/updates/digest/readme":{"label":"Changelog","children":{}},"/updates/digest/digest":{"label":"Digest","children":{}},"/updates/digest/159-2023-whats-new-november":{"label":"What's New in November 2023","children":{}},"/updates/digest/158-2023-whats-new-october":{"label":"What's New in October 2023","children":{}},"/updates/digest/2023-happy":{"label":"Happy 2023","children":{}},"/updates/digest/2022-dwarves-of-the-year":{"label":"Dwarves Of The Year 2022","children":{}},"/updates/digest/2022-in-review":{"label":"2022 In Review","children":{}},"/updates/digest/2022-summit-engineering-a-good-time":{"label":"Summit 2022: Engineering A Good Time","children":{}},"/updates/digest/road-to-100":{"label":"Road To 100","children":{}},"/updates/digest/2022-whats-new-may":{"label":"What's New in May 2022","children":{}},"/updates/digest/2022-whats-new-january":{"label":"What's New in January 2022","children":{}},"/updates/digest/2021-whats-new-december":{"label":"What's New in December 2021","children":{}},"/updates/digest/2021-dwarves-of-the-year":{"label":"Dwarves Of The Year 2021","children":{}},"/updates/digest/2021-whats-new-july":{"label":"What's New in July 2021","children":{}},"/updates/digest/2020-in-review":{"label":"2020 In Review","children":{}},"/updates/digest/2021-in-review":{"label":"2021 In Review","children":{}},"/updates/digest/2019-in-review":{"label":"2019 In Review","children":{}},"/updates/digest/2018-in-review":{"label":"2018 In Review","children":{}}}},"/updates/ogif":{"label":"OGIF","children":{"/updates/ogif/41-20250314":{"label":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","children":{}},"/updates/ogif/39-20250207":{"label":"#39 Frontend report, DB Scaling, AI Workflow","children":{}},"/updates/ogif/38-20250117":{"label":"#38 Erlang automata, AI Trends, Year-End Awards","children":{}},"/updates/ogif/37-20241227":{"label":"#37 AI Fine-tuning, Data archiving, Datalakes","children":{}},"/updates/ogif/28-20241018":{"label":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","children":{}},"/updates/ogif/27-20241011":{"label":"#27 Go weekly, Frontend, AI UX, Finite Automata","children":{}},"/updates/ogif/26-20241004":{"label":"#26 Design insights, Go tools, Trading app, Chatbots, Essays","children":{}},"/updates/ogif/25-20240927":{"label":"#25 Team updates, Hybrid work, AI insights, Go weekly","children":{}},"/updates/ogif/24-20240920":{"label":"#24 Go weekly, AI workflows, Team AI demo, Figma-UI with Claude","children":{}},"/updates/ogif/23-20240913":{"label":"#23 Go weekly, FE report, Hybrid work, AI agents","children":{}},"/updates/ogif/22-20240906":{"label":"#22 Hybrid work, Tech report, Go weekly, AI demo","children":{}},"/updates/ogif/21-20240830":{"label":"#21 Community engagement, Go weekly, Journey of thought for prompt engineering","children":{}},"/updates/ogif/20-20240823":{"label":"#20 Go weekly, Dynamic objects, Devbox, LLM tracing, Cursor AI","children":{}},"/updates/ogif/19-20240821":{"label":"#19 Go weekly, UI design, File sharing, Dify AI","children":{}},"/updates/ogif/18-20240809":{"label":"#18 Go weekly, RAG, UI, FE updates","children":{}},"/updates/ogif/17-20240802":{"label":"#17 Community Call July, C4 Model, Interview Life in the US","children":{}},"/updates/ogif/16-20240726":{"label":"#16 Go weekly, Dune query, AI voice clone, RAG re-ranking","children":{}},"/updates/ogif/15-20240719":{"label":"#15 AI Supervisors, Local-first Software, Code Completion, Bot Commands","children":{}},"/updates/ogif/14-20240712":{"label":"#14 Generic Collections, Pricing Models, and OGIF Summarizer","children":{}},"/updates/ogif/13-20240705":{"label":"#13 Go Weekly updates, Radix Sort, Human Feedback Mechanism, and effective ChatGPT usage","children":{}},"/updates/ogif/12-20240628":{"label":"#12 June updates, Go Performance, eBPF, PGO, Multimodal RAG","children":{}},"/updates/ogif/11-20240621":{"label":"#11 Design patterns: template method \u0026 visitor, Radix sort, and weekly tech commentary","children":{}},"/updates/ogif/10-20240614":{"label":"#10 Behavioral Patterns and Map Content Organization","children":{}},"/updates/ogif/9-20240607":{"label":"#9 What's next for June and Behavior Design Patterns","children":{}},"/updates/ogif/7-20240517":{"label":"#7 Echelon EXPO, Programming patterns, and Moonlighting","children":{}},"/updates/ogif/6-20240510":{"label":"#6 Factory Pattern, Erlang State Machines, and Trading Process","children":{}},"/updates/ogif/5-20240503":{"label":"#5 Singapore Market Report, C4 Modelling, Memo's Nested Sidebar","children":{}},"/updates/ogif/4-20240426":{"label":"#4 DCA, Devbox","children":{}},"/updates/ogif/3-20240419":{"label":"#3 Generative AI, Tokenomics, and Finance Talks","children":{}},"/updates/ogif/2-20240412":{"label":"#2 Devbox as the new Docker, Security Standards, and Understanding Liquidity","children":{}},"/updates/ogif/1-20240405":{"label":"#1 Markdown Presentations, Research Pipeline, Screenshots How-to","children":{}},"/updates/ogif/readme":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/updates/changelog":{"label":"Changelog","children":{"/updates/changelog/2024-10-25-knowledge-base":{"label":"Build your knowledge base","children":{}},"/updates/changelog/2024-09-13-dwarve-updates-ai-llm":{"label":"The Stage of AI and LLM at Dwarves","children":{}},"/updates/changelog/readme":{"label":"Dwarves Updates","children":{}},"/updates/changelog/2023-09-12-growth-stages":{"label":"The Stage of Growth at Dwarves","children":{}},"/updates/changelog/2022-08-26-the-next-leading-chairs":{"label":"The Next Leading Chairs","children":{}},"/updates/changelog/2022-06-26-blockchain-and-data":{"label":"The future is blockchain and data","children":{}},"/updates/changelog/2022-03-31-hiring-stages":{"label":"The stages of hiring at Dwarves","children":{}},"/updates/changelog/2021-12-30-2021-in-review":{"label":"It's a wrap: 2021 in Review","children":{}},"/updates/changelog/2021-12-01-engineering-org-structure":{"label":"Engineering Organizational Structure","children":{}},"/updates/changelog/2021-10-31-path-to-growth":{"label":"The Path To Growth at Dwarves","children":{}},"/updates/changelog/2021-09-29-engineer-performance-review":{"label":"Engineer Performance Review","children":{}},"/updates/changelog/2021-08-23-project-compliance":{"label":"Project Compliance","children":{}},"/updates/changelog/2021-07-11-dalat-office":{"label":"Da Lat Office","children":{}},"/updates/changelog/2021-06-10-dwarves-updates":{"label":"Dwarves Updates","children":{}}}},"/updates/wala":{"label":"WALA","children":{"/updates/wala/001-43-factory":{"label":"43 Factory","children":{}},"/updates/wala/002-dzs-media":{"label":"DZS Media","children":{}},"/updates/wala/003-sp-group":{"label":"SP Group","children":{}},"/updates/wala/readme":{"label":"WALA","children":{}}}}}}}},"/tags":{"label":"Popular Tags","children":{"/tags/earn":{"label":"#earn","children":{},"count":8},"/tags/productivity":{"label":"#productivity","children":{},"count":8},"/tags/quality":{"label":"#quality","children":{},"count":1},"/tags/open-source":{"label":"#open-source","children":{},"count":4},"/tags/liquidity":{"label":"#liquidity","children":{},"count":2},"/tags/ai":{"label":"#AI","children":{},"count":58},"/tags/hiring":{"label":"#hiring","children":{},"count":23},"/tags/case-study":{"label":"#case-study","children":{},"count":29},"/tags/handbook":{"label":"#handbook","children":{},"count":46},"/tags/business":{"label":"#business","children":{},"count":8},"/tags/growth":{"label":"#growth","children":{},"count":2},"/tags/software-development":{"label":"#software-development","children":{},"count":1},"/tags/database-management":{"label":"#database-management","children":{},"count":1},"/tags/icy":{"label":"#icy","children":{},"count":14},"/tags/career":{"label":"#career","children":{},"count":39},"/tags/fullstack":{"label":"#fullstack","children":{},"count":2},"/tags/ux-ui":{"label":"#ux-ui","children":{},"count":13},"/tags/product-design":{"label":"#product-design","children":{},"count":7},"/tags/report":{"label":"#report","children":{},"count":8},"/tags/checklist":{"label":"#checklist","children":{},"count":17},"/tags/presentation":{"label":"#presentation","children":{},"count":1},"/tags/database":{"label":"#database","children":{},"count":8},"/tags/sql":{"label":"#sql","children":{},"count":4},"/tags/data-modeling":{"label":"#data-modeling","children":{},"count":1},"/tags/data-engineering":{"label":"#data-engineering","children":{},"count":5},"/tags/system-design":{"label":"#system-design","children":{},"count":2},"/tags/architecture":{"label":"#architecture","children":{},"count":4},"/tags/etl":{"label":"#etl","children":{},"count":3},"/tags/automata":{"label":"#automata","children":{},"count":1},"/tags/fintech":{"label":"#fintech","children":{},"count":16},"/tags/mobile":{"label":"#mobile","children":{},"count":3},"/tags/go":{"label":"#go","children":{},"count":5},"/tags/error":{"label":"#error","children":{},"count":1},"/tags/community":{"label":"#community","children":{},"count":42},"/tags/startup":{"label":"#startup","children":{},"count":9},"/tags/shares":{"label":"#shares","children":{},"count":1},"/tags/founder":{"label":"#founder","children":{},"count":1},"/tags/entertainment":{"label":"#entertainment","children":{},"count":1},"/tags/culture":{"label":"#culture","children":{},"count":10},"/tags/test":{"label":"#test","children":{},"count":1},"/tags/life-at-dwarves, ai-developer, hybrid-work":{"label":"#life-at-dwarves, ai-developer, hybrid-work","children":{},"count":1},"/tags/hybrid-working":{"label":"#hybrid-working","children":{},"count":2},"/tags/guide":{"label":"#guide","children":{},"count":11},"/tags/security":{"label":"#security","children":{},"count":10},"/tags/reward":{"label":"#reward","children":{},"count":3},"/tags/team":{"label":"#team","children":{},"count":38},"/tags/design":{"label":"#design","children":{},"count":31},"/tags/ux":{"label":"#UX","children":{},"count":2},"/tags/directory-structure":{"label":"#directory-structure","children":{},"count":2},"/tags/file-management":{"label":"#file-management","children":{},"count":2},"/tags/file-system":{"label":"#file-system","children":{},"count":2},"/tags/permissions":{"label":"#permissions","children":{},"count":1},"/tags/database-modelling":{"label":"#database-modelling","children":{},"count":1},"/tags/nda":{"label":"#NDA","children":{},"count":1},"/tags/compliance":{"label":"#compliance","children":{},"count":2},"/tags/people":{"label":"#people","children":{},"count":27},"/tags/operations":{"label":"#operations","children":{},"count":73},"/tags/llm":{"label":"#LLM","children":{},"count":76},"/tags/rag":{"label":"#RAG","children":{},"count":5},"/tags/search":{"label":"#search","children":{},"count":1},"/tags/evaluation":{"label":"#evaluation","children":{},"count":3},"/tags/delivery":{"label":"#delivery","children":{},"count":3},"/tags/reporting":{"label":"#reporting","children":{},"count":1},"/tags/project":{"label":"#project","children":{},"count":16},"/tags/billbyhours":{"label":"#billbyhours","children":{},"count":1},"/tags/engineering":{"label":"#engineering","children":{},"count":56},"/tags/subscription":{"label":"#subscription","children":{},"count":1},"/tags/pricing":{"label":"#pricing","children":{},"count":1},"/tags/product":{"label":"#product","children":{},"count":1},"/tags/blockchain":{"label":"#blockchain","children":{},"count":50},"/tags/evm":{"label":"#evm","children":{},"count":5},"/tags/foundry":{"label":"#foundry","children":{},"count":2},"/tags/search-engine":{"label":"#search-engine","children":{},"count":1},"/tags/duckdb":{"label":"#duckdb","children":{},"count":3},"/tags/transformers.js":{"label":"#transformers.js","children":{},"count":1},"/tags/hybrid-search":{"label":"#hybrid-search","children":{},"count":1},"/tags/erlang":{"label":"#erlang","children":{},"count":1},"/tags/elixir":{"label":"#elixir","children":{},"count":5},"/tags/fsm":{"label":"#fsm","children":{},"count":1},"/tags/design-pattern":{"label":"#design-pattern","children":{},"count":9},"/tags/gang-of-four":{"label":"#gang-of-four","children":{},"count":9},"/tags/observer-pattern":{"label":"#observer-pattern","children":{},"count":1},"/tags/behavior-pattern":{"label":"#behavior-pattern","children":{},"count":2},"/tags/visitor-design-pattern":{"label":"#visitor-design-pattern","children":{},"count":1},"/tags/strategy-design-pattern":{"label":"#strategy-design-pattern","children":{},"count":1},"/tags/market-report":{"label":"#market-report","children":{},"count":32},"/tags/ogif":{"label":"#OGIF","children":{},"count":32},"/tags/guidelines":{"label":"#guidelines","children":{},"count":3},"/tags/feedback":{"label":"#feedback","children":{},"count":2},"/tags/mechanism":{"label":"#mechanism","children":{},"count":1},"/tags/local-first":{"label":"#local-first","children":{},"count":1},"/tags/crdt":{"label":"#crdt","children":{},"count":2},"/tags/data-synchronization":{"label":"#data-synchronization","children":{},"count":1},"/tags/data-ownership":{"label":"#data-ownership","children":{},"count":1},"/tags/real-time-collaboration":{"label":"#real-time-collaboration","children":{},"count":1},"/tags/rust":{"label":"#rust","children":{},"count":10},"/tags/trait":{"label":"#trait","children":{},"count":1},"/tags/error-handling":{"label":"#error-handling","children":{},"count":1},"/tags/data-structure":{"label":"#data-structure","children":{},"count":1},"/tags/bloom-filter":{"label":"#bloom-filter","children":{},"count":1},"/tags/big-o":{"label":"#big-o","children":{},"count":1},"/tags/behavioral-pattern":{"label":"#behavioral-pattern","children":{},"count":1},"/tags/golang":{"label":"#golang","children":{},"count":45},"/tags/behavior-patterns":{"label":"#behavior-patterns","children":{},"count":2},"/tags/algorithms":{"label":"#algorithms","children":{},"count":1},"/tags/sorting":{"label":"#sorting","children":{},"count":1},"/tags/network":{"label":"#network","children":{},"count":2},"/tags/machine-learning":{"label":"#machine-learning","children":{},"count":2},"/tags/zettelkasten":{"label":"#zettelkasten","children":{},"count":1},"/tags/prompt":{"label":"#prompt","children":{},"count":1},"/tags/chatgpt":{"label":"#chatgpt","children":{},"count":1},"/tags/solana":{"label":"#solana","children":{},"count":7},"/tags/amm":{"label":"#amm","children":{},"count":1},"/tags/memo":{"label":"#memo","children":{},"count":14},"/tags/instructions":{"label":"#instructions","children":{},"count":10},"/tags/guideline":{"label":"#guideline","children":{},"count":15},"/tags/ops":{"label":"#ops","children":{},"count":2},"/tags/nft":{"label":"#nft","children":{},"count":3},"/tags/workflow":{"label":"#workflow","children":{},"count":5},"/tags/recording":{"label":"#recording","children":{},"count":1},"/tags/history":{"label":"#history","children":{},"count":1},"/tags/creational-design-pattern":{"label":"#creational-design-pattern","children":{},"count":1},"/tags/moc":{"label":"#moc","children":{},"count":3},"/tags/software-design":{"label":"#software-design","children":{},"count":2},"/tags/software-architecture":{"label":"#software-architecture","children":{},"count":3},"/tags/graphical-notation":{"label":"#graphical-notation","children":{},"count":2},"/tags/techecosystem":{"label":"#techecosystem","children":{},"count":1},"/tags/summit":{"label":"#summit","children":{},"count":4},"/tags/crypto":{"label":"#crypto","children":{},"count":1},"/tags/content":{"label":"#content","children":{},"count":6},"/tags/investment":{"label":"#investment","children":{},"count":1},"/tags/personal-finance":{"label":"#personal-finance","children":{},"count":1},"/tags/dfg":{"label":"#dfg","children":{},"count":6},"/tags/tutorial":{"label":"#tutorial","children":{},"count":7},"/tags/standardization":{"label":"#standardization","children":{},"count":1},"/tags/work-adoption":{"label":"#work-adoption","children":{},"count":1},"/tags/code of conduct":{"label":"#code of conduct","children":{},"count":1},"/tags/research":{"label":"#research","children":{},"count":3},"/tags/field-notes":{"label":"#field-notes","children":{},"count":1},"/tags/innovation":{"label":"#innovation","children":{},"count":2},"/tags/radar":{"label":"#radar","children":{},"count":9},"/tags/bounty":{"label":"#bounty","children":{},"count":4},"/tags/communications":{"label":"#communications","children":{},"count":3},"/tags/token":{"label":"#token","children":{},"count":2},"/tags/brain":{"label":"#brain","children":{},"count":1},"/tags/knowledge-base":{"label":"#knowledge-base","children":{},"count":1},"/tags/engineering/data":{"label":"#engineering/data","children":{},"count":5},"/tags/data-pipeline":{"label":"#data-pipeline","children":{},"count":1},"/tags/vector-database":{"label":"#vector-database","children":{},"count":4},"/tags/payment":{"label":"#payment","children":{},"count":2},"/tags/consulting":{"label":"#consulting","children":{},"count":24},"/tags/partners":{"label":"#partners","children":{},"count":1},"/tags/life-at-dwarves, community-contributor, techie-project":{"label":"#life-at-dwarves, community-contributor, techie-project","children":{},"count":1},"/tags/brainery":{"label":"#brainery","children":{},"count":2},"/tags/devops":{"label":"#devops","children":{},"count":6},"/tags/google-cloud":{"label":"#google-cloud","children":{},"count":1},"/tags/google-data-studio":{"label":"#google-data-studio","children":{},"count":1},"/tags/google-data-fusion":{"label":"#google-data-fusion","children":{},"count":1},"/tags/reliability":{"label":"#reliability","children":{},"count":2},"/tags/cdap":{"label":"#cdap","children":{},"count":1},"/tags/data":{"label":"#data","children":{},"count":14},"/tags/google-dataproc":{"label":"#google-dataproc","children":{},"count":1},"/tags/hadoop":{"label":"#hadoop","children":{},"count":2},"/tags/streaming":{"label":"#streaming","children":{},"count":1},"/tags/life-at-dwarves, alumni, career-growth":{"label":"#life-at-dwarves, alumni, career-growth","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, continuous-learning":{"label":"#life-at-dwarves, backend-engineer, continuous-learning","children":{},"count":1},"/tags/ecommerce":{"label":"#ecommerce","children":{},"count":2},"/tags/dropshipping":{"label":"#dropshipping","children":{},"count":1},"/tags/dwarves":{"label":"#dwarves","children":{},"count":22},"/tags/work":{"label":"#work","children":{},"count":18},"/tags/internal":{"label":"#internal","children":{},"count":11},"/tags/discussion":{"label":"#discussion","children":{},"count":6},"/tags/event":{"label":"#event","children":{},"count":7},"/tags/labs":{"label":"#labs","children":{},"count":24},"/tags/catchup":{"label":"#catchup","children":{},"count":5},"/tags/home":{"label":"#home","children":{},"count":2},"/tags/tauri":{"label":"#tauri","children":{},"count":1},"/tags/htmx":{"label":"#htmx","children":{},"count":2},"/tags/frontend":{"label":"#frontend","children":{},"count":68},"/tags/life-at-dwarves, backend-engineer, community-building":{"label":"#life-at-dwarves, backend-engineer, community-building","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, personal-development":{"label":"#life-at-dwarves, backend-engineer, personal-development","children":{},"count":1},"/tags/backend":{"label":"#backend","children":{},"count":5},"/tags/estimation":{"label":"#estimation","children":{},"count":1},"/tags/code-generation":{"label":"#code-generation","children":{},"count":1},"/tags/typesafe":{"label":"#typesafe","children":{},"count":1},"/tags/internship":{"label":"#internship","children":{},"count":3},"/tags/life-at-dwarves, backend-engineer, teamwork":{"label":"#life-at-dwarves, backend-engineer, teamwork","children":{},"count":1},"/tags/discord":{"label":"#discord","children":{},"count":39},"/tags/workshop":{"label":"#workshop","children":{},"count":1},"/tags/demo":{"label":"#demo","children":{},"count":1},"/tags/protocol":{"label":"#protocol","children":{},"count":2},"/tags/performance-review":{"label":"#performance-review","children":{},"count":2},"/tags/assessment":{"label":"#assessment","children":{},"count":1},"/tags/knowledge":{"label":"#knowledge","children":{},"count":2},"/tags/tech-radar":{"label":"#tech-radar","children":{},"count":1},"/tags/evaluating-tech":{"label":"#evaluating-tech","children":{},"count":1},"/tags/process":{"label":"#process","children":{},"count":9},"/tags/updates":{"label":"#updates","children":{},"count":36},"/tags/life-at-dwarves, product-executive, personal-growth":{"label":"#life-at-dwarves, product-executive, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-member":{"label":"#life-at-dwarves, frontend-engineer, community-member","children":{},"count":1},"/tags/distributed-system":{"label":"#distributed-system","children":{},"count":1},"/tags/data-types":{"label":"#data-types","children":{},"count":1},"/tags/data-structures":{"label":"#data-structures","children":{},"count":2},"/tags/life-at-dwarves, communication-specialist, remote-work":{"label":"#life-at-dwarves, communication-specialist, remote-work","children":{},"count":1},"/tags/life-at-dwarves, qa-engineer, mentorship":{"label":"#life-at-dwarves, qa-engineer, mentorship","children":{},"count":1},"/tags/client":{"label":"#client","children":{},"count":6},"/tags/life-at-dwarves, qa-engineer, quality-standards":{"label":"#life-at-dwarves, qa-engineer, quality-standards","children":{},"count":1},"/tags/guidline":{"label":"#guidline","children":{},"count":1},"/tags/performance":{"label":"#performance","children":{},"count":30},"/tags/playbook":{"label":"#playbook","children":{},"count":4},"/tags/software":{"label":"#software","children":{},"count":8},"/tags/framework":{"label":"#framework","children":{},"count":6},"/tags/learning":{"label":"#learning","children":{},"count":4},"/tags/system design":{"label":"#system design","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, learning-culture":{"label":"#life-at-dwarves, backend-engineer, learning-culture","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, mentorship":{"label":"#life-at-dwarves, backend-engineer, mentorship","children":{},"count":3},"/tags/life-at-dwarves":{"label":"#life-at-dwarves","children":{},"count":2},"/tags/backend-engineer":{"label":"#backend-engineer","children":{},"count":1},"/tags/remote-work":{"label":"#remote-work","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-building":{"label":"#life-at-dwarves, frontend-engineer, community-building","children":{},"count":1},"/tags/enterprise":{"label":"#enterprise","children":{},"count":10},"/tags/australia":{"label":"#australia","children":{},"count":1},"/tags/sargable-queries":{"label":"#sargable-queries","children":{},"count":1},"/tags/zookeeper":{"label":"#zookeeper","children":{},"count":1},"/tags/kafka":{"label":"#kafka","children":{},"count":1},"/tags/sequential-reads":{"label":"#sequential-reads","children":{},"count":1},"/tags/sequential-writes":{"label":"#sequential-writes","children":{},"count":1},"/tags/random-reads":{"label":"#random-reads","children":{},"count":1},"/tags/random-writes":{"label":"#random-writes","children":{},"count":1},"/tags/url-redirect":{"label":"#url-redirect","children":{},"count":1},"/tags/url-rewrite":{"label":"#url-rewrite","children":{},"count":1},"/tags/http":{"label":"#http","children":{},"count":1},"/tags/seo":{"label":"#SEO","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-learning":{"label":"#life-at-dwarves, frontend-engineer, community-learning","children":{},"count":1},"/tags/life-at-dwarves, engineer, work-culture":{"label":"#life-at-dwarves, engineer, work-culture","children":{},"count":1},"/tags/dx":{"label":"#dx","children":{},"count":1},"/tags/life-at-dwarves, software-engineer, mentorship":{"label":"#life-at-dwarves, software-engineer, mentorship","children":{},"count":1},"/tags/machine learning":{"label":"#machine learning","children":{},"count":1},"/tags/r\u0026d":{"label":"#r\u0026d","children":{},"count":1},"/tags/web":{"label":"#web","children":{},"count":9},"/tags/micro-frontend":{"label":"#micro-frontend","children":{},"count":3},"/tags/tool":{"label":"#tool","children":{},"count":3},"/tags/technique":{"label":"#technique","children":{},"count":9},"/tags/vietnam":{"label":"#vietnam","children":{},"count":1},"/tags/write-heavy":{"label":"#write-heavy","children":{},"count":1},"/tags/inventory-platform":{"label":"#inventory-platform","children":{},"count":1},"/tags/scalability":{"label":"#scalability","children":{},"count":1},"/tags/doordash":{"label":"#doordash","children":{},"count":1},"/tags/low-latency":{"label":"#low-latency","children":{},"count":1},"/tags/observability":{"label":"#observability","children":{},"count":5},"/tags/engineer":{"label":"#engineer","children":{},"count":2},"/tags/teamwork":{"label":"#teamwork","children":{},"count":2},"/tags/leadership":{"label":"#leadership","children":{},"count":4},"/tags/multi-column-index":{"label":"#multi-column-index","children":{},"count":1},"/tags/index":{"label":"#index","children":{},"count":1},"/tags/composite-index":{"label":"#composite-index","children":{},"count":1},"/tags/react":{"label":"#react","children":{},"count":15},"/tags/hooks":{"label":"#hooks","children":{},"count":2},"/tags/components":{"label":"#components","children":{},"count":1},"/tags/scrum":{"label":"#scrum","children":{},"count":2},"/tags/technicaldebt":{"label":"#technicaldebt","children":{},"count":1},"/tags/projectmanagement":{"label":"#projectmanagement","children":{},"count":1},"/tags/email":{"label":"#email","children":{},"count":22},"/tags/decoder":{"label":"#decoder","children":{},"count":1},"/tags/json":{"label":"#json","children":{},"count":1},"/tags/materialized-view":{"label":"#materialized-view","children":{},"count":1},"/tags/data-warehouse":{"label":"#data-warehouse","children":{},"count":1},"/tags/mapreduce":{"label":"#mapreduce","children":{},"count":1},"/tags/distributed":{"label":"#distributed","children":{},"count":3},"/tags/form":{"label":"#form","children":{},"count":1},"/tags/uilibraries":{"label":"#uilibraries","children":{},"count":1},"/tags/migrations":{"label":"#migrations","children":{},"count":1},"/tags/agile":{"label":"#agile","children":{},"count":6},"/tags/behavior-driven-development":{"label":"#behavior-driven-development","children":{},"count":1},"/tags/testing":{"label":"#testing","children":{},"count":4},"/tags/ubiquitous-language":{"label":"#ubiquitous-language","children":{},"count":1},"/tags/forward-proxy":{"label":"#forward-proxy","children":{},"count":1},"/tags/apprenticeship":{"label":"#apprenticeship","children":{},"count":3},"/tags/life-at-dwarves, apprenticeship, backend-engineer":{"label":"#life-at-dwarves, apprenticeship, backend-engineer","children":{},"count":1},"/tags/remote":{"label":"#remote","children":{},"count":12},"/tags/showcase":{"label":"#showcase","children":{},"count":1},"/tags/practice":{"label":"#practice","children":{},"count":7},"/tags/life-at-dwarves, backend-engineer, golang":{"label":"#life-at-dwarves, backend-engineer, golang","children":{},"count":1},"/tags/life-at-dwarves, operations, techie-story":{"label":"#life-at-dwarves, operations, techie-story","children":{},"count":1},"/tags/life-at-dwarves, devops-engineer, personal-growth":{"label":"#life-at-dwarves, devops-engineer, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, senior-engineer, mentorship":{"label":"#life-at-dwarves, senior-engineer, mentorship","children":{},"count":1},"/tags/swap":{"label":"#swap","children":{},"count":2},"/tags/quant":{"label":"#quant","children":{},"count":1},"/tags/radio":{"label":"#radio","children":{},"count":3},"/tags/writing":{"label":"#writing","children":{},"count":1},"/tags/english":{"label":"#english","children":{},"count":1},"/tags/life-at-dwarves, data-engineer, remote-work":{"label":"#life-at-dwarves, data-engineer, remote-work","children":{},"count":1},"/tags/apprentice":{"label":"#apprentice","children":{},"count":1},"/tags/life-at-dwarves, ui-designer, design-communication":{"label":"#life-at-dwarves, ui-designer, design-communication","children":{},"count":1},"/tags/life-at-dwarves, engineering-manager, mentorship":{"label":"#life-at-dwarves, engineering-manager, mentorship","children":{},"count":1},"/tags/meeting":{"label":"#meeting","children":{},"count":4},"/tags/us":{"label":"#us","children":{},"count":4},"/tags/funding":{"label":"#funding","children":{},"count":2},"/tags/ventures":{"label":"#ventures","children":{},"count":3},"/tags/mbti":{"label":"#mbti","children":{},"count":6},"/tags/intj":{"label":"#intj","children":{},"count":1},"/tags/istp":{"label":"#istp","children":{},"count":1},"/tags/estj":{"label":"#estj","children":{},"count":1},"/tags/istj":{"label":"#istj","children":{},"count":1},"/tags/personalities":{"label":"#personalities","children":{},"count":1},"/tags/management":{"label":"#management","children":{},"count":4},"/tags/fnb":{"label":"#fnb","children":{},"count":2},"/tags/early-stage":{"label":"#early-stage","children":{},"count":3},"/tags/design-thinking":{"label":"#design-thinking","children":{},"count":2},"/tags/healthcare":{"label":"#healthcare","children":{},"count":1},"/tags/browser-extension":{"label":"#browser-extension","children":{},"count":2},"/tags/git":{"label":"#git","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, growth-mindset":{"label":"#life-at-dwarves, software-engineer, growth-mindset","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, career-change":{"label":"#life-at-dwarves, backend-engineer, career-change","children":{},"count":1},"/tags/marketplace":{"label":"#marketplace","children":{},"count":2},"/tags/tips":{"label":"#tips","children":{},"count":10},"/tags/real-estate":{"label":"#real-estate","children":{},"count":1},"/tags/nocode":{"label":"#nocode","children":{},"count":1},"/tags/hospitality":{"label":"#hospitality","children":{},"count":1},"/tags/ride-hailing":{"label":"#ride-hailing","children":{},"count":1},"/tags/iot":{"label":"#iot","children":{},"count":1},"/tags/macos":{"label":"#macos","children":{},"count":3},"/tags/swift":{"label":"#swift","children":{},"count":7},"/tags/partnership":{"label":"#partnership","children":{},"count":1},"/tags/pm":{"label":"#pm","children":{},"count":4},"/tags/travel":{"label":"#travel","children":{},"count":1},"/tags/operation":{"label":"#operation","children":{},"count":7},"/tags/idea":{"label":"#idea","children":{},"count":1},"/tags/purpose":{"label":"#purpose","children":{},"count":2},"/tags/wasm":{"label":"#wasm","children":{},"count":2},"/tags/transparency":{"label":"#transparency","children":{},"count":1},"/tags/event-sourcing":{"label":"#event-sourcing","children":{},"count":1},"/tags/sdlc":{"label":"#sdlc","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, design-engineering":{"label":"#life-at-dwarves, frontend-engineer, design-engineering","children":{},"count":1},"/tags/modeling":{"label":"#modeling","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, engineering-values":{"label":"#life-at-dwarves, software-engineer, engineering-values","children":{},"count":1},"/tags/goal":{"label":"#goal","children":{},"count":2},"/tags/license":{"label":"#license","children":{},"count":1},"/tags/template":{"label":"#template","children":{},"count":20},"/tags/k8s":{"label":"#k8s","children":{},"count":1},"/tags/js":{"label":"#js","children":{},"count":2},"/tags/clojure":{"label":"#clojure","children":{},"count":1},"/tags/react.js":{"label":"#react.js","children":{},"count":2},"/tags/employee":{"label":"#employee","children":{},"count":2},"/tags/onboarding":{"label":"#onboarding","children":{},"count":1},"/tags/assets":{"label":"#assets","children":{},"count":1},"/tags/company":{"label":"#company","children":{},"count":1},"/tags/tooling":{"label":"#tooling","children":{},"count":9},"/tags/human-resource":{"label":"#human-resource","children":{},"count":1},"/tags/dcos":{"label":"#dcos","children":{},"count":5},"/tags/docker":{"label":"#docker","children":{},"count":11},"/tags/okr":{"label":"#okr","children":{},"count":1},"/tags/oss":{"label":"#oss","children":{},"count":1},"/tags/rfc":{"label":"#RFC","children":{},"count":2},"/tags/newsletter":{"label":"#newsletter","children":{},"count":45},"/tags/web3":{"label":"#web3","children":{},"count":4},"/tags/monitoring":{"label":"#monitoring","children":{},"count":2},"/tags/upptime":{"label":"#upptime","children":{},"count":1},"/tags/mcp":{"label":"#MCP","children":{},"count":3},"/tags/tech-report":{"label":"#tech-report","children":{},"count":15},"/tags/overleaf":{"label":"#overleaf","children":{},"count":1},"/tags/slide":{"label":"#slide","children":{},"count":1},"/tags/office-hours":{"label":"#office-hours","children":{},"count":31},"/tags/btc":{"label":"#btc","children":{},"count":1},"/tags/forward-engineering":{"label":"#forward-engineering","children":{},"count":14},"/tags/weekly-digest":{"label":"#weekly-digest","children":{},"count":15},"/tags/wrap-up":{"label":"#wrap-up","children":{},"count":7},"/tags/real-time":{"label":"#real-time","children":{},"count":1},"/tags/phoenix-live-view":{"label":"#phoenix-live-view","children":{},"count":1},"/tags/timescaledb":{"label":"#timescaledb","children":{},"count":1},"/tags/go-weekly":{"label":"#go-weekly","children":{},"count":24},"/tags/finance":{"label":"#finance","children":{},"count":1},"/tags/agents":{"label":"#agents","children":{},"count":4},"/tags/defi":{"label":"#DeFi","children":{},"count":2},"/tags/aider":{"label":"#aider","children":{},"count":2},"/tags/qwen2.5":{"label":"#qwen2.5","children":{},"count":1},"/tags/openhand":{"label":"#openhand","children":{},"count":1},"/tags/predicted output":{"label":"#predicted output","children":{},"count":1},"/tags/project-management":{"label":"#project-management","children":{},"count":1},"/tags/copilots":{"label":"#copilots","children":{},"count":2},"/tags/team-management":{"label":"#team-management","children":{},"count":1},"/tags/mongodb":{"label":"#mongodb","children":{},"count":1},"/tags/salesforce":{"label":"#salesforce","children":{},"count":1},"/tags/use cases":{"label":"#use cases","children":{},"count":2},"/tags/design-system":{"label":"#design-system","children":{},"count":1},"/tags/storybook":{"label":"#storybook","children":{},"count":1},"/tags/hook":{"label":"#hook","children":{},"count":1},"/tags/cline":{"label":"#cline","children":{},"count":1},"/tags/realtime api":{"label":"#realtime api","children":{},"count":1},"/tags/interface":{"label":"#interface","children":{},"count":1},"/tags/import":{"label":"#import","children":{},"count":1},"/tags/package":{"label":"#package","children":{},"count":1},"/tags/yelp":{"label":"#yelp","children":{},"count":1},"/tags/wala":{"label":"#WALA","children":{},"count":4},"/tags/film":{"label":"#film","children":{},"count":1},"/tags/generics":{"label":"#generics","children":{},"count":2},"/tags/log":{"label":"#log","children":{},"count":1},"/tags/pillar":{"label":"#pillar","children":{},"count":3},"/tags/metric":{"label":"#metric","children":{},"count":1},"/tags/tracing":{"label":"#tracing","children":{},"count":1},"/tags/intent-classification":{"label":"#intent-classification","children":{},"count":1},"/tags/prompting":{"label":"#prompting","children":{},"count":1},"/tags/language":{"label":"#language","children":{},"count":5},"/tags/ai-agents":{"label":"#ai-agents","children":{},"count":2},"/tags/ai-evaluation":{"label":"#ai-evaluation","children":{},"count":1},"/tags/prompt-engineering":{"label":"#prompt-engineering","children":{},"count":4},"/tags/ai-integration":{"label":"#ai-integration","children":{},"count":1},"/tags/networking":{"label":"#networking","children":{},"count":7},"/tags/finite-automata":{"label":"#finite-automata","children":{},"count":1},"/tags/pattern-matching":{"label":"#pattern-matching","children":{},"count":1},"/tags/state-machines":{"label":"#state-machines","children":{},"count":1},"/tags/java":{"label":"#java","children":{},"count":1},"/tags/programming":{"label":"#programming","children":{},"count":1},"/tags/caching":{"label":"#caching","children":{},"count":1},"/tags/devbox":{"label":"#devbox","children":{},"count":17},"/tags/nix":{"label":"#nix","children":{},"count":9},"/tags/generative-ui":{"label":"#generative-ui","children":{},"count":1},"/tags/function-calling":{"label":"#function-calling","children":{},"count":1},"/tags/ton":{"label":"#ton","children":{},"count":2},"/tags/ai-powered":{"label":"#ai-powered","children":{},"count":1},"/tags/pattern":{"label":"#pattern","children":{},"count":1},"/tags/supervisor-architecture":{"label":"#supervisor-architecture","children":{},"count":1},"/tags/document-processing":{"label":"#document-processing","children":{},"count":1},"/tags/information-retrieval":{"label":"#information-retrieval","children":{},"count":1},"/tags/iterators":{"label":"#iterators","children":{},"count":1},"/tags/reinforcement-learning":{"label":"#reinforcement-learning","children":{},"count":3},"/tags/kernel-programing":{"label":"#kernel-programing","children":{},"count":1},"/tags/anchor":{"label":"#anchor","children":{},"count":2},"/tags/containerization":{"label":"#containerization","children":{},"count":4},"/tags/virtualization":{"label":"#virtualization","children":{},"count":4},"/tags/meet-up":{"label":"#meet-up","children":{},"count":4},"/tags/meetup":{"label":"#meetup","children":{},"count":2},"/tags/energy":{"label":"#energy","children":{},"count":1},"/tags/motivation":{"label":"#motivation","children":{},"count":1},"/tags/cybersecurity":{"label":"#cybersecurity","children":{},"count":2},"/tags/serverless":{"label":"#serverless","children":{},"count":1},"/tags/doty":{"label":"#doty","children":{},"count":5},"/tags/websocket":{"label":"#websocket","children":{},"count":1},"/tags/protocols":{"label":"#protocols","children":{},"count":1},"/tags/rendering":{"label":"#rendering","children":{},"count":1},"/tags/dom":{"label":"#dom","children":{},"count":3},"/tags/cssom":{"label":"#cssom","children":{},"count":1},"/tags/render-tree":{"label":"#render-tree","children":{},"count":1},"/tags/iframe":{"label":"#iframe","children":{},"count":1},"/tags/postmessage":{"label":"#postmessage","children":{},"count":1},"/tags/mock-service-worker":{"label":"#mock-service-worker","children":{},"count":1},"/tags/api-mocking":{"label":"#api-mocking","children":{},"count":1},"/tags/web-development-tool":{"label":"#web-development-tool","children":{},"count":1},"/tags/data-fetching":{"label":"#data-fetching","children":{},"count":1},"/tags/frontend,":{"label":"#frontend,","children":{},"count":1},"/tags/graphql":{"label":"#graphql","children":{},"count":1},"/tags/reactjs":{"label":"#reactjs","children":{},"count":2},"/tags/scroll-driven-animations":{"label":"#scroll-driven-animations","children":{},"count":1},"/tags/animations":{"label":"#animations","children":{},"count":1},"/tags/intersection-observer":{"label":"#intersection-observer","children":{},"count":1},"/tags/nextjs":{"label":"#nextjs","children":{},"count":1},"/tags/server-component":{"label":"#server-component","children":{},"count":1},"/tags/caching-data":{"label":"#caching-data","children":{},"count":1},"/tags/social-networks":{"label":"#social-networks","children":{},"count":1},"/tags/foundation-model":{"label":"#foundation-model","children":{},"count":1},"/tags/fine-tuning":{"label":"#fine-tuning","children":{},"count":1},"/tags/vector database":{"label":"#vector database","children":{},"count":1},"/tags/shadow-dom":{"label":"#shadow-dom","children":{},"count":1},"/tags/web-api":{"label":"#web-api","children":{},"count":1},"/tags/swr-infinite":{"label":"#swr-infinite","children":{},"count":1},"/tags/web-design":{"label":"#web-design","children":{},"count":1},"/tags/tuning-llm":{"label":"#tuning-llm","children":{},"count":2},"/tags/langchain":{"label":"#langchain","children":{},"count":1},"/tags/translation":{"label":"#translation","children":{},"count":1},"/tags/profiling":{"label":"#profiling","children":{},"count":1},"/tags/state-mangement":{"label":"#state-mangement","children":{},"count":1},"/tags/global-state-management":{"label":"#global-state-management","children":{},"count":1},"/tags/css":{"label":"#css","children":{},"count":5},"/tags/fonts":{"label":"#fonts","children":{},"count":1},"/tags/variable-fonts":{"label":"#variable-fonts","children":{},"count":1},"/tags/state-management":{"label":"#state-management","children":{},"count":2},"/tags/component":{"label":"#component","children":{},"count":1},"/tags/proof-of-knowledge":{"label":"#proof-of-knowledge","children":{},"count":1},"/tags/fronten":{"label":"#fronten","children":{},"count":1},"/tags/typescript":{"label":"#typescript","children":{},"count":4},"/tags/analytics-tools":{"label":"#analytics-tools","children":{},"count":1},"/tags/analytics-platform":{"label":"#analytics-platform","children":{},"count":1},"/tags/software engineer":{"label":"#software engineer","children":{},"count":1},"/tags/parsing":{"label":"#parsing","children":{},"count":1},"/tags/validation":{"label":"#validation","children":{},"count":1},"/tags/webassembly":{"label":"#webassembly","children":{},"count":1},"/tags/sandbox":{"label":"#sandbox","children":{},"count":1},"/tags/zk-rollup":{"label":"#zk-rollup","children":{},"count":2},"/tags/polygon":{"label":"#polygon","children":{},"count":1},"/tags/starknet":{"label":"#starknet","children":{},"count":1},"/tags/ethereum":{"label":"#ethereum","children":{},"count":2},"/tags/zero-knowledge":{"label":"#zero-knowledge","children":{},"count":1},"/tags/atomic-css":{"label":"#atomic-css","children":{},"count":1},"/tags/client-side":{"label":"#client-side","children":{},"count":1},"/tags/storage":{"label":"#storage","children":{},"count":1},"/tags/frontend/performance":{"label":"#frontend/performance","children":{},"count":2},"/tags/wai-aria":{"label":"#wai-aria","children":{},"count":1},"/tags/accessibility":{"label":"#accessibility","children":{},"count":4},"/tags/polymorphic-component":{"label":"#polymorphic-component","children":{},"count":1},"/tags/threejs":{"label":"#threejs","children":{},"count":1},"/tags/web-performance":{"label":"#web-performance","children":{},"count":2},"/tags/html":{"label":"#html","children":{},"count":4},"/tags/animation":{"label":"#animation","children":{},"count":1},"/tags/zk-proof":{"label":"#zk-proof","children":{},"count":1},"/tags/guides":{"label":"#guides","children":{},"count":1},"/tags/responsive-design":{"label":"#responsive-design","children":{},"count":1},"/tags/hsl":{"label":"#hsl","children":{},"count":1},"/tags/javascript":{"label":"#javascript","children":{},"count":4},"/tags/css-in-js":{"label":"#css-in-js","children":{},"count":1},"/tags/tip":{"label":"#tip","children":{},"count":1},"/tags/dark-mode":{"label":"#dark-mode","children":{},"count":1},"/tags/multisign-wallet":{"label":"#multisign-wallet","children":{},"count":1},"/tags/virtual-dom":{"label":"#virtual-dom","children":{},"count":1},"/tags/native-modules":{"label":"#native-modules","children":{},"count":1},"/tags/vitejs":{"label":"#vitejs","children":{},"count":1},"/tags/esm":{"label":"#esm","children":{},"count":1},"/tags/modules":{"label":"#modules","children":{},"count":1},"/tags/blockchain-bridge":{"label":"#blockchain-bridge","children":{},"count":1},"/tags/foundational-topics":{"label":"#foundational-topics","children":{},"count":5},"/tags/distributed-systems":{"label":"#distributed-systems","children":{},"count":1},"/tags/pos":{"label":"#pos","children":{},"count":1},"/tags/smart-contract":{"label":"#smart-contract","children":{},"count":1},"/tags/atomic-design":{"label":"#atomic-design","children":{},"count":1},"/tags/a11y":{"label":"#a11y","children":{},"count":1},"/tags/useeffect":{"label":"#useeffect","children":{},"count":1},"/tags/concurrency":{"label":"#concurrency","children":{},"count":2},"/tags/parallelism":{"label":"#parallelism","children":{},"count":1},"/tags/engineering/frontend":{"label":"#engineering/frontend","children":{},"count":1},"/tags/wfh":{"label":"#wfh","children":{},"count":1},"/tags/tech radar":{"label":"#tech radar","children":{},"count":1},"/tags/policy":{"label":"#policy","children":{},"count":1},"/tags/vim":{"label":"#vim","children":{},"count":1}}}},"ogifMemos":[{"content":"\n### Topics and Highlights\n\n- **Swap ICY-BTC:** Huy shared updates on the ICY-BTC swap mechanism, explaining the current state and adjustments needed to ensure accurate ICY valuation during swaps.\n- **GitHub BotL:** Thanh introduced a GitHub bot to automate PR reviews, aiming to improve processing speed and consistency in code management.\n- **Memo UI:** The team presented improvements to the Memo user interface, focusing on better data access and user experience.\n- **Agentic: MCP-DB:** Huy discussed the MCP-DB system, highlighting how it handles data storage and retrieval to support agents in automated workflows.\n- **Pocket Turning, Recapable:** Vincent shared progress on the Pocket Turning and Recapable, outlining the completion of core gameplay and next steps.\n- **Funding Rate Arbitrage:**  Antran presented a strategy for funding rate arbitrage across multiple exchanges, addressing technical challenges and execution strategies.\n\n### Vietnamese Transcript\n\n**[05:30]** Hôm nay chắc mình bắt đầu sớm nha. Buổi hôm nay chắc kết hợp với lại anh trong buổi meeting một xíu. Một phần là sẽ làm showcase, cái thứ hai là anh tổng kết một số việc mà bữa trước có trao đổi với mấy anh em á. Cái số hai, cái số ba là mình sẽ bắt đầu cho mấy anh em đăng ký công việc. Hiện tại để mà dễ trước, chắc là mình sẽ để cho Huy Nguyễn đi show mấy cái phần bên Huy trước, liên quan tới ICY một tí, xong rồi show một số cái về tech mà team mình đang làm nè. Để mình có một cái snapshot về chuyện là team tech thì hiện nay như thế nào nhé. Rồi sắp tới thì team mình cần gì, với lại mấy anh em xem contribute được gì vào đó ha.\n\n**[06:35]** Huy, Thành đâu? Nhường sân khấu này nè. Rồi ok, nội dung đầu tiên, chắc là bên ICY Swap trước đi. Mình announce đó, hồi tuần trước, tuần này deploy lên rồi thì giờ những cái khác biệt như thế nào, chắc nhờ Huy đi lại hết mấy series đó.\n\n**[07:29]** Alo, rồi rồi, đã xem màn hình rồi. Thì bây giờ mọi người có thể vào trang ICY Swap để mà swap được rồi. Đây, mình chỉ số liệu nha. Nhưng mà ở trên đây thì nó đang ready hết tất cả mọi thứ rồi. Việc làm duy nhất bây giờ là đang ngồi soát lại mấy cái số ICY á. Tại vì lúc trước vận hành á, thì mình vận hành theo kiểu là mình neo cái giá ICY, nên mình cũng không quan tâm cái lượng lưu thông (circulated) lắm. Nên có mấy trường hợp là mình để vô mấy cái ví của team, hoặc là chuyển qua mấy cái Mochi Balance của em hoặc là của anh Bảo. Thì mấy cái đó đang cần rà soát lại để mà nó ra cái số lưu thông đúng. Tại vì giờ mình sẽ ngồi, cái giá của mình nó sẽ dynamic theo cái pool nên cần ngồi check lại cái đó thì cũng gần xong hết rồi.\n\n**[09:09]** Giờ còn mỗi cái account của anh Bảo là cần kiểm tra lại thôi. Nhớ có đợt là chuyển cho anh Bảo, giờ đang ngồi xem lại cái phần đó rồi cộng trừ lại rồi cắt cái phần đó ra khỏi cái circulated thì số này nó sẽ ra đúng. Còn lại hiện tại muốn swap ủng hộ thì cũng có thể swap được ở trên trang này. Lịch là đang vậy. Em show thử cái list Holder của mình hiện tại cho mấy anh em xem chắc cần biết nhiều hơn xíu. Trước giờ mọi người tham gia không quan tâm nhiều lắm nhưng mà chắc lần này thì mình cần để ý hơn.\n\n**[09:51]** ICY của mình mình deploy ở trên Base, đúng không? Nên khi anh em vào trong cái list Holder, mọi người sẽ thấy được một cái list khoảng tất cả những cái ví nào đang được giữ ICY của team mình, thì là CCK Holder ha. Là một. Rồi thì cái link để mà vô đây chắc Huy share nha. Chứ mọi người lên mà search thì chắc không biết được đâu.\n\nĐầu tiên là anh em cần nắm cái này. Quay qua đoạn này rồi. Anh nghĩ mấy anh em cần quan tâm phần này nhiều hơn xíu. Nó trở thành cái norm của thế giới tech luôn rồi, không cần làm gì mới nữa. Nên anh em nắm được thì sẽ ok hơn.\n\n**[10:33]** ICY của mình hiện đã được list. Trong danh sách này có các ví minter, ví dùng để lập ngân sách cho các hoạt động, và một số ví đang nắm giữ lượng ICY lớn. Các hoạt động liên quan đến staking ICY sẽ được triển khai dần dần trong thời gian tới. Đây là thông tin đầu tiên anh em cần nắm rõ.\n\n**[11:15]** Huy, demo thử luồng swap đi. Có ai có địa chỉ Bitcoin với một ít ICY không? Vincent có ở đây không? Ok, giờ thử swap từ ICY sang Bitcoin. Giá hiện tại được tính theo cơ chế động dựa trên lượng ICY đang lưu hành và pool. Chức năng swap rất đơn giản, chỉ cần điền số lượng, bấm swap là xong.\n\n**[12:27]** Khoan đã, đừng nhập địa chỉ ảo. Ok, vậy là ổn rồi. Khung đầu tiên là ICY như bình thường. Ở dưới thì đang hiển thị đơn vị là satoshi, tức là đơn vị nhỏ nhất của Bitcoin. Khi nhập số lượng vào, nó sẽ tự động chuyển đổi. Tuy nhiên, tỷ giá hiện tại đang bị lệch một chút, khoảng 1.2 thay vì 1.5. Đây chắc là lỗi tính toán nhỏ, chỉnh lại là được.\n\n**[13:28]** Cần có số ICY tối thiểu để swap. Thử nhập 30 ICY xem sao. Refresh lại thử xem có được không.\n\n**[14:43]** Hình như không đủ tiền trong ví rồi. Bạn có ETH trên Base không? Chuyển qua Base và kiểm tra lại xem.\n\n**[15:51]** Không phải lỗi đó đâu. Vấn đề là account chưa được đăng ký nên không thể thực hiện giao dịch. Sẽ fix phần đó sau. Mục tiêu ở đây là giúp mọi người hiểu rõ hơn về cơ chế swap và cách định giá token. Nếu nắm rõ thì sau này sẽ dễ dàng hơn trong việc quản lý tokenomics.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=0LryX12wLbTu3i1m\u0026amp;start=806\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[16:47]** Huy, giải thích nhanh lại cơ chế tính giá đi. Lần trước Quan demo chưa nói kỹ phần đó. Giá của ICY được xác định theo cơ chế minting, nghĩa là giá sẽ không thay đổi mạnh nếu có ai đó swap số lượng lớn. Nó không hoạt động theo kiểu cơ chế tạo lập thị trường tự động (AMM) mà giá sẽ được kiểm soát theo cơ chế minting. Cơ chế này giúp giá duy trì ổn định ngay cả khi có giao dịch lớn.\n\n**[17:43]** Hoàn toàn là nó phụ thuộc vào Bitcoin. Nên nếu giá Bitcoin tăng thì lượng ICY mà anh em đang cầm sẽ tăng về giá trị USD. Còn về cơ chế minting, nhờ Huy giải thích thêm một chút. Nói chung là cơ chế chung của mình trước giờ là mình sẽ cố định giá trị của ICY theo USDC. Anh em không cần quan tâm nhiều, cứ hiểu đơn giản là một ICY tương đương với 1.5 USD.\n\n**[18:37]**Phần đảm bảo này là để giúp team vận hành có thể đảm bảo là tới ngày thì sẽ đổi USDC vào trong contract để mọi người swap. Tỷ giá swap trong contract cũ là cố định ở mức 1.5 ICY, nhưng đó là model cũ. Model mới của mình thì linh hoạt hơn. Nếu anh em đã dùng Uniswap hay các AMM (Automatic Market Maker) khác thì nó cũng tương tự một chút. Ở đây, cơ chế hoạt động là bên dưới có một pool thanh khoản (liquidity pool), trong đó chứa cả ETH và USDC. Tùy vào tình hình của pool lúc đó, tỷ giá sẽ được điều chỉnh dựa trên lượng ETH và USDC trong pool.\n\n**[19:18]** Cơ chế của mình cũng tương tự như vậy. Giá ICY sẽ được quyết định bởi lượng Bitcoin trong pool và lượng ICY đang được lưu hành. Công thức đơn giản thôi: mình có lượng ICY (X), có lượng BTC (Y) trong pool, thì X/Y sẽ ra được giá trị của một ICY tính theo BTC. Công thức này là công thức toán học cơ bản, không có gì phức tạp.\n\n**[19:55]** Do cơ chế hoạt động của mình, sẽ có hai thời điểm làm thay đổi thanh khoản:\n\n1. **Thời điểm đầu tiên** là vào mỗi tháng, team vận hành sẽ đổ thêm BTC vào pool để làm chi phí cho các hoạt động của team. Lúc này giá ICY sẽ tăng lên một chút vì lượng BTC trong pool tăng lên.\n2. **Thời điểm thứ hai** là khi team đẩy thêm ICY vào pool (minting thêm). Khi mint thêm ICY, giá ICY trên thị trường sẽ giảm xuống do lượng ICY trong pool tăng lên.\n\n**[20:35]** Hai trường hợp trên sẽ ảnh hưởng trực tiếp đến giá ICY. Còn nếu giá Bitcoin thay đổi thì giá trị USD của ICY có thể thay đổi, nhưng giá ICY tính theo BTC thì không thay đổi. Market impact từ Bitcoin là yếu tố bên ngoài, không ảnh hưởng trực tiếp đến việc minting hoặc giá trị ICY trong pool.\n\n**[21:12]** Anh em có câu hỏi gì thêm thì đặt câu hỏi, tí nữa sẽ trả lời sau. À, có câu hỏi về việc swap ngược từ BTC về ICY đúng không? Hiện tại thì chưa có chức năng đó. Hiện tại chỉ hỗ trợ swap từ ICY sang BTC thôi, không có chức năng swap ngược lại. Tức là mua vào thì được, nhưng bán ra thì chưa hỗ trợ.\n\n**[21:40]** Cảm ơn Huy. Có gì cần lưu ý thêm không? Cần lưu ý là hiện tại vẫn đang trong giai đoạn thử nghiệm nên có thể có một số trường hợp ngoại lệ. Ví dụ như một số tình huống có thể phát sinh khi swap hoặc thanh khoản chưa đủ. Về cơ bản thì luồng hiện tại vẫn đang hoạt động ổn định.\n\n**[22:00]** Như là số lượng ICY tối thiểu để swap. Vì bản chất là team mình đang cover cái phần phí mà để mà làm gas trên ETH, trên Base và cả trên BTC luôn thì nên đang kiểu đang giới hạn cái số ICY nó swap nhiều tí để mà hạn chế với cái việc mà mọi người swap tầm 1-2 ICY để test á thì nó tốn cái chi phí gas nên đang để tầm trên 20 ICY mới cho mọi người swap trên web.\n\nCái thứ hai là ở cái do cái việc mà mình mint thêm ICY thì nó sẽ làm thay đổi giá thị trường, thì nên em đang disable luôn cái phần mà cơ chế cái ứng lương trước của mình.\n\n**[22:37]** Tức là đồng loạt ứng lương thì nó sẽ ảnh hưởng giá đúng không? Vậy cái lesson learn trong cái này đó là sau đợt này làm thì có vài điểm mà anh đang thấy là bắt đầu team mình đang tập trung vô build những cái tool nó hỗ trợ mình hoạt động. Cũng là một số cái thử nghiệm mới, cũng là một số cái mà hỗ trợ hoạt động thiệt sự. Nhưng mà sau khi xong mấy cái bài này thì nó sẽ ra được một số mấy cái article liên quan thì mấy anh em nếu mà trước đó không có tham gia những cái dự án đó có thể tìm lại những cái bài đó để mà coi được cái game, cái knowledge game từ cái đợt đó là cái gì của mấy anh em làm dự án đó ha.\n\n**[23:24]** Rồi thì trong cái vụ ICY Swap đợt này chắc là được hai ba bài phải không? Dạ, như được ba bài. Còn kiểu viết nhiều thêm thì vẫn có nhiều cái để viết. Ừ, thôi đó cứ thong thả từ từ đi.\n\n**[24:02]** Sau phần của Huy, anh cảm ơn Huy rồi chuyển sang nội dung thứ hai liên quan đến những gì team mình đang làm. Anh Bảo ai nói trước cũng được, nhưng chắc là để Thành nói trước. Thành bảo là em nói trước cũng được, em sẽ gom lại hết để anh cho mọi người biết team đang ở giai đoạn nào. Nhưng anh bảo là để Thành nói trước đi, tại vì đang có người bấm chuông. Rồi anh mời Thành bắt đầu.\n\n**[25:00]** Mọi người, Memo của mình là một trong những cái đợt lớn đợt này, có upgrade format lại cho nhìn nó ok hơn tí. Mình luôn muốn mình tạo những cái map content, những cái thứ mà mình đọc được cái mình up lên đây. Nhưng mà hiện tại cái mô hình đó thật ra nó cũng không có còn quá hiệu quả với chuyện là mấy cái model ra đời nó nén dữ liệu lại, rồi mình query trực tiếp từ đó ra thì nó sẽ hiệu quả hơn.\n\nThì cái point của chuyện là đưa những cái kiến thức mà nó bình thường lên trên Memo thì nó cũng không phù hợp lắm ha. Nên đợt này lúc mà làm lại thì có một cái ý chính để mà muốn nói với anh em đó là Memo hiện tại sẽ được dùng chỉ cho mục đích duy nhất thôi ,  đó là cái knowledge gain mà từ dự án.\n\nCái đó là gần như là những cái mới mà nó xuất phát từ chính cái hoạt động của cái team mình. Gần như trên đây sau này nó sẽ gồm là liên quan tới lĩnh vực gì đó, mình đã làm gì đó trong đó. Nó có nhiều hơn, maybe là sau một giai đoạn thì khi tụi nó train lại cái model thì những cái dữ liệu của mình á thì nó sẽ trở thành một phần của kiến thức chung cho cả cộng đồng.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=MhsFuFFQ5NFKTlYS\u0026amp;start=1556\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[25:39]** Và cái phần này anh nghĩ là nó sẽ giúp ích rất nhiều cho cái chuyện mà mọi người làm kiểu training lại cho AI model sau này, hoặc là mấy cái chuyện mà mình muốn nó có cái việc mà suggestion kiểu tự động ấy.\n\n**[26:24]** Nội dung sẽ trở thành một phần trong mô hình đó hoặc nếu có mấy công cụ tìm kiếm trên internet, thì có thể bài của mình chỉ là một phần nhỏ trong nguồn tài liệu được tham khảo vào thôi, giống như là một phần nhỏ trong citation. Điều này cũng không có vấn đề gì lớn. Nhưng nhìn chung, toàn bộ những nội dung này sẽ gần như trở thành spirit của team.\n\nTrong lần nâng cấp lớn này, có một điểm chính mà Tuấn đã hoàn thành chưa nhỉ? Tuấn ơi, phần liên quan đến việc đồng bộ toàn bộ dữ liệu của team, nhất là về phần nội dung, hiện đang được định hướng như vậy để các thành viên nắm rõ hơn.\n\n**[27:00]** Tức là sau đợt này, các thành viên đang tham gia vào các dự án sẽ có xu hướng ngồi lại với nhau để xem xét kỹ hơn từ những dự án đó, và xác định rõ phần **knowledge gain** (kiến thức thu được) từ chính các dự án đó là gì. Sau đó, team sẽ đưa lên Memo làm nguồn tài liệu nội bộ cho team.\n\nPhần thứ hai là ở cuối mỗi bài sẽ có một phần liên quan đến **group of reading**. Hiện tại phần này vẫn chưa hoàn chỉnh, nhưng ý tưởng là sau khi hoàn thiện, sẽ có thêm phần thông tin tổng hợp về bài viết để người đọc có thể tra cứu và học thêm từ bài viết đó.\n\n**[27:47]** Ngoài ra, tất cả dữ liệu của team được viết ra sẽ được gán định danh ví dụ như **GitHub**, **Discord**, hoặc những kênh nội bộ khác. Dữ liệu này sẽ được upload lên dạng **blockchain storage** trên nền tảng **Arweave (AV)** – một nền tảng lưu trữ phi tập trung. Điều này giúp cho nội dung của team có một định danh rõ ràng và minh bạch.\n\nThêm vào đó, người đọc sẽ có thể xem lại bài viết, đánh giá hoặc để lại phản hồi trực tiếp trên bài viết. Đây là một phần của ý tưởng nâng cấp mới cho trang **Memo** của team.\n\n**[28:39]** Trước đây, team đã có ý định sử dụng Obsidian để quản lý nội dung, nhưng có vẻ như một số thành viên gặp khó khăn trong việc làm quen với công cụ đó. Vì vậy, hiện tại để làm cho mọi thứ đơn giản hơn, team sẽ chuyển sang cơ chế trực tiếp hơn. Cụ thể là thay vì phải làm qua Obsidian, các thành viên có thể submit nội dung trực tiếp vào repository của thư viện chung của team.\n\nCác thành viên chỉ cần đưa nội dung vào và submit trực tiếp qua nền tảng này, không cần phải tuân theo workflow bắt buộc của Obsidian nữa. Nếu ai vẫn muốn dùng Obsidian thì không sao, nhưng nếu không dùng thì cũng không ảnh hưởng gì cả. Đây là thay đổi cơ bản nhất trong hệ thống Memo của team.\n\n**[29:24]** Hiện tại team đang làm một số dự án chính, bao gồm:\n\n1. Bitcoin Swap – đã nhắc tới ở phần trước.\n2. Memo – vừa mới trình bày xong.\n3. Hai dự án nhỏ khác:\n\n- **agentic** – nhóm của Quang và Huy đang phát triển.\n- **github bot** – nhóm của Thành đang thực hiện, hiện đang test thử.\n\nGiờ chắc nhường lại cho Thành để chia sẻ thêm về những nội dung này.\n\n**[30:32]** Dự án này đã được khởi động hơn một tuần và đã chính thức chạy code được hơn một tuần. Mục đích chính của nó là tạo ra một hệ thống nhắc nhở (reminder). Trước đây, team thường gặp tình huống khi tạo pull request (PR), mọi người hay để đó và chờ chạy xong rồi quên luôn việc cần review. Tool này sẽ phục vụ cho việc theo dõi và cập nhật thông tin về các hoạt động hàng ngày trên github hoặc hàng tuần trên các kênh giao tiếp nội bộ của team.\n\n**[31:18]** Hệ thống này được thiết kế dưới dạng một tích hợp đơn giản. Luồng hoạt động cơ bản bao gồm một số use case như: thông báo cho người được assign để review, tương tác với GitHub API, và post thông tin vào các kênh nội bộ như Discord hoặc Slack. Hiện tại, team đang test thử trên Discord. Ngoài ra, team cũng đang thử nghiệm với agentic và một framework mới gọi là **Mastra AI**.\n\nFramework này khác với các tool Python thông thường. Một số thành viên trong team không quen làm việc với Python, nên team muốn thử nghiệm xem liệu sử dụng framework mới này có hiệu quả hơn các giải pháp hiện tại hay không. Framework này hỗ trợ các tính năng như setup môi trường, define các trạng thái để quản lý dữ liệu, và cho phép cấu hình lại tùy theo nhu cầu của team.\n\n**[32:19]** Cấu trúc của hệ thống này có hai phần chính:\n\n1. **Agentic App** – Đây là ứng dụng chính để xử lý các hoạt động của hệ thống.\n2. **Discord App** – Hỗ trợ việc gửi thông báo vào Discord.\n\nNgoài ra, hệ thống còn có một vài component phụ, như workflow để xử lý công việc theo lịch trình, kiểm tra và thông báo cho developer nếu có bất kỳ pull request nào đang chờ được review. Nếu pull request vượt quá một khoảng thời gian nhất định, hệ thống sẽ gửi thông báo để nhắc người thực hiện review.\n\n**[33:12]** Agentic App sẽ expose một vài API cho phép chat và theo dõi trạng thái của các pull request. Khi có một pull request được tạo ra, hệ thống sẽ tự động xác định các điều kiện như trạng thái của pull request (work in progress hay chưa), thời gian tạo pull request, và sẽ gửi thông báo cho người review sau khoảng 30 phút kể từ lúc tạo. Ví dụ: nếu có một pull request cần được review nhưng không có ai assigned hoặc đã quá thời gian xử lý, hệ thống sẽ tự động ping lại người phụ trách.\n\n**[35:02]** Thay vì phải theo dõi thủ công, hệ thống sẽ gắn con agent vào để tự động theo dõi và thông báo thông qua endpoint của hệ thống. Trong phần logic, hệ thống sẽ định nghĩa các điều kiện cụ thể, chẳng hạn như chỉ gửi thông báo nếu pull request được tạo trong vòng 30 phút hoặc đang trong trạng thái work in progress. Nếu pull request được cập nhật hoặc chuyển trạng thái, hệ thống sẽ tự động theo dõi và gửi thông báo cho developer để đảm bảo không bị sót.\n\n**[35:39]** Hệ thống sẽ hoạt động dựa trên code filter thông thường. Ngoài ra, nó sẽ có một số workflow khác như việc gửi thông báo vào cuối ngày để tổng hợp tình trạng của các pull request trên Discord. Hệ thống sẽ tự động gửi thông báo về số lượng pull request đang mở, tình trạng của chúng và trạng thái review hiện tại. Đây là chức năng chính của tool này ,  đóng vai trò như một công cụ reminder.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=Zduog0abeAWXIIM4\u0026amp;start=2107\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[36:24]** Hệ thống cũng có thể tích hợp với các công cụ chat khác. Đơn giản là có thể tạo thêm một command và gửi request tới endpoint của hệ thống. Các request này sẽ được định nghĩa dựa trên schema cụ thể, ví dụ như input là **review ID** hoặc các thông tin khác liên quan đến trạng thái của pull request. Hệ thống sẽ lấy dữ liệu này và hiển thị trên giao diện mà người dùng thường xuyên sử dụng.\n\n**[37:04]** Phần xử lý backend của hệ thống được thực hiện thông qua tool Lippia, một công cụ định dạng dữ liệu JSON thành dạng bảng Markdown table hoặc dạng data binding. Hiện tại team đang test thử hai luồng xử lý này trước khi mở rộng thêm các tính năng khác. Khi hệ thống hoạt động ổn định, các workflow này sẽ được mở cho tất cả các thành viên trong team thử nghiệm và phát triển thêm.\n\n**[38:08]** Hệ thống được thiết kế để mở rộng một cách linh hoạt. Các thành viên trong team có thể tự phát triển và đóng góp các workflow khác nhau. Hệ thống này cho phép xây dựng các tool dưới dạng một đơn vị độc lập (**packaging unit**), sau đó kết hợp các đơn vị này lại để tạo ra các workflow phức tạp hơn. Khi muốn phát hành một workflow mới, các thành viên chỉ cần định nghĩa lại đơn vị cơ bản và tích hợp nó vào hệ thống.\n\nViệc mở rộng các workflow sẽ giúp hệ thống phát triển theo chiều ngang (mở rộng số lượng tính năng), thay vì theo chiều dọc (phát triển tính năng hiện tại). Khi số lượng các workflow tăng lên, hệ thống sẽ càng trở nên linh hoạt và mạnh mẽ hơn.\n\n**[38:54]** Về cơ bản, workflow được coi là lớp ứng dụng (application layer) tương tự như các API data trước đây. Hệ thống này sẽ hoạt động ở cấp độ tool, nhưng người dùng cuối sẽ tương tác với nó qua giao diện của workflow. Hiện tại, vẫn chưa có đơn vị nào triển khai thành công mô hình này ở quy mô lớn. Tuy nhiên, GitHub hiện đã mở rộng API cho các developer tạo các extension và tích hợp chúng trực tiếp vào GitHub.\n\n**[39:40]** Dify đang xây dựng một nền tảng để hỗ trợ các developer phát triển và triển khai các tool và workflow này một cách dễ dàng hơn. Mục tiêu là tạo ra một marketplace để các tool và workflow có thể được phân phối và sử dụng bởi nhiều người dùng khác nhau. Hệ thống này tương tự như một nền tảng mở, cho phép các developer bên thứ ba triển khai các tool và workflow của riêng họ.\n\nTrên nền tảng của Dify đã có khoảng 50 tool khác nhau. Một số tool đã từng được phát hành dưới dạng thử nghiệm, nhưng do chưa có định hướng rõ ràng và thiếu sự hỗ trợ từ cộng đồng, nên chúng chưa đạt được thành công như mong đợi.\n\n**[40:17]** Một số nền tảng trước đây đã thử xây dựng mô hình tương tự nhưng chưa đạt được thành công. Lý do là vì các tool này chỉ được xây dựng dưới dạng form, thiếu khả năng tương tác với dữ liệu bên ngoài và chưa có khả năng kết hợp các workflow phức tạp. Tuy nhiên, Dify đang tập trung vào việc giải quyết các vấn đề này để tạo ra một hệ sinh thái hoàn chỉnh cho các workflow và tool.\n\n**[40:59]** Các công cụ này cũng cho phép người dùng đẩy dữ liệu từ các nguồn bên ngoài vào hệ thống. Người dùng có thể gửi dữ liệu từ các ứng dụng bên ngoài qua các Open Form hoặc API. Dify sẽ tự động xử lý và định dạng dữ liệu để sử dụng trong các workflow của hệ thống.\n\n**[41:56]** Team đang tập trung vào hai hướng phát triển chính:\n\n1. Tiếp tục mở rộng và phát triển các workflow hiện có.\n2. Cải tiến và tối ưu hóa các công cụ hiện tại để hỗ trợ việc triển khai và sử dụng dễ dàng hơn.\n\nHệ thống được xây dựng dựa trên các tiêu chuẩn chung về thiết kế tool và workflow. Công cụ Smithery hiện tại đang đóng vai trò như một Agent để quản lý các workflow. Smithery cũng có thể được sử dụng như một Package Manager để cài đặt và quản lý các tool trong hệ thống.\n\n**[42:53]** Workflow sẽ hoạt động theo cơ chế, nếu một workflow nào đó trở nên phổ biến, mọi người có thể lấy nó về và sử dụng dưới dạng tool. Bản chất của các công cụ này là được thiết kế để phục vụ các domain cụ thể. Ví dụ như một công cụ để tạo file, tìm kiếm hoặc lấy file code chẳng hạn. Nó hoạt động giống như một SDK, tức là một bộ thư viện mà bạn chỉ cần import vào để sử dụng.\n\n**[43:37]** Khi đã tích hợp vào SDK, bạn có thể sử dụng các method sẵn có để thao tác với dữ liệu. Điều này cho phép tích hợp dễ dàng vào các công cụ AI. Hiện tại, chỉ có Cross là hỗ trợ trực tiếp cho các thao tác này. Tuy nhiên, trong tương lai, nó sẽ được chuẩn hóa để các công cụ khác cũng có thể dễ dàng tích hợp. Trường hợp của Manus là một ví dụ. Manus sử dụng rất nhiều tool khác nhau, tuy nhiên khi so sánh với hệ thống agent trong Smithery, về cơ bản chúng là hai lớp hoàn toàn khác nhau.\n\n**[44:15]** Trong hệ thống của Manus, các công cụ được kết hợp lại để tạo ra các workflow tổng quát hơn. Các công cụ này hoạt động ở các lớp khác nhau, trong khi các agent trong Smithery được thiết kế để hoạt động độc lập. Câu hỏi đặt ra là làm thế nào để phân biệt rõ ràng sự khác nhau giữa hệ thống của Manus và hệ thống agent trong Smithery. Có một bài tóm tắt về điều này đã được đăng trong kênh AI Club ,  nội dung chính nói về khả năng suy nghĩ (thinking) và khả năng sử dụng máy tính (computer use).\n\n**[45:09]** Cơ chế của hệ thống Manus là một hệ thống service-oriented. Để kết hợp nhiều tool với nhau trong cùng một workflow, cần phải định nghĩa rõ các bước thực hiện. Ví dụ như bước 1 cần sử dụng tool nào, bước 2 cần sử dụng tool nào, v.v. Điều này đòi hỏi các bước phải được cấu hình cụ thể. Tuy nhiên, hệ thống mới có khả năng suy luận để tự động xác định xem cần sử dụng những công cụ nào để hoàn thành tác vụ. Đây chính là điểm khác biệt giữa hệ thống mới và các hệ thống cũ.\n\n**[45:59]** Cụ thể, hệ thống mới có thể nhận biết được một tác vụ cần sử dụng bao nhiêu công cụ, thực hiện qua các bước nào, và có thể điều chỉnh thứ tự thực hiện một cách thông minh. Đây là một cơ chế đặc biệt và khác biệt so với các hệ thống cũ. Nói cách khác, nó hoạt động như một Supervisor ,  có khả năng suy luận và đưa ra quyết định về thứ tự và phương pháp thực hiện các bước trong workflow.\n\n**[46:35]** Hệ thống Supervisor hoạt động ở lớp cao hơn so với các agent trong  Smithery. Các agent trong  Smithery chỉ đơn giản là các công cụ thực thi một tác vụ cụ thể, trong khi Supervisor có khả năng quản lý và điều phối toàn bộ quá trình thực hiện tác vụ. Việc tích hợp Supervisor cho phép hệ thống hoạt động một cách linh hoạt hơn, đồng thời dễ dàng mở rộng và bổ sung thêm các công cụ mới.\n\n**[47:33]** Mục tiêu của team là hiểu rõ cách hoạt động của hệ thống và nắm được cơ chế điều hành của các workflow. Nếu có thể xác định được cách thức triển khai và quản lý các workflow, thì sẽ có thể chọn lọc và sử dụng các công cụ hiệu quả hơn. Đây là điều mà team đang hướng tới ,  xây dựng một hệ thống có khả năng mở rộng và tối ưu hóa quy trình làm việc.\n\n**[48:24]** Tiếp theo, team sẽ tập trung vào việc xây dựng hệ thống **MCP**. Đây là một hệ thống mới được thiết kế để quản lý dữ liệu và workflow. Team đã tiến hành demo hệ thống này cách đây khoảng hai tuần. Bản chất của hệ thống MCP là xây dựng một agent hoạt động trên nền tảng có sẵn. Người dùng có thể nhanh chóng triển khai và kiểm tra hệ thống thông qua MCP.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=KGQZ4rVPmrc9nMq9\u0026amp;start=2935\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[49:10]** MCP sẽ là một hệ thống hoàn chỉnh, bao gồm một cơ sở dữ liệu (**database**) và một máy chủ (**server**). Điều này cho phép hệ thống hoạt động một cách độc lập và có khả năng xử lý dữ liệu lớn. Khác với các hệ thống cũ, MCP sẽ cho phép người dùng điều chỉnh cấu hình và quản lý dữ liệu dễ dàng hơn.\n\n**[49:58]** Bản chất của MCP là một agent, được định nghĩa theo một cấu trúc input và output cụ thể. Điều này cho phép các hệ thống khác nhau có thể kết nối và tương tác với MCP thông qua các giao thức tiêu chuẩn. Nói cách khác, MCP có thể được tích hợp vào bất kỳ hệ thống nào thông qua các giao thức được định nghĩa sẵn.\n\n**[50:35]** MCP cũng cho phép người dùng quản lý dữ liệu thông qua Knowledge Database, bản chất nó là timescale database, dump hết mọi data về hoạt động của team vào trong đó. Đây là một cơ sở dữ liệu dạng time-series, cho phép ghi nhận các sự kiện theo thời gian thực, ai làm backend sẽ quen dạng event sourcing, event log. Ví dụ: ghi nhận thông tin về các thành viên của team, trạng thái hoạt động của hệ thống, hoặc các sự kiện quan trọng khác.\n\n**[51:13]** Knowledge Database sẽ lưu trữ toàn bộ dữ liệu hoạt động của team, bao gồm các thông tin như ai đã thực hiện tác vụ gì, trạng thái của hệ thống vào từng thời điểm cụ thể, và các thông tin khác liên quan đến hoạt động nội bộ của team. Điều này cho phép team theo dõi và phân tích hiệu suất làm việc, từ đó đưa ra các quyết định điều chỉnh hợp lý.\n\n**[51:51]** Concept của hệ thống sẽ có một thành phần gọi là Landing Zone. Landing Zone có nghĩa là mọi dữ liệu mà mình đang có ,  khoảng mười mấy đến hàng chục bộ dữ liệu (database) ,  sẽ được tập kết vào đây. Trước đây, khoảng ba đến năm năm trước, nếu muốn xây dựng một hệ thống lưu trữ dữ liệu mình sẽ tạo một con bot để thu thập mọi hoạt động của team và đưa vào trong cơ sở dữ liệu của mình.\n\nVới mô hình Meta mới, tất cả các dữ liệu lớn (Big Data) sẽ được dump vào một kho lưu trữ tạm thời dưới dạng file .dat trên S3 hoặc GCS (Google Cloud Storage). Con MCP này sẽ có khả năng đọc trực tiếp từ Landing Zone. Nếu hệ thống thấy rằng dữ liệu trong Landing Zone có giá trị và cần thiết, nó có thể tự động chuyển đổi dữ liệu đó sang dạng Time Series Database (TSDB) để sử dụng lâu dài. Đây chính là end game (kết quả cuối cùng) của hệ thống này.\n\nCòn lại, vấn đề sẽ là xây dựng các Use Case (trường hợp sử dụng) dựa trên các dữ liệu đã được tổ chức trong hệ thống ,  theo hướng mà team mong muốn. Đây là định hướng phát triển quan trọng của hệ thống MCP trong thời gian tới.\n\n**[52:25]** Vậy là hiện tại team sẽ có một hệ thống cơ sở dữ liệu cũ ,  đó là cơ sở dữ liệu dạng table kiểu cũ, nằm ở phần bên dưới của hệ thống (có thể thấy trên diagram với các khối màu xanh dương). Giờ đây, team đang bổ sung thêm hai thành phần mới:\n\n- Thành phần **Landing Zone** ,  nằm trong khối màu vàng phía trên của hệ thống.\n- Thành phần **Time Series Database (TSDB)** ,  được kết nối trực tiếp với các thành phần trong hệ thống cũ để phân tích và khai thác dữ liệu.\n\nTeam đang lưu trữ các dữ liệu thô trong Landing Zone. Về bản chất, việc tập kết dữ liệu trong Landing Zone giống như việc gom quân ,  tập trung tất cả dữ liệu về một chỗ, sau đó mới quyết định cách phân tích và xử lý. Đây là cơ chế giúp hệ thống vận hành linh hoạt hơn và dễ dàng mở rộng khi có thêm dữ liệu mới.\n\n**[53:11]** Điểm đặc biệt của hệ thống này là khả năng tự động chuyển đổi dữ liệu từ Landing Zone sang Time Series Database. Cơ chế này xuất phát từ nhu cầu ngày càng tăng về phân tích dữ liệu cục bộ (local analytics). Đây là xu hướng đang nổi lên trong bối cảnh sự phát triển của AI (Trí tuệ nhân tạo).\n\nSự trỗi dậy của AI đã làm gia tăng nhu cầu về các hệ thống phân tích dữ liệu theo thời gian thực. Khi các dữ liệu thô được tập kết vào Landing Zone, hệ thống sẽ tự động nhận diện dữ liệu có giá trị và chuyển chúng sang TSDB để phân tích chi tiết hơn. Đây là một bước tiến quan trọng trong việc xây dựng hệ thống phân tích dữ liệu hiệu quả và có khả năng thích ứng với những thay đổi của thị trường.\n\n**[53:45]** Hiện tại team đã có thể chạy analytic trực tiếp cho phần dữ liệu được lưu trữ trên local. Hệ thống này cho phép chạy analytic ngay trên dữ liệu Data Lake mà không cần phải chuyển dữ liệu đi xa. Đối với phần dữ liệu trong Landing Zone ,  tức là phần file packet mà Huy đang show trên màn hình ,  đây là phần mà team cần tập trung nghiên cứu thêm. Vấn đề này có liên quan đến text processing, nên mấy anh em cần phải pick up (nắm bắt) chủ đề này. Cái này cũng không khó lắm, chắc học trong vòng nửa ngày là có thể nắm được cơ bản.\n\nPhần Prompt để tìm kiếm và khai thác dữ liệu cũng khá nhanh và đơn giản, không phức tạp. Đây là phần rất đáng để thử nghiệm vì nó liên quan đến cơ chế knowledge discovery (khám phá tri thức) trong hệ thống. Đây là một trong những phần nâng cấp mới mà Huy vừa nhắc tới.\n\n**[54:22]**  Điểm nổi bật nhất của hệ thống trong đợt nâng cấp này chính là **Knowledge Hub**. Đây là nơi mà team sẽ tập trung toàn bộ dữ liệu để phục vụ cho việc phân tích và khai thác tri thức. Knowledge Hub sẽ trở thành một dạng **data pool** chung của toàn team. Bất kỳ ai cũng có thể thêm dữ liệu vào đây, và hệ thống sẽ xử lý, chuyển đổi dữ liệu theo format tiêu chuẩn.\n\nĐiều quan trọng là khi hệ thống đã được thiết lập xong, mọi người trong team sẽ có chung một **protocol** để sử dụng. Các module hoặc component khác nhau sẽ có thể **share (chia sẻ)** chung một cấu trúc dữ liệu và truy cập trực tiếp vào Knowledge Hub. Đây sẽ là nền tảng chung để đồng bộ dữ liệu và xử lý dữ liệu trong nội bộ team.\n\n**[54:58]** Về phần cơ sở dữ liệu (DB), hệ thống sẽ có hai lớp:\n\n- **DB cũ:** Dùng để hỗ trợ các nghiệp vụ hiện có và xử lý các dữ liệu có cấu trúc sẵn.\n- **DB mới:** Được thiết kế để kết nối trực tiếp với **Knowledge Hub** và hỗ trợ phân tích dữ liệu theo thời gian thực.\n\nĐiểm đặc biệt là phần **MCP** sẽ đóng vai trò như một **protocol** để các module khác nhau có thể giao tiếp với nhau. Điều này có nghĩa là bất kỳ dữ liệu nào cần được truy cập hoặc xử lý, chỉ cần đưa vào đúng đường dẫn của hệ thống thì nó sẽ tự động được xử lý theo cấu trúc tiêu chuẩn. Đây là cách để hệ thống đồng nhất dữ liệu và tránh xung đột khi có nhiều nguồn dữ liệu cùng được xử lý.\n\n**[55:43]** Từ giờ, team sẽ cần làm quen với các cơ chế xử lý dữ liệu mới. Mọi người nên dành thời gian để tìm hiểu thêm về các thành phần trong hệ thống mới. Khi các thành phần này hoạt động ổn định, các dự án mới của team sẽ tận dụng các công cụ này để triển khai nhanh hơn và hiệu quả hơn. Đây sẽ là bộ công cụ chính để phục vụ cho các dự án trong tương lai.\n\nHệ thống này có tiềm năng trở thành **requirement** bắt buộc trong các dự án tiếp theo. Nếu bạn muốn bắt kịp với hệ thống mới, hãy bắt đầu từ việc tìm hiểu các nguyên lý cơ bản về MCB và các protocol liên quan.\n\n**[56:40]** Trước đây, khi team triển khai hệ thống trên S3 hoặc GCS (Google Cloud Storage), việc xử lý dữ liệu khá mất thời gian. Tuy nhiên, với cơ chế mới, dữ liệu từ Landing Zone sẽ được xử lý nhanh hơn và dễ dàng hơn.\n\nHệ thống đã được thử nghiệm trên nhiều nền tảng khác nhau, bao gồm **S3** và **GCS**. Tuy nhiên, vì hạ tầng hiện tại của team đang chạy trên **GCS**, nên các dữ liệu từ Landing Zone sẽ được xử lý trên GCS trước. Mặc dù vậy, về mặt kỹ thuật, hệ thống này có thể mở rộng sang các nền tảng khác mà không gặp trở ngại lớn.\n\n**[57:45]** Cơ chế hoạt động của Landing Zone khá đơn giản:\n\n- Các dữ liệu từ nhiều nguồn khác nhau sẽ được tập trung vào Landing Zone.\n- Các dữ liệu này sẽ được lưu dưới dạng **file Parquet** theo từng ngày.\n- Hệ thống có khả năng đọc lại các file này thông qua cơ chế **Time Series Database** (TSDB).\n\nHiện tại, một số file **Parquet** mẫu đã được tạo và đang trong quá trình kiểm tra. Nếu cần, team có thể chạy thử demo trên các dữ liệu mẫu này để kiểm tra tính nhất quán của hệ thống.\n\n**[58:24]** Những hoạt động của team giống như kiểu **AI sub** hoặc **Memo** thì nó cũng được đẩy hết lên đây. Nhiệm vụ của **Landing Zone** là lưu trữ mọi dữ liệu mà team muốn, ai muốn lưu trữ gì thì cứ đẩy hết vào đây rồi sau đó hệ thống sẽ quyết định xử lý dữ liệu đó như thế nào. Hệ thống cũng đã cung cấp một số công cụ để mọi người có thể đẩy dữ liệu lên, ví dụ như là các **API proxy** để forward các sự kiện. Mọi người muốn push thông tin lên Landing Zone thì chỉ cần gọi API là được.\n\nMemo hiện tại đang sử dụng cơ chế này để lấy dữ liệu từ các **nền tảng xã hội** và đồng bộ vào hệ thống. Cơ chế này cũng đã được thử nghiệm thành công. Còn đối với những loại dữ liệu có tính đặc thù như là **Discord messages** hoặc **data từ Basecamp**, team cần phải xây dựng các **crawler** hoặc các **connector** để thu thập dữ liệu. Hiện tại, team đã có một số template sẵn cho những loại dữ liệu này.\n\n**[58:59]** Về hướng phát triển tiếp theo, team sẽ tập trung vào việc khai thác dữ liệu từ Landing Zone. Nếu bạn muốn tham gia vào dự án này, lời khuyên là hãy bắt đầu từ một **vertical cụ thể**. Ví dụ:\n\n- Xác định một **use case** rõ ràng.\n- Tìm hiểu xem **dữ liệu nào** cần cho use case đó.\n- Định nghĩa lại cơ chế khai thác dữ liệu theo hướng **từ trên xuống dưới**.\n\nThay vì kiểu thấy dữ liệu nào hay thì lưu lại, team nên nghĩ theo hướng là **xác định use case trước** rồi mới quyết định lưu trữ dữ liệu. Điều này giúp hệ thống hoạt động một cách có tổ chức và dễ dàng quản lý hơn.\n\nVí dụ cụ thể là nếu có một use case về **Project Nghệ Nhân** thì team sẽ cần tạo một **Git Agent** để thu thập dữ liệu từ Git, sau đó đẩy dữ liệu đó vào **Knowledge Hub** thông qua MCP. Từ đó, hệ thống sẽ định nghĩa các công cụ khai thác dữ liệu cho use case này.\n\n**[1:00:16]** Ngoài ra, team đang phát triển một MCP Server nhỏ. MCP Server này thực chất là một server cơ bản, sử dụng các thành phần kỹ thuật thông thường của hệ thống internet hiện tại. Nó định nghĩa các input và output rõ ràng, cho phép kết nối với nhiều loại giao diện khác nhau.\n\nVí dụ:\n\n- Nếu có một MCP để xử lý dữ liệu từ Slack, team sẽ định nghĩa các API cho từng loại dữ liệu.\n- Nếu cần có các công cụ để đọc dữ liệu từ Google Sheets hoặc phân tích dữ liệu về tình trạng check-in trong tuần, team có thể tạo các MCP tool để xử lý những dữ liệu đó.\n\nMCP sẽ là một thành phần trung gian để đồng bộ và xử lý dữ liệu từ nhiều nguồn khác nhau. Mọi người có thể truy cập các công cụ này từ Editor, Command Line, hoặc bất kỳ giao diện nào khác.\n\n**[1:01:07]** Bản chất của MCP là nó sẽ đóng vai trò như một **API Gateway** để kết nối các công cụ. Nếu bạn cần theo dõi việc check-in hàng tuần của mọi người trong team, bạn có thể tạo một MCP để thu thập dữ liệu từ **Knowledge Hub** và Google Sheets, sau đó so sánh dữ liệu để xem ai đã check-in và ai chưa check-in.\n\nHệ thống hiện tại đang dừng ở mức độ triển khai MCP Server cơ bản. Giao diện hiện tại sử dụng **Command Line** để gọi MCP, nhưng về cơ bản team có thể mở rộng để kết nối với các công cụ khác nhau.\n\n**[1:01:43]** Hệ thống đang tập trung vào việc triển khai cơ chế xác thực (authentication) và phân quyền (authorization).\n\n- Authentication – Xác thực người dùng để truy cập vào hệ thống.\n- Authorization – Phân quyền cho các hoạt động xử lý dữ liệu.\n\nHệ thống đang được sử dụng nội bộ trong team, chưa công khai ra bên ngoài. Nếu bạn muốn sử dụng MCP, bạn sẽ cần nhập vào **private key** để xác thực quyền truy cập.\n\n**[1:02:23]** Về mặt kỹ thuật, MCP có thể mở rộng ra các thành phần khác nhau trong hệ thống. Mọi người có thể tích hợp MCP vào các ứng dụng hiện tại hoặc các công cụ hiện có mà không cần phải viết lại quá nhiều code.\n\nTeam vẫn đang thử nghiệm tính năng này và tập trung vào việc hoàn thiện các phần về bảo mật và quản lý quyền truy cập. Khi hệ thống đã ổn định, mọi người có thể tích hợp MCBP vào các quy trình xử lý dữ liệu hiện có.\n\n**[1:03:00]** Chỉ là đang dừng lại ở đây thôi, chưa xử lý được các bài toán phức tạp về authorization. Sau khi hoàn thành các bước hiện tại thì mới đến việc xử lý các bài toán phức tạp hơn liên quan đến authorization và quyền sử dụng hệ thống. Mọi người có thể tập trung vào các vấn đề cơ bản trước đã.\n\nRồi, cảm ơn Huy nhé. Đây là một trong những phần phát triển kỹ thuật quan trọng của team. Nếu theo dõi các hoạt động trên tech và AI Club, mọi người sẽ nhận ra team đang tiến tới các bước tiếp theo trong quá trình phát triển. Về mặt kỹ thuật, mọi người nên chú ý vào các từ khóa quan trọng mà Huy vừa đề cập. Nếu chưa hiểu rõ thì có thể xem lại bản ghi để nắm được đầy đủ thông tin.\n\n**[1:03:45]** Team core vẫn đang tiếp tục phát triển hệ thống. Yêu cầu tất cả các thành viên tham gia vào dự án để có thể **transfer knowledge** hiệu quả hơn. Dự án này là môi trường để mọi người học hỏi và thực hành.\n\nĐây là cơ hội để các thành viên mới trong team tiếp cận và nắm bắt các khía cạnh kỹ thuật quan trọng. Nếu cảm thấy chưa sẵn sàng thì có thể tham khảo các phần hướng dẫn và tài liệu nội bộ để bắt kịp. Việc training sẽ được thực hiện trong quá trình làm việc chứ không có các buổi training riêng. Đây là môi trường thực hành trực tiếp để vừa làm vừa học.\n\n**[1:04:29]** Bên cạnh việc phát triển hệ thống, team cũng đang thực hiện knowledge transfer từ các dự án đã hoàn thành. Dự kiến cuối tháng sẽ có một buổi tổng hợp lại các bài học rút ra từ các dự án này. Nếu ai chưa thực sự hiểu rõ thì có thể tham khảo hoặc hỏi các thành viên đã làm qua để nắm thêm thông tin.\n\nNếu cảm thấy chưa sẵn sàng hoặc cần thêm thông tin thì có thể hỏi trực tiếp các thành viên trong team. Mọi người có thể ping các thành viên có kinh nghiệm hơn để nhận được sự hỗ trợ.\n\n**[1:05:07]** Team có hai nhóm khác nhau đang hoạt động song song:\n\n- **Team của Tuấn** đang phát triển một số game và ứng dụng nhỏ.\n- **Team build** đang làm việc trên các ứng dụng thử nghiệm để kiểm tra tính khả thi của hệ thống.\n\nCác hoạt động này tương tự với các nhóm **Build Club** và **AI Club** trong team Foundation. Một số sản phẩm đã bắt đầu có **output** tốt. Tuấn và team đang phát triển một trò chơi dựa trên **Turing Machine**.\n\n**[1:06:38]** Trò chơi **Turing Machine** mà team Tuấn phát triển được chuyển thể từ phiên bản board game thành phiên bản trên thiết bị di động. Mục tiêu của trò chơi là đoán một chuỗi gồm **ba số**. Để đoán đúng chuỗi số này, người chơi sẽ nhận được các **clue** (gợi ý).\n\nVí dụ:\n\n- Nếu gợi ý nói rằng “một trong ba số phải lớn hơn 1” → Người chơi có thể nhập số vào và hệ thống sẽ xác định xem đáp án có đúng hay không.\n- Nếu hai số sai nhưng một số đúng thì hệ thống sẽ phản hồi ngay để người chơi có thể tiếp tục điều chỉnh.\n\nLuật chơi khá phức tạp nên có thể gây khó khăn cho người chơi mới. Tuấn và team đang tiếp tục điều chỉnh để trò chơi trở nên dễ tiếp cận hơn mà không mất đi tính thử thách.\n\n**[1:07:23]** Tên trò chơi là [**Pocket Turing**](https://pocket-turing.vercel.app/) bởi vì phiên bản board game gốc của nó liên quan đến các thẻ đục lỗ – giống như cơ chế hoạt động của Turing Machine trong lập trình máy tính. Tuy nhiên, mình đã điều chỉnh và phát triển thêm các yếu tố mới để phù hợp hơn với phiên bản di động.\n\nMÌnh có kế hoạch tinh chỉnh và mở rộng trò chơi trong các phiên bản tiếp theo. Ngoài ra, cũng đang kiểm tra xem có thể triển khai thêm các tính năng thu phí hoặc các tùy chọn nâng cao để tăng khả năng monetize.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=bP3ZjI3af1fVijle\u0026amp;start=3997\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:08:16]** Mình đang thử nghiệm phiên bản beta của trò chơi. Trò chơi đã hoàn thiện về mặt gameplay và người chơi có thể trải nghiệm trọn vẹn các tính năng. Bước tiếp theo là thử nghiệm với nhóm người dùng rộng hơn để thu thập phản hồi và cải thiện sản phẩm.\n\n**[1:09:15]** Mục tiêu tiếp theo là đưa trò chơi vào App Store và Google Play để tiếp cận nhiều người dùng hơn. Trước mắt, team muốn đảm bảo trò chơi hoạt động ổn định và không phát sinh lỗi nghiêm trọng.\n\nTuấn kỳ vọng trò chơi sẽ thu hút được ít nhất **100 người dùng** trả phí trong giai đoạn thử nghiệm đầu tiên. Nếu nhận được phản hồi tích cực  sẽ mở rộng thêm các tính năng mới và cải thiện trải nghiệm người chơi. Mong nhận được phản hồi từ các thành viên khác để có thể điều chỉnh và hoàn thiện sản phẩm tốt hơn. Tuấn đã chia sẻ link tải trò chơi cho các thành viên trong team để mọi người có thể trải nghiệm và đóng góp ý kiến.\n\n**[1:10:13]** Nếu anh em hứng thú với việc build sản phẩm thì giai đoạn này là thời điểm phù hợp để bắt đầu. Trước đây team đã thử nghiệm nhiều lần nhưng lần này là cơ hội tốt để làm bài bản hơn. Việc phát triển các sản phẩm nội bộ không chỉ giúp cải thiện năng lực kỹ thuật mà còn mở ra cơ hội thương mại hóa trong tương lai.\n\nNgoài game của Tuấn, team đang phát triển thêm các công cụ khác. Nếu có ý tưởng hay, anh em có thể đóng góp để cùng xây dựng và thử nghiệm. Cách bán hoặc thương mại hóa sản phẩm thì tính sau, quan trọng là hoàn thiện các tính năng cốt lõi trước.\n\n**[1:10:58]** Tiếp theo là phần của An. An từng làm một tool gọi là **Rec** để tổng hợp thông tin theo dạng giống với hệ thống của **Apple**. Phiên bản 1 của Rec yêu cầu người dùng tự sắp xếp thông tin, còn phiên bản 2 hiện tại đã được tích hợp AI để hỗ trợ sắp xếp tự động.\n\nTuy nhiên, AI vẫn có một số hạn chế trong việc nhận diện nội dung đầy đủ. Đôi khi AI không thể xác định được toàn bộ ngữ cảnh nên kết quả trả về chưa thực sự hoàn hảo. Tuy nhiên, các nội dung quan trọng vẫn được sắp xếp và hiển thị đầy đủ.\n\n**[1:11:56]** Tool này đang trong giai đoạn hoàn thiện, nhưng các chức năng cốt lõi đã ổn định. Hiện tại, team đang tập trung vào việc cải thiện phần giao diện và tối ưu trải nghiệm người dùng. An dự kiến sẽ tiếp tục phát triển thêm các tính năng bổ sung để hỗ trợ người dùng tốt hơn.\n\n**[1:12:51]** Các dự án của team hiện đang ở giai đoạn thử nghiệm và cải tiến. Nếu ai có thắc mắc hoặc góp ý, có thể trực tiếp trao đổi với An hoặc các thành viên khác trong team. Hiện tại, các dự án đã showcase gần hết. Các phần chi tiết hơn sẽ được đề cập vào buổi sau.\n\n**[1:13:57]** Bên đội mình, anh luôn nói về chuyện kiến thức liên quan tới liquidity và game in general, thì anh em thật sự muốn team mình đẩy theo hướng đó một chút. Vì nó có lợi cho gần như là cái life skill luôn, đúng không? Nên anh muốn team mình đi theo hướng đấy trong đợt này. Mấy anh em, đặc biệt là những người hứng thú với trading, tức là lấy data về để tìm kiếm cái Alpha trên đó, Intel trên đó, để ra được những cái market-making dựa trên điều kiện nào đó.\n\n**[1:14:43]** Nó là một cái, hoặc có thể đi xa hơn để làm một luồng rất tuyệt vời. Hình như hiện tại chỉ là ước mơ của anh thôi. An đã làm được một version, anh thấy khá ok. Đây là cơ hội để cho anh em biết trong team đang có những tiến triển như vậy. Đang chạy ha, mời An. Nói chung là game kiếm tiền thôi. Coi tụi nó kiếm tiền sao thì mình làm vậy. Mấy cái thường thường thì có biết một cái gì để thử, nó cũng là dạng **Delta neutral**, đúng không? Thì mình cũng research những thứ đó. Rồi đi build và research xong để có kiến thức ship.\n\n**[1:15:30]** Chơi cái cột này hết thôi, không nhìn tới đâu nữa. Mọi người thấy màn hình terminal chưa? Có thấy chưa? Có thấy rồi, ok, chạy để chạy thử. Chắc phải zoom lên, zoom lên một hai level, hơi nhỏ, rồi ok rồi. Đây là arbitrage để ăn funding free, thì có nhiều thể loại arbitrage. Cái này chỉ là một trong những loại đó thôi, ăn trên chênh lệch phantom giữa các sàn. Đang tập trung vào ba sàn: Binance, OKX ,  thằng OKX này sàn của nó không có nhiều dữ liệu lắm ,  nên em có cái diagram cho cái đó không, An?\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=IevTgfLbxwcu6MOh\u0026amp;start=4506\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:16:26]** Nghĩ mọi người sẽ hơi khó hình dung. Nhìn cái này chắc không hiểu nó là gì. Có diagram không? Ok, không có vẽ à? Có cái này, to, nhưng là lý thuyết, không cụ thể ra được high level. Không thấy, chắc phải ngồi vẽ lại sơ sơ. Thấy chưa? Chắc nhìn hình của anh đi, hình của anh, biết ngay là cái này luôn. PRL à? Ủa, nó đang chạy lộn, quên, nó đang vào mấy cái socket của…\n\n**[1:17:47]** Tụi nó để lấy real-time data về. Đang lấy dữ liệu từ bao nhiêu account? Ba cái: Binance, Bybit, với cái gì nữa? Ok rồi, init để lấy giá về, đúng không? Lấy giá, lấy funding, lấy phí chưa? Lấy mấy cái data như phí thì đang code theo calculation, chưa xài để lấy về. OKX chắc không có. Mấy cái trade này thì thường chỉ nằm ở hai cái chính: Binance, Bybit. Ừ, setup ok rồi, nó sẽ có mảng thể hiện cái vị nào đang có chênh lệch funding, thì có profit. Em tính được nếu mình vào thì nó sẽ bao nhiêu. PH là số lần thu funding để hòa vốn phí, monitoring cái cao nhất giữa hai sàn. Bước một là lấy chênh lệch funding giữa hai bên, đúng không? Không, em nói là lấy trên ba exchange, so với góc của nó vẫn là exchange net, đúng không? Ừ, exchange net tính sau thôi.\n\n**[1:19:49]** Funding thì giả sử tụi nó thường, trên lập giá thì không có nhiều, kiểu một thằng dương, hai thằng đều dương, hoặc hai thằng đều âm, thì chênh lệch ít hơn. Thật ra mình đặt counter trên cái chênh khác, chỉ là offset giá di chuyển, để không lỗ bởi giá. Trên exchange net, không có chênh lệch đó, mình làm funding lúc nào cũng bằng 0. Vì không có phí swap, thay vì counter trên sàn khác bằng cái đó, mình counter lúc funding bằng 0. Hiện tại chấp nhận ít lợi hơn, nhưng version đầu tiên vậy.\n\n**[1:20:33]** Lấy giá, lấy **funding**, rồi coi có deploy capital thôi, đúng không? Chạy thử chưa? Chưa đổ tiền vô. Ừ, cái này kiểu game scale, cần nhiều tiền mới ăn, vài trăm ngàn thì vô không thấy gì. Hiểu, ok. Về kỹ thuật thì em làm gì? Từ lúc price crash, em làm những gì?\n\n**[1:21:25]** Đầu tiên em research chart trước, coi nó thế nào, có chênh lệch gì không. Xong rồi ship, code hết bằng Cloud 3.7. Phải lấy data sàn trước qua socket, từ API dock của sàn, quăng lên WebSocket client. Sàn nào cũng có dock, lấy về ship lên, tự view được.\n\n**[1:22:15]** Web cho ba sàn xong, có form đầy đủ. Sau đó build chart, giải thích cho nó, build từ từ. Check data, sai thì tự build sample để đảm bảo data đúng. Vì format giữa các sàn khác nhau.\n\n**[1:23:01]** Khác hết, nên cần test data valid mới compare được. Tiếp theo build con để vào lệnh, khi phát hiện thì có thằng đứng ra vào lệnh, watch bot xem lỗ không, làm từ từ, hợp lý. Quá trình hết bao lâu? Một tuần.\n\n**[1:23:58]** Bước tiếp theo của tool này là gì? Em sẽ check data trước, xem sàn nào dễ kiếm tiền, có lời. Quản lý rủi ro, lấy phí structure của sàn. Vì phí ảnh hưởng lớn, phải tính chính xác, đảm bảo lời mới vào lệnh. Có back system không? Có history để backtest không? Có, nhưng không chính xác.\n\n**[1:25:38]** Đây là showcase kỹ thuật, hướng này team tự lập, ok. Anh em showcase game trading, có bước đẩy tiếp, đang trên đường làm cái muốn làm, rất good. Công nghệ, techno house, xài thế nào thôi. Quan trọng nhất là…\n\n**[1:26:19]** Hoạt động team hiện tại vậy nhé. Productivity gần đây bắt đầu sync. Tom comment trước, productivity team giờ bao nhiêu? 2/10 hay 4/10? So với 6-7/10 cần, anh thấy setup tốt rồi.\n\n**[1:27:01]** Bước tiếp theo về mặt catch up cái công nghệ tool link để hỗ trợ mình vận hành đội theo mô hình này, nó đang được improve từ từ lên. Bên thị trường, thị trường **funding** nói chung và những sản phẩm bắt đầu cũng rục rịch quay trở lại. Người ta thấy công nghệ ổn định hơn. Bên **crypto** thì do macro ảnh hưởng nhiều, nhưng cứ có trend nào về tech là sẽ vô cắn thôi, là vậy ha. Anh đang thấy sắp tới tín hiệu để nó resume lại thì đâu đó khoảng 50/50. Trước đó anh nhìn thì cái market rất tệ, kiểu mọi thứ chưa sẵn sàng. Dù có học nhiều, làm nhiều thì cũng không ra kết quả liền.\n\n**[1:27:40]** Nhưng đợt này anh nghĩ mấy anh em sẽ phải có sự yêu cầu về chuyện tham gia mấy cái này ha. Tuần sau chắc nhờ Huy, Tom với Thành thống kê lại, xem ngoại trừ dự án anh em đang làm thì hoạt động tham gia những dự án side project như vậy, anh em nào đang làm gì ha. Đó là thần sau nội dung, thì cũng trao đổi gần hết rồi. Tuần sau còn một số cái core flow tiếp, nhưng chắc cũng không ảnh hưởng quá nhiều tới mọi thứ.\n\n**[1:28:22]** Hôm nay là ngày 14, hy vọng đến cuối tháng này, buổi họp team tiếp theo sẽ show được nhiều progress hơn. Tất cả những thứ mình đang làm rất quan trọng ha. Toàn bộ này đều đang được đưa lên Memo, tụi anh đang sử dụng Memo đó không chỉ để share trên đó không ăn thua.\n\n**[1:29:01]** Shill khắp nơi mấy công ty khác mình biết, bắt đầu mở rộng network ra để xem tìm kiếm user cần thiết. Chuyện là mình biết những thứ này rồi thì làm sao mình mound được khả năng mình profit từ kiến thức của mình, ý là vậy. Ok ha, đó là cái skin mà team đang chạy theo. Tóm lại, thị trường nhận định đang như vậy. Tuần sau mấy anh em sẽ phải đăng ký làm cái registration vô cho những cái phần, với lại Huy, Tom và Thành là những bên mạng bắt buộc.\n\n**[1:29:45]** Còn bên mấy cái hobby club, như kiểu Build hay gì đó, thì anh không yêu cầu cao vào bên đó, ra cái kỹ thuật để apply, nó cũng không quan trọng lắm. Quan trọng là output nhiều hơn. Hai nhóm khác nhau: một nhóm là những phần mà core project mình sẽ làm, tập trung vào làm sao tăng activity, tăng cái **knowledge base** của mọi người; còn cụm kia tập trung vào cái **skill set**, chuyện develop product sao launch, làm sao làm onboarding tốt hơn, user các kiểu. Là một cái nhóm skill set khác.\n\n**[1:30:27]** Đặc biệt là Huy, Huy đang co cho cái việc quay trở lại office để bắt đầu làm shadowing cho chuyện **knowledge transfer**, thì nếu được thì cứ tiếp tục để nó diễn ra. Rồi xem số liệu như thế nào thì report lại cho anh ha. Hopefully khi nào có con số đây thì mấy anh em xem thảo luận tiếp, làm sao setup cái vụ shadowing đó trên mấy cái dự án mà mình, mấy cái site mà mình tham gia, để có cái case share với nhau ha.\n\n**1:31:05** Toàn bộ là vậy. Nếu bây giờ không có gì khác thì chắc mình kết thúc ở đây. Đây có bao nhiêu bạn nhờ? 28 bạn hả? Không, đang bao nhiêu bạn trong con này nhờ? Chuẩn bị spam ICY, có vấn đề để transfer ICY chưa? Để cái này mai mốt lấy acc anh, hoài căng quá. Có vấn đề trên ICY nhờ. Mọi người ra random nha, giật cô hồn nha. Amount 28 thì mình sẽ drop 14 token ICY, entry là 14 rồi. Xin mời, duration là 5 giây. Ok, let’s go. Một ICY hồi nãy là tương đương khoảng 100 Satoshi rồi đó.\n\n**[1:32:09]** Tuần sau lịch vậy nha, mọi người xem phối hợp với nhau để làm việc cho hiệu quả rồi. Bye bye.\n\n---\n\n### English Transcript\n\n**[05:30]** Hello, can you hear me? Oh, okay, it’s fine now. Today, I think we’ll start a bit early. Today’s session will probably combine with the brother in the meeting for a little bit. One part will be to do a showcase, the second part is that the brother will summarize some things that were discussed with the guys previously. The second and third parts are that we’ll start letting the guys register for tasks. For now, to make it easier, I’ll probably let Huy Nguyễn go first to show the parts related to Huy, which involve ICY a little, and then show some tech stuff that our team is currently working on. This will give me a snapshot of how the tech team is doing right now. Then, moving forward, what our team needs and what the guys can contribute to it. Alright, let’s get started.\n\n**[06:35]** Huy, where’s Thành? Let’s give them the stage now. Okay, for the first content, let’s start with ICY Swap. We announced it, last week or this week it was deployed, so now how are the differences, I’ll probably ask Huy to go over that whole series again.\n\n**[07:29]** Hello, alright, I’ve seen the screen already. So now everyone can go to the ICY Swap page to swap. Here, I’ll show the data. But up here, everything is fully ready now. The only thing left to do is that we’re currently reviewing the ICY numbers. Because previously, when we were operating, we operated by pegging the ICY price, so we didn’t really care much about the circulating supply. So there were some cases where we put it into the team’s wallets or transferred it to Mochi Balances for me or for brother Bảo. Those things need to be reviewed again to get the correct circulating supply number. Because now we’ll sit down, and the price will be dynamic based on the pool, so we need to check that again, and it’s almost done.\n\n**[09:09]** Now, the only thing left is brother Bảo’s account that needs to be checked again. I remember there was a time we transferred to brother Bảo, so now we’re reviewing that part, doing the addition and subtraction, and cutting that part out of the circulating supply, then this number will come out correct. For now, if anyone wants to swap to support, they can swap on this page. That’s the current schedule. I’ll show the list of our current Holders so the guys can see, probably need to know a bit more. Up until now, people participated without paying much attention, but this time we need to be more mindful.\n\n**[09:51]** Our ICY is deployed on Base, right? So when the guys go into the Holder list, everyone will see a list of all the wallets currently holding our team’s ICY, which are the CCK Holders. That’s one thing. And then the link to access this, Huy will share it, I guess. Because if people go search for it, they probably won’t find it.\n\nFirst, the guys need to understand this. Moving on to this part now. I think the guys need to pay more attention to this part. It’s become the norm in the tech world already, no need to do anything new anymore. So if the guys grasp this, it’ll be better.\n\n**[10:33]** Our ICY is now listed. In this list, there are minter wallets, wallets used to budget for activities, and some wallets holding large amounts of ICY. Activities related to staking ICY will be rolled out gradually in the coming time. This is the first piece of information the guys need to understand clearly.\n\n**[11:15]** Huy, demo the swap process for us. Does anyone have a Bitcoin address with some ICY? Is Vincent here? Okay, now let’s try swapping from ICY to Bitcoin. The current price is calculated dynamically based on the circulating ICY amount and the pool. The swap function is very simple, just enter the amount, press swap, and it’s done.\n\n**[12:27]** Wait, don’t enter a fake address. Okay, it’s good now. The first frame is ICY as usual. Below it, it’s displaying the unit in satoshi, which is the smallest unit of Bitcoin. When you enter the amount, it will automatically convert. However, the current exchange rate is slightly off, around 1.2 instead of 1.5. This is probably a small calculation error, we can fix it.\n\n**[13:28]** You need a minimum amount of ICY to swap. Try entering 30 ICY and see how it goes. Refresh it and check if it works.\n\n**[14:43]** It seems like there’s not enough money in the wallet. Do you have ETH on Base? Transfer it to Base and check again.\n\n**[15:51]** It’s not that error. The issue is that the account hasn’t been registered, so it can’t perform the transaction. We’ll fix that part later. The goal here is to help everyone understand the swap mechanism and how token pricing works better. If you understand it well, it’ll be easier to manage tokenomics later on.\n\n**[16:47]** Huy, quickly explain the pricing mechanism again. Last time Quan demoed it but didn’t go into detail about that part. The price of ICY is determined by the minting mechanism, meaning the price won’t fluctuate heavily if someone swaps a large amount. It doesn’t operate like an automated market maker (AMM) mechanism; the price will be controlled through the minting mechanism. This mechanism helps keep the price stable even with large transactions.\n\n**[17:43]** It completely depends on Bitcoin. So if Bitcoin’s price goes up, the amount of ICY you guys are holding will increase in USD value. As for the minting mechanism, Huy, explain a bit more. Generally, our overall mechanism so far is that we fix ICY’s value to USDC. You guys don’t need to worry too much, just understand simply that one ICY is equivalent to 1.5 USD.\n\n**[18:37]** This assurance part is to help the operating team ensure that by the deadline, USDC will be added into the contract for everyone to swap. The swap rate in the old contract was fixed at 1.5 ICY, but that was the old model. Our new model is more flexible. If you guys have used Uniswap or other AMMs (Automated Market Makers), it’s somewhat similar. Here, the mechanism works with a liquidity pool underneath, which contains both ETH and USDC. Depending on the pool’s situation at that time, the exchange rate will be adjusted based on the amount of ETH and USDC in the pool.\n\n**[19:18]** Our mechanism works similarly. The price of ICY will be determined by the amount of Bitcoin in the pool and the amount of ICY currently in circulation. The formula is simple: we have the amount of ICY (X), we have the amount of BTC (Y) in the pool, then X/Y will give us the value of one ICY in terms of BTC. This formula is basic mathematics, nothing complicated.\n\n**[19:55]** Due to our operating mechanism, there will be two moments that change liquidity:\n\n1. **The first moment** is every month when the operating team adds more BTC into the pool to cover the costs of the team’s activities. At this point, the price of ICY will increase slightly because the amount of BTC in the pool increases.\n2. **The second moment** is when the team adds more ICY into the pool (minting more). When more ICY is minted, the market price of ICY will decrease because the amount of ICY in the pool increases.\n\n**[20:35]** The two cases above will directly affect the price of ICY. However, if the price of Bitcoin changes, the USD value of ICY might change, but the price of ICY in terms of BTC will not change. The market impact from Bitcoin is an external factor and does not directly affect the minting or the value of ICY in the pool.\n\n**[21:12]** If you guys have any more questions, feel free to ask, and we’ll answer them later. Oh, there’s a question about swapping back from BTC to ICY, right? Currently, that function isn’t available. Right now, we only support swapping from ICY to BTC, not the reverse swap. Meaning you can buy in, but selling out isn’t supported yet.\n\n**[21:40]** Thank you, Huy. Anything else to note? One thing to note is that we’re still in the testing phase, so there might be some exceptional cases. For example, some situations might arise during swaps or when liquidity isn’t sufficient. Fundamentally, though, the current flow is still operating stably.\n\n**[22:00]** Like the minimum ICY amount required to swap. Because essentially, our team is covering the gas fees for transactions on ETH, on Base, and even on BTC, we’re kind of limiting it so that the ICY amount swapped needs to be a bit higher. This is to avoid situations where people swap just 1-2 ICY to test, which would cost gas fees, so we’ve set it at around above 20 ICY to allow swapping on the web.\n\nThe second thing is that since minting more ICY will change the market price, I’ve disabled the part about our previous salary advance mechanism.\n\n**[22:37]** Meaning if everyone advances salaries at the same time, it would affect the price, right? So the lesson learned from this is that after this round, there are a few points I’m noticing. Our team is starting to focus on building tools to support our operations. These are also some new experiments and some things that genuinely support our activities. But after finishing these tasks, we’ll produce some articles related to them. So if any of you didn’t participate in those projects earlier, you can look back at those articles to understand the game, the knowledge gained from that round, and what the guys working on those projects achieved.\n\n**[23:24]** So with this ICY Swap round, we’ll probably get two or three articles, right? Yes, like three articles. And if we want to write more, there’s still plenty to write about. Yeah, alright, take it slow and steady.\n\n**[24:02]** After Huy’s part, I thank Huy and move on to the second topic related to what our team is currently doing. Brother Bảo, whoever wants to go first is fine, but I’ll probably let Thành speak first. Thành said it’s okay for him to go first, he’ll gather everything to let everyone know what stage the team is at. But I said let Thành go first because someone’s ringing the bell. Alright, I invite Thành to start.\n\n**[25:00]** Everyone, our Memo is one of the big things this round, and we’ve upgraded its format to make it look a bit better. We always want to create content maps, things that we can read and upload here. But currently, that model isn’t really that effective anymore because new models compress data, and querying directly from there would be more efficient.\n\nSo the point is that putting ordinary knowledge onto Memo isn’t very suitable anymore. For this round, when reworking it, there’s one main idea I want to tell you all: Memo will now be used for one sole purpose ,  the knowledge gained from projects.\n\nThat’s almost like the new things that come directly from our team’s activities. In the future, it’ll mostly consist of what field it’s related to and what we’ve done in that field. There’s more to it ,  maybe after a period when they retrain the model, our data will become part of the shared knowledge for the whole community.\n\n**[25:39]** And I think this part will be very helpful for things like retraining AI models later or for cases where we want it to provide automatic suggestions.\n\n**[26:24]** The content will become part of that model, or if there are internet search tools, our articles might just be a small part of the referenced materials, like a small piece in a citation. That’s not a big issue. But overall, all this content will pretty much become the spirit of the team.\n\nIn this major upgrade, there’s one key point that Tuấn has completed, right? Tuấn, the part about syncing all the team’s data, especially the content, is currently being directed this way so the members can understand it better.\n\n**[27:00]** Meaning after this round, the members participating in projects will tend to sit down together to review those projects more closely and determine exactly what the **knowledge gain** from those projects is. After that, the team will upload it to Memo as internal reference material for the team.\n\nThe second part is that at the end of each article, there will be a section related to a **group of reading**. This part isn’t fully complete yet, but the idea is that once it’s finished, there will be an additional section summarizing information about the article so readers can look up and learn more from it.\n\n**[27:47]** In addition, all the data written by the team will be tagged with identifiers such as **GitHub**, **Discord**, or other internal channels. This data will be uploaded to a **blockchain storage** form on the **Arweave (AV)** platform ,  a decentralized storage platform. This ensures that the team’s content has a clear and transparent identifier.\n\nOn top of that, readers will be able to review the articles, rate them, or leave feedback directly on the articles. This is part of the new upgrade idea for the team’s **Memo** page.\n\n**[28:39]** Previously, the team intended to use Obsidian to manage content, but it seems some members had difficulty getting familiar with that tool. Therefore, to make things simpler now, the team will switch to a more direct mechanism. Specifically, instead of having to go through Obsidian, members can submit content directly to the repository of the team’s shared library.\n\nMembers just need to input the content and submit it directly through this platform, without having to follow Obsidian’s mandatory workflow anymore. If someone still wants to use Obsidian, that’s fine, but if they don’t, it won’t affect anything. This is the most fundamental change in the team’s Memo system.\n\n**[29:24]** Currently, the team is working on several main projects, including:\n\n1. Bitcoin Swap ,  already mentioned in the previous section.\n2. Memo ,  just presented.\n3. Two smaller projects:\n    - **Agentic** ,  being developed by Quang and Huy’s group.\n    - **GitHub Bot** ,  being worked on by Thành’s group, currently in testing.\n\nNow, I’ll probably hand it over to Thành to share more about these contents.\n\n**[30:32]** This project was started over a week ago and has officially been running code for more than a week. Its main purpose is to create a reminder system. Previously, the team often encountered situations where, after creating a pull request (PR), people would leave it there, wait for it to finish running, and then forget about the need to review it. This tool will serve to track and update information about daily activities on GitHub or weekly activities on the team’s internal communication channels.\n\n**[31:18]** This system is designed as a simple integration. The basic workflow includes several use cases, such as notifying the person assigned to review, interacting with the GitHub API, and posting information to internal channels like Discord or Slack. Currently, the team is testing it on Discord. Additionally, the team is experimenting with Agentic and a new framework called **Mastra AI**.\n\nThis framework is different from typical Python tools. Some team members aren’t familiar with working in Python, so the team wants to test whether using this new framework is more effective than current solutions. The framework supports features like setting up the environment, defining states to manage data, and allowing reconfiguration based on the team’s needs.\n\n**[32:19]** The system’s structure has two main parts:\n\n1. **Agentic App** ,  This is the main application for handling the system’s activities.\n2. **Discord App** ,  This supports sending notifications to Discord.\n\nAdditionally, the system has a few auxiliary components, such as workflows to handle scheduled tasks, check, and notify developers if there are any pull requests waiting for review. If a pull request exceeds a certain amount of time, the system will send a notification to remind the person responsible for reviewing it.\n\n**[33:12]** The Agentic App will expose a few APIs that allow chatting and tracking the status of pull requests. When a pull request is created, the system will automatically identify conditions like the pull request’s status (work in progress or not), the time it was created, and will notify the reviewer after about 30 minutes from the creation time. For example, if a pull request needs review but no one is assigned or it has exceeded the processing time, the system will automatically ping the responsible person again.\n\n**[35:02]** Instead of having to track manually, the system will attach an agent to automatically monitor and notify through the system’s endpoint. In the logic part, the system will define specific conditions, such as only sending notifications if the pull request was created within 30 minutes or is in a work-in-progress state. If the pull request is updated or changes status, the system will automatically track and notify the developer to ensure nothing is missed.\n\n**[35:39]** The system will operate based on standard code filters. Additionally, it will have some other workflows, like sending notifications at the end of the day to summarize the status of pull requests on Discord. The system will automatically send notifications about the number of open pull requests, their statuses, and the current review status. This is the main function of this tool ,  acting as a reminder tool.\n\n**[36:24]** The system can also integrate with other chat tools. It’s simple ,  you can create an additional command and send a request to the system’s endpoint. These requests will be defined based on a specific schema, such as the input being a **review ID** or other information related to the pull request’s status. The system will take this data and display it on the interface that users frequently use.\n\n**[37:04]** The backend processing of the system is handled through the Lippia tool, which formats JSON data into Markdown tables or data-binding formats. Currently, the team is testing these two processing flows before expanding to additional features. Once the system is stable, these workflows will be opened up for all team members to test and further develop.\n\n**[38:08]** The system is designed to scale flexibly. Team members can independently develop and contribute different workflows. This system allows the creation of tools as standalone **packaging units**, which can then be combined to create more complex workflows. When wanting to release a new workflow, members just need to redefine the basic unit and integrate it into the system.\n\nExpanding workflows will help the system grow horizontally (increasing the number of features) rather than vertically (developing existing features). As the number of workflows increases, the system will become more flexible and powerful.\n\n**[38:54]** Fundamentally, workflows are considered the application layer, similar to previous data APIs. This system will operate at the tool level, but end users will interact with it through the workflow interface. Currently, no entity has successfully implemented this model on a large scale. However, GitHub has now expanded its API for developers to create extensions and integrate them directly into GitHub.\n\n**[39:40]** Dify is building a platform to support developers in developing and deploying these tools and workflows more easily. The goal is to create a marketplace where tools and workflows can be distributed and used by various users. This system is similar to an open platform, allowing third-party developers to deploy their own tools and workflows.\n\nOn Dify’s platform, there are already about 50 different tools. Some tools were previously released as experiments, but due to a lack of clear direction and community support, they didn’t achieve the expected success.\n\n**[40:17]** Some platforms in the past tried building similar models but didn’t succeed. The reason is that those tools were only built as forms, lacking the ability to interact with external data and unable to combine complex workflows. However, Dify is focusing on solving these issues to create a complete ecosystem for workflows and tools.\n\n**[40:59]** These tools also allow users to push data from external sources into the system. Users can send data from external applications via Open Forms or APIs. Dify will automatically process and format the data for use in the system’s workflows.\n\n**[41:56]** The team is focusing on two main development directions:\n\n1. Continuing to expand and develop existing workflows.\n2. Improving and optimizing current tools to support easier deployment and use.\n\nThe system is built based on common standards for tool and workflow design. The Smithery tool is currently acting as an Agent to manage workflows. Smithery can also be used as a Package Manager to install and manage tools within the system.\n\n**[42:53]** Workflows will operate on the mechanism that if a workflow becomes popular, people can take it and use it as a tool. The nature of these tools is that they are designed to serve specific domains. For example, a tool for creating files, searching, or retrieving code files. It works like an SDK, meaning a library that you just need to import to use.\n\n**[43:37]** Once integrated into the SDK, you can use the available methods to manipulate data. This allows easy integration into AI tools. Currently, only Cross directly supports these operations. However, in the future, it will be standardized so other tools can also integrate easily. The case of Manus is an example. Manus uses many different tools, but when compared to the agent system in Smithery, they are fundamentally two completely different layers.\n\n**[44:15]** In Manus’s system, tools are combined to create more general workflows. These tools operate at different layers, while agents in Smithery are designed to work independently. The question is how to clearly distinguish the difference between Manus’s system and the agent system in Smithery. There’s a summary article about this posted in the AI Club channel ,  the main content discusses the ability to think (thinking) and the ability to use computers (computer use).\n\n**[45:09]** The mechanism of the Manus system is a service-oriented system. To combine multiple tools into a single workflow, the execution steps need to be clearly defined. For example, step 1 uses which tool, step 2 uses which tool, and so on. This requires the steps to be specifically configured. However, the new system has the ability to reason and automatically determine which tools are needed to complete a task. This is the key difference between the new system and older systems.\n\n**[45:59]** Specifically, the new system can recognize how many tools a task requires, which steps to go through, and can intelligently adjust the execution order. This is a special mechanism and a difference compared to older systems. In other words, it operates like a Supervisor ,  capable of reasoning and making decisions about the order and method of executing steps in a workflow.\n\n**[46:35]** The Supervisor system operates at a higher layer than the agents in Smithery. Agents in Smithery are simply tools that execute a specific task, while the Supervisor has the ability to manage and coordinate the entire task execution process. Integrating the Supervisor allows the system to operate more flexibly while making it easy to expand and add new tools.\n\n**[47:33]** The team’s goal is to understand how the system works and grasp the mechanics of managing workflows. If we can determine how to deploy and manage workflows, we’ll be able to select and use tools more effectively. This is what the team is aiming for ,  building a system capable of scaling and optimizing workflows.\n\n**[48:24]** Next, the team will focus on building the **MCP** system. This is a new system designed to manage data and workflows. The team conducted a demo of this system about two weeks ago. The essence of the MCP system is to build an agent that operates on an existing platform. Users can quickly deploy and test the system through MCP.\n\n**[49:10]** MCP will be a complete system, including a **database** and a **server**. This allows the system to operate independently and handle large amounts of data. Unlike older systems, MCP will allow users to adjust configurations and manage data more easily.\n\n**[49:58]** The essence of MCP is an agent, defined with a specific input and output structure. This allows different systems to connect and interact with MCP through standard protocols. In other words, MCP can be integrated into any system via predefined protocols.\n\n**[50:35]** MCP also allows users to manage data through the Knowledge Database, which is essentially a timescale database where all the team’s activity data is dumped. This is a time-series database that enables recording events in real-time, something backend developers will recognize as event sourcing or event logs. For example, it records information about team members, the system’s operational status, or other significant events.\n\n**[51:13]** The Knowledge Database will store all the team’s activity data, including details like who performed which task, the system’s status at specific times, and other information related to the team’s internal operations. This allows the team to track and analyze work performance, thereby making reasonable adjustment decisions.\n\n**[51:51]** The system’s concept includes a component called the Landing Zone. The Landing Zone means that all the data we currently have ,  about a dozen to tens of datasets (databases) ,  will be centralized here. Three to five years ago, if we wanted to build a data storage system, we’d create a bot to collect all the team’s activities and input them into our database.\n\nWith the new Meta model, all large data (Big Data) will be dumped into a temporary storage in the form of .dat files on S3 or GCS (Google Cloud Storage). The MCP will have the ability to read directly from the Landing Zone. If the system determines that the data in the Landing Zone is valuable and necessary, it can automatically convert that data into a Time Series Database (TSDB) for long-term use. This is the end game (final outcome) of this system.\n\nThe remaining issue will be building Use Cases based on the organized data in the system ,  in the direction the team desires. This is a key development direction for the MCP system in the near future.\n\n**[52:25]** So currently, the team will have an old database system ,  a traditional table-based database located at the bottom of the system (visible in the diagram with blue blocks). Now, the team is adding two new components:\n\n- The **Landing Zone** component ,  located in the yellow block at the top of the system.\n- The **Time Series Database (TSDB)** component ,  directly connected to the old system’s components for data analysis and exploitation.\n\nThe team is storing raw data in the Landing Zone. Essentially, centralizing data in the Landing Zone is like rallying troops ,  gathering all the data in one place before deciding how to analyze and process it. This mechanism makes the system more flexible and easily scalable when new data is added.\n\n**[53:11]** The special feature of this system is its ability to automatically convert data from the Landing Zone to the Time Series Database. This mechanism stems from the growing need for local data analytics. This is an emerging trend in the context of AI (Artificial Intelligence) development.\n\nThe rise of AI has increased the demand for real-time data analysis systems. When raw data is centralized in the Landing Zone, the system will automatically identify valuable data and transfer it to the TSDB for more detailed analysis. This is a significant step forward in building an efficient and adaptable data analysis system to market changes.\n\n**[53:45]** Currently, the team can already run analytics directly on the data stored locally. This system allows running analytics right on the Data Lake without needing to transfer data elsewhere. For the data in the Landing Zone ,  the file packets that Huy is showing on the screen ,  this is the part the team needs to focus on researching further. This issue relates to text processing, so the guys need to pick up this topic. It’s not too difficult; it’ll probably take about half a day to grasp the basics.\n\nThe Prompt for searching and exploiting data is also quite fast and simple, not complicated. This is a part very worth experimenting with because it relates to the knowledge discovery mechanism in the system. This is one of the new upgrades Huy just mentioned.\n\n**[54:22]** The most standout feature of the system in this upgrade is the **Knowledge Hub**. This is where the team will centralize all data to serve analysis and knowledge exploitation. The Knowledge Hub will become a common **data pool** for the entire team. Anyone can add data here, and the system will process and convert the data into a standard format.\n\nThe important thing is that once the system is fully set up, everyone in the team will have a common **protocol** to use. Different modules or components will be able to **share** a common data structure and access the Knowledge Hub directly. This will be the common foundation for syncing and processing data within the team.\n\n**[54:58]** Regarding the database (DB), the system will have two layers:\n\n- **Old DB:** Used to support existing operations and process pre-structured data.\n- **New DB:** Designed to connect directly with the **Knowledge Hub** and support real-time data analysis.\n\nThe special thing is that the **MCP** will act as a **protocol** for different modules to communicate with each other. This means that any data needing access or processing just needs to be fed into the system’s correct pathway, and it will be automatically processed according to the standard structure. This is how the system unifies data and avoids conflicts when multiple data sources are processed simultaneously.\n\n**[55:43]** From now on, the team will need to get familiar with the new data processing mechanisms. Everyone should take the time to learn more about the components in the new system. Once these components are stable, the team’s new projects will leverage these tools to deploy faster and more efficiently. This will be the main toolkit to serve future projects.\n\nThis system has the potential to become a **requirement** for upcoming projects. If you want to keep up with the new system, start by learning the basic principles of MCP and related protocols.\n\n**[56:40]** Previously, when the team deployed systems on S3 or GCS (Google Cloud Storage), data processing took quite a bit of time. However, with the new mechanism, data from the Landing Zone will be processed faster and more easily.\n\nThe system has been tested on various platforms, including **S3** and **GCS**. However, since the team’s current infrastructure runs on **GCS**, the data from the Landing Zone will be processed on GCS first. That said, technically, the system can expand to other platforms without major obstacles.\n\n**[57:45]** The Landing Zone’s operating mechanism is quite simple:\n\n- Data from various sources will be centralized in the Landing Zone.\n- This data will be stored as **Parquet files** by day.\n- The system can read these files back through the **Time Series Database (TSDB)** mechanism.\n\nCurrently, some sample **Parquet** files have been created and are being tested. If needed, the team can run a demo on these sample data sets to check the system’s consistency.\n\n**[58:24]** The team’s activities, like **AI sub** or **Memo**, are also fully pushed up here. The task of the **Landing Zone** is to store all the data the team wants ,  anyone who wants to store something can push it all here, and then the system will decide how to process that data. The system has also provided some tools for people to push data up, such as **API proxies** to forward events. If anyone wants to push information to the Landing Zone, they just need to call the API.\n\nMemo is currently using this mechanism to pull data from **social platforms** and sync it into the system. This mechanism has been successfully tested. For more specific data types like **Discord messages** or **data from Basecamp**, the team needs to build **crawlers** or **connectors** to collect the data. Currently, the team already has some ready-made templates for these data types.\n\n**[58:59]** For the next development direction, the team will focus on exploiting data from the Landing Zone. If you want to join this project, the advice is to start with a specific **vertical**. For example:\n\n- Identify a clear **use case**.\n- Find out **which data** is needed for that use case.\n- Redefine the data exploitation mechanism in a **top-down** approach.\n\nInstead of storing whatever data seems interesting, the team should think in terms of **defining the use case first** and then deciding what data to store. This helps the system operate in an organized and easily manageable way.\n\nA specific example is if there’s a use case about **Project Nghệ Nhân**, the team would need to create a **Git Agent** to collect data from Git, then push that data into the **Knowledge Hub** via MCP. From there, the system would define data exploitation tools for this use case.\n\n**[1:00:16]** Additionally, the team is developing a small MCP Server. This MCP Server is essentially a basic server, using standard technical components of the current internet system. It defines clear inputs and outputs, allowing connection to various interfaces.\n\nFor example:\n\n- If there’s an MCP to process data from Slack, the team will define APIs for each data type.\n- If tools are needed to read data from Google Sheets or analyze weekly check-in status data, the team can create MCP tools to handle that data.\n\nMCP will act as an intermediary component to sync and process data from various sources. Everyone can access these tools from the Editor, Command Line, or any other interface.\n\n**[1:01:07]** The essence of MCP is that it will serve as an **API Gateway** to connect tools. If you need to track everyone’s weekly check-ins in the team, you can create an MCP to collect data from the **Knowledge Hub** and Google Sheets, then compare the data to see who has checked in and who hasn’t.\n\nThe current system is at the stage of deploying a basic MCP Server. The current interface uses the **Command Line** to call MCP, but fundamentally, the team can expand it to connect with various other tools.\n\n**[1:01:43]** The system is focusing on implementing authentication and authorization mechanisms.\n\n- **Authentication** – Verifying users to access the system.\n- **Authorization** – Assigning permissions for data processing activities.\n\nThe system is currently being used internally within the team and has not been made public externally. If you want to use MCP, you’ll need to input a **private key** to authenticate your access rights.\n\n**[1:02:23]** Technically, MCP can expand to different components within the system. Everyone can integrate MCP into existing applications or tools without needing to rewrite too much code.\n\nThe team is still testing this feature and focusing on completing the security and access management parts. Once the system is stable, everyone can integrate MCP into their existing data processing workflows.\n\n**[1:03:00]** It’s just paused here for now; we haven’t tackled the complex authorization problems yet. After completing the current steps, we’ll move on to addressing more complex issues related to authorization and system usage rights. For now, everyone can focus on the basic issues first.\n\nAlright, thank you, Huy. This is one of the important technical development parts for the team. If you follow the activities on the tech and AI Club, you’ll notice the team is moving toward the next steps in the development process. Technically, everyone should pay attention to the key terms Huy just mentioned. If you’re not clear on them, you can review the transcript to get the full information.\n\n**[1:03:45]** The core team is still continuing to develop the system. We request all members to participate in the project so we can **transfer knowledge** more effectively. This project is an environment for everyone to learn and practice.\n\nThis is an opportunity for new team members to get acquainted with and grasp important technical aspects. If you feel unprepared, you can refer to the internal guides and documents to catch up. Training will happen during the work process rather than in separate sessions. This is a hands-on environment where you learn while doing.\n\n**[1:04:29]** Alongside system development, the team is also conducting knowledge transfer from completed projects. We expect to have a session at the end of the month to summarize the lessons learned from these projects. If anyone doesn’t fully understand yet, they can refer to or ask members who’ve worked on them for more information.\n\nIf you feel unprepared or need more details, you can directly ask team members. Everyone can ping more experienced members to get support.\n\n**[1:05:07]** The team has two different groups working in parallel:\n\n- **Tuấn’s team** is developing some games and small applications.\n- **The build team** is working on experimental applications to test the system’s feasibility.\n\nThese activities are similar to the **Build Club** and **AI Club** groups within the Foundation team. Some products have started showing good **output**. Tuấn and his team are developing a game based on the **Turing Machine**.\n\n**[1:06:38]** The **Turing Machine** game that Tuấn’s team is developing is adapted from the board game version into a mobile version. The game’s goal is to guess a sequence of **three numbers**. To guess the correct sequence, players receive **clues**.\n\nFor example:\n\n- If the clue says “one of the three numbers must be greater than 1” → Players can input numbers, and the system will determine if the answer is correct.\n- If two numbers are wrong but one is correct, the system will respond immediately so players can continue adjusting.\n\nThe rules are quite complex, which might be challenging for new players. Tuấn and the team are continuing to tweak it to make the game more accessible without losing its challenge.\n\n**[1:07:23]** The game is called [**Pocket Turing**](https://pocket-turing.vercel.app/) because the original board game version involves punched cards ,  similar to how the Turing Machine works in computer programming. However, I’ve adjusted and added new elements to make it more suitable for the mobile version.\n\nI plan to refine and expand the game in future versions. Additionally, I’m checking if we can implement premium features or advanced options to increase monetization potential.\n\n**[1:08:16]** I’m testing the beta version of the game. The gameplay is complete, and players can fully experience the features. The next step is to test it with a broader user group to gather feedback and improve the product.\n\n**[1:09:15]** The next goal is to bring the game to the App Store and Google Play to reach more users. For now, the team wants to ensure the game runs stably without serious bugs.\n\nTuấn hopes the game will attract at least **100 paying users** in the initial testing phase. If we get positive feedback, we’ll expand with new features and improve the player experience. I’d love to hear feedback from other team members to adjust and perfect the product further. Tuấn has shared the game download link with team members so everyone can try it and provide input.\n\n**[1:10:13]** If you guys are excited about building products, this is a good time to start. The team has experimented many times before, but this is a chance to do it more systematically. Developing internal products not only improves technical skills but also opens up future commercialization opportunities.\n\nBesides Tuấn’s game, the team is working on other tools. If you have any good ideas, feel free to contribute so we can build and test together. How to sell or monetize the products can be figured out later; the priority is completing the core features first.\n\n**[1:10:58]** Next is An’s part. An once made a tool called **Rec** to aggregate information in a format similar to **Apple**’s system. Version 1 of Rec required users to manually organize information, while the current Version 2 has integrated AI to support automatic organization.\n\nHowever, the AI still has some limitations in fully recognizing content. Sometimes it can’t grasp the entire context, so the results aren’t completely perfect. Still, the important content is organized and displayed fully.\n\n**[1:11:56]** This tool is in the refinement stage, but the core functions are stable. Currently, the team is focusing on improving the interface and optimizing the user experience. An plans to continue developing additional features to better support users.\n\n**[1:12:51]** The team’s projects are currently in the testing and improvement phase. If anyone has questions or suggestions, they can directly discuss with An or other team members. For now, we’ve showcased almost all the projects. More detailed parts will be covered in the next session.\n\n**[1:13:57]** On our team’s side, I always talk about the knowledge related to **liquidity** and **game in general**, right? We really want the team to push a little in that direction because it’s beneficial for almost like a **life skill**, you know? So I want our team to head in that direction this time. Especially those of you who are really interested in **trading**, meaning getting data to find the **Alpha** on it, the **Intel** on it, to come up with some **market-making** strategies based on certain conditions or something like that.\n\n**[1:14:43]** It’s one thing, or it could even go further to create a really awesome flow. It feels like it’s just my dream for now. An has already made a version that I think is pretty okay. Just taking this chance to let you guys know that the team has this kind of progress going on. It’s running, right? Let’s invite An. Okay, okay, generally it’s just a money-making game. See how they make money, and we’ll do the same. The usual stuff has a bit of something to test, a bit of it is also in the form of **Delta neutral**, right? So we also research those things there. Then go build and research to have the knowledge to ship it.\n\n**[1:15:30]** Play this column until it’s all used up, that’s it, no looking over there. Do you all see the terminal screen? Do you see it? Yes, okay, let’s run it. Let’s run it. I think we need to zoom in, zoom in one or two levels, it’s still a bit small, okay now. Yeah, this is the arbitrage to eat the **funding free frost**, right? There are many, many types of arbitrage like that. This is just one of those types, which is eating off the difference, the **phantom bin**, between the exchanges. We’re focusing on three exchanges: **Binance**, **OKX** ,  that devil OKX, their exchange doesn’t have too much stuff ,  so, do you have a diagram for that, An?\n\n**[1:16:26]** I think it’ll be a bit hard for everyone to visualize. Looking at this, they won’t understand what it is. Is there one? Okay, no diagram? Oh, there’s this, big one, just theory, nothing concrete comes up at a high level? I guess we’ll have to sketch it roughly again. Do you see it yet? Maybe look at my diagram, yeah, my diagram, so you know it’s this right away. The PRL? Wait, it’s messed up, running wrong, forgot, it’s going into the sockets of…\n\n**[1:17:47]** Those guys to pull **real-time data** back, and it’s currently pulling data from how many accounts? Three accounts ,  **Binance**, **Bybit**, and what? Okay, initialized to get the price back, right? Getting the price, getting the **funding** back, getting the price yet? Getting some data like fees and fee-related data, it’s kind of coded according to that calculation, but it hasn’t been used to fetch yet. OKX probably doesn’t have it. Those trades, okay, don’t have it, so usually it’s just on the two main ones, which are **Binance** and **Bybit**. Yeah, setup is okay, and then it’ll have an array to show which pair has the difference, the difference in **funding**, then it’ll have profit, and I calculate that if we enter, how much it would be. PH is the number, the number of times we collect the **funding** to break even with the fees. It’s monitoring, monitoring the highest between the two exchanges, that’s it. Step one is getting the difference, the difference in **funding** between the two sides, right? Between the two sides that I’m talking about, no. Because I said this is getting from three exchanges, so compared to its angle, it’s still on the exchange net, right? Yeah, exchange net is something calculated later because…\n\n**[1:19:49] Funding**, let’s say normally, on the price setup, they usually don’t have much, like one is positive, two are positive, or two are negative, so the difference is smaller. Actually, we place a counter on the other difference, it’s just the offset, the price movement offset, so it doesn’t lose due to the price. On the exchange net, there won’t be that difference, we make the **funding** always equal to zero, right? Because there’s no swap fee there, and instead of countering on another exchange with that thing, we counter at the moment when the **funding** is also zero. Currently, we accept that it won’t be as profitable, but that’s how the first version is.\n\n**[1:20:33] b**So we get the price, get the top, get the **funding**, then see if it’s just about deploying capital, right? Have you tried running it yet? Not yet, haven’t poured money in. Yeah, this is like a scale game, you need a lot of money to profit, but with just a few hundred or a few thousand, it goes in, and it doesn’t look like much. Got it, got it, okay, understood. But on the technical side, technically, for me to do this, what did I apply from start to finish? From when it crashed, what did I do? Yeah…\n\n**[1:21:25]** First, I researched that chart beforehand, checked how it was, whether it had this or that, all those things. Yeah, got those dots sorted. We’ll ship it for that, yeah, that guy will code everything with code, okay? Using this **Cloud 3.7**, right? First, you have to get the exchange’s data before calculating anything. The main exchanges will pull from sockets, and the setup is, first, on the API docks of the exchanges, right? This one, yeah, then pull their docks back, throw it to this guy, it ships up to the **WebSocket client**. Every exchange has docks, all of them, pull them back, ship them up, and it’ll auto-view.\n\n**[1:22:15]** The web for all three exchanges is done, with forms and everything. After that, we start building it up, yeah, this part, this chart. The first step is probably explaining it to it and stuff, kind of it builds slowly up. Then check the data and all, if it’s wrong, it auto-builds itself. I built it, and it auto, every time there’s something new, it’ll auto-build a sample to check the data before finding it again for us. Cool, to ensure the data is correct or not. Because the thing between the exchanges, the format is different, the data format is…\n\n**[1:23:01]** Different, everything is different, right? So to compare it all into one final form for it to compare, it needs a section to test pulling the data back, ensuring the data is valid, then it starts comparing. That’s the step. Next, it’ll be building things like, next is, yeah, building the guy to place orders. When it detects these, there’ll be a guy standing by to place orders, watching our bot to see if it’s losing or whatever, slowly, reasonably. The whole process took how long? About a week, yeah, cool, huh? So now the next step…\n\n**[1:23:58]** The next step of this tool I’m working on, what’s the next step? I’ll check the data first, check the data to see which one makes money easily, which one is profitable, which one you put money into and it’s all profitable. There’ll be data to check those profits, then manage more of our stuff, like risks and all that, then, yeah, pull all the data back about the fee structure of the exchanges. Because if you use those trusts or whatever, the exchange fees affect the thing a lot, so you need the exact fees, then calculate…\n\n**[1:24:51]** How to ensure it’s profitable in the end before placing orders, right? Next will probably be those steps. Okay, that’s the step, the step of when to place orders, that’s the final thing, right? The rest needs to filter the data first. Is there a back system built? Because I think this data, does it have history or not? Does it? Or is it just at that moment? It does, if you can get the history, it’s backtest history, but I think it’s not accurate, huh? Yeah, not accurate, not there. I don’t think so. The other stuff might have it, but this arbitrage is a bit hard to get accurate.\n\n**[1:25:38]** This is a technical showcase. I think with this direction in the team, independently, our team, regarding this direction, it’s okay. You guys showcasing the trading game have started having steps that the team is pushing forward to do. I think fundamentally, fundamentally, you guys are all on a path, on the way to getting to what you want to do, which is very good. The thing is, with technology, with that tech know-how, how we bring it out and use it, right?\n\n**[1:26:19]** The team’s activities in general are like this, okay? Regarding productivity, it feels like recently everyone has started syncing with each other to a certain degree. But with Tom, Tom is probably out, but Tom had a comment from before when you guys were sitting and chatting. We were thinking, what’s the team’s productivity level right now? How much would Tom rate it? 2/10 or 4/10, huh? If compared to the level we need, you guys at like 6-7/10, the general average, I think we’re in a very good setup right now.\n\n**[1:27:01]** The next step regarding the quality, the technology tool link to support us in operating the team according to this model, it’s being improved slowly but surely. On the market side, the **funding** market in general and the products are starting to stir and come back. People see the technology becoming more stable. In **crypto**, it’s heavily influenced by macro factors, but whenever there’s a tech trend, they’ll jump in and bite, that’s how it is, right? I’m seeing signals for it to resume soon, about 50/50 right now. Before this, I looked at the market, and it was really bad, like everything wasn’t ready yet. Even if you studied a lot and worked a lot, results wouldn’t come immediately.\n\n**[1:27:40]** But this time, I think you guys will need to have some requirements about participating in these things, okay? Next week, I’ll probably ask Huy,Tom and Thành to compile some stats, to see besides the projects you’re working on, what’s the participation in side projects like that, who’s doing what, alright? That’s the follow-up after the content, we’ve discussed almost everything. Next week, there are still some core flow parts left, but they probably won’t affect things too much.\n\n**[1:28:22]** Today is the 14th, I hope by the end of this month, the next team meeting will show more progress. Everything we’re doing is very important, right? Another important thing is we have Sister Minh here, Nicki. Probably past the out time already. All of this is being uploaded to **Memo**, and we’re using that **Memo** not just to share on it ,  that’s not enough ,  but the channels we’re working on are being sent out…\n\n**[1:29:01]** To everywhere, to other companies we know, starting to expand the network to look for necessary users. The thing is, we know these things already, so how do we mound our ability to profit from our knowledge? That’s the idea, okay? That’s the skin the team is following. In summary, the market assessment is like this. Next week, you guys will need to register for those parts, and Huy, Tom, and Thành are the mandatory segments.\n\n**[1:29:45]** As for the hobby clubs, like Build or something, I don’t have high demands there, producing technical stuff to apply, it’s not that important. The output matters more. Two different groups: one group is the core project parts we’ll work on, focusing on how to increase activity, increase everyone’s **knowledge base**; the other group focuses on the **skill set**, how to develop products for launch, how to do onboarding better, users and all that. It’s a different skill set group. You guys next week jump in and start thinking, especially…\n\n**[1:30:27]** Especially Huy, Huy is co-handling the return to the office to start shadowing for **knowledge transfer**. If it works, just keep it going, then see how the numbers look and report back to me, okay? Hopefully, when we have the numbers, you guys discuss further, figure out how to set up that shadowing on the projects, the sites we’re involved in, to have cases to share with each other, right? Like Sister An, finishing this in a week is super solid, doing everything herself, using new workflows and all, it’s great…\n\n**[1:31:05]** Alright, you guys, that’s the whole thing. If there’s nothing else now, we’ll probably end here. How many people are here? 28 people, huh? No, how many in this call right now? Preparing to spam ICY, is there an issue with transferring ICY yet? Everyone go random, grab it like ghosts, okay? Amount is 28, so we’ll drop 14 ICY tokens, entry is 14 already. Go ahead, duration is 5 seconds. Okay, let’s go. One ICY earlier was about 100 Satoshi already.\n\n**[1:32:09]** Now starting, don’t know when the boss updates the multiplier price, just estimate it for now. Today’s early, next time seeing Bitcoin, it looks cool. Happy Weekend, bye bye everyone.\n","title":"OGIF Office Hours #41 - ICY-BTC Swap, GitHub Bot, MCP-DB, Pocket Turing, Recapable, and Arbitrage Strategy","short_title":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","description":"In OGIF 41, the team covered key updates on the ICY-BTC swap, GitHub bot automation, MCP-DB system for agent workflows, and progress on the Pocket Turning and Recapable projects and we also shared insights into funding rate arbitrage strategies.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Thu Mar 20 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/41-20250314.md","slugArray":["updates","ogif","41-20250314"]},{"content":"\n### Topics and Highlights\n\n- **Team check-ins \u0026 workflow**: Kicked off with roll-call vibes, planning speed-run topics, and assigning tasks to Hải, Cường, and Tom. Encouraged quick 10-minute concept pitches.\n- **Frontend updates**: Hải’s January report covered React 19 and Next.js 15.1, spotlighting the View Transition API for smoother stage animations and Deno Deploy’s new server-side rendering support.\n- **Tooling \u0026 libraries**: Explored Transformer for running Python models in JS, Neon’s switch from Webpack to a lighter setup with better hot reloads, and HTMX’s rise with logic-in-HTML simplicity.\n- **Database design practices**: Cường recapped scaling databases with business growth, emphasizing DBA roles, migrations, CI systems, and versioning for managing schema changes and avoiding API breaks.\n- **AI-driven development**: Tom showcased a full-cycle approach, leveraging AI for rapid planning, task breakdowns, and proposals.\n- **Skillset spotlight**: Highlighted team strengths, security/performance (Thành), user/data flow (Tom), and how to align them with proposals, from MVP to real-time app concepts.\n- **Process optimization**: Detailed Tom’s AI-assisted workflow: extracting insights, crafting prompts, validating concepts, and scaling tasks with 90% accuracy, plus burning questions for client rapport.\n- **Q\u0026A \u0026 next steps**: Wrapped with open questions, a nod to future Tom-led sessions, and a promise to refine skills like real-time handling and proposal structuring.\n\n### Vietnamese transcript\n\n**[00:00]** Bắt đầu thôi nào. Chào mấy anh em, cảm ơn đã đợi. Thành với Cường đâu rồi? Cường có lên phòng chưa? Thấy đăng ký thứ Sáu mà giờ lên đây rồi, đúng không? Tuần này Thành đâu rồi? À, lên rồi, đứng đây nè. Tuấn, Tom lên stage luôn nha. \n\n**[04:51]** Đang xem mấy cái bài, tự nhiên cái link này Tom ơi, đẹp chưa? Để anh sửa lại. Ngày hôm nay 186 giao dịch, 1 user, 30 ICY member như cũ, 5 cái inactive, 1482 giả mạo. Hai channel chat nhiều nhất, ba channel chat nhiều nhất, mấy người chat nhiều nhất là ai? Ờ, tiêu rồi! Còn ai nữa không? Hôm nay thiếu ai không? Có hai chủ đề cũ: một cái là \"run and report\". Sáng nay anh post link lên rồi, chắc vậy, để kiểm tra lại. Cái thứ hai là bài design của Cường, anh chưa biết nội dung.\n\n**[06:03]** Bài này là cái gì vậy, ngồi nghe mà chẳng hiểu gì luôn. Bài số ba là nối tiếp cái series hôm trước, mấy anh em viết xong, làm xong, giờ nó thành hình cụ thể rồi. Qua 3 tháng thì team cũng có vài cập nhật mới, hướng đi này rõ ràng hơn chút. Hệ thống thấy cũ rồi, tí anh forward link cho mọi người đọc trước qua email.\n\n**[07:10]** Đăng ký dùng thử đi, tí nữa vào xem. Plan là vậy. Chắc ship bài của Hải trước, rồi tới bài của Cường, rồi tới bài của Tom, mấy phần Tom làm đó. Nội dung hôm nay chắc vậy. Anh em xem thử còn thiếu ai không, hay thấy ngắn quá, có gì liên quan nữa không? Ai thiếu vậy? Thành lên chưa? Ờ, đệ Thành đỉnh quá, hết việc để làm rồi. Anh cũng nghĩ vậy.\n\n**[08:55]**  Đợi chút nha, đợi đủ người rồi tụi mình speed-run mấy chủ đề này. Chủ đề cũng đơn giản thôi. Anh em cố gắng tóm gọn bài của mình, nói concept, idea trong 10 phút thôi, đừng dài quá, để dành thời gian cho buổi kia. Nếu cần hơn 10 phút thì nói dài hơn chút, vậy nha. Tuần sau có lịch lên văn phòng, tuần này check-in bình thường thôi. \n\n**[09:57]** Tuần sau dựa trên danh sách đăng ký, anh sẽ đề xuất với Huy Nguyễn làm trò điểm danh cho đủ mặt. Thành policy luôn rồi. Tuần sau làm điểm danh cho đông đủ. Đoạn tiếp theo thì mấy dự án cũ giờ gần xong hết rồi. Giờ dep blockchain với AI là vua của mọi nghề, anh em nào muốn làm trực tiếp thì phải lên kế hoạch cái đó. Có ai trùng gì không, hay còn ý gì nữa không?\n\n**[10:58]** Chắc bắt đầu với bài của Hải trước nha. Hải ơi, mời em trình bày. Dạ, mọi người thấy hình của em rồi đúng không? Em tóm tắt Frontend report tháng 1. Tháng 12 năm ngoái, React 19 release đi kèm với nó là thằng Next.JS 15.1 cũng tung ra một phiên bản mới.\n\n**[12:07]** Để hỗ trợ cả thằng Next.JS lẫn thằng React 19 luôn. Bên Reactthì em thấy nó đang làm một cái API khá hay, gọi là View Transition. Browser giờ đã có API View Transition này rồi, nhưng trước đây thì React chưa hỗ trợ. Một số thư viện đã viết và dùng cái API của bên kia, nhưng khi đưa lên React thì gặp vài vấn đề về performance. Ờ, tụi nó đang đợi API này từ React để hỗ trợ tốt hơn, giúp giải quyết vấn đề performance rõ ràng hơn. \n\n**[12:48]** API này dùng để làm animation khi chuyển giữa hai stage của trang web. Ví dụ như anh kéo xuống dưới đây, nó sẽ như ví dụ bên dưới này, cái stage đầu tiên là box nằm trên, stage thứ hai thì box nằm dưới. Thay vì chuyển stage mà nó nhảy thẳng xuống luôn, thì View Transition này hỗ trợ mình tạo hiệu ứng animation, nhảy qua nhảy lại các kiểu. Tương tự, với mấy cái như hình ảnh, nó cũng tạo hiệu ứng animation. \n\n**[13:28]** Khi chuyển đổi hình ảnh, thay vì chỉ nhảy sang hình khác ngay lập tức. Dạ, cái API này vẫn đang trong giai đoạn thử nghiệm thôi. Phải dùng phiên bản thử nghiệm thì mình mới xài được. Nhưng nó hứa hẹn sẽ tăng performance khi sử dụng. Vì trước đây, thằng Motion cũng đã hỗ trợ rồi, nhưng chỉ trong môi trường thuần thôi. Còn nếu lên thì nó gặp vài vấn đề performance, tại vì nó phải xử lý cả trước và sau khi set.\n\n**[14:07]** Cho cái phần này, bên SCS thì có mấy thứ như thằng Deno Deploy. Lúc trước nó chỉ hỗ trợ deploy static site thôi, nhưng giờ nó đã hỗ trợ hoàn toàn để deploy cả thằng Next.JS luôn, kể cả server-side rendering. Giờ mình có thể dùng Deno thay thế, hòa chung được, để deploy một ứng dụng NS. Dạ, cái này vẫn chưa có gì để nói hết. Còn cái thư viện Transformer Z này cũng khá hay. Bản chất của nó là đang biến mấy cái model. \n\n**[15:03]** Bản chất của nó là đang biến mấy cái model viết bằng Python lên thành JS, để mình có thể chạy trực tiếp mấy cái model này trên trình duyệt luôn, không cần qua API hay ngôn ngữ Python gì hết. Như trong bài này, nó chạy được cái sentiment testing. Ví dụ như positive hay negative, hoặc là object detection, như phát hiện con mèo. Bản chất thì em nghĩ mấy model khác, mấy cái pipeline khác, vẫn chạy được, miễn là nó được hỗ trợ bởi thư viện này.\n\n**[18:33]** Bọn em buộc phải hỗ trợ kiểu dù có mạng hay không, data vẫn phải lưu được hết. Sau đó chọn cách lưu xuống IndexedDB, rồi khi có kết nối trở lại, mới đẩy data lên server. Kiểu như vậy. Ở dưới đây nó có hướng dẫn step-by-step để xử lý. Làm vậy thì sẽ gặp vài vấn đề, như list data bị fail khi sync chẳng hạn. Nó chỉ ra một số cách để giải quyết mấy vấn đề đó.\n\n**[19:22]** Kiểu như vậy. An mới post link gì đó à? Zero là con gì? An mới bảo gì kìa, có liên quan không? Bữa trước thấy Lập, cũng bảo cái vụ \"local first\", chắc giống vậy đúng không? Mọi người chung bài toán, thi nhau đi giải. Tiếp theo, bên Win thì có nhắc. Bài này có update chút, giờ nó support thằng đó luôn rồi. Lúc trước Node.js thì phải có command line để combine thằng Typescript ra js mới chạy được. Còn giờ nó chạy trực tiếp luôn.\n\n**[20:01]** Như nó chạy bằng cái command line, load file luôn. Theo em thấy, còn một bài nữa về anh dec này, kể về chuyện các dependency ở bên MBM. Nó cứ ra version mới hoài, kiểu mỗi version lại kèm theo mấy cái breaking change. Ổng nói làm vậy khá cực, muốn update version nhưng sợ app không theo kịp. Không phải lúc nào cũng có thời gian để xử lý hết. Nên ổng không thích thằng React lắm, chọn hướng khác. Ổng bảo thằng này sẽ ổn định hơn, ít bị thay đổi như vậy. Ổng ưu tiên thằng này hơn. Thằng HTMX thì cũng nổi lên đang đứng top 1.\n\n**[21:07]** Dạ, còn một bài cuối nhanh về thằng Neon. Thằng này cung cấp dịch vụ về database. Nó vừa chuyển từ Webpack sang cái khác. Trong quá trình đó, nó gặp vài vấn đề, nhận ra một số hạn chế của Webpack. Như là nó không support tốt, có một danh sách dài những khó khăn ngay đây. Nhưng kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái mới ổn hơn Webpack. Thứ nhất, nó ít lỗi hơn, reliable hơn thằng Webpack. Thứ hai, config của nó đơn giản hơn. Như nó nói, chỉ cần mười mấy, hai mươi cái plugin của Webpack là làm cho nó nhẹ hơn nhiều. Em cũng không biết tại sao nó để vậy.\n\n**[22:03]** Nhưng mà cái kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái hot reload của nó ok hơn thằng Webpack. Nó ít kiểm khi bị full reload hơn thằng Webpack. Thứ hai là config của nó, nó simple hơn. Như nó nói là nó cỡ mười mấy, hai mươi cái plugin của Webpack gì đó, nó làm cho cái của nó nhẹ hơn nhiều.\n\n**[23:01]** Bài này nó chủ yếu là nói về những cái khó khăn và những cái kết quả cuối cùng khi mà nó chuyển từ Webpack sang cái kia. Dạ, vậy là cái của mấy anh em đang thay đổi à? Đang chuyển qua từ cái Webpack chuyển qua cái con kia là một đúng không? Cái React ở trên kia thì sao?\n\n**[23:50]** Chuyển qua HTMX hả? Là hai rồi, còn gì khác nữa không? Xài con Deno à? Với lại TP hả? TP thành main framework hả? Ừ, dạ, cho nó rồi. Còn mấy bài khác thì mọi người có thể đọc thêm trong cái này. Dạ, cái gì nhờ Hải post lại cái link nhé? Cảm ơn Hải, cảm ơn mấy anh em đã cho cái reply. HTMX nó là cái gì mà tại sao lại được chọn vậy? HTML nhưng mà có logic trong đó hả? Kiểu nó sẽ thêm một số thằng trực tiếp vô cái HTML, rồi dùng để trực tiếp ông lại chê nhau thôi. Cái trò này từ thời Backbone.js với lại Knockout.js. \n\n**[25:06]** Đây cả chục năm, giờ mới làm y chang vậy mà. Anh em có câu hỏi gì không? Cho một phút comment thêm. Có gì cần update thêm không? Có gì nhờ Hải post link vô, cho vô ngoài random hay vô group chat nhé. Mời bạn tiếp theo. Mời Cường đi nhanh qua chủ đề về database design. Dạ, bắt đầu luôn. Tiết học lịch sử hả? Cái này, cái bài mấy cái practice này là có từ 2017 rồi.\n\n**[26:17]** Em chỉ recap lại thôi à? Tổng kết hả? Tổng kết cái kỹ năng thiết kế dữ liệu, tip entity hả? Dạ, không, không hẳn là quản lý dữ liệu. Kiểu mấy cái practice để mà mình handle mấy cái kiến thức trong quá trình mình phát triển, mình grow cái database của mình lên. Dạ, em xin vô luôn. Database với lại cái hệ thống mà mình phát triển thì lúc nào cũng đi đôi với nhau. Khi mà phần mềm của mình scale up để bắt kịp cái business demand, thì mình bắt buộc phải scale up cái database của mình lên để quản lý số lượng lớn các.\n\n**[26:51]** Dữ liệu trải qua từng năm. Ví dụ như từ 2015, Amazon mới có khoảng 50 triệu dữ liệu, thì bắt đầu tới 2020 đã phát triển lên tới mức phải handle 200 triệu dữ liệu. Vậy tại sao cần phải có những cái practice này? Khi mà cái database của mình có tới cả trăm hoặc cả ngàn schema, thì cái management system như SQL Server hay mấy cái hệ thống quản lý dữ liệu khác, mình nhìn vào sơ đồ schema, table hay data thì không thể biết hết được tất cả.\n\n**[27:27]** Các cái context. Tại sao những cái change này đã được apply vào trong hệ thống? Để đúc kết ra được thì sẽ có một vài practice. Bắt buộc phải có sự kết hợp giữa con người và hệ thống để quản lý các kiến thức này. Tất cả những cái này chỉ là practice, không bao gồm việc lựa chọn hệ thống quản lý database hay thiết kế database schema. Nó bao gồm cách mà mình chia sẻ kiến thức database, lưu trữ những kiến thức này. Và khi những cái database change được boost lên thì sẽ có một hệ thống riêng để quản lý mấy cái change này, như continuous integration và những cái tương tự.\n\n**[28:02]** Đó là những cái change này sẽ bắt buộc phải follow một vài refactoring rules. Về no-sharing thì bình thường trong tổ chức của mình sẽ có một người gọi là DBA. Người này sẽ quản lý cũng như phải chia sẻ tất cả kiến thức và các sự thay đổi của database được apply vào hệ thống. Ví dụ, nếu mình có nhiều team dev, dev 1 khi phát triển phần mềm A, dev 2 quản lý phần mềm B, thì cả hai khi push change lên database của hệ thống sẽ phải hỏi qua người DBA. DBA này sẽ verify từng change xem nó có tác dụng gì, để quyết định cái change đó có make sense với database chính hay không.\n\n**[28:34]** Khi từng dev push cái database của mình lên, thì dev này sẽ verify với hệ thống chính để xem các API gọi đến database có bị ảnh hưởng gì không. Sau đó sẽ đánh giá cái change này có cần thiết không. Nếu cái change này ảnh hưởng quá lớn đến hệ thống, thì người DBA có thể reject cái change đó, bắt người dev phải update, refactor hoặc chỉnh sửa lại cho hợp lý. Khi cái change đã được approve, thì người DBA sẽ phải document lại rằng cái change này có ý nghĩa gì, tại sao cần cái change đó, rồi post một cái migration lên cho database master bắt đầu cập nhật.\n\n**[29:14]** Những cái dữ liệu này còn phải được lưu trữ ở một chỗ nào đó mà tất cả mọi người đều dễ dàng truy cập và tìm kiếm để biết tại sao những thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào một cái repository, giống như một coding project. Cái repository này chứa tất cả database artifact, bao gồm script chạy database, credential login, configuration, và mức độ dung lượng tối đa mà các instance này có thể quản lý, cũng như các documentation của hệ thống. Cái repository này cũng tương tự như một coding project, sẽ được quản lý bởi một version control.\n\n**[29:51]** Cũng như là tìm kiếm để biết được là tại sao những cái thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào trong một cái repository giống như một coding project vậy. Mọi người có gì hỏi thêm không?\n\n**[30:39]** Để mọi người có thể check, cũng như kiểm tra các cái change, context và history của những thay đổi này trong hệ thống, thì mỗi lần thay đổi, người push cái migration này sẽ tạo một cái pull request và thêm description. Description này giải thích tại sao cần cái change này, nó cần thiết ra sao, và những hệ thống nào sẽ bị ảnh hưởng bởi cái change đó. Người review, đa số là các dev của những API mà cái change này tác động trực tiếp tới, sẽ vào xem xét.\n\n**[31:14]** Sau khi những thay đổi này được merge vào nhánh master, sẽ có versioning để mình có thể rollback hoặc deploy các version này vào từng hệ thống để dev, testing, và cuối cùng là đưa lên production. Khi mà mình có nhiều dev instance giữa các version, thì lúc dev từng hệ thống riêng, mình sẽ phải checkout ra từ một instance của master database để sử dụng cho việc development. Như vậy, khi thay đổi gì đó hoặc migration một cái mới, mình không ảnh hưởng trực tiếp tới cái database chính.\n\n**[31:52]** Khi đó, mình cần có một hệ thống CI. Mỗi khi thay đổi gì trong instance mà mình dev, mình có thể dễ dàng verify xem cái change này có break master database hay không. Đồng thời, khi ai đó push một cái change mới lên master database, mình sẽ được thông báo về schema thay đổi hoặc resource conflict trước khi làm chậm tiến độ dev. Khi boost một thay đổi trên database, những thay đổi này bao gồm mấy bước như sau: thay đổi một cái database schema. \n\n**[32:25]** Khi push một thay đổi, mình phải tạo một migration script lên database đó. Sau khi script này được merge, mình phải đổi database access code để API có thể dùng cái change mới đó. Đối với những database change như thêm một column mới, thì có thể không nhất thiết phải thay đổi access layer của API khi change này được push lên. Vì một số API không cần dùng tới cột mới đó. Ví dụ, mình có bảng user với name và address, một service mới cần thêm field birthday vào bảng user, thì các service cũ như service gom nhóm user theo address không cần thay đổi gì trong API để tích hợp cái change mới này.\n\n**[33:07]** Đối với những change ảnh hưởng lớn, như giới thiệu một non-null value hay tách bảng, thì tất cả service phụ thuộc vào nó cần phải đổi data access layer để tránh lỗi. Ví dụ như bảng user vừa nãy, nếu tách bảng user ra, thì service nào dùng bảng đó phải thay đổi toàn bộ access layer để không bị lỗi. Ngoài ra, có thể dùng một cái gọi là transition interface để dần dần apply các thay đổi mới, rồi boost cái change đó mà không làm crash API cũ.\n\n**[33:45]** Sau khi đã refactor và apply change lên master database, mình còn phải notify tất cả các service dùng database này để tránh break mấy cái API đó. Đồng thời, mọi người có thể contact nhau để resolve config khi thay đổi master database. Về phần recap, trong quá trình develop một software, khi phần mềm phát triển thì bắt buộc database của mình cũng phải phát triển theo. Để mọi người đều nắm được thông tin và context của từng cái change trong database này, cần vận dụng tất cả kiến thức để chia sẻ và sắp xếp kiến thức của mình.\n\n**[34:32]** Đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian, mọi người resource conflict giữa các cái database change. Bài này thấy nó có giá trị ở chỗ góc nhìn. Chắc là giống như góc nhìn dev, nhưng mà nó đứng góc nhìn về chuyện là thay đổi đối tượng làm việc chính.\n\n**[35:21]** Thông tin và cũng như context của những từng cái change bên trong database này thì cần phải vận dụng tất cả những kiến thức để mà know sharing cũng như là sắp xếp các cái kiến thức của mình và đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian mọi người resource conflict giữa các cái database change. Hết rồi. Mọi người có gì hỏi thêm không?\n\n**[35:58]** Là không phải codebase mà là cái database đúng không? Theo hướng đó nhiều. Cứ nghe tới đoạn này thấy hơi meta quá, kiểu hệ thống lớn chắc mới quan tâm, còn hệ thống như hiện tại thì hơi khó áp dụng hả? Khoảng hệ thống cỡ 20 table là thấy hơi lâu lâu, nhìn vô cũng hơi chóng mặt rồi. Đúng rồi. Vậy cái này liên quan tới chuyện documentation, quản lý versioning, với cả làm monitoring. Không phải version monitoring, mà là notification cho mấy cái team khác đúng không? Dạ, vậy nó còn ít lắm, nhưng mà đúng rồi. Mấy cái này đưa vô thì hợp lý, vì có góc nhìn.\n\n**[36:50]** Quản trị data trước tới mấy cái kia. Logic thì logic ở đây, mai mốt data chạy rồi. Anh em có hỏi gì Cường không? Không thì sẽ kết thúc ở đây. Bài này có giá trị về góc nhìn. Nghĩ mấy anh em khi làm backend mà muốn làm giàu thì sẽ phải theo dự án suốt đời. Dự án càng lâu thì nghĩa là dự án càng có tiền. Thấy vậy, đi được với dự án càng lâu thì về bản chất nó sẽ ok. Nhưng mà thường dev thì nó sẽ lười. Dev thấy cái gì mà làm lâu quá thì bị chán, hành vi rất là lạ. \n\n**[37:40]** Trước khi qua bài tiếp theo, để đóng góp cho buổi hôm nay, một cái keyword của tuần này, trong quá trình đi ngồi đọng lại, có một keyword mới, mới học được. Từ mới dành cho những bạn chưa biết, giống như anh chưa biết. Đây là kiểu hôm nay lên, có một trường phái tên là Luddism. Luddism là một cái chữ xuất thân từ thế kỷ 19, khi cuộc cách mạng công nghiệp diễn ra. Ngành những ngành liên quan tới dệt may được tự động hóa, thì cái đó tức là những người theo có cửa, họ tên là Luddites sao á, mới đi đốt mấy cái máy đó.\n\n**[38:34]** Mấy cái máy đó cướp việc của mình, cướp chén cơm của mình, nên họ đi phá mấy máy đó. Thành ra cái này trở thành một trường phái Luddism. Tức là tầng lớp working class đi chống lại xu hướng hiện đại hóa. Rồi chữ khóa tiếp theo đi sâu tiếp thì sẽ ra Neo-Luddism, với lại cái thằng Luddism ngay đây. Cái gì anh em đọc thêm nhé, thấy khá là relevant với mình sắp tới. Theo những dự đoán mà hôm trước.\n\n**[39:18]** Mình ngồi nói với nhau á, thì sắp tới chắc sẽ nhiều người dậy lắm. Ở trên Reddit thì nó có một cái bài cách đây 2 năm, có cái làn sóng Neo-Luddism mới sẽ xuất hiện. Giờ lên thấy cũng nhiều lắm ha. Thì cố gắng, góc nhìn anh thì cố gắng, anh em không nên, không nên theo trường phái này. Hồi phát triển sẽ đi tiếp, không nên chống lại bánh xe lịch sử. Rồi có luôn cái subreddit tên là Luddism luôn, nói từ Luddism luôn. Không chỉ nói về automation, mà nói về đủ thứ trả lại công nghệ trên đời. Cảm giác lạc lỏng, cảm giác thế này thế kia. Đây là keyword khá là thú vị, ha, anh em.\n\n**[40:08]** Không bị dính vào đây ha. Rồi cái số hai nữa là có cái liên quan đến cái này. Thằng vừa rồi mới ngồi, mới ngồi tìm ra này, đó là U.S. geopolitical. Có một góc nhìn về chuyện nước Mỹ phát triển như thế nào. Anh nghiên cứu về thị trường vốn, có cái dòng tiền đầu tư nó chảy đâu, nên vô tình lọt vô cái chủ đề này. Đây là chủ đề thứ hai, thấy cũng khá thú vị. Maybe anh em sẽ quan tâm. Chủ đề này liên quan tới macro economy. Thì ra nó được, từ cái trị nó chuyển qua thành macro economy.\n\n**[40:56]** Nước Mỹ sẽ có xu hướng có hai phái thôi. Một là isolationism, tức là cô lập hóa. Một chữ khác thể dùng cái đó, tính làm đây ha. Thì trong cái movement này, nó nói gì? Nước Mỹ sẽ có xu thế là nó co mình lại, không deploy mấy cái resource đi khắp nơi để giao thương nữa, mà gom cái đó về, đứng đó phòng thủ. Đây là cái cụm thứ nhất. Hiện tại, tất cả những tin tức mình thấy được á, thì nó đang trong cái đó, protectionism hoặc là isolationism. Cụm này, hướng thứ hai mình thấy là globalization.\n\n**[41:41]** Globalization thì những cái sáng chế, những cái công việc sẽ tập trung vào chuyện trading với nhau nhiều hơn, giao thương nhiều hơn. Nước này nước nọ quăng những cái đó đi khắp nơi. Mỹ sẽ có xu hướng là out ra ngoài, những anh chị em theo cái phái đó. Những cái nước theo cái phái đó cũng sẽ có xu hướng cởi mở hơn, chạy khắp nơi. Thì nó là cái tình trạng trong trạng thái mà nó diễn ra từ report, từ năm 45 tới gần đây, thì đã đi thành những cái cụm nhỏ. \n\n**[42:20]** Trong giai đoạn sau chiến tranh với Nhật, đi nút cho Nhật hai cú xong rồi, thì giúp Nhật với Đức sau Chiến tranh. Nó sẽ giúp tái thiết lại, thì bắt đầu nó deploy, nó globalization theo hướng đó. Đó là cái phase ban đầu. Nó bắt cái đoạn đó, đến khi mà Nhật mạnh quá rồi phải không, thì bắt đầu sẽ bị nerf lại bằng một số sự kiện nhất định. Ở đây có sự kiện này, với cả sự kiện tên là VIA này, ra thông tin hơn. Nhưng cơ bản là vậy. Thì idea chính là gì? Idea chính là đang có cái xu hướng học từ lịch sử trước đây.\n\n**[43:09]** Từ cái Great Depression năm 1930 cho tới giờ, hiện nay 2020, có một cái nước Mỹ đang trở lại với trường phái protectionism. Sẽ dẫn đến tất cả những nước khác cũng sẽ đi theo cái này. Ai cũng sẽ là dân tộc mình là cái chính. Thì cái chuyện mà mình nhảy khắp nơi sẽ ít lại hơn, so với giai đoạn này. Đường đỏ là đường Trung Quốc nè, đường màu này là đường của Nga nè. Nga sau năm 91 cũng được buff xong rồi, nó đi, nó quất Crimea, cái bị nerf lại. Hiện đang tới Trung Quốc. \n\n**[43:54]** Mà cái này sẽ ảnh hưởng gì? Tới thế sẽ ảnh hưởng là thị trường thì nó sẽ khó khăn. Theo cái hướng nó sẽ favor một số nước nhất định. Không biết Việt Nam, Việt Nam hiện nay trong top 4 mấy cái nước có delta import-export với Mỹ vẫn cao, nhưng mà vẫn được buff. Không biết có được ăn nhậu gì không, nhưng về cơ bản thì mọi người sẽ chạy chậm với tiền mình hơn. Thì hai cái hướng chính nè. Một hướng là công nghệ nó ra, nó replay liên tục để trường hợp mà cái cụm từ này lại được gọi tên lần nữa. \n\n**[44:35]** Với cả cái xu thế về kinh tế toàn cầu đang dày, anh đáng là nó sẽ đi kèm với cái gì mình từng nói với nhau. Thị trường càng ngày càng khó tính, được proven qua cái này ha. Dễ dàng thấy với mình thì mình sẽ phải behave như thế nào. Mời Tom lên show hàng những kỹ năng của Tom. Anh nghĩ là mấy anh em trong team sẽ cần đấy. Anh em trong team mình sẽ cần những kỹ năng mà Tom nó được từ cái làm việc với ai ha. \n\n**[45:31]** Trong team mình hiện tại á, có một số cái mình không nói về hướng phát triển của software nữa nha. Cái đó thì nói với nhau suốt rồi. Nhưng mà trong quá trình làm việc với Tôm, anh nhận ra Tôm có một kỹ năng rất hay. Đó là gần như nguyên cái life cycle của chuyện làm phần mềm, một mình Tôm gần như dùng khả năng viết code, tự động hóa bằng tool, tự mình viết agent luôn. Thì gần như cả quá trìn h từ dev ban đầu, capture cái insight dự án, xong rồi lên planning.\n\n**[46:07]** Cả các thứ, Tom xử lý rất OK. Nên hiện tại anh muốn em show một tí [âm nhạc] về cái approach của em trong quá trình làm việc. Khi em nhận được đề bài cho tới lúc em đến cái planning của em, nó như thế nào, em đã làm ra sao? À, OK, chắc để em share screen. Hy vọng không có gì nhạy cảm. Anh nghĩ là mình lấy luôn cái đề bài mà tí nữa mình sẽ đi sâu, sẵn đó. Ô, đề bài chơi cái đó luôn đó. Mình đang không biết là cái gì đó, mình cũng chưa đi sâu luôn. Thì giờ cái phong hợp nhất đang gần như là zero.\n\n**[46:55]** Nó có một cái ví dụ về đề bài thôi đấy, cho tới lúc mà em đến cái kia như nào. OK, để em share screen, tìm lại cái chỗ đó, đúng không? OK, thông thường, logic phía em là như thế nào? Mình có data, mình muốn gỡ ra những ý của cái data này. Nếu mình có mấy cái ảnh này, thì ví dụ em sẽ cởi hết chỗ này, sau đó extract ra. Cái app này nó có những cái gì mình sẽ phải để ý. OK, sau đó là những direction mình muốn cho nó, giải thích cho mình. Vì luôn luôn là có thể mọi người xem cái app này.\n\n**[47:59]** Có thể là Airbnb, hoặc là dạng app cho personal trainer và lifestyle trainer. Thì ví dụ ở đây, em muốn tìm kiếm kiểu \"What the hell\", thì trước tiên em sẽ sắp xếp một cái prompt. Một là, nếu context là bây giờ em just about to have a meeting with client that asks us to improve their user experience. Sau đó là ý context của bên ngoài, rồi context của bên mình là \"I have some idea of what they may want\". Câu hỏi là có cần input luôn cả cái brief của cái đưa mình không? Ở đây không có. Sau đó là, this chính là cái này.\n\n**[49:08]** Nó sẽ là objective, adjective, context. This is the email they sent to us. Sau đó, em muốn cái vision chính là \"What is the vision, goals, and objectives for them asking us to help improve?\". Từ cái này, em sẽ sinh ra một số context em dùng để gửi lại cho bên phía AI. Thực tế thì cái này nó chắc em làm rồi chứ? Ờ, cái đứng ra là model nào cũng được, nhưng thinking model sẽ giúp mình kéo ra những góc nhìn mà mình không phát hiện ra. Những thinking model rất là siêu về mấy cái đấy.\n\n**[50:15]** Thì cũng hơi functional, user app-centric. Từ cái context này, em sẽ biết app nó là gì, sau đó hình dung cái vision họ đang muốn cần là cái gì. À, chính là có cái gì chi tiết hơn về user, user experience. Thì như vậy, em sẽ hỏi câu hỏi là \"What images, what bọn này muốn?\". Bọn này không muốn gì đâu, bọn này đang muốn là sẽ clone cái app này, chứ không phải là improve cái app này đâu. Cái app đã có sẵn rồi, và giờ nó muốn clone lại, mirroring đúng không?\n\n**[51:07]** Cái này là một cái đã có sẵn, lại mình làm. Với cái chuyển trường hợp này thì sẽ sinh ra một số câu hỏi như \"Are there ways their app is extending to? What are your thoughts?\". Sau đó, dần dần em xây dựng một cái picture. Từ cái picture này, sẽ sinh ra một cái prompt model cuối cùng để gửi cho bên phía làm cho mình. Ví dụ là tiếp tục về một cái dự án đã proven model shortcomings. Như vậy mình sẽ có một số cái mình phải chú ý. Chú ý bên phía mình sẽ phải thử bằng tay những cái gì nhỉ? \n\n**[52:15]** Chi tiết về concept validation này. Chính là cái gì nó work rồi dùng cái đó thôi. Concept như vậy thì em sẽ làm một cái prompt là \"Give me a proposal to pass on what I learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what the story is đằng sau họ đang muốn cái gì, and what they want us to consult, develop?\". \n\n**[53:23]** Về cái chuyện proposal này nó sẽ ra dạng như thế nào,  sau đó từ cái này, vì mỗi thứ mình dùng với AI nó sẽ có reference sẵn rồi, em sẽ copy một cái reference mình có sẵn. Là cái proposal đã làm sẵn, ví dụ trước là cái này. Sau đó là mình sẽ copy cái proposal của bên phía chẳng hạn đi, nhân đi, đi này đi nhỉ. Hình như là hình như internet đâu á? À, chắc copy nhầm này. Đúng ra là mọi người có thể ra cái này, hoặc là download. Use reference to create the proposal, or just in case, don’t take elements. \n\n**[55:16]** From but do follow the proposal format để adapt to what we learned and what they wanting to meet the trust. Sau đó, luôn luôn là mình sẽ expect cái proposal này nó không ổn định. Nó sẽ ổn định lúc mình bỏ thêm những idea, những idea mình thấy là mình có thể involve bản thân mình vào. Vì do mình đang xem khía cạnh của họ là dạng như thế này, thì bên phía mình sẽ làm được cái gì? Ví dụ skillset bên phía em khuyến khích là giỏi về user experience, user flow, data flow. Trong này mình có Mirror được cái app và optimize cái data flow, user flow chẳng hạn.\n\n**[56:10]** Hoặc là bên phía anh Thành là optimize về security và performance. Làm như thế nào để apply đúng cái project proposal này? Thêm về mấy cái kiểu good-to-have: performance và security. Nếu là dạng MVP thì mấy cái này sẽ không consider mấy chuyện hack. Là những cái design liên quan với data. Ví dụ bên phía em thì hay thiết kế data dạng là temporal state, event store, hoặc là thiết kế uniform.\n\n**[56:51]** Như thế nào để apply đúng kỹ năng của mình trên computer science về cái này? Cho nó không phải đơn giản quá, nhưng sẽ simplify, maintain cho cái chuyện cái app này nó đưa ra. Nếu mà trên đây với cái tham khảo này đi, bây giờ sẽ tới kiểu anh sẽ cần mấy cái để chốt được cái deal đúng không? Mình sẽ phải cần những câu hỏi để hỏi xem với bọn đó như thế nào. Giống như con open deal, mình open book á. Mà mình nói đi thì phải cần mấy câu hỏi đấy nữa, kèm với chuyện gần như phải suggest được cái lịch làm việc, cái milestone làm việc tiếp theo giữa mình với bọn đó.\n\n**[57:28]** Cần mấy câu hỏi đấy nữa, kèm với chuyện là gần như phải. Em làm như nào? Building rapport, sau đó là xem về burning questions chúng nó. Thì nếu mình có chuyên về nghề của mình, thì mình sẽ suy ra mấy cái câu hỏi cũng không khó lắm. Nhưng nếu mình thấy là mình hơi bị stuck, mình có cái block gì đó, thì mình sẽ nhờ AI cho hỏi mấy cái question. \n\n**[58:14]** \"So we haven’t met with this partner yet, with this client yet, but we want to make a deal with them. What should I do to help build rapport and meet the three burning questions I need to get this deal off the ground and solve any technical concerns?\". Thì cái này là good start, mình sẽ dùng cái này cho bên phía AI suy ra một số câu cho mình. Sau đó mình sẽ dựa trên cái này suy ra thêm. Nếu mình có suy ra thêm thì mình sẽ bổ sung thêm ở trên proposal và add thêm cũng realistic thôi. Không phải riêng bên phía Gemini, nhưng có một số app như Claude hoặc là ChatGPT, mình sẽ phải làm như thế nào. \n\n**[59:12]** Những cái due diligence mình sẽ phải làm như thế nào? Những cái burning question, ví dụ ở trên này mình không có context của trước, thì dùng đi. Nó kiểu như thế ngoài đó. Mình muốn đặt mấy cái goal như vậy, đứng ra là ở trên cái proposal đầu tiên, mình đang hơi nghi ngờ là mirroring là tại sao họ mirror? Nó sẽ hở ra ở trong cái intent của cái proposal đầu tiên mình xây dựng cho họ. Nên là nó sẽ liên quan với cái này. Lúc mình có thêm không nhất thiết. \n\n**[59:50]** Sẽ dùng luôn cái này, nhưng từ cái này em sẽ suy ra là, à, maybe góc nhìn về handling real-time thì sao? Maybe bên phía họ thì không phải real-time, nó sẽ kiểu như booking appointment app. Và nếu mình ghi về dạng real-time, họ có muốn đi hướng vision đó không? Để đem ra consult xem là họ muốn cái app nó kiểu đẹp hơn, ổn hơn, hay là họ muốn cái mới hơn, hoặc kiểu risky hơn? Nó sẽ là mấy cái step mình hỏi, mình chém, để xem họ reply như thế nào thôi. Và nó không có hại.\n\n**[01:00:34]** Vì nó cũng là câu hỏi hợp lý mà. Rồi, ví dụ như bước tiếp theo dev này nó hit đi, thì sau đó cái đoạn mà lên to-do rồi, kể mọi thứ thì như nào? Dạ, dạ, nó cứ hình dung. Em có một số cái cứ hình dung là cái điều này đã OK rồi. Sau đó em bỏ sung cái technical direction mình đồng ý để đi tiếp theo với họ. Ví dụ là real-time đi, \"We think they want something like this, but are open to the idea of a more real-time something like Grab, Uber for the personal trainer\". Trước tiên em sẽ xây dựng cái Technical proposal.\n\n**[01:01:33]** Như chắc không cần đâu, thông thường em sẽ xây dựng cái đó để làm rõ góc nhìn. Nhưng từ khía cạnh này, thì ví dụ là \"Help me create tasks for frontend, backend\". Tại vì cái đoạn giữa mà Tôm em sẽ figure out ra tất cả mấy cái diagram, flow, rồi tất cả mọi thứ. Phải chốt cái đấy trước, mới base cái đấy bắt đầu làm cái breakdown đúng không? Nên để đơn giản hóa hôm nay mình sẽ nhờ bên phía AI suy ra luôn.\n\n**[01:02:11]** Cái này nó là một cái góc sơ sơ, nhưng mình sẽ bổ sung thêm là \"We are planning to use Timescale and RxJS to do the sync and part real-time features of the app. We are most comfortable with React for frontend, and our house mostly uses all this in mind. Create and format tasks with description, user story, and acceptance criteria\". Mình sẽ nhờ bên phía AI viết giúp mình cái này luôn. Sau đó, nếu mình dùng thì mình sẽ copy cái copy epic là cái gì, copy story là cái gì, copy cái story, sau đó bỏ xuống cái criteria.\n\n**[01:03:44]** Cái này thì bên phía em thì làm thêm cho về cũng là cho bản thân. Vì ở đây đang là story, giải thích cái story, xử lý cái story. Lúc mình đến technical, technical nó chỉ cần confirm là nó có đạt đúng tiêu chí của story không. Vì nếu story đó nó tồn tại chung với cái vision của họ, coi như mình làm thành công bên phía họ rồi phải? Nhưng mà dự như cái sườn này là bắt đầu scale lên được một cái chất.\n\n**[01:04:23]** Chờ cho tất cả những cái liên quan cho backend. Thông thường trong technical proposal hoặc là cái context, em sẽ bỏ xuống thêm boilerplate, những cái code mình đã dùng rồi, những cái concept mình muốn apply ở trên cái app này. Với goal chính là goal của mình dựa trên goal của họ. Copy bên phía họ thì nếu có cái lúc có cái đấy xong, sau đó xây dựng mấy cái test này, thì sẽ có đầy đủ để mình breakdown đúng cái task mình cần thiết nhất. Ờ, đúng là nó sẽ độ chính xác tầm 90 phần trăm, nhưng 10 phần trăm còn lại nó sẽ bị thừa.\n\n**[01:05:02]** Nhưng mà đỡ hơn là mình bắt đầu ở chỗ kiểu zero đúng không? Rồi, chắc tới đây thôi. Giờ Tom anh bắt đầu có con, với lại khách hàng thật rồi. Tí nữa giao hết cho Tom nhé. Nay chốt tới đây thôi bạn ơi. Đây là nghĩa là bước đầu tiên để show được quá trình làm phần mềm á. Nếu mà mình có một kỹ năng mềm tốt, với lại capture được cái domain và tất cả quá trình làm việc á, có thể leverage AI rất là nhiều để mà quá trình làm ra một. \n\n**[01:05:39]** Người ban đầu lúc trước, một cái quá trình như vậy sẽ tốn khoảng 2 ngày, 3 ngày, 4 ngày gì đấy. Giờ quá trình làm xong, soạn rồi, vẽ diagram rồi, present cái idea, những hệ thống kiểu cũ á, nó nhanh rất là nhiều ha. Nên khi xong là đây là một cái skill trong team mình, Tom đang ở mức độ này. Ờ, mà Tôm đang tự tin là nó đang khoảng bao nhiêu phần trăm hả anh? Anh không rõ lắm. Mà anh nghĩ chắc đâu đó, chắc sẽ trên 50 phần trăm ha, trên 50 bé hơn 90. Hy vọng là những cái bước về sau thì sẽ có những buổi sau.\n\n**[01:06:21]** Mình lại làm thêm vài buổi với Tom. Còn giờ chắc là tạm thời dừng ở đây. Các câu hỏi có liên quan thì anh em sẽ hỏi sau. Giờ anh đây. Bye bye, hẹn gặp lại mấy anh em nhé.\n\n---\n\n### English transcript\n\n**[00:00]** Let’s get started. Hey everyone, thanks for waiting. Where are Thành and Cường? Has Cường joined the room yet? I saw he registered for Friday, but he’s up here now, right? Where’s Thành this week? Oh, he’s here, standing right there. Tuấn, Tom, hop on stage now.\n\n**[04:51]** We’re going through some articles, and suddenly this link, Tom, isn’t it great? Let me fix it. Today’s stats: 186 transactions, 1 user, 30 ICY members as usual, 5 inactive, 1482 fakes. Which are the top two most active chat channels? The top three? Who’s chatting the most? Oh, we’re in trouble! Anyone else around? Are we missing someone today? There are two old topics: one is “run and report”, I posted the link this morning, I think, let me double-check. The second is Cường’s design piece; I don’t know the details yet.\n\n**[06:03]** What’s this one about? I’m sitting here listening and totally lost. The third piece follows up on the series from before you guys wrote it, worked on it, and now it’s taken solid shape. After three months, the team’s got some small updates, and this direction’s getting a bit clearer. The system feels outdated, though; I’ll forward a link later for everyone to review via email.\n\n**[07:10]** Sign up and try it out, we’ll dive in later. That’s the plan. We’ll probably ship Hải’s piece first, then Cường’s, then Tom’s the parts Tom worked on. That’s today’s content, I think. Guys, check if anyone’s missing or if it feels too short. Anything else related we should add? Who’s not here? Has Thành joined yet? Oh, bro Thành’s on fire out of work to do. I think so too.\n\n**[08:55]** Hold on a sec, let’s wait till everyone’s here, then we’ll speed-run these topics. They’re pretty straightforward. Try to sum up your piece of concept, idea, n 10 minutes max. Don’t go overboard so we can save time for the other session. If you need more than 10, stretch it a bit, alright? Next week’s the office schedule; this week’s just regular check-in.\n\n**[09:57]** Next week, based on the sign-up list, I’ll suggest to Huy Nguyễn we do a roll-call game to get everyone in. It’s basically policy now, full attendance next week. The next part: those old projects are nearly wrapped up. Now it’s all about deploying blockchain and AI, they’re the kings of the game. Anyone wanting to work on them directly needs to plan it out. Any duplicates? Any more ideas?\n\n**[10:58]** Guess we’ll start with Hải’s piece first. Hải, go ahead and present! Uh, everyone can see my visuals, right? I’ll summarize the frontend report for January. Last December, React 19 dropped, and alongside it, Next.js 15.1 rolled out a new version too.\n\n**[12:07]**\n\nTo support both Next.js and React 19. On the React side, I see they’re working on a pretty cool API called View Transition. Browsers already have this View Transition API, but React didn’t support it before. Some libraries have built on that external API, but when integrated into React, they hit a few performance snags. Yeah, they’re waiting for React’s version of this API to improve support and tackle those performance issues more cleanly.\n\n**[12:48]** This API’s for animating transitions between two stages of a webpage. Like, if you scroll down here, it’s like this example below. The first stage has the box up top, the second stage has it below. Instead of the stage just jumping straight down, View Transition helps us create an animation effect, sliding back and forth smoothly. Same deal with images, it adds animation effects too.\n\n**[13:28]** When switching images, instead of instantly jumping to the next one. Yeah, this API’s still in the experimental phase. You’ve got to use the experimental version to try it out. But it promises a performance boost when implemented. Before, Motion supported this, but only in a vanilla environment. When scaled up, it ran into some performance hiccups because it had to handle pre- and post-set states.\n\n**[14:07]** On this front, over at SCS, there’s stuff like Deno Deploy. It used to only support static site deployment, but now it fully supports deploying Next.js too, including server-side rendering. Now we can use Deno as a replacement, blending it in to deploy an NS app. Yeah, nothing much to say on that yet. Then there’s this Transformer Z library, pretty neat. At its core, it’s about converting models.\n\n**[15:03]** At its core, it’s about converting models written in Python into JavaScript, so we can run these models directly in the browser without needing APIs or Python itself. Like in this article, it can handle sentiment testing say, positive or negative or object detection, like spotting a cat. Essentially, I think other models or pipelines can work too, as long as they’re supported by this library.\n\n**[18:33]** We had to support a setup where, network or not, all data still gets saved. So we chose to store it in IndexedDB, then push it to the server once the connection’s back. That’s the gist of it. Down here, it’s got step-by-step instructions for handling it. Doing it this way runs into a few issues, like data lists failing during sync, for example. It points out some ways to tackle those problems.\n\n**[19:22]** Something like that. Did An just post a link or something? What’s Zero? What did An just say related or not? The other day, I saw Lập mention this “local first” thing probably the same deal, right? Everyone’s tackling the same problem, racing to solve it. Next up, there’s a mention from the Win side. This one’s got an update, it supports that thing now. Before, with Node.js, you had to use a command line to compile TypeScript into JS to run it. Now it runs directly.\n\n**[20:01]** Like, it runs straight from the command line, loading the file as-is. From what I see, there’s another piece about this dev guy, talking about dependencies at MBM. They keep dropping new versions, and each one comes with breaking changes. He says it’s a pain, wants to update versions but worries the app can’t keep up. There’s not always time to fix everything. So he’s not big on React, went a different route. He says this one’s more stable, less prone to constant shifts. He prefers it over the others. Meanwhile, HTMX is popping off, sitting at number one.\n\n**[21:07]** Yeah, one last quick bit about Neon. This one’s a database service provider. They just switched from Webpack to something else. During the process, they hit some snags and realized Webpack’s got limitations. Like, it doesn’t support things well, there’s a long list of issues right here. But the end result after switching? They feel the new setup beats Webpack. First, it’s got fewer bugs, more reliable than Webpack. Second, its config is simpler. They say with just a dozen or two Webpack plugins, it makes their setup way lighter. I’m not sure why they went with that.\n\n**[22:03]** But the final outcome after the switch is they think its hot reload is better than Webpack’s. It triggers fewer checks during full reloads compared to Webpack. Second, its config is simpler. Like they said, with about a dozen or twenty Webpack plugins or so, it keeps their setup much lighter.\n\n**[23:01]** This piece mostly covers the challenges and the final results of switching from Webpack to that other thing. So, does that mean what we’re working on is shifting too? Are we moving from Webpack to this new one as well? What about that React stuff up there?\n\n**[23:50]** Switching to HTMX, huh? That’s two now., what else is there? Using Deno? And TP, is that the main framework now? Yeah, alright, it’s in. For the other articles, you guys can check them out in here. Uh, what was it—Hải, can you repost that link? Thanks, Hải, and thanks, everyone, for the replies. What’s HTMX, and why’d it get picked? HTML with logic baked in? Like, it injects some stuff straight into the HTML and uses it to handle things directly, no fuss. This trick goes back to Backbone.js and Knockout.js days. \n\n**[25:06]** A decade ago, and now they’re doing it the same way again. Any questions, guys? One minute for extra comments. Anything need updating? If there’s something, Hải, toss the link in random channel or group chat, whatever. Next up. Let’s move quick to Cường’s topic on database design. Alright, starting now. History lesson, huh? This stuff, these practices, they’ve been around since 2017.\n\n**[26:17]** Just a recap, right? Summing it up? Summing up data design skills, entity tips? Nah, not exactly data management. More like practices for handling the knowledge as we build and scale up our database. Alright, I’ll dive in. The database and the system we’re developing always go hand in hand. When our software scales up to meet business demands, we’ve got no choice but to scale the database too, to manage a huge amount of data over the years.\n\n**[26:51]** Take Amazon: in 2015, they had about 50 million data points, then by 2020, it grew to needing to handle 200 million. So why do we need these practices? When your database hits hundreds or thousands of schemas, management systems like SQL Server or other data management tools, you look at the schema diagrams, tables, or data, and you can’t possibly grasp it all.\n\n**[27:27]**\n\nThe context why were these changes applied to the system? To boil it down, there are a few practices. It’s gotta be a combo of people and systems to manage this knowledge. All of this is just practices not about picking a database management system or designing the schema itself. It’s about how we share database knowledge, store that knowledge. And when database changes get rolled out, there’s a separate system to manage those changes like continuous integration and stuff like that.\n\n**[28:02]** Those changes have to follow a few refactoring rules. Regarding no-sharing, we usually have someone called a DBA in our organization. This person manages and shares all the knowledge and changes applied to the database within the system. For example, if we’ve got multiple dev teams, say Dev 1 working on Software A and Dev 2 on Software B, both need to check with the DBA when pushing changes to the system’s database. The DBA verifies each change to see what it does and decides if it makes sense for the main database.\n\n**[28:34]** When a dev pushes their database changes, they verify with the main system to check if the APIs calling the database are affected. Then they assess whether the change is necessary. If it impacts the system too heavily, the DBA might reject it and ask the dev to update, refactor, or adjust it to fit better. Once the change is approved, the DBA documents what it means, why it’s needed, and posts a migration for the master database to start updating.\n\n**[29:14]** That data also needs to be stored somewhere everyone can easily access and search, so they understand why these changes matter. All these changes go into a repository, much like a coding project. This repository holds all the database artifacts, including scripts to run the database, login credentials, configurations, and the maximum capacity these instances can handle, plus system documentation. It’s similar to a coding project and gets managed with version control.\n\n**[29:51]** And searchable, so we know why these changes are necessary. All those changes get stored in a repository, just like a coding project. Any questions, guys?\n\n**[30:39]** So everyone can check and review the changes, their context, and history in the system, each time a change happens, the person pushing the migration creates a pull request with a description. This description explains why the change is needed, how essential it is, and which systems it’ll affect. The reviewers, mostly devs from the APIs directly impacted by this change, step in to take a look.\n\n**[31:14]** After those changes get merged into the master branch, there’s versioning so we can rollback or deploy these versions to individual systems for development, testing, and finally production. When we’ve got multiple dev instances across versions, and we’re working on separate systems, we have to check out from an instance of the master database for development use. That way, when we tweak something or add a new migration, it doesn’t directly mess with the main database.\n\n**[31:52]** At that point, we need a CI system. Whenever we change something in the instance we’re developing on, we can easily verify if the change breaks the master database. Plus, when someone pushes a new change to the master database, we get notified about schema updates or resource conflicts before it slows down our dev progress. When rolling out a database change, it involves a few steps, like modifying a database schema.\n\n**[32:25]** When pushing a change, we have to create a migration script for that database. Once the script’s merged, we update the database access code so the API can use the new change. For database changes like adding a new column, it might not always require tweaking the API’s access layer when the change goes live, since some APIs don’t need to touch that new column. For instance, if we’ve got a user table with name and address, and a new service needs to add a birthday field to the user table, older services, like one grouping users by address, don’t need API changes to integrate this new update.\n\n**[33:07]** For changes with big impacts, like introducing a non-null value or splitting a table, all dependent services need to update their data access layer to avoid errors. Take that user table from earlier, for example. If we split the user table, every service using it has to overhaul its access layer to prevent bugs. Alternatively, we could use something called a transition interface to gradually apply the new changes and roll them out without crashing the old APIs.\n\n**[33:45]** After refactoring and applying the change to the master database, we still need to notify all services using this database to prevent breaking those APIs. At the same time, folks can coordinate to resolve config issues when the master database changes. For a recap, during software development, as the software grows, the database has to grow too. To keep everyone in the loop about the info and context of each database change, we need to leverage all our knowledge to share and organize it effectively.\n\n**[34:32]** Also, all these changes have to be released thoroughly to avoid timing conflicts or resource clashes between database updates. This piece has value in its perspective. It’s probably like a dev’s viewpoint, but it focuses on shifting the main object of work.\n\n**[35:21]** The info and context of each change within this database require us to use all our knowledge for knowledge sharing and to organize it well. Plus, all these changes must be released thoroughly to avoid timing conflicts or resource clashes among database updates. That’s it. Any questions, guys?\n\n**[35:58]** It’s not about the codebase but the database, right? Leaning heavily that way. Hearing this part feels a bit meta, like it’s more relevant to big systems. For systems like ours now, it’s kinda tough to apply, huh? A system with about 20 tables already feels a bit sluggish, and looking at it gets overwhelming. Exactly. So this ties into documentation, managing versioning, and monitoring. Not version monitoring, but notifications for other teams, right? Yeah, it’s still limited, but spot on. Bringing this in makes sense because of the perspective.\n\n**[36:50]** Data management comes before the other stuff. The logic’s here, and tomorrow the data will run. Any questions for Cường, guys? If not, we’ll wrap up here. This piece has value in its perspective. Thinking about it, for backend devs wanting to strike it rich, you’ve got to stick with projects long-term. The longer the project, the more money it’s got. That’s how it seems. Sticking with a long-running project is solid at its core, but devs usually get lazy. When something drags on too long, they get bored, and their behavior turns weird.\n\n**[37:40]** Before moving to the next piece, to contribute to today’s session, here’s a keyword of the week. While reflecting on stuff, I picked up a new one, a fresh term for those who don’t know yet, like me. Today I came across this school of thought called Luddism. Luddism’s a word from the 19th century, tied to the Industrial Revolution. Industries like textiles got automated, and that ticked off some folks, called Luddites or something, who went and smashed those machines.\n\n**[38:34]** Those machines stole their jobs, their livelihoods, so they wrecked them. That turned into a movement called Luddism, where the working class pushed back against modernization trends. The next keyword digging deeper is Neo-Luddism, alongside this Luddism stuff right here. Check it out if you want, guys. It feels pretty relevant to what’s coming up for us, based on predictions from the other day.\n\n**[39:18]** When we were chatting, we figured there’d be a lot of pushback soon. On Reddit, there’s a post from two years back about a new Neo-Luddism wave popping up. Now it’s everywhere up there, huh? So, from my angle, I’d say we shouldn’t jump on this bandwagon. Progress keeps moving forward, and we shouldn’t fight the wheel of history. There’s even a subreddit called Luddism, diving straight into it. Not just about automation, but all sorts of tech pushback, feelings of being lost or out of place. Pretty interesting keyword, right, guys?\n\n**[40:08]** Not getting stuck in that, huh? Then there’s a second thing tied to this. I just sat down and dug into it recently, and it’s U.S. geopolitics. There’s a perspective on how the U.S. is evolving. I was researching capital markets, tracking where investment money flows, and stumbled into this topic. It’s the second theme, pretty interesting. Maybe you guys will care about it. This ties into macroeconomics. Turns out it stems from that angle and shifts into macroeconomics.\n\n**[40:56]** The U.S. is trending toward just two camps. One is isolationism, meaning pulling back. There’s another word we could use for it, figuring that out here. So, in this movement, what’s it saying? The U.S. will likely shrink inward, stop spreading resources everywhere for trade, and gather them up to hunker down defensively. That’s the first cluster. Right now, from all the news I’m seeing, it’s leaning that way, protectionism or isolationism. This cluster aside, the second direction I see is globalization.\n\n**[41:41]** With globalization, innovations and jobs focus more on trading with each other, boosting cross-border commerce. Nations toss stuff all over the place. The U.S. would trend outward, along with allies in that camp. Countries following that path would also open up more, moving freely everywhere. That’s the state of things, based on reports from 1945 up to recently, breaking into smaller phases.\n\n**[42:20]** In the post-war phase with Japan, after hitting them hard twice, they helped Japan and Germany rebuild after the war. That kicked off globalization in that direction. It’s the initial phase. It started there, and when Japan got too strong, right? It got dialed back by certain events. There’s this event here, plus one called VIA, with more details out there. But that’s the basics. So what’s the main idea? The core idea is there’s a trend we’re learning from history.\n\n**[43:09]** From the Great Depression in 1930 up to now, 2020, there’s a sense the U.S. is swinging back to protectionism. That’ll pull other countries along too. Everyone’s putting their own nation first. So, hopping around everywhere will slow down compared to this phase. The red line’s China, this colored line’s Russia. Russia got a boost after ’91, went for Crimea, then got dialed back. Now it’s China’s turn.\n\n**[43:54]** So how’ll this affect things? Globally, it’ll mean tougher markets. It’ll favor certain countries in that direction. Not sure about Vietnam. Vietnam’s in the top four for import-export delta with the U.S., still getting a boost. Not sure if we’ll cash in big, but generally, folks will move slower with their money. Two main paths here. One is tech keeps pumping out stuff, replaying constantly, so this term might pop up again.\n\n**[44:35]** Plus, the global economic trend’s getting thick. I reckon it ties into what we’ve talked about. Markets are getting pickier, proven by this, huh? Easy to see how we’ll need to adapt. Let’s get Tom up to showcase some skills. I think the team could use them. Our crew needs the skills Tom’s picked up from working with whoever.\n\n**[45:31]** In our team right now, there’s some stuff I won’t dive into about software dev trends. We’ve hashed that out plenty. But working with Tom, I noticed he’s got a slick skill. Almost the whole software dev life cycle, Tom handles solo, using coding chops and automating with tools, even writing his own agents. From initial dev to capturing project insights, then planning it out.\n\n**[46:07]** Everything, Tom nails it. So I want him to show a bit [music] about his approach during work. From getting the brief to reaching your planning stage, how’s it go, what’ve you done? Oh, cool, I’ll share my screen then. Hope there’s nothing sensitive. I figure we’ll grab the brief we’ll dive into soon, right there. Yeah, let’s roll with that one. We don’t even know what it is yet, haven’t dug in. So the starting point’s basically zero.\n\n**[46:55]** It’s just got a sample brief, up to when I get to that part. Alright, I’ll share my screen, find that spot, yeah? Cool, so usually my logic’s like this. We’ve got data, and I want to unpack its key points. If we’ve got these images, say, I’ll strip it all down, then extract stuff. What’s this app got that we need to watch? Okay, then it’s the directions I want it to take, explaining it for me. Cause it’s always possible folks see this app—\n\n**[47:59]** As maybe Airbnb, or some personal trainer and lifestyle trainer app. So here, I’m trying to figure out “What the hell,” right? First, I’d set up a prompt. Say the context is I’m about to meet a client asking us to improve their user experience. Then there’s their external context, and ours is “I’ve got some guesses on what they might want.” Question is, do we need to input our whole brief too? Not here. Then it’s this, the main bit.\n\n**[49:08]** It’s objective, adjective, context. This is the email they sent us. Then I want the core vision, “What’s the vision, goals, and objectives for them asking us to help improve?” From that, I’ll generate some context to send back to the AI side. In reality, I’ve probably done this already, huh? Yeah, any model works, but a thinking model helps pull out perspectives we miss. Those thinking models are ace at that stuff.\n\n**[50:15]** So it’s kinda functional, user app-centric. From this context, I’ll figure out what the app is, then picture the vision they’re after. Oh, it’s really about something more detailed on users, user experience. So I’d ask, “What images, what do these guys want?” They don’t want much, they’re looking to clone this app, not improve it. The app’s already there, and now they want to mirror it, right?\n\n**[51:07]** It’s an existing thing we’re redoing. With this shift, it sparks questions like, “Are there ways their app’s extending? What’re your thoughts?” Then I gradually build a picture. From that picture, I’ll craft a final prompt model to send to our side’s team. Like, moving forward on a proven model’s shortcomings. That way, we’ve got stuff to watch out for. We’ll need to manually test what, exactly?\n\n**[52:15]** Details on this concept validation. It’s just using what already works. With that concept, I’d make a prompt like, “Give me a proposal to pass on what I’ve learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what’s the story behind what they want, and what they want us to consult, develop?”\n\n**[53:23]** On how this proposal will shape up, after that, since each AI tool we use has ready references, I’ll copy an existing one we’ve got. A pre-made proposal, say from before, like this. Then we’d copy a proposal from their side, maybe, duplicate it, tweak it here and there. Wait, internet’s out? Oh, probably copied the wrong thing. Should be, you guys can pull this up or download it. Use the reference to create the proposal, or just in case, don’t lift elements.\n\n**[55:16]** From it, but follow the proposal format to adapt to what we’ve learned and what they’re aiming for to build trust. After that, we always expect this proposal won’t be stable. It’ll firm up once we toss in ideas, ideas I think we can bring ourselves into. Since we’re seeing their angle like this, what can our side deliver? For example, my skillset leans toward excelling at user experience, user flow, data flow. Here, we can mirror the app and optimize its data flow, user flow, stuff like that.\n\n**[56:10]** Or, from anh Thành’s side, it’s optimizing for security and performance. How do we apply this correctly to the project proposal? Adding in some good-to-have stuff like performance and security. If it’s an MVP, we wouldn’t consider hacking concerns much. It’s more about designs tied to data. For example, my side often designs data with temporal state, event store, or uniform patterns.\n\n**[56:51]** How do we apply our computer science skills to this properly? Not overly simple, but simplifying and maintaining what this app delivers. If we go with this and the reference here, now it’s like anh needs some stuff to lock in the deal, right? We’ll need questions to figure out how it works with them. Like an open deal, all cards on the table. To get there, we need those questions, plus we’ve got to suggest a work schedule and next milestones between us and them.\n\n**[57:28]** Need those questions, along with the fact we’ve pretty much got to do it. How do I handle it? Building rapport, then digging into their burning questions. If we’re sharp in our craft, coming up with questions isn’t too hard. But if I feel stuck, hitting some block, I’d ask AI for question ideas.\n\n**[58:14]** “So we haven’t met with this partner yet, this client yet, but we want to make a deal with them. What should I do to help build rapport and find the three burning questions I need to kick this deal off and address any technical concerns?” That’s a solid start. I’d use it to have the AI spit out some questions for us. Then I’d build on that. If I come up with more, I’d add them to the proposal, keeping it realistic. Not just Gemini, but apps like Claude or ChatGPT, how do we approach it?\n\n**[59:12]** How do we handle the due diligence? For burning questions, say we’ve got no prior context up here, just use it. It’s like that out there. I want to set goals like that, starting with the first proposal. I’m a bit skeptical about mirroring, why do they want to mirror? It’ll show in the intent of the initial proposal we built for them. So it ties into this. When we’ve got more, it’s not always set.\n\n**[59:50]** I’d use this as is, but from here I’d figure, maybe a real-time handling angle? Perhaps their side isn’t real-time, more like a booking appointment app. If we pitch real-time, do they want that vision? To consult on whether they want the app prettier, stabler, or newer, riskier even? It’s steps where we ask, throw stuff out, see how they reply. No harm in it.\n\n**[01:00:34]** Cause it’s a fair question anyway. Say this dev step lands, then what’s next with the to-do list and laying it all out? Yeah, yeah, it’s like picturing it. I’ve got some stuff I picture as already sorted. Then I flesh out the technical direction we agree on to move forward with them. Like real-time, “We think they want something like this, but are open to a more real-time thing, like Grab or Uber for personal trainers.” First, I’d draft the technical proposal.\n\n**[01:01:33]** Probably not needed though, usually I’d build that to clarify the angle. But from this view, it’s like, “Help me create tasks for frontend, backend.” Cause in that middle stretch, Tôm here figures out all the diagrams, flows, everything. Gotta lock that down first, then base the breakdown on it, right? So to simplify today, we’ll have AI churn it out.\n\n**[01:02:11]** It’s a rough angle, but we’ll add, “We’re plannin","title":"OGIF Office Hours #39 - Frontend updates, Database scaling, AI workflow, and Macro insights","short_title":"#39 Frontend report, DB Scaling, AI Workflow","description":"In OGIF 39, the team explored React 19 and Deno Deploy updates, database scaling with CI and migrations, Tom’s AI-driven dev workflow, and macro insights on protectionism and globalization trends.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Wed Feb 12 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/39-20250207.md","slugArray":["updates","ogif","39-20250207"]},{"content":"\n### Topics and Highlights\n\n- **Erlang automata part 2:** Minh Lưu shared a practical breakdown of Erlang’s gen_statemachine vs gen_server, using a TCP-to-Redis connection module as a case study, implemented in Elixir for clarity.\n- **AI \u0026 market landscape**: Minh Lê recapped AI agent products on Solana, emerging macro trends in Southeast Asia’s tech investment (e.g. challenger banks, neobanks), and how Y Combinator and a16z are repositioning around fintech and stablecoins.\n- **Research direction**: The team discussed how recent AI and Web3 shifts align with internal priorities, reinforcing a focus on high-signal innovation zones and how to build for impact in 2025.\n- **Dwarves of the Year \u0026 Team Awards**: Celebrated contributors for embodying the AMA model, sharing AI research, and supporting team growth. Awards also highlighted excellence in consulting, performance, and community engagement.\n- **Hybrid work shift**: Announced a post-Tết operational update, ending remote-by-default in favor of 3-days-a-week in-office commitment for Saigon, Hanoi, and Danang hubs. Emphasis on adaptability in a shifting tech market.\n- **Operation updates**: Confirmed no referral bonus for 2025. 13th-month salary was processed based on average pay. Noted that profit-sharing would be paused due to cautious market outlook.\n- **Looking ahead**: The team reflected on the importance of physical collaboration, the benefits of dense information zones, and emerging builder trends post-layoffs, from build-in-public to token-based MVPs.\n\n### Vietnamese transcript\n\n**[01:29]** Ok anh em. Lâu quá mới lên sân khấu. Lịch trình hôm nay theo kế hoạch của anh Thành là có bốn bài như thường lệ. Thấy nhắc bốn bài như mọi khi, nhưng tí nữa có tổng kết nên anh chắc chen thêm một bài.\n\n**[07:48]** Xíu đổi lịch trình nha anh Thành. Lịch cũ thì bên đó làm bài tiếp tục series Engineering, cái mà bên nghệ nhân đã chuẩn bị và trình bày. Nhưng anh thấy tạm thời dời lại. Chắc để tuần sau hoặc qua Tết.\n\n**[08:09]** Gộp thành combo, từ từ giới thiệu cho anh em sau đợt hai tháng trước. Giờ mấy topic mở rộng thế nào, làm gì tiếp thì để sau. Lên phần Biên nha, phần hai của Minh Lưu về Erlang. Bữa trước cậu ấy làm bài rồi, giờ tính review lại ý kiến mọi người.\n\n**[08:32]** Buổi trước mọi người thấy sao? Có còn nhớ gì không? Bài này 50-50. Bài Minh Lê thì chắc lấy bài Minh Lê lên xem thử.\n\n**[08:53]** Lần này, bữa trước thấy anh em team bắt đầu viết, nhìn rất ok. Cuối cùng đăng hôm nay. Nên mình có hai, hoặc hai rưỡi topic cho mọi thứ. Thiếu mấy đứa rồi. Anh đâu rồi? Mấy nhóc kia đâu, biến hết rồi?\n\n**[09:34]** Lên nào, lên tổng kết tí. Em ơi, anh đâu rồi? Đây nè. Rồi, đủ mặt, đủ mấy nhóc vô xem. Ừ, Tuấn, Tuấn đi chơi về vui không? Mua gì không? Tuấn trốn trong văn phòng rồi đúng không? Vui quá ha. Nếu đổi lịch thì làm bài Minh Lê trước nha. Hai tuần trước.\n\n**[10:32]** Thấy mấy bài đã public rồi. Anh muốn nghe trực tiếp, xem chất lượng bài market report thế nào. Mọi người xem thử luôn, anh chen vô tí về thị trường. Tiếp theo là tổng kết, có vài chính sách muốn thông báo.\n\n**[10:54]** Sắp tới có update nhỏ, rồi sau đó tổng kết năm. Inno viết bài rồi. Ngồi đọc chung xem sao. Rồi tới bài cuối của Minh Lưu. Chắc làm luôn, bài trước cũng là Minh Lưu. \n\n**[11:22]** Bài trước của Minh Lưu nói về cái gì? Finite state machine trong Erlang đúng không? Hôm trước anh em còn nhớ gì không? Đánh giá sao, cho rating một phút. Còn nhớ nói gì không? Độ tập trung thế nào?\n\n**[11:57]** Bài tháng trước hả? Minh Lưu làm về finite state machine. Còn nhớ nó quan trọng sao không? Nếu nhớ thì sao dùng Erlang khi Elixir cũng làm được? Huy Nho có bài đó không? Huy Nho có tham gia không? Anh thấy bên nhóm của em có nhắc tới làm state machine khá nhiều.\n\n**[12:26]** Huy có góp mặt không vậy? Bên chỗ Minh Lưu là bên Yolo, tụi nó đang xài cái đó cho mấy dự án bên này, đúng không? Còn bên em thì ở Ascenda, cả hai chỗ đều có xài rồi. Làm cái framework, rồi làm cái controller, tới cả stage controller nữa, đúng không? Bên mình thì đang thử mấy cái đó, kiểu như là làm sao để tối ưu hóa được cái luồng xử lý state trong mấy dự án hiện tại.\n\n**[12:56]** Chuyển kiểu round-robin đúng không? Bên này thì define mấy cái state trước, rồi cái state nó sẽ set một cái context, kiểu như là enter state hay state hiện tại là gì, để bên ngoài chỉ cần switch qua lại giữa các state thôi. Còn bên Yolo thì em chưa biết rõ lắm, em chưa xem kỹ bài của Minh Lưu. \n\n**[13:19]** Chắc được ha? Ok, có mấy người mới nhảy vô luôn kìa, đông vui rồi đây. Vậy thì chắc cho Minh Lưu lên trước đi thôi. Minh Lưu chuẩn bị đi nha, Minh Lê ngồi đợi xíu. Lên nào, Minh Lưu, nhanh lên, nói một chút thôi cũng được, vì cái này cũng quan trọng, anh em cần hiểu rõ hơn về hướng đi này.\n\n**[13:41]** Em có 10 phút thôi, 10 phút là ổn. Bài này nối tiếp cái trước đó, tí nữa em sẽ nhắc lại tại sao cái này nó quan trọng với team mình. Em muốn anh em thấm cái này thật kỹ, vì đây là một kỹ thuật không dùng nhiều trong Erlang, nên nó khá đặc biệt. Mấy cái này bên mình làm xong thì có thể áp dụng qua mấy dự án khác nữa.\n\n**[14:12]** Order Minh Lưu nha, em có slide sẵn đây rồi. Để em review lại bài trước một chút cho anh em nhớ. Trong đó có cái behavior, tức là một pattern để mình define một cái module. Nó sẽ set sẵn một số hook, rồi mình dựa vào đó để xử lý các trường hợp cụ thể trong code, kiểu như là chuẩn bị trước một bộ khung cho mấy cái chức năng chính.\n\n**[14:49]** Mình sẽ implement mấy cái hook đó. Hai cái phổ biến nhất là gen_server và gen_statemachine. Đa số module thì viết dưới dạng server là được rồi, đặc biệt với mấy cái state machine đơn giản thì gen_server đủ sức đáp ứng. Nhưng mà tùy trường hợp thì mình mới chọn cái nào, không phải lúc nào cũng xài bừa được đâu.\n\n**[15:14]** Gen ở đây là viết tắt của generic, tức là generic server đó. Mình chỉ nên dùng gen_statemachine khi thật sự cần mấy tính năng nâng cao, ví dụ như insert event—tự chèn một event vào trong stage—or là cần trigger một cái action cụ thể khi switch giữa các stage. Cái này thì gen_server không làm được tốt bằng, nên phải cân nhắc kỹ.\n\n**[15:36]** Khi chuyển từ stage này sang stage khác mà mình muốn nó tự động chạy một action sẵn, thì cái đó gọi là state entry. Hoặc là mấy cái liên quan tới timeout, kiểu như set thời gian chờ giữa các bước. Trong thực tế gen_statemachine thường được dùng khi mình muốn implement một cái gì đó persistent, tức là cần giữ trạng thái lâu dài.\n\n**[15:59]** Ví dụ như TCP connection chẳng hạn, người ta hay dùng state machine để xử lý mấy cái này. Khi mình đã có một set các state cố định và cái connection đó cần persistent, thì lúc đó gen_statemachine là lựa chọn hợp lý nhất. Còn với mấy trường hợp bình thường khác thì gen_server là đủ rồi, không cần phức tạp quá. Hôm nay em sẽ code một ví dụ cụ thể để anh em hình dung rõ hơn.\n\n**[16:24]** Như anh Minh có hỏi bữa trước, tại sao phải dùng Erlang trong khi Elixir cũng làm được cái này? Thì đúng là vậy thật, behavior trong Erlang thì bên Elixir cũng gọi lên được và implement tương tự luôn. Nên hôm nay em quyết định làm ví dụ bằng Elixir để anh em dễ so sánh, xem cái nào tiện hơn trong trường hợp này.\n\n**[16:49]** Code bằng Elixir thì nhìn nó trực quan hơn một chút. Có slide không nhỉ? Slide về gen này đây, để em cho anh em xem thêm chi tiết, biết rõ hơn cách nó hoạt động. Ok, em chèn slide vô luôn. Hôm nay mình sẽ làm một module để connect TCP tới Redis server, giữ nó đơn giản thôi, chỉ có hai stage để dễ hiểu.\n\n**[17:34]** Hai cái stage thôi, tức là nó sẽ là một module nằm giữa user và Redis server. Module này sẽ implement phương thức để connect tới đó và làm trong suốt quá trình connect tới server. Ví dụ, khi process của mình bắt đầu một connection tới server thì tất cả request từ user tới nó sẽ trả về trạng thái disconnect hết. User không biết process thật sự mình gọi tới server ra sao, chỉ thấy qua process của mình thôi. Nên nó có hai stage: thứ nhất, khi khởi động process lên thì nó ở trạng thái disconnect.\n\n**[18:21]** Mình sẽ thiết lập một connection tới Redis server; sau khi connection thành công thì chuyển sang trạng thái connect. Còn nếu connection bị lỗi hay gặp vấn đề gì đó mà đứt, thì mình quay lại trạng thái disconnect và cố gắng restart lại connection. Những request, tức event request từ client, sẽ nhận dưới dạng event. Event có thể tới lúc disconnect hoặc connected; khi tới lúc disconnect thì mình chỉ đơn giản trả về cho client trạng thái là disconnect.\n\n**[18:41]** Connection thì những request, tức event request từ client, sẽ được nhận dưới dạng một event. Event này có thể tới lúc disconnect hoặc connected. Khi tới lúc disconnect thì mình chỉ đơn giản trả về cho client trạng thái disconnect. Còn nếu tới lúc connected thì mình gửi request đó lên Redis server, lấy data trả về cho client. Tại sao cần dùng state machine? Vì mình cần mấy tính năng như insert event mà em vừa nói. Đây, mọi người thấy không, để em zoom lên.\n\n**[19:39]** Trước tiên, mình implement behavior trên state machine, define cái data trong behavior này. Nó có hai thứ: State là stage thể hiện trạng thái của module, và Data chứa dữ liệu khi state chuyển đổi, mang theo một số data để xử lý trong module. Data gồm host, port để connect tới Redis DB, và request là map chứa danh sách ID của client cùng key là ID để biết trả về cho client nào. Khi start process này lên, việc đầu tiên là connect tới Redis server, chạy trong background và trả về trạng thái disconnect cho user.\n\n**[21:04]** Trước tiên, mình define callback mode gồm hai thứ: State function—tức đặt tên hàm trong module theo tên stage, ví dụ stage disconnect thì module tự chạy vào hàm disconnect để xử lý dựa trên tên và số tham số—và state enter, tí em giải thích sau. Bắt đầu bằng hàm init, mình trả về next_event, tức module tự insert event internal_connect với data là host, port để connect tới Redis server, rồi trả ngay trạng thái disconnect. Khi trả về disconnect, nó chạy vào hàm disconnect để xử lý. Hàm disconnect với internal_connect sẽ mở TCP socket bằng gen_tcp tới Redis server, bắt đầu thiết lập connection trong background.\n\n**[21:49] C**ái state enter thì tí em sẽ giải thích sau, ý nghĩa của state function là như vậy. Xong rồi mình bắt đầu bằng hàm init như hồi nãy em nói. Trong init thì làm gì? Đơn giản là khi bắt đầu hàm, mình dùng một term là next_event. Next_event là gì? Nó sẽ tự insert, tức module này tự chèn một event vào trong tay nó. Event này là internal_connect, dùng với data là host và port để connect tới Redis server, rồi trả về ngay lập tức trạng thái disconnect.\n\n**[22:43]** Khi trả về trạng thái disconnect như vậy thì việc đầu tiên là nó chạy vào hàm disconnect để xử lý. Hàm disconnect với internal_connect này làm gì? Thứ nhất, nó mở một TCP socket bằng gen_tcp tới Redis server. Rồi mình chợt thấy hơi nhiều chi tiết quá, em gộp mấy hàm lại thử được không anh em? Anh chen vô cho anh coi mấy chỗ define hai cái stage của nó, coi hơi rối quá rồi. Chỗ nào define hai trạng thái đâu đây? Mọi người thấy đó, nó có cái list ở kia, mình có hai trạng thái thôi ha.\n\n**[23:48]** Ở đây là hai trạng thái, bốn cái overlap function của disconnect và connect. Minh có bốn overload của connect, còn hai trạng thái thì bình thường thôi. Bên Elixir thì nó overload dựa trên param list đưa vào, nó biết chạy cái nào. Chỗ thứ hai là chỗ nào chuyển trạng thái đâu? Ờ, nó nằm trong từng hàm. \n\n**[24:42]** Ví dụ hàm connect này, khi connect thành công thì nó trả về một atom next_stage, chuyển trạng thái sang connect ngay đây, cùng với data đó. Hiểu rồi, đây là chỗ gọi để chuyển từ state này sang state kia. Nhưng mà với mỗi overload function vậy, em gọi implicit thế này thì có đúng không ta? Thường mấy cái này sẽ có dạng controller để quản lý, chứ gọi chi tiết kiểu này nhìn hơi rối. Có 10 stage trong đây mà define xong gọi thì quản lý khùng luôn.\n\n**[25:01]**\n\nCái ngay chỗ này nè, ngay backend của mình á. Vì state function thì mình có một cái gọi là handle_event function. Khi chuyển sang xài mode như thế này, mình chỉ cần define một function là handle_event thôi. Ví dụ vậy thôi, mình chỉ cần define xong là quản lý hết tất cả state trong cái function đó. Nếu muốn kiểu cách như vậy, nó cho mình lựa chọn giữa việc define function theo state hoặc define một handle_event function. \n\n**[26:14]** Giờ kêu hình đâu, Bảo Hân Trần có nhìn vô cái này, có xài cái này bao giờ chưa? Để trực quan thì em có copy một cái thư viện connection, cũng tới TCP đây. Nó viết bằng gen_server, không xài state machine, theo kiểu button như vậy. Connect fail thì nó back off, rồi retry, cũng quản lý user trong một cái map. Nhưng nó hơi dài hơn tí, phải implement lại mấy thứ có sẵn ở bên này, như timeout chẳng hạn, nó phải tự implement lại hết. Rồi câu hỏi là chị hỏi em cũng mới học, mới làm cái này. Mình biết cái này lâu rồi đúng không, nhưng lúc tiếp cận thì thấy sao? Tại sao nó quan trọng? Nó define một số cách giúp mình làm tiện hơn, anh. Ví dụ như xử lý timeout, nó define dựa trên data mình trả về.\n\n**[27:47]** Ví dụ thế này đây. Khi connection của mình bị drop, mình chạy vô hàm disconnect để xử lý, nó cho mình trả về một atom timeout cộng với thời gian. Sau khoảng thời gian đó, nó tự động chạy vô đây xử lý cho mình. Ừ, đây, nó giúp mình làm mấy cái đó tiện hơn. Thay vì viết bằng gen_server cũng được, nhưng sẽ dài hơn. Ok, ok, cảm ơn Minh. Minh Trần có hỏi gì không? Ai nữa? \n\n**[29:14]** Nhờ Minh Trần code cái này nào, code Elixir nữa, nhờ anh biết có đụng tới đây chưa. Rồi đặt câu hỏi nha, phổ biến nhanh cho anh em: cái này nó như vậy, sự khác biệt cơ bản trong lập trình của nhóm mình, trong tất cả các ngôn ngữ lập trình hiện đại hiện tại thì không có ngôn ngữ nào có sẵn thư viện để quản lý state machine như con Erlang. Erlang là con duy nhất sinh ra để chạy mấy cái hệ máy tự động không xuất phát từ góc nhìn làm server đứng đó đợi gọi tới gọi lui theo mô hình client-server từ đầu.\n\n**[29:30]** Con này sẵn đúng không? Với cái vụ mà nó build-in trong đây thì lúc em lập trình, nó ra được hai thứ. Thứ nhất là nó ép cả đội học ngôn ngữ này, có tooling này sẽ hình thành một mental model, một mô hình tư duy giống nhau. Gặp đúng trường hợp thì mình moi cái tool ra xài, suy nghĩ giải bài toán theo cách này.\n\n**[30:07]** Lúc làm mấy cái model như C4 á, hồi đó cũng đụng tới đây thôi đúng không? Nó cung cấp cái tool cho mình. Thứ hai là nó unify tư duy lại với nhau. Tất nhiên gen_server thì vẫn code kiểu bình thường, anh em xài vẫn ok, không sao hết. Nhưng điểm đặc biệt của Erlang là có cái này.\n\n**[31:13]** Khi team thấm rồi, quản lý code base dưới dạng state machine, chuyển state vòng vòng mấy cái object, thì nó giúp quản lý source tốt hơn. Tránh việc code implicit, đẩy code đi vòng vòng, hồi không biết mình chuyển qua đâu. Mình tập trung vào logic, vào góc nhìn nhiều hơn là lo cái data bên dưới nó thế nào. Chứ không thì lỗi xảy ra rất nhiều trong code base lớn, không nhìn theo kiểu này là lỗi đầy ra, phải đứng ra làm lại hết. Đó là lý do con này đặc biệt, với Erlang thì đặc biệt vậy thôi. Về Elixir, mấy cái thằng kia không build thư viện này lên, nên xài vẫn phải chọt trực tiếp trên Erlang.\n\n**[32:08]** Ừ, thôi đó là tóm tắt nhanh bài của Minh Lưu. Nếu Minh Lưu làm bài tiếp theo, anh đề nghị làm cái gì phức tạp hơn xíu, thực tế hơn xíu. Bài kia có hai stage nhìn còn đơn giản. Hoặc kiếm mấy cái clip open source của tụi nó, quản lý state nhiều hơn. Có cái ví dụ của một nhóm nào đó, xài quản lý mấy WebSocket, nhiều stage lắm. Chỗ đó cũng có xài một phần gen_statemachine, một file lên tới ngàn dòng. Lúc gộp hàm lại thì thấy cấu trúc rõ ràng, không phải kiểu implement đẩy qua đẩy lại lung tung, có một con manager đứng quản lý. Anh em lâu lâu mất não quay lại nhìn vẫn dễ chịu hơn là nhìn mấy hàm tự đặt tên, nhìn mệt lắm.\n\n**[34:18]** Cái chuyện hình thành mental model chung của một nhóm cực kỳ quan trọng. Cảm ơn Minh Lưu, mời Minh Lê. Dạ, để em coi em có bao nhiêu phút? 10 phút không? Ừ, 10 phút, tại em đang có bốn bài rồi hả? Lúc trước bên team consulting có ra ý tưởng viết series hàng tuần, mỗi tuần một bài tổng hợp thông tin liên quan tới bên test mình. Nhưng nó không sâu về test quá, chỉ chung chung. Bài đầu tiên em viết từ giữa tháng 12 năm ngoái, cover lại chuyện đợt Google ra Gemini 2.0, rồi Open AI ra model chain video.\n\n**[34:38]** Mấy cái hình ảnh của nó cho người bệnh, hoặc mấy người khỏe mạnh đeo vào, như mấy cái đồng hồ anh em mình đeo để theo dõi nhịp tim ấy. Rồi bên consumer tập trung vào mấy cái bí mật, như mấy cái để render ra video giống con Sora. Ở trên crypto thì nó cũng gắn AI vào fintech, gaming, infrastructure này nọ.\n\n**[35:21]** Bên Y Combinator cũng tương tự, họ kêu gắn AI vào mọi chủ đề hiện tại. Họ đang tập trung bảo mọi người nghiên cứu stablecoin. Lúc trước thì kêu làm Bitcoin với Ethereum để thanh toán, nhưng sau một thời gian thử nghiệm trên thị trường, họ thấy mấy cái đó không hợp lý. Nên giờ họ chuyển qua nghiên cứu hướng stablecoin, kiểu đủ thị phần để mấy công ty bỏ tiền nghiên cứu, rồi build giải pháp thanh toán.\n\n**[36:21]** Ở đây em nói sơ về một product, một cái AI agent platform trên Solana. Họ khẳng định đây là nền tảng cho mình tạo mấy con AI agent. Mấy con AI agent này giúp quản lý ví, tạo coin, tự trade, nói chung là tự động hóa mấy thứ mà dân crypto với DeFi hay làm. Product này giúp người ta làm vậy dễ hơn với sự hỗ trợ của AI. Đó là bài thứ nhất, qua bài thứ hai. Bài thứ hai trong bốn bài, nhiều lắm. Giờ câu hỏi chính là qua bốn bài tháng vừa rồi, em nghĩ cái gì benefit team mình?\n\n**[37:13]** Ừ, em nghĩ chắc mình đang đi đúng hướng, tập trung vào mấy công nghệ AI, làm quen với agent, blockchain này nọ. Nó đang phát triển rất mạnh, đang bùng nổ thị trường, rất hot. Sắp tới chắc cũng có nhiều product tập trung vào đó. Còn mấy mảng như Y Combinator hay a16z đề xuất thì hơi vượt xa tầm với của thị trường mình, nên cũng khó nhảy vào. Nhưng tuần vừa rồi em coi một cái thống kê sơ về nguồn tiền chảy ra chảy vào ở Đông Nam Á, quý cuối 2024 vừa rồi, thì đây là top mấy ngành đang được đổ tiền vào nhiều.\n\n**[38:23]** Ngành thứ nhất là challenger banks. Nó khác ngân hàng truyền thống, tập trung hoàn toàn vào ngân hàng số. Ở Việt Nam mình hình như có một cái ngân hàng số, nhưng không được định nghĩa là challenger bank hoàn toàn. Như mấy thằng Timo này nọ, nó được gọi là neobank nhiều hơn. Challenger bank là kiểu có giấy phép hoạt động như ngân hàng thật, tự đưa ra sản phẩm mới trong ngân hàng số của nó. \n\n**[40:08]** Còn neobank ở Việt Nam thường có ngân hàng truyền thống đứng sau, bật giấy phép, không tự phát hành sản phẩm ngân hàng được. Ở Đông Nam Á, mấy VC đang đầu tư mạnh vào challenger banks, vì họ nghĩ nó giải quyết vấn đề tiếp cận tài chính cho mấy vùng không có điều kiện, hơn là crypto.\n\n**[40:30]** Nghe nói em có kiểm tra lương bổng, mấy con số lương phải trả cho ngành IT ở các nước Đông Nam Á. Bên Philippines đang có giá cả khá cạnh tranh, dân số thì đông, giáo dục cũng đang phát triển ổn. Ngành nhân lực IT của họ đông lắm, làm cho mấy công ty nước ngoài, tiếng Anh thì rất tốt, mà giá lại cạnh tranh hơn so với mấy nước Đông Nam Á như Việt Nam mình.\n\n**[41:26]** Việt Nam mình vừa rồi quý 4 bị giảm đầu tư khá nhiều, tới hơn 80% luôn, còn Philippines thì tăng mạnh, phát triển lắm. Họ đang đẩy mạnh thương mại điện tử, mấy cái digital, giống kiểu mình cách đây 3-4 năm trước. Giờ họ được đầu tư nhiều. Ok, interesting, kéo lên trên tí. Vậy mấy phần trên anh thấy toàn liên quan tới tiền, đúng không? Banking, ngoại tệ, rồi finance, toàn dính tới tài chính. Đông Nam Á chắc đang đầu tư mạnh vào mấy ngành này.\n\n**[42:39]** Nếu vậy, tuần tới mấy bữa nữa ngồi xem tiếp, soát thử danh sách tiềm năng, coi mấy cái ecosystem map hay system hiện tại của tụi nó, tìm được không? Dạ, để em kiếm thử, mấy report thường có mấy cái đó. Ừ, ok. Còn gì nữa ngoài cái này không? Thấy tuần này là tuần Giáng sinh, ít tin tức, cũng chán. Em có để mấy product blockchain mới, đang được người ta đổ tiền vào nhiều, lock vốn nhiều. Blockchain hả? Cho anh xem thử coi. Liquid rồi, ok, phút phút, tụi nó share trong kênh chat rồi, ok lắm.\n\n**[44:19]** Nếu vậy có hướng này. Thật ra để viết bài này, anh thấy có góc nhìn thế này. Nếu được, mấy anh em ngồi xem, em có thể trả lời theo góc nhìn: slogan của team mình từ đầu là “empower innovation”. Tức là mình tìm mấy cái innovation đang xảy ra, để có cơ hội làm chung với tụi nó, hoặc hỗ trợ, thậm chí invest luôn thì quá đẹp. Cái interest lớn nhất khi anh em làm cái này là tìm xem innovation đang ở đâu. Thay vì điểm tin chung chung, em có thể chỉ rõ mấy điểm nhịp của market, chỗ này chỗ kia, innovation sẽ diễn ra ở đó. Trong đó, business model hay financial model là gì, trả lời đúng trọng tâm của mình, cũng là kiến thức dễ ping bài toán kinh doanh.\n\n**[47:25]** Coi thử hướng đó nha. Ok, cảm ơn Minh nhiều. Trước giờ mình thiếu cái đó, kiến thức trôi nổi, không tập trung. Góc nhìn này sẽ giúp team hiểu rõ, khi soát dự án cứ nhìn kiểu: chỗ nào sale, chỗ nào có tiền. Cách nói khác là innovation đang ở đâu, mọi người làm gì mới ở đó. Ngành software của mình, chỗ nào đang build mới thì mình mới có cơ hội nhảy vào review. Còn mấy cái bùng nổ quá thì chỉ làm retainer, dán dự án, chán lắm. Ok, chắc hết phần này. Giờ tổng kết năm nhanh xíu. Mấy anh em ra ngoài offline là xong. Câu hỏi nhanh: Huy với Thành chuẩn bị mấy phần rồi, out hết chưa? Chắc hết rồi. \n\n**[47:50]**  Năm nay thị trường thay đổi nhiều quá, mấy cái assumption trước đây nó thay đổi hết, cũng không biết đâu mà lần. Nhân sự mình cũng có sự chuyển dịch tương đối, đúng không?\n\n**[48:53]** Nhiều thứ định hướng của mình cũng shift đi, từ mô hình cũ tập trung vào enterprise, nhưng AI ra đời thì wipe hết thị trường luôn, đủ trò hết. Định hướng sắp tới mấy em nhìn chung thấy thị trường đang chuyển dịch theo hướng tiếp cận tài chính, blockchain. Nói chung AI đang hot, nhưng tính ứng dụng vào doanh nghiệp chưa nhiều lắm, đang bùng, mọi người thi nhau bùng thôi. Còn hướng chính vẫn là đánh về tài chính.\n\n**[49:55]** Hướng đó thì vô tình Engineering của mình đi research cái đó, học cái đó, đang diễn ra nhiều. Nên Year-end hiện nay công bố luôn nhờ, có bao nhiêu giải? Năm nay team có bốn giải, còn một giải trong phần community. Vẫn có một giải cao nhất thuộc dạng kiểu vầy cơ. Còn mấy giải kia thì dạng honorable mention. Tất cả giải thưởng này rút ra là để vinh danh mấy bạn high performer trong năm vừa rồi qua các mảng chính của team mình. Chắc để công bố luôn, bảy giải từ bốn giải là gì?\n\n**[51:05]** Một giải đội hả? Ba giải còn lại gì? Giải thứ nhất cho bạn execute cái model AMA của mình tốt nhất vừa rồi. Giải thứ hai thuộc về giải thí sinh cho team lead, bạn nào dẫn dắt team mình ok nhất, được number one. Giải thứ ba thì cho bạn perform tốt nhất ở bên phần consulting. Project có hai performance được anh em trong team đánh giá cao và khách hàng đánh giá cao vừa rồi. Giải thứ ba thì cho tập thể team nào đó perform cộng, có hai đầu: một là perform để ghép được phần inhousing, hai là nâng cao độ tin nhiệm và upsell thêm cho team đó.\n\n**[53:38]** Giải cuối thì cho community, mấy bạn supporter không thuộc official team nhưng có setting với mình, tham gia hoạt động, sharing cá nhân. Ok, vậy là bốn cái đúng không? Developer of the Year tức là mô hình MMA: meaning, mastery, autonomy. Giải thứ hai dành cho hướng style research, là giải động team lead. Giải thứ ba là consulting, ba giải cá nhân và một giải đồng đội là dự án thấy có impact nhất. Mấy dự án khác không impact bằng thì không tính sau nhé. Cuối cùng là giải community, mà community ở đây đâu có ai đâu nhờ? Năm vừa rồi thấy ít mà, phải không? Ừ, thật ra là có bạn làm việc với team mình khoảng mấy tháng thôi.\n\n**[54:26]** Rồi, chắc làm cái công bố nhanh thôi, mấy anh em in-out cái đó cho dễ, ok không? Chứ anh làm cho chuẩn thì lâu lắm, rồi tính tiếp. Cho xin cái reaction cái nào, zalo hay slack để mình capture lại nhờ. Hai con VIP kêu rồi, ok. Rồi, good. Giải thứ hai nhờ Thành công bố cái gì hay giải gì nhờ, cần soát tin rồi check, ok.\n\n**[55:58]** Cho phút lead xin cái reaction. Ủa, còn setting này là cá nhân hả? Đúng rồi. Ok, có thể điểm sơ qua được không? Trước giờ performance mình thấy cũng như mấy năm trước thôi, không biết cụ thể sao, mời. Ừ, vừa rồi thì chắc tí nữa Huy sẽ cho comment chi tiết hơn. Nhưng đánh giá tổng quan mà Thành xem được của Phúc vừa rồi thì thực ra trước giờ, mấy năm trước Phúc đa phần làm internal project với consulting, chủ yếu đợt này qua làm bên team Yolo.\n\n**[57:21]** Thực ra với role nó hơi yếu đúng không, so với expectation dạng depth của backend các thứ. Trước đó role của Phúc focus chủ yếu liên quan đến iOS thôi. Ban đầu thì ông này bên đó cũng skeptical về Phúc, nhưng thấy bảo adapt rất ok. Trong đâu đó năm sáu tháng, anh em bên đó chắc đánh giá ưng nhất, mà nghe nói cũng khó tính. Chi tiết thì chắc Huy có comment chính xác hơn, vì trong project của mình thì Huy chọn mà. Mắt em comment là sao, anh này comment về em mà. Thật ra em nghĩ mấy bạn làm thì cũng tương đối nhiều, mặt kháng giá, lập này nọ.\n\n**[58:46]** Nhưng em nghĩ chỗ anh Thành chọn Phúc là vì thấy sự vừa expectation của mọi người tí. Lúc trước Phúc cũng có làm dự án, nhưng khá im, không giao tiếp vào xong thôi. Dù tài năng rất tốt, nhưng trong năm nay thì có nhiều cái vượt xa mức đó. Ví dụ thành quả là mấy cái style khác, loop rồi, đi mấy cái style JavaScript, TypeScript các thứ, dù lúc trước không liên quan lắm. Rồi còn liên quan đến việc bắt đầu có mấy cái cần soát với khách hàng, deadline kiểu này không ổn. Đỉnh nhất là vừa rồi, chỗ bảo muốn làm cái này, nhưng Phúc bảo không kịp đâu, từ từ làm. Đó là dấu hiệu ban đầu, rồi cũng bắt đầu có việc trọn Phúc cho mấy cái phát triển vừa bực hơn, ghê hơn, chứ không hẳn là nhất. Nhưng expectation của Phúc có vẻ tốt nhất. Ok, đúng là Superman ha. Phúc có lên phát biểu không?\n\n**[61:03]** Chế độ này ngồi sao được, em gắn headphone vô. Em cũng thấy hơi bất ngờ, tự nhiên được cái này. Đang bắt xe xuống cái từ thiện, em cũng không biết nói gì, mà thấy có nhiều hết nghe rồi. Ô, Ngọc Thành vô nè, lâu mới gặp Ngọc Thành, nghe Phúc ơi. Thức nó sao giờ, nó mute luôn rồi. Ở ngoài đường thì hiểu cảm nghĩ là giải thứ hai, em nghĩ cái này có xứng đáng không, thấy hơi ngại, chào đúng không, khó quá. Đủ để anh nhận hội. Dạ, đúng rồi. Giải tiếp theo nhờ Huy, xin mời cho team, thì vô cho bên key team của bên Yolo. Ok, team Yolo tức là như nào?\n\n**[62:28]** Là Yolo đang nghĩ nó là một cái tiêu biểu nhá. Ừ, đồng ý. Giải cuối cùng rồi, còn lại thì trao bạn này. Cho ai cũng cần đạt thì cũng có khoảng thời gian đâu đó 2-3 tháng làm intern với mình, chủ yếu là em research liên quan đến mấy thứ liên quan đến phần mềm.\n\n**[63:20]** Thực ra trước đó, Đạt cũng có trước khi join mình làm intern thì cũng tương đối active trên server mình rồi. Kể cả sau khi kết thúc intern, vẫn tiếp tục với mấy cái về build trên AI. Những bài share thì được mấy anh em phía core fork, hướng lại tương đối nhiều. Một tiêu biểu mà bên cộng đồng mình đang muốn khai thác, và Đạt chắc nên phát biểu tí. Đạt ơi, team có dành phần quà community dành cho em, giờ đang thế nào, đang ở đâu rồi?\n\n**[64:26]** Em đang ngoài đường, chưa tới giờ hết đi họp hết trơn rồi à? Em cảm ơn mọi người đã dành thời gian cho em. Phần lớn thì anh Tom với anh Thành giúp em rất nhiều trong câu chuyện onboarding với cả share lại cho team sao cho ok, kiểu có thể delivery được kiến thức cho mọi người. Còn gì nữa không? Dạ, chắc không, em nghĩ như vậy. Cảm ơn Đạt đã dành thời gian ha.\n\n**[65:11]** Ở style với mấy anh em, việc mình đọc cái gì hay thì chia sẻ cho mọi người, rất là appreciate mấy phần của Đạt. Even là hiện tại, mấy anh em thấy trong hoạt động chia sẻ kiến thức trong team, cái đó là cái chính. Tại hướng đi innovation nó cứ thay đổi suốt, mình kiếm được người đồng điệu, cơ hội làm việc trực tiếp thì chưa đến. Nhưng về cái chung, thấy những thứ thú vị, mình ngồi chia sẻ với nhau, đó là cái rất đáng quý ha. Cảm ơn Đạt. Chắc tới giải cuối cùng, Thành ơi, rồi sau đó mình kết thúc.\n\n**[66:14]** Giải Developer of the Year dành cho bé Biên đây. Anh em có in-out giùm cái. Biên ơi, bên đâu rồi? Xin cho biết lý do đề cử là như nào, xin mời. Từ từ anh em, ừ, rồi. Năm ngoái, năm trước nữa thì anh em đều biết, mọi người đâu đó mình có post một bài viết muốn execute theo model là AMA, viết tắt của mastery, autonomy, với cả meaning. Những cái để giải đáp thì thấy chủ yếu quan sát là bạn nào trong team execute được model đấy tốt nhất trong khoảng 1 năm trở lại.\n\n**[68:23]** Cái gì đấy, một cái mà team đang muốn triển khai trong giai đoạn mà AI có thể giúp mình làm phần unit work, rồi solution với design system các thứ. Vừa rồi nó quan trọng hơn đấy. Biên là một trong những anh em ở đâu đó mà đang nghĩ là execute mấy cái gọi là ninh với core goal của team rõ ràng nhất. Mọi người rất ưng khi làm việc với Biên vừa rồi, thì đó là mấy cái chính.\n\n**[69:18]** Mời Biên cho vài lời, xong rồi mình qua phần cuối cùng nhờ. Em ơi, theo comment của anh Thành thấy sao? Cảm ơn mọi người ơi, nếu có thay đổi gì thì cũng không sao rồi. Kết thúc ở đây nhé, phần của Biên dậy xong nhé.\n\n**[70:21]** Giờ tí nữa trước khi mấy anh em họp mình ra quán hết năm với nhau, thì có vài cái mới để định hướng lại sau Tết. Buổi này là buổi gặp tuần như cuối rồi nha, tuần sau đâu có gặp đâu, đúng không? Tại mình giảm thời lượng xuống còn hai tuần một lần. Mấy cái topic đang share với nhau từ cái idea đầu là mình fit mấy cái keyword để ngồi coi tiếp, học tiếp, tìm hiểu những cái mới để phát triển bản thân. Hiện tại mình giảm thời lượng họp xuống còn hai tuần một buổi, thì buổi cuối tháng 1 rồi, chặp sau Tết mới bắt đầu có buổi khác ha.\n\n**[71:49]** Hiện tại sau Tết, định hướng của team mình, mấy cái dự án mà tụi anh, phần Minh Lê á, nó vơi ra về chuyện innovation diễn ra ở những lĩnh vực như vậy. Bản thân mấy anh em trong team management cũng thấy cái đó, thật ra dự án như vậy đang dần về, mình đang build, làm hết trơn rồi. Cả những sản phẩm cũng theo hướng đó. Như vậy là sau Tết có một pha rất thú vị về chuyện làm startup, rồi build public cái trend mà sau khi nhiều engineer bị layoff. Có cái là mấy bạn đó chuyển qua tự build cho bản thân, tự kinh doanh, tham gia hội build public á, lo một cái token hoặc triển khai một quan alpha. Tự nhiên anh thấy cái flow ra, chuyện bốn thứ đó có điểm chung rất thú vị, vô tình hướng team đi theo hướng mà anh nghĩ từ đây đến giữa năm sẽ đẩy nhiều hơn, trên vực tài chính in general.\n\n**[73:07]** Nếu add cái framework đó vô thì gần như nó là một button repeatable, mình vừa có thể làm consulting trên đó, vừa apply hết kiến thức engineering vốn trước giờ khi làm mấy sản phẩm bình thường. Sản phẩm trước giờ chỉ làm trên dataset là list hay array thôi, giờ mình sẽ làm nhiều thứ khác, giải bài toán scale lớn hơn, làm application nhiều hơn được ha. Nếu muốn tự detect, tự lo cái trend của mình cho thị trường đầu cuối thì vẫn được luôn. Cái hướng rất thú vị, chắc để sau Tết, mấy buổi họp kế tiếp sẽ bắt đầu tiết lộ cho anh em. Với định hướng đó thì kỳ vọng mọi người cũng theo đó, thường định hướng của team sẽ quyết định tới nhân sự rất nhiều.\n\n**[74:28]** Nửa là sẽ đổi chính cho đầu năm sau. Với suy nghĩ như vậy, hiện nay anh muốn tạm thời quên đi cái chuyện team mình là team làm việc remote default nữa. Tức là không còn default là vô thì sẽ bị remote với nhau. Anh mong muốn sau mấy buổi qua, team mình sẽ setup lại sao cho mình sẽ sắp xếp lại cái loop của mình tí, với mấy dự án hiện nay đang hơi chểnh mảng, chưa biết giải quyết thế nào.\n\n**[75:15]** Toàn bộ dự án hơi cảm giác nhẹ nhàng á, anh đang gắn cái mác đó là retainer rồi check, cứ vô trỏng ngồi bình thường bình thường. Thì muốn có một cái policy công bố với anh em: từ sau Tết, mọi người sẽ phải đảm bảo lên văn phòng, với các bạn đang ở Sài Gòn nhé. Ở Hub Sài Gòn, mình sẽ không còn set cái chuyện làm việc remote default nữa, mà sẽ làm việc theo hub được ha.\n\n**[76:07]** Hai cái hub official đang có, even là ba cái: Sài Gòn, Đà Nẵng, Hà Nội, thì sẽ phải kiếm chỗ ngồi lại với nhau, resume lại physical connection. Anh em sẽ phải commit 3 ngày một tuần. Hub Long An thì sao? Hub Chân Giang mấy đó thua mấy đó thua mấy nơi. Trong giai đoạn vừa rồi, cảm giác khi mọi người làm remote thì có cái alone zone rất ok. Alone zone sẽ work khi kiến thức đã sẵn, không thay đổi.\n\n**[77:05]** Nhưng khi thị trường thay đổi thì mức độ trao đổi thông tin với nhau, nơi nào diễn ra càng nhiều thì nhân sự ở đó sẽ dễ thích nghi và phát triển hơn. Vì vậy, mấy anh em đang làm remote có khả năng rất cao là sẽ bị tụt lại, chưa biết sao. Có tuyển lại đó, anh sẽ post lại requirement sau cho chỗ Tân Nhật hả, Tân Nhật đúng không? Sẽ sắp xếp lại. Mấy anh em giai đoạn vừa rồi làm remote, khi thị trường không thay đổi về kiến thức thì có alone zone để tập trung làm việc rất thoải mái. Nhưng khi thị trường đổi tí, nơi nào thông tin diễn ra nhiều hơn thì anh em sẽ phát triển dễ thích nghi hơn.\n\n**[77:43]** Đó là lý do tại sao nó diễn ra như hiện tại. Hoặc mọi người phải cực kỳ active trên online, hoặc phải gặp nhau offline, đó là cái buộc ha. Với policy đó, anh em sẽ có 1 tháng để xem thử thứ của mình như nào. Với cái đà thị trường chuyển dịch, event là Meta mới layoff, mới bị magaling, không update thêm đống người, 50% của đó, 3600 người, bỏ hết. Tất cả doanh nghiệp đều rất skeptical về chuyện phát triển cái gì mới, nên cơ hội làm remote hoàn toàn đang hạn chế dần.\n\n**[78:35]** Đang không request mấy anh em thay đổi liền, nhưng anh đang có kế tiếp trước về chuyện như vậy. Trước nhất, anh em đang ở Hà Nội và Sài Gòn sẽ phải đảm bảo ngồi với nhau. Anh sẽ cố gắng sắp xếp team theo khu vực để mọi người resume lại cái của mình ha.\n\n**[79:29]** Đó là 3 ngày/tuần. Trước mắt giữa mấy anh em với nhau thì chỗ anh Thành sẽ… Anh Thành từ khoảng giữa năm thôi, không, anh Thành sẽ relocate lại về Sài Gòn là một. Huy Nguyễn thì đang run cái office ở Sài Gòn rồi. Mấy bạn hay lên office Sài Gòn không vấn đề gì lắm. Hiện tại sẽ test trước với cái đó, nó là policy chính thức luôn sau Tết nhé. Với mấy bạn mà tụi anh biết là đang ở Sài Gòn thì sẽ require cái đó ha.\n\n**[80:14]** Đó là thay đổi chính nhất trong vận hành. Chuyện thứ hai có thay đổi nữa là năm nay sẽ không có ref sharing. Những năm trước thì mình có chương trình ref sharing, năm nay chỉ có lương tháng 13 thôi. Cách đây khoảng tiếng rưỡi là anh đã gửi lệnh đi rồi, giờ mấy anh em đã nhận được lương tháng 13 của mình rồi. Nó là trung bình lương 12 tháng thôi, bạn nào vô trước thì theo tháng, đủ là đủ. Còn sharing năm nay thì không có.\n\n**[81:26]** Một chương trình khác, sharing là khi doanh thu giữ mức cũ hoặc cao hơn, thì thường anh sẽ chích ra bao nhiêu phần trăm trong doanh thu để chia lại theo mức seniority của bạn trong team. 8 năm thì khác, 5 năm thì số khác, mà năm nay sẽ không có cái đó ha. Toàn bộ điểm tin nhanh cơ bản của team sẽ có những thay đổi đó. Nếu anh em quan tâm hơn thì chỗ Inno có gửi một bài lên trên kia rồi. Mấy nay đọc hết chưa nhờ? Đây, mọi người nhìn màn hình nhé. Có bài điểm qua, anh thấy việc team vận hành hiện nay đang rất tốt, mọi thứ đã vào rượt với nhau hết rồi.\n\n**[82:41]** Bây giờ chỉ có lựa chọn thị trường nào, cơ hội nào để có biến động lớn, đột biến về thu nhập thôi, đó là cái chính. Còn với mô hình, cách vận hành hiện tại thì anh nghĩ rất happy với những gì đang diễn ra nhé. Nếu không còn gì khác thì chúc anh em tối nay ăn tối nhẹ nhàng, tình cảm với nhau ở hai địa chỉ. Hẹn gặp lại sau Tết với kế hoạch chi tiết hơn, recruitment mới hơn, cũng như mở ra lĩnh vực mới về tài chính. Anh nghĩ cơ hội đột biến tài chính cũng sẽ nhiều đó ha.\n\n---\n\n### English transcript\n\n**[01:29]** Alright, folks. It’s been a while since we were on stage. According to Thành’s plan, there are four presentations today as usual. I heard Phát mention four, just like every other time. But since there’s going to be a recap later, I think Thành wants to squeeze in one more.\n\n**[07:48]** Let’s shift the schedule a bit, Thành. Originally, the other team was going to continue the Engineering series the one the artisan group had prepared and presented. But I think for now, we’ll postpone that. Maybe next week or after Tết.\n\n**[08:09]** We’ll bundle it into a combo session and gradually introduce it to everyone, especially after that two-month stretch. As for the expanded topics and what’s next, we’ll save that for later. Let’s move to Biên’s part this is the second part of Minh Lưu’s talk on Erlang. He already did one last time, and now we want to review and get everyone’s thoughts.\n\n**[08:32]** What did you think of the last session? Do you still remember it? This time the vote is kind of split. I guess we’ll bring up Minh Lê’s talk to check it out.\n\n**[08:53]** Last time, I saw a few folks in the team starting to write things up it looked pretty solid. In the end, it got published today. So we’ll probably have two, maybe two and a half topics in total for everything. A few people are still missing. Where’s Anh? Where are the others? Everyone’s disappeared?\n\n**[09:34]** Alright, come on up. Let’s do a quick recap. Hey, where’s Anh? There he is. Okay, looks like we’ve got everyone. Let’s get the others in too. Tuấn, you back from your trip? Did you buy anything? You’re hiding in the office, right? Must’ve had fun. If we’re changing the schedule, then let’s start with Minh Lê’s session first. That was two weeks ago.\n\n**[10:32]** I saw a few posts already went live. I’d like to hear it directly and get a sense of the market report quality. Let’s let everyone see it too. I’ll also chime in a bit with the market section. After that, we’ll do the wrap-up. There are a few policy updates I want to announce.\n\n**[10:54]** There’s going to be a small update coming up. Then we’ll do a year-end wrap-up. Inno already wrote a post for that. Let’s read it together and see how it looks. After that, we’ll get to the final presentation by Minh Lưu. I think it’s the same one Minh Lưu did last time.\n\n**[11:22]** What was Minh Lưu’s last session about again? Wasn’t it about finite state machines in Erlang? Do you still remember anything from that? How would you rate it, just a quick take, like in one minute. Do you remember what it covered? How focused it was?\n\n**[11:57]** That was last month, right? Minh Lưu did a talk on finite state machines. Do you remember how important that was? If so, why use Erlang for it when Elixir can do the same thing? Did Huy Nho work on that one? Did he join the session? I remember your team mentioning a lot about building state machines.\n\n**[12:26]** Was Huy involved? Minh Lưu’s team Yolo is using that stuff in their projects, right? And on your side, at Ascenda, I think both teams have already adopted it. They’re building a framework, then the controller, and even a stage controller, right? On our side, we’re also experimenting with those ideas trying to figure out how to optimize state processing flow in our current projects.\n\n**[12:56]** Like using round-robin mode, right? On this side, we define the states first. Then each state sets a context, like whether it’s entering a state or which state it currently is, so the outside logic just switches between them. As for the Yolo side, I’m not too sure, I haven’t read Minh Lưu’s write-up in detail yet.\n\n**[13:19]** That should be good, right? A few more folks just jumped in nice. Okay, let’s have Minh Lưu go first. Minh Lê, hang tight for a bit. Come on up, Minh Lưu. Just share a bit, it’s important. The team needs to understand this direction better.\n\n**[13:41]** You’ve got 10 minutes. That should be enough. This session continues from the last one. In a moment, I’ll explain again why this matters for our team. I really want everyone to understand this deeply. It’s a technique not widely used in Erlang, which makes it a bit special. Once we figure this out, we can apply it to other projects too.\n\n**[14:12]** Alright, Minh Lưu, you’re up. I’ve got the slides ready. I’ll review a bit from the previous session so everyone can recall it better. That one included a behavior pattern, basically a way to define a module. You predefine some hooks, then handle different scenarios based on those. It’s kind of like a scaffold for core logic.\n\n**[14:49]** Then you implement those hooks. The two most common are gen_server and gen_statemachine. For most modules, you can just write a server. Especially for simple state machines, gen_server is usually enough. But depending on the use case, you have to be careful, don’t just use them interchangeably.\n\n**[15:14]** “Gen” stands for “generic” generic server. You should only use gen_statemachine when you really need more advanced features. For example, inserting events manually into a stage, or triggering a specific action when transitioning between stages. Those are things gen_server doesn’t handle well.\n\n**[15:36]** If you want to automatically run an action when entering a new stage, that’s called a state entry. Or when you want to set timeouts between steps. In practice, gen_statemachine is used when you’re building something persistent—something that needs to hold state over time.\n\n**[15:59]** Like a TCP connection, for instance. State machines are commonly used for that. When you’ve already defined a fixed set of states and need the connection to persist, gen_statemachine is a solid choice. For simpler stuff, gen_server is enough—no need to complicate it. Today, I’ll walk you through an actual code example so it’s easier to understand.\n\n**[16:24]** Like Minh asked last time why use Erlang when Elixir can already handle this? And yeah, that’s true. You can implement behavior in Elixir the same way you do in Erlang. So today, I’ll do the example in Elixir so everyone can compare and see which approach is more practical.\n\n**[16:49]** Elixir code looks a bit more readable. Do we have the slides? Here’s the one on gen, let me show you more detail so you can see how it works. Okay, I’ll insert the slides now. We’ll build a module that connects to a Redis server over TCP. I’ll keep it simple, just two stages so it’s easier to follow.\n\n**[17:34]** Two stages only. This module sits between the user and the Redis server. It implements methods to establish and maintain the connection. For example, when the process starts a connection, all user requests will return a “disconnected” status. The user has no idea what the actual connection state is they only see what the proxy process returns. So, two stages: when the process starts, it’s in “disconnected”.\n\n**[18:21]** It’ll try connecting to Redis. If the connection succeeds, it switches to “connected”. If the connection fails or drops, it goes back to “disconnected” and retries. The incoming requests client event requests are received as events. These can arrive in either disconnected or connected state. If disconnected, we just return that state to the client.\n\n**[18:41]** If connected, we forward the request to Redis, get the data, and return it to the client. Why a state machine? Because we need features like insert event, like I mentioned. Here let me zoom in so you can see.\n\n**[19:39]** First, we implement the behavior in the state machine and define the data structure. It has two parts: State represents the current stage of the module, and Data carries information between transitions—like host, port for connecting to Redis DB, and a map of client requests with client IDs. When the process starts, the first step is to connect to Redis in the background, while returning “disconnected” to the user.\n\n**[21:04]** First, we define the callback mode with two things: the state function which means naming the functions in the module according to the stage name, for example, if the stage is disconnect, then the module will automatically run the disconnect function to handle it based on the name and arity and the state enter, which I’ll explain in a bit. We start with the init function, which returns next_event, meaning the module will automatically insert an internal event called internal_connect, with data being the host and port to connect to the Redis server, and then it immediately returns the disconnect state. When it returns disconnect, it jumps into the disconnect function to handle things. The disconnect function with internal_connect will open a TCP socket using gen_tcp to connect to the Redis server and start setting up the connection in the background.\n\n**[21:49]** The part about state enter, I’ll explain that in a bit. That’s what the state function means. After that, we begin with the init function like I mentioned earlier. What does it do? It’s simple: at the start of the function, we use a term called next_event. What’s next_event? It means the module will insert like automatically insert an event into its own queue. This event is internal_connect, using the data host and port to connect to the Redis server, and then immediately return the state disconnect.\n\n**[22:43]** When it returns the disconnect state like that, the first thing it does is run the disconnect function to handle the logic. What does disconnect with internal_connect do? First, it opens a TCP socket using gen_tcp to connect to the Redis server. But then I realized there are too many details, so I thought maybe I should try combining a few of the functions. You mind if I jump in and show you the part where it defines the two stages? That part looks a bit messy. Where exactly are the two states defined? You can see here it has a list over there, and we just have two states.\n\n**[23:48]** Here are the two states, and four overlapping functions between disconnect and connect. Minh has four overloads of connect, but only two states, which is normal. In Elixir, function overloading works based on the parameter list, so it knows which one to run. What about the part where the state transition happens? Oh right, it’s inside each function.\n\n**[24:42]** For example, in this connect function, when the connection is successful, it returns an atom next_stage, which switches the state to connect right there, along with the data. Got it, that’s the part that triggers the state transition. But with each overload like that, calling it implicitly like this is that okay? Normally, stuff like this should have a controller to manage it. Calling each one like this feels a bit messy. If there are 10 stages and we define and call them all this way, it becomes impossible to manage.\n\n**[25:01]** Right at this part here, in our backend. Since we’re using state functions, there’s something called a handle_event function. When we switch to this mode, we only need to define a single function called handle_event. Just one function to manage all the states in there. If you want to write it that way, the system lets you choose between defining per-state functions or defining one handle_event function.\n\n**[26:14]** Now where’s the diagram, have you looked into this or used it before? For a clearer picture, I copied a connection library that also connects to TCP. It’s written using gen_server, doesn’t use a state machine. It’s built kind of like a button logic. If the connection fails, it backs off and retries. It also manages users inside a map. But it’s a bit longer, you have to reimplement some of the built-in things like timeout, for example. It doesn’t come built-in so you have to code that manually. And to answer the earlier question yes, I just recently learned and started working with this. I’ve known about it for a while, but only recently started applying it. So how did it feel when I actually got into it? Why is it important? Because it defines some structured ways that make things more convenient. For example, for handling timeouts, it defines that based on the data we return.\n\n**[27:47]** Here’s an example. When your connection drops, it jumps into the disconnect function to handle it, and you can return an atom timeout along with a time value. After that time passes, it’ll automatically jump into this function to process it for you. Yeah, so it helps make these things more convenient. You could write it using gen_server too, but that would be a lot longer. Okay, okay, thanks Minh. Minh Trần, any questions? Anyone else?\n\n**[29:14]** Let’s have Minh Trần code this, and code it in Elixir. Since you know it already, have you touched this part before? And ask questions to help spread the knowledge to the team. So here’s how it works: the fundamental difference in programming with our team is that among all the modern programming languages right now, none of them have a built-in library for state machine management like Erlang. Erlang is the only one that was born for running these kinds of automatic systems, not from the traditional client-server mindset where a server just sits there waiting for requests.\n\n**[29:30]** This is built-in, right? And with this kind of built-in feature, when you write code, two things happen. First, it forces the whole team to learn this language, and once they start using the tooling, it shapes a shared mental model a way of thinking that everyone can follow. When you encounter the right use case, you pull out the right tool and solve the problem by thinking about it this way.\n\n**[30:07]** Back when we were doing things like the C4 model, we were already touching this stuff, weren’t we? It gives you the tooling. The second benefit is that it unifies everyone’s mindset. Of course, you can still code using gen_server the traditional way, that’s totally fine. No problem with that. But what makes Erlang special is that it has this built in.\n\n**[31:13]** Once the team really gets this, managing a codebase through state machines, switching between states and objects, helps you keep the codebase much more maintainable. You avoid writing implicit code, bouncing it around in different places, not knowing where the transitions are actually happening. Instead, you can focus more on the logic and the structure, rather than getting caught up in the underlying data. Otherwise, in big codebases, bugs will pile up everywhere, and if you don’t look at it with this kind of mindset, it gets messy fast. That’s why this thing is special. For Erlang, this is what makes it stand out. In Elixir, there’s no official library built like this, so you still have to call into Erlang directly.\n\n**[32:08]** Alright, that was a quick wrap-up of Minh Lưu’s talk. If he’s going to do a follow-up session, I’d recommend something a bit more complex, something more realistic. The last example only had two stages, which still looked simple. Maybe look for some open-source code from other teams, ones that manage more complex state. There’s this example I saw from another group that manages WebSocket state—it had a ton of stages. That example also used gen_statemachine, and the file was over a thousand lines. When they consolidated the functions, the structure became clear it wasn’t just code pushing back and forth randomly. There was a dedicated manager handling everything. So even if you lose track and come back later, it’s still easier to read than trying to understand a bunch of functions with custom names. That gets exhausting fast.\n\n**[34:18]** Building a shared mental model within a team is super important. Thanks, Minh Lưu. Now over to Minh Lê.\n\nYeah, let me check how many minutes I’ve got. Ten minutes, right? Okay, ten minutes. I think I have four posts already?\n\nBack then, the consulting team had the idea of doing a weekly series—one post per week summarizing info related to our testing side. But it wasn’t too deep on testing more of a general roundup. The first one I wrote was back in mid-December last year. It covered the release of Google’s Gemini 2.0 and OpenAI’s model that generates video chains.\n\n**[34:38]** They had visuals showing it applied to patients, or healthy people wearing something like smartwatches to track their heart rates like what we wear. In the consumer space, the focus was more on secret tech, like rendering video similar to Sora. On the crypto side, they’re also embedding AI into fintech, gaming, infrastructure, and so on.\n\n**[35:21]** Same goes for Y Combinator. They’re also saying AI should be applied to everything right now. At the moment, they’re encouraging research into stablecoins. In the past, they were promoting Bitcoin and Ethereum for payments. But after testing it on the market, they realized it wasn’t a good fit. So now they’ve shifted to studying stablecoins—aiming for enough market share that companies would invest money into R\u0026D and build payment solutions.\n\n**[36:21]** Here I’ll briefly mention a product an AI agent platform on Solana. They’re positioning it as a platform to build AI agents. These agents help manage wallets, generate tokens, trade automatically basically automating the kind of stuff crypto and DeFi users normally do. This product helps people get it done easier with AI support.\n\nThat was the first post. Moving on to the second. Out of the four, there are a lot. Now the real question is out of the four posts from last month, what do I think actually benefits our team?\n\n**[37:13]** Yeah I think we’re on the right path now focusing on AI tech getting used to agent stuff blockchain and all that it’s growing fast booming really hot right now probably gonna see a lot of products diving into that soon the other tracks like what Y Combinator or a16z suggest might be a bit out of reach for our market so harder to jump in but last week I saw this report about money flow in and out of Southeast Asia for Q4 2024 these were the top sectors getting capital.\n\n**[38:23]** First one’s challenger banks different from traditional banks fully digital I think in Vietnam we’ve got one that’s sorta digital but it’s not really what they define as a challenger bank like Timo they usually call that a neobank challenger banks are the ones that have a banking license they can issue their own digital banking products.\n\n**[40:08]** Neobanks in Vietnam usually sit under a traditional bank’s license so they can’t release their own banking products in Southeast Asia VCs are pouring money into challenger banks because they think it solves the access to finance problem in underserved areas more so than crypto.\n\n**[40:30]** I checked salary data too for IT in Southeast Asia Philippines is super competitive big population education’s solid IT workforce is huge working with foreign companies English is good pricing better than Vietnam even.\n\n**[41:26]** Vietnam saw a huge drop in Q4 over 80% down meanwhile Philippines is booming ecom’s growing fast all the digital stuff kinda like where we were three or four years ago now they’re getting the investment okay interesting scroll up a bit everything above sounds like it’s tied to money right banking currency finance heavy focus on finance seems like that’s where SEA is betting big.\n\n**[42:39]** If that’s the case maybe next week we can sit down and go through some potential lists try finding those ecosystem maps or system overviews from them can we find that yeah I’ll look for it most reports have those yeah okay anything else besides that this week’s quiet it’s Christmas not much news kinda dull I did bookmark a few new blockchain products they’re getting heavy capital inflows and locked funds blockchain huh show me Liquid yeah hold on they shared it in the chat already it’s solid.\n\n**[44:19]** If that’s the direction then for writing this post I think there’s a good angle if you can frame it this way our team’s slogan from the start has been empower innovation meaning we look for where innovation is happening to find ways to work with them support them even invest if we can biggest value from doing this is spotting where innovation is instead of broad headlines we can pinpoint where the market’s pulsing here and there that’s where innovation’s happening inside that what’s the business model financial model answer those clearly it lines up with our core direction and helps us analyze business cases better too.\n\n**[47:25]** Try that angle yeah thanks Minh we’ve been missing that knowledge has been kinda scattered this perspective helps when we review projects just ask where’s the sale where’s the money another way to ask is where’s innovation happening who’s building something new there’s where we can jump in and give it a look the stuff that already exploded we just retain and patch it up boring stuff alright that’s it for this part now quick year-end wrap up everyone’s going offline after this quick one Huy and Thành got your sections ready done already looks like it.\n\n**[47:50]** This year the market shifted so much all the assumptions from before are out the window even our own people have moved around a bit right\n\n**[48:53]** Our direction’s also shifted we used to focus on enterprise but then AI came and wiped out the whole thing now there’s chaos heading into finance and blockchain is where the flow is AI’s booming yeah but actual enterprise use still low everyone’s just chasing hype main trend is still around finance.\n\n**[49:55]** Turns out our engineering team’s been researching and learning those exact topics so for year-end announcements how many awards do we have team got four awards plus one from community there’s one top-level award too others are kind of honorable mentions these are to recognize high performers across key areas of our team okay let’s announce them seven awards based on four main ones right.\n\n**[51:05]** One team award then three others first one’s for the person who executed the AMA model best second one’s team lead of the year whoever led their squad the strongest third one’s for top consulting performer two projects that got great feedback both from team and from the client third one’s also a team award either for successful inhousing or trust-building and upselling for that team.\n\n**[53:38]** Final award goes to the community folks supporters who aren’t official team members but still worked with us joined activities shared stuff so that’s four right Developer of the Year is based on MMA model meaning mastery autonomy second is research-oriented team lead third is consulting three individual awards one team award for the most impactful project others that didn’t have impact won’t be mentioned last one’s the community award and wait who’s even in the community this year felt pretty quiet right yeah someone did work with us a few months though.\n\n**[54:26]** Alright let’s do a quick announcement let people in-out easily okay if I make it formal it’ll take too long so let’s roll with it someone react on Zalo or Slack so we can screenshot it two VIPs already confirmed good okay second award Thành you got that one what was it again check the notes yeah.\n\n**[55:58]** Lead give us a quick reaction wait is this one individual yeah okay can we do a brief overview so far performance looks about the same as previous years not sure on the details go ahead yeah Huy might add more in a bit from what Thành saw and how Phúc did honestly in previous years Phúc mostly worked on internal and consulting this year moved to team Yolo.\n\n**[57:21]** Honestly his role was kinda light compared to backend expectations before he focused mostly on iOS at first folks were skeptical about him but heard he adapted well in like five six months the team over there seemed pretty happy with him and they’re known to be tough Huy probably has more accurate comments since it was his project wait is that your feedback Huy he’s talking about me well I think he handled quite a lot pushed back where needed made suggestions.\n\n**[58:46]** I think Thành picked Phúc because he met people’s expectations before he’d finish projects but stay quiet didn’t really engage his skills were always good but this year went beyond that like picking up new styles getting into loops exploring JavaScript TypeScript stuff he hadn’t touched before plus started getting into deadlines client timelines and pushing back like recently someone asked if he could finish fast and Phúc said nope not gonna make it let’s do it slower that’s a good sign and then he started getting assigned tougher development tracks not necessarily best overall but his trajectory’s impressive alright Superman Phúc are you coming up to say something.\n\n**[55:58]** Lead, give us a quick reaction. Wait, this one’s personal, right? Yeah. Okay, can we go over it briefly? So far performance seems like previous years, not sure on the details, over to you. Right, Huy might give a more detailed comment later. But based on what Thành saw about Phúc this past year, actually in previous years, Phúc mostly worked on internal projects and consulting. This time he moved over to the Yolo team.\n\n**[57:21]** Honestly, the role was kind of weak compared to backend depth expectations. Before that, Phúc was mainly focused on iOS. At first the team over there was skeptical about him, but apparently he adapted really well. In five or six months, the team there seemed to rate him highly, and I heard they’re pretty tough. Huy probably has the more accurate comments, since it was his project. Hey, this guy’s commenting about me. Actually I think those who worked on this contributed a lot, handling pressure, estimates, all that.\n\n**[58:46]** But I think Thành picked Phúc because he matched expectations well. He’d done projects before but stayed quiet, didn’t communicate much, just got it done. Super talented, but this year he went way beyond that. For example, picked up new styles, loops, worked with JavaScript, TypeScript, even though before he wasn’t really involved with those. He also started reviewing stuff with clients, catching issues with deadlines. The best was when someone wanted to rush something and Phúc said no way, it won’t make it, take it slow. That was a good sign, and then he started being trusted with tougher, more intense features. Not necessarily the best, but probably the most solid expectations-wise. Alright, Superman Phúc. You coming up to speak?\n\n**[61:03]** How am I supposed to sit like this, I just plugged in my headphones. I was pretty surprised, honestly. Was catching a ride to go do charity stuff and didn’t expect this. Don’t really know what to say, but seems like a lot of people heard already. Oh hey, Ngọc Thành just joined, long time no see. Phúc, can you hear us? He’s muted, I guess. I’m outside right now, so this is the second award. Do I think I deserve it? A bit shy to say, but honored. Yes, that’s right. Next award is from Huy, let’s go ahead and present it to the Yolo core team. Okay, Yolo team meaning?\n\n**[62:28]** Yolo’s considered a standout team. Yeah, agreed. This is the last award, the rest goes to this person. This one goes to someone who spent about 2–3 months interning with us, mostly researching software-related stuff.\n\n**[63:20]** Actually, Đạt had already been pretty active on our server even before interning. Even after the internship ended, he kept contributing to AI builds. The posts he shared got forked and reshared quite a bit by core team members. A great example of someone we want to support in the community. Đạt should probably say something. Đạt, the community award is yours. Where are you now?\n\n**[64:26]** I’m outside right now, still on the way. Everyone’s done with meetings already? Thanks so much for the time. Honestly, Tom and Thành helped me a lot during onboarding and sharing stuff back to the team in a way that could be understood and used. Anything else? No, I think that’s it. Thanks again for your time.\n\n**[65:11]** The way you’ve been sharing what you read with everyone is super appreciated, Đạt. Even now, in terms of internal knowledge sharing, that’s been one of the most valuable things. Because innovation is always shifting, and we haven’t had a chance to work closely together yet. But in the bigger picture, sharing cool ideas with each other like that is really meaningful. Thanks, Đạt. Now, final award, Thành. After that we’ll wrap up.\n\n**[66:14]** Developer of the Year award goes to Biên. Everyone, give a quick in-out reaction. Biên, where are you? Please share the reason behind the nomination, go ahead. Alright, hold on everyone, okay. Last year and the year before, as you all know, we had this idea posted in the group we wanted to execute based on the AMA model: mastery, autonomy, and meaning. The way we determine the winner is by observing who in the team executed that model the best over the past year.\n\n**[68:23]** This is something the team really wants to roll out, especially now that AI can help with unit work, solutioning, design systems, all that stuff. It’s become even more important lately. Biên’s been one of those folks who really seems to embody the core goals of the team most clearly. Everyone really enjoyed working with Biên recently, so that’s basically the main point.\n\n**[69:18]** Biên, please say a few words, then we’ll move to the final part. What do you think, based on Thành’s comments? Thanks everyone. Even if things change, that’s alright. Let’s wrap this up here, that’s it for Biên’s part.\n\n**[70:21]** Later, before we all head out to wrap up the year together, we’ve got a few new things to reorient after Tết. This is basically the final weekly sync, right? Next week we’re not meeting anymore, since we’re cutting the frequency down to once every two weeks. The topics we’ve been sharing starting from keywords and ideas are for us to keep learning and exploring to develop ourselves. Since the syncs are now biweekly, the next one will be the final one in January. After Tết, we’ll pick it up again.\n\n**[71:49]** After Tết, the direction of our team and the projects we’ve been building especially the ones Minh Lê’s been involved in are leaning more into innovation in those specific fields. Even the management team sees that. Those types of projects are starting to come in, and we’ve been building them. Our products are heading that way too. So after Tết, there will be a very interesting phase of startup-style building and joining the build-in-public trend, especially after many engineers were laid off. Some of them have started building on their own, launching businesses, participating in build-in-public communities, launching a token, or deploying alpha features. I’ve noticed this natural flow of four themes coming together, and it’s led our team in a direction I think will continue through mid-year—focused more on the finance sector in general.\n\n**[73:07]** If we plug that framework in, it’s almost like a repeatable button. We can do consulting based on that, and at the same time apply all the engineering knowledge we’ve built up from working on regular products. Products used to just be about datasets lists, arrays. Now we’re moving toward more complex problem-solving, scaling, and building real applications. If you want to self-detect and manage your own trend in the end market, that’s also possible. It’s a really interesting direction, and probably after Tết, we’ll start revealing more during future syncs. With that direction in mind, the team’s strategy will influence personnel decisions a lot too.\n\n**[74:28]** Some changes will kick in at the start of next year. With that in mind, I want to temporarily move away from the idea that our team is default-remote. Meaning, it won’t be assumed that everyone just works remotely by default anymore. After the last few syncs, I’m hoping we can reset our rhythm a bit. Some of our current projects feel a bit scattered, and we’re not quite sure how to handle that yet.\n\n**[75:15]** The whole project vibe feels a bit too casual. I’ve been labeling it as retainer work just show up and coast. So I want to announce a new policy: after Tết, everyone in Saigon needs to start working at the office again. We’re removing the remote-default setup for the Saigon Hub. Instead, we’ll move to a hub-based working model.\n\n**[76:07]** We have two, even three, official hubs: Saigon, Danang, and Hanoi. Everyone should find a place to sit together again, rebuild that physical connection. You’ll be expected to show up 3 days a week. Yeah, those are a bit different. Over the past stretch, working remotely has been great for the “alone zone.” Alone zone works when knowledge is stable and not changing.\n\n**[77:05]** But when the market shifts, the teams that communicate the most are the ones who adapt and grow the fastest. So if you’re working remotely, there’s a high chance of falling behind. We’re even thinking of reopening some hiring, I’ll repost the requirements soon. We’ll restructure things. Remote worked great when the knowledge was stable, but now that things are moving, you need those dense information zones to evolve faster.\n\n**[77:43]** That’s why things are shifting the way they are. Either you need to be extremely active online or you meet people offline. That’s the hard rule. With this policy, you’ll have one month to figure out what your setup looks like. Given the way the market is moving, Meta just laid off another batch, didn’t update the rest, dropped 3,600 people everyone’s skeptical about building new stuff. So fully remote opportunities are starting to shrink.\n\n**[78:35]** We’re not asking anyone to change overnight, but this is the direction. First, everyone in Hanoi and Saigon should start sitting together again. I’ll try to arrange teams by region so you can get back into your flow.\n\n**[79:29]** So that’s 3 days per week. At the moment, with our current members—Thành, for example he’ll relocate to Saigon sometime mid-year. Huy Nguyễn is already running the Saigon office. Folks who’ve been going in regularly shouldn’t have any issue. We’ll test this out first, and it’ll become the official policy after Tết. Everyone we know who’s currently based in Saigon will be required to follow it.\n\n**[80:14]** That’s the main change in how we operate. The second change is: there will be no ref-sharing this year. In the past, we had a ref-sharing program, but this year there’s only the 13th-month salary. About 90 minutes ago I pushed the button, so everyone should have received it by now. It’s calculated as the average of your past 12 months’ salary. For newer folks, it’s prorated by month. But no extra sharing this year.\n\n**[81:26]** The sharing program usually kicked in when revenue stayed stable or grew. In those cases, I’d take a percentage of revenue and distribute it based on team seniority. Someone with 8 years would get more than someone with 5, etc. But this year, there won’t be any of that. That’s a quick summary of the core changes in how the team’s operating. If anyone wants more info, Inno posted an article earlier have you all read it? Alright, take a look at the screen. There’s a post there. I think the way we’re running things now is going really well. Everything’s clicking together.\n\n**[82:41]** Now it’s just a matter of choosing the right market and spotting opportunities that can trigger major income jumps that’s the real goal. With our current model and the way we operate, I’m really happy with how things are going. If there’s nothing else, enjoy dinner tonight with the team at either of the two meetup spots. We’ll reconnect after Tết with more detailed plans, new recruitment, and new directions especially around the finance sector. I think there’ll be a lot of breakout financial opportunities ahead.","title":"OGIF Office Hours #38 - Erlang automata p2, market report, DOTY, Year end celebrations ","short_title":"#38 Erlang automata, AI Trends, Year-End Awards","description":"In OGIF 38, the team explored Erlang automata, AI and fintech market shifts in Southeast Asia, year-end reflections with team awards, and a new hybrid work direction for 2025.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sun Jan 19 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/38-20250117.md","slugArray":["updates","ogif","38-20250117"]},{"content":"\n### Topics and Highlights\n\n- **Session setup \u0026 check-in**: Kicked off with a casual vibe, confirming no all-hands this week and setting up three talks. Phát skipped his slot, and the team troubleshooted screen sharing for demos.\n- **AI fine-tuning overview**: Explored fine-tuning vs. retraining, using a doctor’s note example to show how fine-tuning embeds knowledge while retraining leans on token-heavy prompts.\n- **Fine-tuning demo**: Showcased a Duty 40 Mini fine-tuning job on Open AI (~4800 tokens), comparing pre- and post-tuning results, with a nod to local vs. hosted model trade-offs.\n- **Data archiving essentials**: Biên broke down archiving vs. backup for apps with 50K-1M daily transactions, focusing on metadata, cloud storage, and recovery to optimize query performance.\n- **Archiving tools \u0026 Q\u0026A**: Highlighted tools like AWS, Google Cloud, and Timescale, plus hot/warm/cold storage options (Azure, Backblaze), with audience questions on scheduling and platform quirks.\n- **Datalake foundations**: An traced datalakes from 1980s databases to today’s cloud systems, contrasting warehouse ETL (structured) with datalake ELT (raw data) workflows.\n- **Notion’s datalake scaling**: Detailed Notion’s growth to 96 instances and 400+ shards by 2023, shifting from warehouse to datalake with Debezium CDC, Kafka, Hudi, and S3 for analytics.\n- **Interactive wrap-up**: Fielded questions on datalake vs. replication, async processing, and external data handling (e.g., social media), ending with reflections on big data skillset.\n\n### Vietnamese transcript\n\n**[00:00]** Hình như tuần sau mới có all-hand. Không thấy ai tạo event gì hết, chắc tuần này cứ bình thường thôi nhá. Anh em kiểm tra xem màn hình sharing có vấn đề gì không. Hay lên luôn nhỉ? Hôm nay chắc có ba bài thôi đâu đó. Phát vừa bảo tuần này cậu không có gì mới, chắc skip hôm nay rồi. Anh em thử share màn hình cá nhân xem sao nào. Xem trước được không?\n\n**[10:42]** Theo lịch chắc anh nhỉ, để em lên trước nhá. Fine-tuning này, chủ đề này không mới lắm đâu. Bài này chỉ là 100.5 thôi, không phải 101, nên chỉ giới thiệu sơ sơ, chưa đi sâu được đâu. Tại em cũng mù mờ lắm, nên chắc giới thiệu sơ vậy thôi. Hôm nay em giới thiệu bài fine-tuning. Đây là agenda của bài này nè.\n\n**[12:24]** Introduction là nếu mọi người dùng AI, chắc có nghe tới khái niệm fine-tuning rồi. AI có mấy mô hình đa số được fit vào dữ liệu từ một ngày nào đó, với mấy cái data privacy hoặc data của domain riêng. Dữ liệu này không xuất hiện trong knowledge của mô hình nền tảng (foundation model). Để mô hình có được kiến thức đó, người ta thường dùng retraining, đúng không? Nhưng còn một cách khác gọi là fine-tuning. Cuối bài, em sẽ so sánh hai cách này, xem lúc nào nên dùng cái nào, lúc nào không.\n\n**[13:08]** Trước mắt, cứ hiểu fine-tuning như cách để mở rộng kiến thức cho mô hình nền tảng vậy. Fine-tuning là gì? Hiểu đơn giản là mọi người retrain lại mô hình, lấy một mô hình nền, đưa vào một dataset gì đó để fine-tune, nghĩa là retrain lại nó. Sau khi fine-tune xong, ta được một mô hình đã điều chỉnh, gọi là fine-tuned model.\n\n**[13:44]** Tại sao fine-tuning mang được kiến thức mới? Hiểu đơn giản là trong một AI model, kiến thức được lưu qua mấy cái mạng nơ-ron. Fine-tuning sẽ cập nhật các weight, các thông số của mạng nơ-ron đó, để nó phù hợp với kiến thức mới. Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi.\n\n**[14:19]** Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi. Khi fine-tune mô hình trong thực tế, không phải chỉ ném dataset vào rồi retrain là xong. Đúng là ra một mô hình fine-tuned, nhưng không biết cái mô hình sau retrain này có tốt hay không. Em sẽ giới thiệu một workflow mà bên ngoài thường dùng để fine-tune.\n\n**[14:59]** Cái flow này, nó gồm nhiều bước như thế này. Mọi người có thể chia thành hai cụm, hai cụm nhá. Cái flow này, em sẽ chia thành hai cụm. Em đi qua cụm một trước. Cụm đầu tiên là cụm ở bên trái, hiểu đơn giản là một base model ban đầu. Sau đó, mọi người có một dataset mới, một cái gì đó mới, mọi người quăng vô, fine-tune nó. Rồi nó ra một model, mọi người sẽ supervise nó, có nghĩa là mọi người retrain nó dưới kiểu là retrain nó. \n\n**[15:38]** Sẽ cho nó thêm kiến thức, với input này thì output sẽ ra như này. Nó sẽ ra một cái gọi là supervised fine-tuning. Sau đó, để em đi qua phần tiếp. OK, cái fine-tuning này là retrain on data, nghĩa là sẽ cho một cặp input-output trong dataset để nó học. Nó sẽ học được những kiến thức mới đó. Để bước này hoàn hảo, dataset phải được clean. Nó phải clean, không được lẫn với những cái khác. Nghĩa là nó phải specific cho cái domain mà mình muốn train nó.\n\n**[16:31]** Quay lại hình này, sau khi mọi người có một cái model đã được retrain xong, mọi người mang lên production, mọi người dùng, đúng không? Lúc này, bên ngoài sẽ sử dụng một cái system gọi là human feedback. Kiểu như là response của model này có làm bạn hài lòng không, chấm từ 1 tới N sao, kiểu vậy á. Mọi người sẽ collect cái data đó. Nó nằm ở bước này, mọi người sẽ thu thập human feedback từ cái retrained model của mọi người.\n\n**[17:06]** Dựa vào cái feedback đó, mọi người gọi bước này hơi tốn tài nguyên chút. Mọi người sẽ phải retrain một cái model riêng. Cái model này dùng để đánh giá xem response này được chấm bao nhiêu điểm. Kế đó, mọi người tới bước thứ ba, bước cuối. Ở bước này, mọi người sẽ dùng thuật toán như reinforcement learning để kết hợp với cái retrained model và cái reward model của mọi người.\n\n**[17:46]** Mọi người retrain, mọi người lại fine-tune cái model một lần nữa. Nó sẽ ra cho mọi người một cái gọi là model tối ưu. Cái vòng lặp này cứ tiếp tục, tiếp tục mãi. Mọi người có cái model đã retrain xong, thu thập human feedback, rồi kết hợp ba cái đó để retrain cái model thêm lần nữa. Càng ngày, cái model sẽ càng ok hơn với những gì mà mình muốn. Đó là cái flow mà em thấy bên ngoài, trong production, người ta hay dùng. \n\n**[18:24]** Trong quá trình fine-tuning, ,ọi người sẽ thường nghe tới khái niệm gọi là catastrophic forgetting. Nghĩa là sao? Nghĩa là khi mọi người retrain kiến thức mới vào, nó sẽ làm giảm performance với những kiến thức cũ. Tại sao chuyện này xảy ra? Như em đã nói, kiến thức của một model dựa vào mấy cái weight, dựa vào kiến trúc của cái model đó và những tham số của nó. Tham số dynamic trong một model là mấy cái weight. Khi mọi người retrain, mấy cái weight này thay đổi, đúng không?\n\n**[19:00]** Khi nó thay đổi, có phải là kiến thức cũ sẽ bị giảm bớt độ chính xác đi không? Nếu trong dataset của mọi người có nhiều dữ liệu bị overfitting, nghĩa là dataset của mọi người quá đúng, quá đúng trong cái dataset đó. Khi một người quăng cái gì mới vào, nó sẽ sai với những cái cũ đi. Người ta gọi đó là overfitting, nghĩa là nó bị fold in quá mức vào những cái training data. Khi gặp data mới, nó sẽ giảm performance. \n\n**[19:42]** Nên lúc này, bên ngoài người ta sử dụng một kỹ thuật gọi là parameter-efficient fine-tuning, gọi là PEFT. Nó có nhiều cách, nhiều kỹ thuật trong method này, như LoRA này kia. Nhưng trung quy, đa số bọn họ không phải update hết tất cả các weight trong cái model đó. Bọn họ sẽ chỉ đóng băng những layer nào không cần thiết. Họ sẽ đóng băng mấy cái layer không cần thiết, rồi chỉ update một số lượng nhất định các weight thôi. Để tránh trường hợp kiến thức cũ bị mất đi quá nhiều. Đó là cái cơ bản. Còn sâu hơn về mấy cái algorithm đằng sau, mọi người có thể tự tìm hiểu. \n\n**[20:27]** Quay lại câu hỏi lúc ban đầu, ha, nó với retraining khác nhau thế nào, nên dùng cái nào? Có cái bảng đây, mọi người có thể dễ dàng nhận ra. Retraining là dữ liệu phụ thuộc vào database của mọi người. Cứ quăng vào, quăng vào, lúc nào data cũng được update liên tục. Còn fine-tuning là mọi người retrain lại model, nên lúc nào data cũng chỉ ở cái chỗ mà mọi người đã retrain thôi. \n\n**[21:16]** Kế tiếp là customize and learning style. Nghĩa là cái retraining, mục đích của nó là cho mình một cái knowledge base để mình lấy mấy cái knowledge base đó ra tham chiếu, sử dụng. Còn fine-tuning thì sao? Nó upgrade cái não của model lên, để nó có sẵn cái knowledge đó luôn. Còn mấy cái ở dưới thì chắc mọi người tự tìm hiểu tiếp ha.\n\n **[21:57]** Em có một cái ví dụ như vậy. Ví dụ như là mọi người muốn làm một cái system để giải thích những cái note của bác sĩ, đúng không? Những cái note của bác sĩ, mọi người có thể biết là những cái note của bác sĩ nó có rất là nhiều từ chuyên ngành. Và những từ chuyên ngành đó nó còn viết tắt, viết kiểu luộm thuộm nữa. \n\n**[22:44]** Nếu mọi người sử dụng fine-tuning á, mọi người sẽ cho nó học hết tất cả những cái kiến thức luộm thuộm, những cái shorthand, những cái handwriting đó của bác sĩ. Nên khi mọi người input một cái note của bác sĩ vô, nó sẽ trả lời được rất đúng. Còn nếu mọi người dùng retraining á, khi mọi người input một cái note của bác sĩ vô, nó sẽ kiếm được những cái relevant data, mang ra đọc. Nhưng bản chất là cái model nó không hiểu được những từ đó, nên nó cũng sẽ không đưa cho mọi người một câu trả lời chính xác.\n\n**[23:16]** Mọi người có thể hiểu như này: fine-tuning là mình nhờ một bác sĩ đọc một cái note của bác sĩ. Còn retraining là mọi người đưa cho một người có kiến thức rất rộng đọc một cái note của bác sĩ. Người đó có thể kiến thức rất rộng, nhưng về mấy cái chuyên ngành, mấy cái chuyên ngành thật sự, thì nó không đủ sâu như của một bác sĩ thực thụ. Nên độ chính xác sẽ không cao.\n\n**[23:57]** Thứ hai, mọi người có thể nói, bây giờ với retraining, mình dùng một cái system prompt để list hết mấy cái shorthand của bác sĩ ra trong system prompt, nó sẽ tự hiểu thôi. Nhưng làm vậy, mọi người sẽ bị tốn token, đúng không? Tại vì khi mọi người dùng retraining, mọi người lấy hết cái retraining data ra, quăng một cái knowledge retraining vô, lại cộng thêm đống cái zero-shot, mấy cái description, mấy cái đi kèm theo nó trong một cái prompt á, thì nó rất tốn token.\n\n**[24:34]** Và khi ở trong một cái long conversation với một cái model, nó đâu phải chỉ dựa vào câu hỏi của mình đâu. Nó sẽ dựa vào tất cả các cuộc trò chuyện từ trước tới giờ của mình mà nó trả lời cho mình. Lúc này, nó sẽ dẫn tới trường hợp là nó bị limit bởi token. Đó là cái drawback khi sử dụng retraining, là nó sẽ tốn token. Tại vì mọi người cần token để chạy cái system prompt của mọi người nữa. Còn fine-tuning, bản chất là model nó đã có kiến thức rồi, nên không cần phải có system prompt.\n\n**[25:14]** Đó là sơ qua về fine-tuning. Chắc có cái demo cho mọi người xem sẽ rõ hơn ha. Bây giờ em sẽ fine-tune một cái model là Duty 40 Mini ha. Em có một cái dataset như này. Ừ, như này thì mỗi thứ nó sẽ có một cái system như retraining, rồi user hỏi cái này thì muốn nó trả lời vậy, đúng không? Em cộng 10 cái, 10 record trong cái dataset này, em sẽ fine-tune nó.\n\n**[26:13]** Trước khi fine-tune, em sẽ cho nó chạy qua một đoạn code để em estimate được. Tại vì em dùng Open AI, nên sẽ tốn tiền. Nên mình sẽ tính được estimate là nó sẽ charge mình bao nhiêu. Em dùng xong, khúc cuối nó sẽ kiểu, tầm khoảng 4800, sắp xỉ 4800 token. Cái này chỉ là tham khảo thôi, nhưng em thấy nó cũng đúng. Sau đó, em sẽ upload cái file data này lên Open AI. Nó sẽ cho em cái file ở trên cái Open AI của em.\n\n**[27:03]** Rồi em sẽ training nó. Em sẽ tạo một cái fine-tuning job. Lúc này, ở trên Open AI, nó sẽ chạy một cái job này. Mọi người có thể lên đây, mọi người đọc, mọi người quan sát. Nó sẽ không trả kết quả liền, nó sẽ tạo một cái job để pending ra đó, để trên Open AI nó fine-tune cho mình. Trong lúc chờ, mình có thể theo dõi quá trình của nó như thế nào. Sau khi xong đâu rồi, nó sẽ thông báo cho mình. Mình cứ stamp cái câu này, cứ check cái câu này để coi nó đã hoàn thành hay chưa. Mình đọc ở cái chỗ đó.\n\n**[27:58]** Sau khi xong, nó sẽ cho mình mấy cái result status. Sau khi fine-tune xong, với cùng một câu hỏi, ví dụ đây là cái câu hỏi em sử dụng, em dùng câu hỏi này. Cái câu hỏi này gần giống với một cái record trong đống dataset của em. Sau khi em chạy, nó sẽ trả lời như vậy. Nhưng trước khi fine-tune, em dùng một cái model bình thường nha, model bình thường thì nó sẽ trả lời kiểu vậy.\n\n**[28:49]** Có nghĩa là em fine-tune thì nó đã thành công. Đó là cái cách em sử dụng Open AI để fine-tune một cái model. Demo của em tới đây thôi. Anh em có câu hỏi gì không? Đúng rồi, cái này demo em xài tuning chứ để tự fine-tune bằng local mà xịn xịn thì chắc không đủ đồ. Dạ, đồ ngon nhõ thôi. Thực ra có mấy cái model trước, tô nó trên LoRA các thứ, cũng có thể demo được. Nhưng tô không, bài này easy, bài này kiểu một...\n\n**[29:47]** Lẻ tẻ trầm mấy á. Đúng, chắc cũng ổn mà. Nói chung, những đội enterprise hay không muốn tốn thời gian xây dựng GPU thì sẽ dùng cách này. Diagram GPT hồi trước, GPT-4o Mini ra thì fine-tuning đã miễn phí, dùng cái này cũng tiện lợi cho họ. Cái cửa hàng demo cho anh em là sử dụng một cái như kiểu service ấy. Open AI cung cấp service fine-tuning, đưa lên mấy cái model của nó luôn. Mình pick mấy cái model, chắc là pick model mini á. Chắc chi phí nó không cao lắm.\n\n**[30:37]** Đấy cũng là một cách. Nhưng vấn đề thực ra là mình vẫn không phải người own cái model đấy. Bản chất là vẫn host ở trên server của họ. Còn có một cách khác là tự build server và tự running. Trường hợp hôm nay đã khác. Anh em xem, hôm qua em có thử một cái model có 3 billion parameter thôi. Nhưng nó chạy hai ba tiếng, nó chưa xong đâu anh. Thực ra bài này, cái version nó đơn giản hơn một cái bây giờ, nhỏ hơn của bài trước. đầu.\n\n**[31:26]** Nó là một cái full flow liên quan đến gì ta, reinforcement feedback. Ý là cái em giới thiệu ở bên ngoài production á, là khi người ta tuning á. Người ta không phải chỉ fine-tune xong là dùng liền, người ta phải đánh giá lại coi nó có đúng không. Người ta phải cho nó vô cái cycle để càng cải tiến cái fine-tuned model nữa, kiểu vậy. Đây là một cái flow như vậy. Bản chất nó cũng model thôi, đâu có gì đâu. Quan trọng là mọi người biết được những cái cost để đánh giá cái approach thôi.\n\n**[32:05]** Tại ra nó vẫn là bài toán accuracy, đúng không? Mình chọn cách nào để làm cái output nó chính xác hơn. Những cái method như retraining hay fine-tuning, nó sẽ có những nhược điểm khác nhau. Và thực ra kể cả fine-tuning, nó cũng có nhiều method fine-tuning khác nhau. Chắc là cần đi sâu hơn để xác định mấy cái đó. Cái này vẫn hơi general. Chắc vậy, Hoàng. Nếu có điều kiện thì chắc đi sâu hơn tí nữa.\n\n**[32:48]** Sâu hơn theo kiểu là có mấy cái method liên quan đến phần retraining các thứ. Sử dụng fine-tuning method á, có một số cái method nó tương đối tiết kiệm về mặt tài nguyên. Tất nhiên, nó sẽ đánh đổi với một số thứ khác, kiểu vậy. Giới thiệu cái đó để anh em xem thử đâu đó. Mọi người hỏi, Đạt hỏi là khi nào cần fine-tuning. Nói là fine-tuning cần khi mà mọi người muốn nó có một cái kiến thức, một cái specific topic nào đó. Mọi người có thể cân nhắc sử dụng fine-tuning.\n\n**[33:39]** Nhưng trong tất cả trường hợp, em thấy bên ngoài, đa số mọi người sẽ prefer dùng retraining. Tại vì nó dễ và tốn ít tài nguyên hơn. Nhưng một số trường hợp như lúc này, cái ví dụ em nói về cái note của bác sĩ á, suppose là nên dùng tuning. Rồi tùy cái kiến trúc, tùy cái mình chia system của mình ra nhiều system nhỏ, system nhỏ nó như thế nào nữa, tùy. Có thể có một vài cái use case như kiểu chúng nó muốn host mấy cái model bé bé, model bé chẳng hạn.\n\n**[34:18]** Chỉ kiểu dành để làm một cái task cụ thể thôi. Ví dụ như phân tích thời tiết, độ ẩm các thứ để perform cái action nào đấy. Ví dụ như thay đổi cái theme của điện thoại hay để chỉ action nào đấy chẳng hạn. Có thể retrain cái model bé bé để chỉ cần làm chuyện đó thôi, không cần phải cần network các thứ gì cả. Chắc vậy. Từ giờ chắc là Biên ha? \n\n**[36:13]** Dụng cái và build cái recovery process cho nó. Chi tiết như nào thì nó sẽ có một vài phần chính. Trước tiên là cái lý do mà mình cần cái kỹ thuật này và so sánh nó với một cái quen thuộc hơn là backup. Sau đó là đi vào việc để mình build và những cái mình cần để ý, những cái gì. Đầu tiên là trên thực tế, thường có những tổ chức, những công ty mà chạy những cái app với lưu lượng dữ liệu cao á. Ví dụ như giao dịch chứng khoán này nọ. Như em ví dụ này là kiểu 50.000 transaction.\n\n**[37:12]** Như em ví dụ này là kiểu 50.000 mỗi ngày là ít á, kiểu vọt lên 500.000, triệu transaction mỗi ngày. Sau khoảng thời gian, cái lượng data nó sẽ phồng lên rất rất lớn, ảnh hưởng đến cái việc mà mình query data và ảnh hưởng đến cái trải nghiệm người dùng. Trong những cái data đó, sẽ có những cái data mà dùng rồi thì nó rất ít được access lại nữa. Ví dụ như lịch sử trên 7 năm trước chẳng hạn. Nó sẽ dẫn đến một cái vấn đề, làm sao để mình giải quyết cái đống data đó. Nên mình mới dùng cái kỹ thuật là data archiving.\n\n**[38:14]** Nó sẽ có những cái lợi ích để counter lại những chuyện bên trên. Đầu tiên là cái data mà mình sử dụng, mà nó set liên tục á, query ghi đọc liên tục á, thì nó thường tốn chi phí cao. Mình sẽ dùng cái kỹ thuật này, mình sẽ đem data của mình bỏ qua một cái chỗ khác, chi phí rẻ hơn, access ít hơn. Từ đó, nó sẽ làm tăng được cái performance của app của mình trong việc query hay aggregate data các thứ.\n\n**[39:07]** Về mặt pháp lý hay reusable, những cái data đó nó sẽ được bảo vệ an toàn, không bị ảnh hưởng bởi những yếu tố bên ngoài. Để sau này khi mình dùng lại, mình có thể lấy ra dùng được. Như mọi người hay nói, mọi người sẽ liên tưởng đến cái data backup, thường dùng trong việc restore data, restore cái system hay app nếu có lỗi xảy ra. Mà hai thằng này, nó sẽ khác nhau ở chỗ là data backup á, nó sẽ dùng cho cái việc hotfix cái system nhiều hơn. Còn cái thằng archiving...\n\n**[39:59]** Data archiving thì nó hướng về cái việc lưu trữ data một cách lâu dài. Nó sẽ có cái chi tiết so sánh như này. Để mình đi build một cái architecture, một cái system để archive data, xong rồi dùng nó để recovery lúc mình cần thì sẽ làm như sau. Mọi người thấy, nó sẽ có ba cái note chính. Thứ nhất là mình lưu data lại, mình dùng metadata để interact với những cái data đó, rồi mình bỏ lên một chỗ, ví dụ như những cái cloud-based service, cloud storage service, để mình lưu trữ cái data đó.\n\n**[40:51]** Về chi tiết, để lưu trữ cái dữ liệu á, đầu tiên mình phải xác định những cái dữ liệu cần được lưu trữ. Phải phân tích xem dữ liệu nào hay được sử dụng, dữ liệu nào không được sử dụng, ít được truy cập. Sẽ có nhiều công cụ để mình làm những cái đó. Ví dụ như phân tích từ business requirement, hoặc từ các công cụ phân tích, mấy cái công cụ phân tích á. Từ đó, mình mới biết cái data nào là cần, cái nào có thể đem đi archive lại.\n\n**[42:05]** Sau đó, mình sẽ gói nó lại, dùng một vài biện pháp như vector hóa nó, encode nó, rồi dùng checksum các thứ để đảm bảo cái data nó sẽ đúng. Sau này, khi mình sử dụng lại, mình truy cập lại một cách nhanh chóng. Tại vì những cái database này, nó gói lại ở một cái storage khác với cái mình hay set, nên mình cần phải lưu lại cái metadata của nó. Ví dụ như lưu theo tháng ha, hoặc lưu theo account, để sau mình query lại thì dễ hơn.\n\n**[43:07]** Sau khi archive xong, mình muốn sử dụng lại á, thì vừa đây là cái ví dụ em để recovery. Mình sẽ tận dụng những cái metadata lúc nãy, mình search lại những cái block data mà mình cần, rồi đưa về cái môi trường tính toán lại nó khi cần thiết. Cái này nó có lợi ích là khi mình làm những chuyện này, nó sẽ không tác động đến cái data production của cái ứng dụng đang chạy. Mình có thể làm song song được. Mình muốn làm gì với nó thì làm, không chọc ngoáy vào trong cái production, sẽ đảm bảo an toàn được.\n\n**[43:51]** Cho cái trải nghiệm người dùng, như sản phẩm của mình đó. Nói đến đây, có một vài cái practice cho việc sử dụng, xây dựng cái hệ thống này. Nó cũng đơn giản lắm nhỉ. Mình sẽ phải review lại những cái policy mà mình đặt ra để cái hệ thống này chạy, xem data nó có trọn vẹn hay không. Mình sẽ automation những cái step của cái process này. Hiện tại cũng có nhiều tool hỗ trợ mình rồi, ví dụ như AWS, hay Google, đều có những cái như...\n\n**[45:14]** Google Cloud chẳng hạn. Mình chỉ cần viết những cái đơn giản để đẩy lên trên đó thôi. Và mình không thể thiếu cái monitoring để xem data này có hoạt động tốt hay không. Xong rồi, có những kỹ thuật khác như checksum này nọ, để đảm bảo data của mình luôn trọn vẹn. Khi mình cần, cũng sẽ có những chiến lược như schedule trước cái data. Tại vì những cái data này nó tồn tại lâu, nó cũng sẽ lớn, cũng sẽ phồng lên trên cái storage, cái cloud storage mà mình dùng để lưu trữ nó.\n\n**[46:05]** Nên sẽ có những chiến lược như khi nào cần thì phải schedule trước, bao nhiêu thời gian đó để nó replicate data cho mình chẳng hạn. Kế của em chỉ như vậy thôi. Lý thuyết kiểu để giải quyết cái mục đích cuối cùng là nói mọi người về việc giải quyết những cái data tồn động lâu dài, nhưng không sử dụng đến nhiều trong cái hệ thống mà mình build thôi. Ví dụ như bên ngân hàng chẳng hạn, sẽ có kiểu user trade, trade của user nó lên đến cả trăm triệu record chẳng hạn.\n\n**[46:45]** Sau này, nó sẽ lên nữa. Tức là query những cái data gần thôi, nhưng nó cũng rất tốn thời gian, kiểu vậy. Đó là những cái mà em nói hôm nay, hết. Mọi người có hỏi gì không? Khi mà store data, zip data, là mình sẽ zip một cái đoạn fragment trong quá khứ mà nó không sử dụng data đấy cho mục đích hiện tại, đúng không? Dạ, đúng rồi, đúng rồi. Đồng ý, việc em sẽ phải xóa. Khi xong, em phải xóa cái đó, đúng rồi. Nên mình sẽ có những cái load lại để tính toán khi cần.\n\n**[47:41]** Nên mình mới có mấy cái kiểu để mình làm nó an toàn. Mình có hỏi kìa. Em chưa biết cái cơm của Thỏ có biết cái này không, so sánh được không? Đứng ra là Timescale, nó có cơ chế move chunk. Ví dụ là mình compression như bình thường thôi. Thêm về cái vụ là mình có hot, warm, và cold storage. Ví dụ mình backup hàng tuần thì để trên hot storage của Azure. Nếu là cũ quá, ví dụ 2, 3, 4, 5 năm, thì để trên cold storage của Azure, hoặc là Backblaze. Nó sẽ có riêng cái dịch vụ cho mình move cái chất data đó.\n\n**[48:53]** Đúng cái vị trí object hoặc block storage, mình tương tác với Timescale để đảm bảo lúc mình cần tiết kiệm tiền với data cũ. Có thể tiết kiệm được, vốn có thể query, với trade-off là mình sẽ query hơi chậm với data hơi cũ thôi. Dạ, cái em hiểu là để tùy vào cái platform mình dùng để build ha anh. Ví dụ như bên Microsoft thì cũng sẽ có những cái tùy vào thời gian của database, hoặc tùy vào tuổi thọ của data, hay dung lượng này nọ, thì sẽ có những cái level khác nhau.\n\n**[49:40]** Ví dụ nó sẽ có delay, hay bình thường vẫn access, hay delay cho những cái mà không dùng một thời gian lâu nữa. Cái đó là để mình cụ thể trên từng tool thôi. Còn chung chung, nó là anh đang cái này làm gì thì đứng ra là Timescale thì phù hợp cho cái kiểu pattern này, cho về time series. Bên phía Azure thì họ làm cho nó phù hợp với status, hơi giống như Timescale, nhưng nó kiểu giúp mình partition và shard đúng theo kiểu mình mong muốn.  \n\n**[50:39]** Mỗi một cái nó sẽ có ưu điểm, nhược điểm riêng. Với AWS thì đứng ra là với cái dịch vụ này thì phải coi chừng cái hardware cho lưu cái data này, nó có ổn định không. Ví dụ bên phía Azure cold storage thì nó dùng đĩa, đĩa gì ta, đĩa hơi khá đặc trưng. Phải dùng cái máy laser để in vào trong đó. Nên query rất nhanh, nhưng insert thì cũng hơi chậm, kiểu insert một đống cũng mất vài phút. Vì phải có một cái laser cứng để in ở trên đó, không có virtualization layer. \n\n**[51:23]** Mỗi một service và mỗi một cái kiểu tool mình dùng cho compress và lưu trữ sẽ có ưu điểm, nhược điểm riêng, theo cái platform mình subscribe. Dạ, đúng rồi. Cái này không chỉ là mấy cái tool kiểu như AWS hay Google service. Nó là kiểu mình cũng có thể cân nhắc cho cái business của mình nữa. Nên cái này kiểu chung chung thôi. Còn từng platform, nó sẽ dùng những kỹ thuật khác nhau. Mục đích chung cuối cùng là để giải quyết cái vấn đề data nó lớn lên, nhưng ảnh hưởng đến cái việc mình query, mình nó chạy thôi\n\n**[52:16]** Nhiều cách giải quyết cho câu chuyện optimize query, đúng không? Khi mà vấn đề là do data quá lớn, thì có một vài cách. Cách của biên là một cách, tức là sẽ có một phần data mình đang không xài đến, thì ta cắt đi ra, lưu đâu đấy. Về sau mà có cần đến past data thì insert lại xài sau. Còn mình để đâu đó tầm bao nhiêu phần trăm data hiện tại, đủ để xài mục đích hiện tại, query đi nó nhanh hơn. \n\n**[52:59]** Còn một số cách khác thì xài thằng tooling, có một số kiểu database hay kiểu như Timescale, thì nó sẽ optimize luôn cho chuyện query với lượng data lớn lớn. Em nghĩ là bên dưới thì nó cũng sẽ tự động kiểu nó buff lên đâu đó, nó giữ giúp mình thôi, đúng không anh? Nên mình tỉ mỉ bên dưới, mình dùng là interface thôi. Cái bên dưới thì gần gần như nhau, như các em ta. Cảm ơn biên, vậy thôi. Chắc bài cuối của An, không biết có liên quan không. Không biết còn liên quan một tí gì đến cái cộng đồng viên không.\n\n**[54:00]** Chắc có thì chắc cũng nói sơ sơ thôi, cũng không nhiều cái. Cũng gần giống như bài của Biên, nhưng use case cũng gần giống á. Nó mở rộng ra tí thôi. Tí rồi thì bài này là nói sơ về cái datalake với lại cái use case của thằng Notion. Mình nói cái datalake trước. Datalake thì chắc mọi người nghe miết rồi, xưa giờ cũng hơi lâu rồi đó. Mình nhìn lại cái quá trình phát triển của tụi datalake này, coi là mình đang đi tới đâu.\n\n**[54:54]** Thật ra từ lúc bắt đầu, hồi tầm 1980 gì đó, là thời đại của thằng database, mấy thằng database warehouse, mấy cái mà mình đang xài hiện tại á. Về table các kiểu, tạo table rồi xử lý data. Sau này, tới cái đợt tầm năm 2000 các kiểu, tụi mấy thằng big tech bắt đầu thu thập data nhiều á. Rồi nó tận dụng mấy data đó, thì mới sinh ra mấy thằng để giải quyết vấn đề lưu trữ data và xử lý data trên dữ liệu lớn. Như là mấy dữ liệu lưu theo dạng file đồ á. Mấy cái này, mấy thuật ngữ như là cái MapReduce này nè.\n\n**[55:44]** Hình như trong cái memo của mình có một bài về MapReduce. Nếu mọi người không biết thì có thể search lại, tìm đọc thử xem cái MapReduce hồi xưa nó làm cái gì. Nó là cái tiền thân của tuổi. Sau này nó tích hợp vô thôi, giờ không xài nữa, nhưng chắc là nó tích hợp sẵn hết rồi. Sau cái thời gian phát triển của thằng này, mới bắt đầu 2010, thì mới đẻ ra, trước 2010 tí, đẻ ra khái niệm về datalake, big data, cloud, là cái internal data warehouse á, trên cloud á. Nó cloud thôi.\n\n**[56:28]** Sau này, cái đợt bây giờ á, thì nó bắt đầu phát triển hơn nữa, là về cái lake và datamart. Lake chắc bản chất là kết hợp giữa mấy cái của tụi datalake và cái warehouse thôi, để rồi đặt thành cái house. Như là mấy thằng như thằng Datadog, nó đang làm sao không biết, nhưng mình chắc là đang nói về cái này hơi đi sau thời đại tí. Để tập trung vào, chắc mình coi sơ một cái data architecture chung chung trước. Cái này, bữa cái bài của Tom có đăng, cũng có một cái diagram. Nó cũng tinh gọn hơn cái này, tinh gọn hơn tí, là cũng về cái data đi qua mấy cái layer, là processing rồi mới tới thằng gì đó. \n\n**[57:20]** Cái này nó sẽ thể hiện rõ hơn tí, là trong một cái datalake, mình sẽ lưu những loại data gì. So với thằng data warehouse, mình chỉ lưu mấy thằng structured data thôi, hoặc là mấy cái như lưu table data clean hết rồi. Còn thằng datalake này thì nó raw data, nó sẽ cả structured, unstructured, semi-structured data luôn. Nó sẽ lưu dạng raw, sau đó nó mới xử lý data, transform data, rồi nó quăng qua cho cái đám bên BI analytics, hoặc là quăng vô cái warehouse khác để chứa cái data đã được process rồi á.\n\n**[58:18]** Còn cái layer mà analytics sandbox này, thì nó là một cái layer để cho tụi data scientist, hoặc mấy thằng mà cần dùng cái raw data, process data, mà nó không ảnh hưởng tới cái process chính. Bên đây á, thì nó sẽ làm việc trên cái sandbox này để xử lý data cho tụi mấy thằng đó, mấy thằng cần raw data, nhưng không ảnh hưởng trực tiếp tới cái ruồng chính. Cái giống như hồi nãy Biên có nói á, có làm á đó, là nó sẽ lấy data, rồi nó lưu ở đâu đó để sử dụng sau này, hoặc để process gì đó không biết, nhưng mà nó không muốn ảnh hưởng tới process chính của cái app, thì nó sẽ là cái đống này.\n\n**[59:09]** Ở chỗ này, mọi người thấy là mình có khái niệm là cái ETL á, là extract, transform, và load. Bên cái warehouse xưa giờ mình làm á, nó sẽ là extract, transform, và load, nó đi theo thứ tự đó luôn. Nhưng trong cái này, mình sẽ thấy rõ là cái thằng datalake á, nó sẽ là extract và load trước. Rồi sau khi nào cần á, nó bắt đầu process data, là transform. Transform sẽ đứng sau, load sẽ đứng trước. Đó là cái khác biệt giữa hai thằng.\n\n**[59:52]** Đây là chỗ so sánh khác biệt giữa thằng data warehouse và datalake thôi. Đó là dữ liệu bên warehouse, nó được clean, structured, organized thành cái table. Còn thằng này thì nó lưu dạng file, raw data các thứ, semi-structured rồi đó, CSV hoặc mấy cái JSON. Cái process nó cũng sẽ khác nhau giữa thằng lake và lake này. Truy vấn thì thằng warehouse sẽ truy vấn bằng SQL, còn kia thì xử lý trực tiếp trên cái dữ liệu luôn. Mấy thằng hỗ trợ xử lý trực tiếp dữ liệu, như thằng Spark đó, thì nó sẽ hỗ trợ mấy cái đó. Nói qua về cái thằng Notion.\n\n**[01:00:46]** Datalake thì cái use case của thằng Notion, mọi người biết là Notion mình xài cũng hơi nhiều rồi đó. Hồi xưa, nó cũng đi từ từ thôi. Mấy cái tổ chức, mấy cái block hồi xưa, nó tổ chức thì cũng kiểu data bình thường, giống như mình, là mấy cái app nhỏ nhỏ. Mấy cái block của nó bắt đầu tăng dần. Block của nó được hiểu là mấy cái gì, rồi nó sẽ bao gồm cái title trong trong đó. Nó sẽ gọi là block. Số lượng block của nó tăng lên liên tục theo ngày giờ.\n\n**[01:01:35]** Gì đó thì bắt đầu sau này, nó phình ra, nó sẽ bắt đầu sử dụng mấy cái kỹ thuật như là sharding, sharding xưa. Như nhớ có bài của Hải Vũ có xe gì đó, nó scale horizontally. Nó bắt đầu tách ra sharding này nọ, rồi mấy cái instance. Trong giai đoạn từ 2021 đến 2023, nó sẽ có 32 instance. Mỗi instance sẽ có 15 cái shard. Rồi từ 2023 trở đi á, nó bắt đầu chia lại, nó lại tăng lên. Số lượng tăng lên nữa, thì đó là 96 cái instance. Và mỗi cái instance, nó sẽ là 5 cái shard. Nhân lên tầm 400 mấy á, bốn trăm mấy.\n\n**[01:02:27]** Để mà xử lý thì lúc này, nó hơi to, đúng không? Khi mà data nó bắt đầu to lên á, nó sẽ có những nhu cầu. Sau này sẽ có những nhu cầu về cái analytics, hoặc là mấy cái về làm bên machine learning á, tập dữ liệu này nọ, mẹo mẹo rồi. Nó sẽ bắt đầu setup một cái data warehouse architecture của nó. Cái này là cái tiền thân trước khi setup cái datalake. Nó sẽ làm data warehouse để xử lý data. Cái luồng cơ bản của nó setup để thu thập data, mấy cái về thay đổi data của mấy cái block trong từng shard.\n\n**[01:03:21]** Nó sẽ sử dụng cái file transfer để nó ingest mấy cái data từ mấy shard này nè. Nó đổ về cái gì, rồi nó gộp mấy thằng đó lại thành một cái single database to. Cái này sẽ gặp khó khăn trong việc là nãy mình nói, nó đang có khoảng bốn trăm mấy cái shard, đúng không? Nó sẽ gặp khó khăn trong việc là quản lý bốn trăm mấy connection thằng này. Xong rồi mấy cái khó khăn trong việc scaling. Số lượng data thay đổi trong mỗi cái block của thằng Notion, nó xảy ra thường xuyên và nó rất nặng, sẽ...\n\n**[01:04:13]** Gây khó khăn trong việc đọc ghi trong cái table to này. Sau đó, nó mới bắt đầu setup một cái internal datalake của nó. Cái internal datalake này, có note là nó sẽ không thay thế thằng này hoàn toàn, mà nó chỉ sử dụng cái mới thôi. Còn cái này, nó vẫn tận dụng trong một vài tác vụ, kiểu nhẹ hơn, cho mấy cái table thay đổi data không có nặng lắm. Với lại nó cần cái gì. Còn thằng này, nó expect cái luồng này á, là nó sẽ đánh những cái data nó cần để cho những mục đích mà analytics hoặc là machine learning.\n\n**[01:05:08]** Data nó có thể chấp nhận cái độ trễ là vài tiếng, vài phút, tiếng gì đó. Nó sẽ sử dụng cái data trong đây. Cái lượng setup thì cũng đơn giản thôi. Nó sẽ sử dụng cái thằng Debezium CDC này nè. Nó là cái capture data change á, để nó watch cái thằng database này, bắn về Kafka. Sau khi nó bắn cái đống event data change về Kafka, thì có một thằng bên đây là Hudi hay gì đó, nó lấy event đó, nó quăng về thằng S3. Rồi bắt đầu từ thằng này, thằng nào muốn sử dụng thì vô đây, nó lấy về, nó setup tiếp, xài data warehouse hoặc xài mấy cái chủ đích về shard gì đó, thì vô đây nó lấy, nó xài.\n\n**[01:05:51]** Cái đó là cái thật ra, cái case Notion. Chắc là có thể xài thằng này thử. Vì nó cũng là cái thằng đứng ở ngoài, nó watch vô cái đống đó. Nếu mà xài AWS hay retraining á, sẽ xài cái một là cái thằng Redshift hay gì quên rồi. Nó sẽ watch thằng đó, những thay đổi trên cái database, xong rồi nó sẽ lưu hết về trong một cái bucket hay cái gì đó. Xong rồi từ đó, mình bắt đầu xử lý sau. Cái luồng bên này là có thể sử dụng cái này. Hồi nãy setup một cái demo, nhưng mà có vẻ hơi fail rồi.\n\n**[01:06:51]** Tại vì nó chưa có được cái thằng server, nên là nó fail. Để sau đi, rồi chắc chỉ có như đó. Với lại có cái kiểu góc nhìn đó, là cái process này nè. Là cái process mà chắc tụi enterprise, nó sẽ có thể áp dụng. Nó là process kiểu chung chung mà đa số tụi enterprise sau này, em nghĩ là nó có thể. Nhu cầu của nó khi mà cái data lớn lên á, thì cái nhu cầu của nó cũng sẽ đi theo hướng này thôi. Đó là nó cần data, thu thập data để làm cái gì đó, và không ảnh hưởng tới cái luồng chính.\n\n**[01:07:52]** Mình thì xưa giờ toàn focus vào cái việc làm việc với mấy cái model AI. Nhưng mà mình nghĩ là sau này, mình cũng cần cái skill set gì đó để mình biết cách xử lý những data như thế này, tụi mà nó data lớn hơn kiểu vậy. Cho xin lỗi cái, anh nào đây? Anh đang nhìn nhận cái process này, thì nó khác gì với chuyện là mình replicate cái database của mình ra một instance khác để phục vụ chuyện retraining ấy anh? Là tại vì ở đây, đứng ra là ý ở đây, thật ra là kiểu em đang sinh giống như kiểu sinh data sang một...\n\n**[01:08:54]** Cái shard khác, đúng không? Data warehouse, đúng không? Và sử dụng upload kit process cho những cái tác vụ mà nó không, kiểu mình làm mình làm async được ấy, chứ không cần phải trực tiếp trên nguồn data chính. Câu hỏi là đối với cả mấy cái model dạng như sharding hay sử dụng master-slave ấy, thì sao không theo kiểu cứ duplicate cái database của mình ra thôi? Duplicate data thì nó vẫn chỉ là một cái data warehouse ở dưới dạng table ha. Còn thật ra cái này, nó chỉ là cái process, nghĩa là một process cho database thôi\n\n**[01:09:40]**\n\n. Nó có thể có những cái event khác. Như là ví dụ, mình sẽ có nhiều cái external data, không hẳn là mình chỉ có một cái battery, database không. Ví dụ mình có mấy cái capture như là từ social media, hoặc là mấy cái tụm lum la nào đó, chả biết. Nhưng mà nó có thể là nhiều loại data khác nhau, gom về, quăng qua thằng này. Thằng Hudi bạn này, nó sẽ là thằng chịu trách nhiệm xử lý cái raw data đó, để nó quăng vào cái thằng S3 này. Nó lưu...\n\n**[01:10:23]**\n\nMọi thứ dưới cái đống này. Nó vô luôn, mọi thứ về dạng file gì đó, gom hết vô đây, để bắt đầu sau này, mấy thằng ngoài sao n mới có cái slot để xử lý. Thật ra tụi nó cũng có một câu hỏi là tại sao không dùng mấy thằng database như MySQL hay PostgreSQL á? Nó sẽ có mấy cái... Tại sao phải sử dụng cái thằng capture data change mà không sử dụng mấy thằng đó? Mấy thằng đó, nó có cơ chế để streaming mấy cái event change của nó luôn. Mấy thằng đó, event stream, nó thường sẽ stream trực tiếp từ database này qua database khác.\n\n**[01:11:09]**\n\nCòn thằng này, nó sẽ là capture cái event đó và đưa đâu cũng được. Vì là nếu mình không có thằng Kafka này ở đây á, thì mình cần một service nào đó, mình cần cái real-time data xử lý liền luôn á, mình không cần phải vô Kafka. Cái thằng CDC này vẫn có thể bypass qua đó được, kiểu kiểu vậy, chứ không hẳn là từ database sang database kiểu như vậy. Ta cũng có nhìn ý là kiểu, thấy là nếu mà theo góc nhìn về operation chẳng hạn ấy. Tất nhiên nếu mà có multiple datasource và sử dụng những cái partition tool các thứ, nó khác nhau ấy.\n\n**[01:11:50]**\n\nDatabase khác nhau, thì cái này cũng sẽ cả, thật ra là gom nó lại vào một cái datalake, sao cho một số cái tác vụ, nó cụ thể thôi ấy. Thực ra là một số team, như kiểu team AI hay team về mặt làm report, hay data, thì người ta cũng sẽ chỉ cần work trên cái data warehouse này thôi, kiểu vậy. Hoặc là có extend cho bên nào khác nữa, thì cũng sẽ make sense, phân vùng data riêng cho từng cái team riêng, đúng không? Có họ thêm cái vụ mà cái button, cái button ETL bên database bình thường với cả bên datalake, thì nó sẽ là ELT, đúng không?\n\n**[01:12:39]**\n\nĐúng rồi, ELT, anh sẽ hiểu là mình extract, nghĩa là mình tìm đúng file, đúng không? Mình load cái file đấy lên đây, và transform nó thành dạng kiểu structured data ha. Ý là nó sẽ transform, nó chỉ là cái action, nó xảy ra ở sau khi mình có raw data rồi. Còn ETL, nghĩa là extract là sẽ lấy data từ cái đám data source á. Xong rồi nó sẽ có cái quá trình log thẳng vào cái raw data, thẳng vào mấy cái gì đó của mình. Nó gọi là cái raw landing, cái layer raw landing. Xong rồi mình mới có cái gọi là transform. \n\n**[01:13:34]** Sau đó, sau cái landing sẽ có transform để xử lý data, thì nó sẽ ra sau. Còn cái thằng kia là nó extract xong, rồi transform, nó mới quăng thẳng vào cái warehouse, đó là cái database của mình. Hay anh em có hỏi gì không? Rồi, cảm ơn An, đúng rồi. Anh em nhé, rồi bye anh em, mỗi tuần vui vẻ.\n\n---\n\n### English transcript\n\n**[00:00]** It seems like the all-hands meeting is next week. I don’t see anyone creating any events, so this week will probably just be normal, right? Guys, check if there’s any issue with screen sharing. Should we just start? Today, I think we’ll have about three presentations. Phát just said he doesn’t have anything new this week, so he’ll probably skip today. Guys, try sharing your personal screens and see how it goes. Can we preview it first?\n\n**[10:42]** According to the schedule, it’s probably you, right, bro? Let me go first then. This fine-tuning topic, it’s not really that new. This presentation is just 100.5, not 101, so it’s only a brief intro, not going deep into it yet. Honestly, I’m pretty clueless about it too, so I’ll just give a quick overview. Today, I’ll present about fine-tuning. Here’s the agenda for this session.\n\n**[12:24]** The introduction is, if you guys use AI, you’ve probably heard of the concept of fine-tuning. AI has these models that are mostly fitted to data from some specific day, with stuff like data privacy or data from a particular domain. That data doesn’t show up in the knowledge of the foundation model. To get that knowledge into the model, people usually use retraining, right? But there’s another way called fine-tuning. At the end of this, I’ll compare these two methods, looking at when to use which one and when not to.\n\n**[13:08]** For now, just think of fine-tuning as a way to expand the knowledge of a foundation model. What is fine-tuning? Simply put, you retrain the model. You take a foundation model, feed it a dataset to fine-tune it, meaning you retrain it. After fine-tuning, you get an adjusted model, called a fine-tuned model.\n\n**[13:44]** Why does fine-tuning bring in new knowledge? Simply put, in an AI model, knowledge is stored through neural networks. Fine-tuning updates the weights, the parameters of that neural network, to fit the new knowledge. When you throw new knowledge in, those weights change, and at that point, the model has updated its knowledge.\n\n**[14:19]** When you throw new knowledge in, the weights change, and at that point, the model has updated its knowledge. When fine-tuning a model in practice, it’s not just about throwing a dataset in and retraining it. Sure, you get a fine-tuned model, but you don’t know if that retrained model is any good. I’ll introduce a workflow that people outside commonly use for fine-tuning.\n\n**[14:59]** This flow, it’s got several steps like this. You can split it into two clusters, two clusters, alright? This flow, I’ll divide it into two clusters. I’ll go through the first cluster first. The first cluster is the one on the left, simply understood as a base model to start with. Then, you have a new dataset, something new, you throw it in, fine-tune it. Then it produces a model, and you supervise it, meaning you retrain it in a way that’s like retraining it.\n\n**[15:38]** It’ll add more knowledge, with this input, the output will come out like this. It results in something called supervised fine-tuning. After that, let me move to the next part. Alright, this fine-tuning is retraining on data, meaning you give it a pair of input-output in the dataset for it to learn. It’ll pick up that new knowledge. For this step to be perfect, the dataset has to be clean. It has to be clean, not mixed with other stuff. Meaning it has to be specific to the domain we want to train it on.\n\n**[16:31]**\n\nBack to this diagram, after you have a model that’s been retrained, you bring it to production, you use it, right? At this point, people outside use a system called human feedback. It’s like, does the response from this model satisfy you? Rate it from 1 to N stars, something like that. You’ll collect that data. It’s part of this step—you’ll gather human feedback from that retrained model of yours.\n\n**[17:06]** Based on that feedback, people call this step a bit resource-intensive. You’ll have to retrain a separate model. That model is used to evaluate how many points this response gets. Next, you move to the third step, the final one. In this step, you’ll use algorithms like reinforcement learning to combine it with the retrained model and your reward model.\n\n**[17:46]** You retrain, you fine-tune the model one more time. It’ll give you something called an optimized model. This loop keeps going, going forever. You have a retrained model, collect human feedback, then combine those three things to retrain the model again. The more you do it, the better the model gets at what we want. That’s the flow I’ve seen people use out there in production.\n\n**[18:24]** During the fine-tuning process, you’ll often hear about a concept called catastrophic forgetting. What does that mean? It means when you retrain with new knowledge, it reduces performance on the old knowledge. Why does this happen? As I said, a model’s knowledge depends on its weights, its architecture, and its parameters. The dynamic parameters in a model are those weights. When you retrain, those weights change, right?\n\n**[19:00]** When they change, doesn’t that mean the old knowledge gets less accurate? If your dataset has a lot of overfitting data, meaning your dataset is too perfect, too perfect within that dataset, then when someone throws something new in, it’ll mess up the old stuff. They call that overfitting, meaning it’s folded in too much to the training data. When it encounters new data, performance drops.\n\n**[19:42]** So at this point, people out there use a technique called parameter-efficient fine-tuning, or PEFT. It has lots of methods, techniques within this approach, like LoRA and stuff. But generally, most of them don’t update all the weights in the model. They’ll just freeze the layers that aren’t necessary. They freeze those unneeded layers and only update a certain number of weights. That’s to avoid losing too much of the old knowledge. That’s the basic idea. For deeper stuff about the algorithms behind it, you can look it up yourselves.\n\n**[20:27]** Back to the question from the start, how’s it different from retraining, and which should we use? There’s a table here, you can easily see it. Retraining depends on your database. You keep throwing stuff in, throwing stuff in, and the data gets updated constantly. But with fine-tuning, you retrain the model, so the data stays only where you retrained it.\n\n**[21:16]** Next is customize and learning style. Meaning, retraining’s purpose is to give us a knowledge base that we can pull from to reference and use. But fine-tuning? It upgrades the model’s brain, so it has that knowledge built-in already. The stuff below that, you guys can probably look into it more yourselves, yeah?\n\n**[21:57]** I’ve got an example like this. For instance, say you want to build a system to explain doctors’ notes, right? Doctors’ notes, as you might know, have tons of technical terms. And those technical terms are often abbreviated, written all sloppy too.\n\n**[22:44]** If you guys use fine-tuning, you’ll make it learn all that messy knowledge, the shorthand stuff, the handwriting stuff from doctors. So when you input a doctor’s note, it’ll give you a really accurate answer. But if you use retraining, when you input a doctor’s note, it’ll find the relevant data and pull it up to read. But the thing is, the model doesn’t actually understand those terms, so it won’t give you an accurate answer either.\n\n**[23:16]** You can think of it like this: fine-tuning is like asking a doctor to read a doctor’s note. Retraining is like giving it to someone with really broad knowledge to read a doctor’s note. That person might know a ton, but when it comes to the specialized stuff, the real technical terms, they don’t have the depth of an actual doctor. So the accuracy won’t be high.\n\n**[23:57]** Second, you might say, alright, with retraining, we can use a system prompt to list out all the doctor’s shorthand in the system prompt, and it’ll figure it out on its own. But doing that, you’ll end up using a lot of tokens, right? Because when you use retraining, you pull out all the retraining data, throw in a retraining knowledge base, plus a bunch of zero-shot stuff, descriptions, and whatever else goes with it in a prompt, that takes up a ton of tokens.\n\n**[24:34]** And when you’re in a long conversation with a model, it’s not like it only relies on your question. It bases its answers on all the conversations you’ve had with it from the start. At that point, it leads to a situation where it’s limited by tokens. That’s the drawback of using retraining—it eats up tokens. Because you need tokens to run your system prompt too. But with fine-tuning, the thing is, the model already has the knowledge, so you don’t need a system prompt.\n\n**[25:14]** That’s a quick rundown on fine-tuning. Probably having a demo for you guys to see would make it clearer, yeah? Now I’ll fine-tune a model called Duty 40 Mini. I’ve got a dataset like this. Yup, like this, each thing has a system like retraining, then the user asks this and wants it to answer like that, right? I’ve got 10 things, 10 records in this dataset, and I’ll fine-tune it.\n\n**[26:13]** Before fine-tuning, I’ll run it through a piece of code so I can estimate it. Since I’m using Open AI, it’ll cost money. So we’ll calculate an estimate of how much it’ll charge me. After I run it, at the end it’s like, around 4800, close to 4800 tokens. This is just a reference, but I think it’s pretty accurate. Then I’ll upload this data file to Open AI. It’ll give me the file up on my Open AI account.\n\n**[27:03]** Then I’ll train it. I’ll create a fine-tuning job. At this point, on Open AI, it’ll run this job. You guys can go up here, read it, check it out. It won’t give results right away, it’ll create a job and leave it pending there, so Open AI can fine-tune it for us. While waiting, we can track how the process is going. Once it’s done, it’ll notify us. We just keep stamping this sentence, checking this sentence to see if it’s finished or not. We read it from that spot.\n\n**[27:58]** Once it’s done, it’ll give us some result statuses. After fine-tuning, with the same question. For example, this is the question I used, I used this question, it’s pretty close to one of the records in my dataset pile. After I run it, it’ll answer like this. But before fine-tuning, I used a normal model, just a regular model, and it answered like that.\n\n**[28:49]** Meaning, when I fine-tuned it, it worked. That’s how I used Open AI to fine-tune a model. That’s it for my demo. Any questions, guys? Yeah, for this demo, I used tuning, but to do fine-tuning locally with something fancy, I probably don’t have the gear. Yup, just small-time gear. Actually, with some earlier models, I ran them on LoRA and stuff, and they could’ve been demoed too. But I didn’t, this one’s easy, it’s like a basic one.\n\n**[29:47]** A bit scattered and slow, huh? Yeah, it’s probably fine though. Generally, enterprise teams or those who don’t want to spend time building GPUs will use this method. Back with Diagram GPT, when GPT-4o Mini came out, fine-tuning was free, so using this was pretty convenient for them. The demo shop for you guys is using something like a service. Open AI provides a fine-tuning service, putting it right up on their models. We pick some models probably the mini ones, I guess. The cost probably isn’t too high.\n\n**[30:37]** That’s one way to do it. But the issue is, we’re still not the ones owning that model. The thing is, it’s still hosted on their server. There’s another way, like building your own server and running it yourself. Today’s case was different. Check it out, guys, yesterday I tried a model with just 3 billion parameters. But it ran for two or three hours and still wasn’t done, bro. Actually, this one, its version is simpler than what we have now, smaller than the previous one from earlier.\n\n**[31:26]** It’s a full flow related to what was it reinforcement feedback. The point is, what I introduced about production out there is when people tune stuff. They don’t just fine-tune it and use it right away, they have to evaluate it again to see if it’s right. They put it into a cycle to keep improving the fine-tuned model, something like that. This is one of those flows. At its core, it’s just a model, nothing special. The key is you guys knowing the costs to judge the approach.\n\n**[32:05]** Because it’s still about accuracy, right? We pick a method to make the output more accurate. Methods like retraining or fine-tuning they’ve got different downsides. And honestly, even with fine-tuning, there are lots of different fine-tuning methods. We’d probably need to dig deeper to figure those out. This is still kinda general. Probably so, Hoàng. If we’ve got the chance, we should dive a bit deeper.\n\n**[32:48]** Deeper in the sense of looking at methods related to retraining and stuff. With fine-tuning methods, some of them are pretty resource-efficient. Of course, there’s a trade-off with some other stuff, like that. I’m introducing this so you guys can check it out somewhere. People asked—Đạt asked when we need fine-tuning. I’d say fine-tuning is needed when you want it to have knowledge on a specific topic. You can consider using fine-tuning then.\n\n**[33:39]** But in all cases, from what I’ve seen out there, most people prefer retraining. Because it’s easier and uses fewer resources. But in some cases, like right now, the example I gave about doctors’ notes, I’d suppose tuning is better. Then it depends on the architecture, how we split our system into smaller systems, what those smaller systems are like it varies. There might be some use cases where they want to host small models, tiny ones maybe.\n\n**[34:18]** Just for doing a specific task. Like analyzing weather, humidity, stuff like that, to perform some action. For example, changing your phone’s theme or triggering some action or whatever. You could retrain a small model just for that, no need for a network or anything fancy. Probably like that. From now on, it’s Biên’s turn, yeah?\n\n**[36:13]** Using it and building a recovery process for it. How it works in detail, it’s got a few main parts. First is the reason we need this technique and comparing it to something more familiar like backup. Then it’s about how we build it and the things we need to watch out for, what stuff. To start, in reality, there are often organizations, companies running apps with high data traffic. Like stock trading stuff, for example. My example here is something like 50,000 transactions.\n\n**[37:12]** My example is like 50,000 a day is low it could shoot up to 500,000, a million transactions a day. After a while, that data volume swells up huge, affecting how we query data and impacting the user experience. In that data, there’s stuff that, once used, barely gets accessed again. Like history from over 7 years ago, for instance. That leads to a problem how do we deal with that pile of data? So we use a technique called data archiving.\n\n**[38:14]** It’s got benefits to counter those issues up there. First off, the data we use, the stuff that’s constantly being set, queried, read, and written all the time, it usually costs a lot. With this technique, we take our data and move it somewhere else, somewhere cheaper with less access. That way, it boosts our app’s performance when querying or aggregating data and such.\n\n**[39:07]** In terms of legal stuff or reusability, that data will be kept safe, not affected by external factors. So later, when we need to use it again, we can pull it out and use it. As people often say, you’ll think of data backup, which is usually used to restore data, restore the system, or the app if something goes wrong. But these two things are different in that data backup is more for hotfixing the system. As for archiving data archiving focuses on storing data long-term.\n\n**[39:59]** It has a detailed comparison like this. To build an architecture, a system to archive data, and then use it for recovery when we need it, here’s how it works. You guys see, it has three main notes. First, we store the data, we use metadata to interact with that data, then we put it somewhere, like cloud-based services or cloud storage services, to keep that data stored.\n\n**[40:51]** In detail, to store the data, first we have to figure out which data needs to be stored. We need to analyze which data gets used a lot, which doesn’t get used, or gets accessed rarely. There are lots of tools to help us do that. For example, analyzing from business requirements or using analytics tools, those analytics tools. From there, we figure out which data is necessary, which can be archived.\n\n**[42:05]** After that, we’ll package it up, using a few methods like vectorizing it, encoding it, then using checksums and stuff to make sure the data stays correct. Later, when we use it again, we can access it quickly. Because these databases are packaged in a storage different from what we usually set, we need to save its metadata. For instance, store it by month, yeah, or by account, so it’s easier to query later.\n\n**[43:07]** After archiving, when we want to use it again, just now I gave an example for recovery. We’ll use that metadata from earlier, search for the data blocks we need, then bring it back to the computing environment when necessary. The benefit here is that when we do this stuff, it doesn’t mess with the production data of the running app. We can do it in parallel. Whatever we want to do with it, we do, without poking around in production, so it keeps things safe.\n\n**[43:51]** For the user experience, like our product. Speaking of this, there are a few practices for using and building this system. It’s pretty simple, right? We’ll have to review the policies we set up for this system to run, check if the data stays intact. We’ll automate the steps of this process. Nowadays, there are plenty of tools supporting us already, like AWS or Google, they’ve got stuff like...\n\n**[45:14]** Google Cloud, for example. We just need to write some simple stuff to push it up there. And we can’t skip monitoring to see if this data is working well or not. Then, there are other techniques like checksums and such, to ensure our data always stays intact. When we need it, there’ll also be strategies like scheduling the data beforehand. Because this data sticks around for a long time, it’ll grow big, it’ll swell up in the storage, the cloud storage we use to keep it.\n\n**[46:05]** So there’ll be strategies like, when we need it, we have to schedule in advance, how much time it’ll take to replicate the data for us, for instance. That’s my plan, that’s it. The theory is kind of to address the ultimate goal of explaining to you guys about handling data that sticks around long-term but isn’t used much in the system we build. Like in banking, for example, there’s stuff like user trades, user trades hitting hundreds of millions of records or something.\n\n**[46:45]** Later on, it’ll grow even more. Meaning querying just the recent data, but it still takes a ton of time, something like that. That’s what I talked about today, done. Any questions, guys? When we store data, zip data, it’s like we zip up a fragment from the past that doesn’t use that data for current purposes, right? Yup, exactly, exactly. Agreed, I’ll have to delete it. Once it’s done, I’ve got to delete that, yeah. So we’ll have ways to reload it for calculations when needed.\n\n**[47:41]** That’s why we’ve got these methods to keep it safe. I’ve got a question over here. I don’t know if Thỏ’s crew knows about this, can we compare it? Standing out is Timescale, it’s got a chunk-moving mechanism. For example, we compress it like normal. Plus, there’s this thing about having hot, warm, and cold storage. Like, if we back up weekly, it goes on Azure’s hot storage. If it’s too old, say 2, 3, 4, 5 years, then it’s on Azure’s cold storage or Backblaze. It’s got a separate service for us to move that data stuff.\n\n**[48:53]** Right to the object or block storage spot, we interact with Timescale to make sure we save money with old data when we need to. It can save costs, it can still query, with the trade-off being that querying is a bit slow with older data. Yup, what I get is it depends on the platform we use to build, right, bro? For example, with Microsoft, it’ll depend on the database’s timing or the data’s lifespan or capacity and stuff, so there’ll be different levels.\n\n**[49:40]** For example, there’ll be delays, or normal access still, or delays for stuff that hasn’t been used in a long time. That’s for us to specify on each tool. But generally, it’s like, bro, what’s this doing? Standing out is Timescale, it fits this kind of pattern, for time series stuff. On Azure’s side, they make it fit the status, kinda like Timescale, but it helps us partition and shard the way we want.\n\n**[50:39]** Each one has its own pros and cons. With AWS, standing out is that with this service, you’ve got to watch the hardware storing this data, whether it’s stable or not. For example, Azure’s cold storage uses disks, what kind of disks, some pretty unique ones. They’ve got to use a laser machine to burn it in there. So querying is super fast, but inserting is kinda slow, like inserting a bunch takes a few minutes. Because it needs a hard laser to burn it on there, no virtualization layer.\n\n**[51:23]** Each service and each type of tool we use for compressing and storing has its own pros and cons, depending on the platform we subscribe to. Yup, exactly. This isn’t just about tools like AWS or Google services. It’s like we can also weigh it for our business too. So this is kinda general. Each platform uses different techniques. The ultimate common goal is to tackle the issue of data growing big but affecting how we query, how it runs.\n\n**[52:16]** Lots of ways to solve the query optimization problem, right? When the issue is that the data’s too big, there are a few approaches. Biên’s way is one approach, meaning there’s a chunk of data we’re not using, so we cut it out, store it somewhere. Later, if we need past data, we insert it back to use it. For now, we keep some percentage of current data, enough for current purposes, so querying is faster.\n\n**[52:59]** Other ways use tooling, some database types, like Timescale, optimize querying for huge data right off the bat. I think underneath, it kinda auto-buffs it somewhere, holds it for us, right, bro? So we fuss over the details underneath, we just use the interface. Underneath, it’s pretty much the same, like us kids. Thanks, Biên, that’s it. Probably An’s last piece, not sure if it’s related. Not sure if it ties a bit to the community stuff.\n\n**[54:00]** If there is, it’s probably just a quick rundown, not a lot. Pretty similar to Biên’s piece, but the use case is kinda close too. It expands a bit more. Alright, so this piece is a quick talk about datalakes and Notion’s use case. Let’s talk datalakes first. Datalakes, you guys have probably heard about them tons, been around for a while now. Let’s look back at how these datalakes evolved, see where we’re at.\n\n**[54:54]** Actually, from the start, around 1980 or something, it was the era of databases, database warehouses, the stuff we’re using now. Table stuff, creating tables and processing data. Later, around the 2000s or so, the big tech folks started collecting tons of data. They used that data, so new stuff popped up to handle storing and processing data on big datasets. Like data stored as files and such. These things, terms like MapReduce, for instance.\n\n**[55:44]** I think in my memo, there’s an article on MapReduce. If you guys don’t know, you can search it up, check it out to see what MapReduce did back then. It was the ancestor of this era. Later, it just got integrated in, not used standalone anymore, but it’s probably all built-in now. After that development phase, around 2010, it started giving birth, a bit before 2010, to concepts like datalakes, big data, cloud, internal data warehouses on the cloud. It’s just cloud stuff.\n\n**[56:28]** Now, these days, it’s evolving further, into lakes and datamarts. Lakes are probably just a mix of datalake stuff and warehouses, then turned into a house. Like Datadog or whatever they’re doing, I don’t know, but we’re probably talking about this a bit behind the times. To focus in, let’s take a quick look at a general data architecture first. This one, Tom’s piece the other day posted it, had a diagram too. It’s a bit more streamlined than this, a bit more concise, about data going through layers, processing then to some other thing.\n\n**[57:20]** This one shows it a bit clearer, about what kinds of data we store in a datalake. Compared to a data warehouse, we only store structured data, or stuff like table data that’s all cleaned up. But this datalake, it’s raw data, it’ll handle structured, unstructured, semi-structured data all together. It stores it raw, then it processes the data, transforms it, and tosses it over to the BI analytics crew or into another warehouse to hold the processed data.\n\n**[58:18]** Then there’s this analytics sandbox layer, which is a layer for data scientists or folks who need to use raw data, process data, without messing with the main process. Over here, they’ll work on this sandbox to handle data for those guys, the ones who need raw data but don’t directly affect the main flow. It’s like what Biên said earlier, doing that stuff, taking data and storing it somewhere for later use or to process something, I don’t know, but it doesn’t want to mess with the app’s main process, so it’s this pile.\n\n**[59:09]** Here, you guys see we’ve got this concept called ETL, extract, transform, and load. With warehouses, what we’ve done so far is extract, transform, and load, it follows that order straight up. But in this one, you’ll clearly see the datalake does extract and load first. Then when it’s needed, it starts processing the data, that’s transform. Transform comes after, load comes before. That’s the difference between the two.\n\n**[59:52]** This is just the spot comparing the differences between data warehouses and datalakes. With warehouses, the data is cleaned, structured, organized into tables. But this one stores it as files, raw data and stuff, semi-structured already, CSV or JSON files. The processing is different between this lake and that lake too. Querying, the warehouse uses SQL, while over there, it processes directly on the data itself. Tools that support direct data processing, like Spark, handle that stuff. Moving on to Notion.\n\n**[01:00:46]** For datalakes, the use case of Notion, you guys know we’ve been using Notion quite a bit already. Back in the day, it started slow. The organizations, the blocks from before, they were organized like normal data, just like us, small apps. Its blocks started growing gradually. Blocks are understood as what, and they’ll include the title in there. They call it a block. The number of blocks keeps increasing constantly by the day and hour.\n\n**[01:01:35]** Something like that, then later it started swelling up, and it began using techniques like sharding, old-school sharding. Like I remember Hải Vũ’s article mentioning something about it, scaling horizontally. It started splitting into sharding and stuff, then instances. From 2021 to 2023, it had 32 instances. Each instance had 15 shards. Then from 2023 onward, it started splitting again, increasing even more. The number went up again, so that’s 96 instances. And each instance had 5 shards. Multiply that, it’s around 400-something, four hundred and some.\n\n**[01:02:27]** To handle that, at this point it’s pretty big, right? When the data starts getting big, it’ll have some needs. Later on, it’ll have needs for analytics or stuff related to machine learning, datasets and tricks and all that. It started setting up its own data warehouse architecture. This was the precursor before setting up the datalake. It did a data warehouse to process data. The basic flow it set up was to collect data, stuff about data changes in the blocks across each shard.\n\n**[01:03:21]** It used file transfers to ingest the data from these shards here. It dumped it into something, then merged those things into one big single database. This ran into issues because, like I said earlier, it’s got about four hundred-something shards, right? It struggled with managing four hundred-something connections for this thing. Plus the scaling challenges. The amount of data changing in each block of Notion happens often and is super heavy, so it made reading and writing in this big table tough.\n\n**[01:04:13]** After that, it started setting up its own internal datalake. This internal datalake has a note that it won’t completely replace this one, it just uses the new stuff. The old one, it still uses for some tasks, lighter ones, for tables where data changes aren’t too heavy. And it needs something. But this one, it expects this flow to tag the data it needs for purposes like analytics or machine learning.\n\n**[01:05:08]** The data can handle a delay of a few hours, a few minutes, something like that. It’ll use the data in here. The setup amount is pretty simple. It uses this thing, Debezium CDC, you know. It’s the capture data change thing, to watch this database and shoot it over to Kafka. After it shoots that pile of event data changes to Kafka, there’s a thing over here, Hudi or something, that grabs those events and tosses them to S3. Then from this point, whoever wants to use it goes in here, grabs it, sets it up further, uses it for data warehouses or some shard-related purposes, they take it from here and use it.\n\n**[01:05:51]** That’s actually the Notion case. We could probably try using this thing. Because it’s also one of those that stands outside, watching that pile. If we use AWS or retraining, it’d use something like Redshift or whatever, I forget. It’d watch that, the changes on the database, then save it all into a bucket or something. From there, we start processing afterward. This flow here could use that. Earlier, I set up a demo, but it kinda flopped.\n\n**[01:06:51]** Because it didn’t have the server yet, so it failed. Let’s leave it for later, probably just that much for now. Plus there’s this perspective, this process here. It’s a process that enterprise folks could probably apply. It’s a kinda general process that most enterprises later on, I think, might use. Their needs, when the data grows big, will probably head in this direction anyway. It’s that they need data, collect data to do something, without messing with the main flow.\n\n**[01:07:52]** For us, so far we’ve always focused on working with AI models. But I think later on, we’ll also need some skill set to know how to handle data like this, stuff where the data’s bigger, you know. Sorry, which bro is this? You’re looking at this process, how’s it different from us replicating our database to another instance for retraining purposes, bro? Because here, standing out, the point is, it’s like I’m kinda generating data to another different shard, right?\n\n**[01:08:54]** And using an upload kit process for tasks that don’t, like, we can do async, you know, instead of needing to work directly on the main data source. The question is, for all those models like sharding or using master-slave setups, why not just duplicate our database? Duplicating data, it’s still just a data warehouse in table form, yeah. But actually this, it’s just a process, meaning a process for the database.\n\n**[01:09:40]** It could have other events. Like, for example, we’ll have lots of external data, not just one battery, a database, you know. Say we’ve got captures from social media or some random messy stuff, who knows. But it could be lots of different data types, gathered up, tossed to this thing. This Hudi buddy here, it’s the one responsible for processing that raw data, to throw it into this S3 thing. It stores everything under this pile.\n\n**[01:10:23]** It goes right in, everything in some file format, all dumped in here, so later on, the outside folks have a slot to process it. Actually, they had a question too, why not use databases like MySQL or PostgreSQL? They’ve got their own... Why use this capture data change thing instead of those? Those ones, they’ve got mechanisms to stream their event changes already. With them, event streams usually stream straight from one database to another.\n\n**[01:11:09]** But this one, it captures that event and sends it wherever. Because if we don’t have this Kafka here, we’d need some service, we’d need real-time data processed right away, without going through Kafka. This CDC thing can still bypass to there, kinda like that, not exactly from database to database like that. We also noticed something, like, it feels like, from an operations angle, for instance. Of course, if there are multiple datasources and we use partition tools and stuff, they’re different.\n\n**[01:11:50]** Different databases, so this will also, actually, bundle it into a datalake, so some tasks are specific, you know. Actually, some teams, like the AI team or the reporting team or data folks, they’d probably just need to work on this data warehouse, like that. Or if it extends to other sides too, it’d make sense, splitting data zones for each team separately, right? They added this thing about the button, the ETL button in regular databases versus datalakes, it’d be ELT, right?\n\n**[01:12:39]** Yup, ELT, you’d get it as extract, meaning we find the right file, right? We load that file up here and transform it into something like structured data, yeah. The idea is it transforms, it’s just an action, it happens after we’ve got the raw data. But ETL means extract is pulling data from the data source pile. Then it’s got a process to log straight into the raw data, straight into some stuff of ours. They call it raw landing, the raw landing layer. Then we’ve got what’s called transform.\n\n**[01:13:34]** After that, after the landing, there’s transform to process the data, so it comes out later. But the other one extracts, then transforms, then tosses it straight into the warehouse, that’s our database. Any questions, guys? Alright, thanks An, yup. See you, guys, bye, have a fun week","title":"OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion","short_title":"#37 AI Fine-tuning, Data archiving, Datalakes","description":"In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q\u0026A to boost our technical skills.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sun Dec 29 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/37-20241227.md","slugArray":["updates","ogif","37-20241227"]},{"content":"\n**Topic Highlights**\n\n- **Go Weekly #16**: Phat discussed concurrent data structures in Go, focusing on `sync.Map`. He explored its structure, use cases, and performance trade-offs in high-read, low-write scenarios. He also touched on garbage collection issues reported by the Go team.\n- **Generative AI UX Design Patterns**: Nam presented on UX design patterns for AI integration, covering System Scope Relationship, Spatial Relationship, and Functional Relationship. He explained how AI can be incorporated at various levels in digital products and discussed different ways to present AI features in user interfaces.\n- **Yelp Usecase AI**: Dat presented real-world AI use cases from Yelp, explaining how AI is used for recommendation systems, text editing, and image summarization. He explored AI applications in generating datasets, spam detection, and auto-generating short video reviews for restaurants.\n- **LLM Pattern**: Hoang introduced design patterns for integrating LLMs (Large Language Models) into applications. Key patterns included in-context learning, data preprocessing, and multi-agent collaboration, highlighting their practical use in AI-powered systems.\n- **Dify Git Analyze**: Cat demonstrated a Git repository analysis tool built using Dify. The tool scrapes content from repositories and supports diagram generation for code structure analysis, with a focus on optimizing the knowledge retrieval process in large datasets\n\n---\n\n**Vietnamese Transcript**\n\n**0:28** Chủ đề hôm nay vẫn có Go Weekly, và Nam đang thử nghiệm phần commentary về thiết kế hàng tuần. Chúng ta sẽ theo dõi trong vài tuần tới xem nội dung như thế nào.\n\n**11:19** Nam sẽ trình bày tiếp cho anh em, và sau đó sẽ có một vài bài của Hoàng, Cát, Đạt. Chúng ta đang nghiên cứu về các trường hợp sử dụng mà các công ty khác đang áp dụng, hoặc các công cụ mà dev đang sử dụng, và có thể sẽ mở một bài chia sẻ trong tuần này hoặc tuần sau. Bài hôm nay sẽ xoay quanh việc tạo một nút thiết kế UX. Trước đây, có rất nhiều câu hỏi về phạm vi mà AI đang áp dụng và vai trò của nó sẽ như thế nào – liệu nó chỉ đóng góp như một thành phần nhỏ riêng lẻ hay là cả một ứng dụng trong các sản phẩm số. Hôm nay, em sẽ giải đáp thắc mắc đó, tức là AI đang đóng vai trò như thế nào và cách thức hoạt động của nó ra sao.\n\n**12:11** Đầu tiên, em sẽ nói về \"System Scope Relationship.\" Hình ảnh này sẽ mô tả AI được tích hợp vào các hệ thống ở nhiều cấp độ khác nhau, từ một thành phần nhỏ lẻ đến một hệ sinh thái toàn diện hơn. AI có thể chỉ là một phần nhỏ trong một thành phần hoặc có thể phát triển thành các tính năng lớn hơn, giúp tự động hóa nhiều chức năng. Điều này sẽ giúp người dùng trải nghiệm ứng dụng dễ dàng hơn. AI có thể đóng vai trò trong bất kỳ phần nào của sản phẩm số – từ thành phần, luồng xử lý, tính năng cho đến toàn bộ ứng dụng, hoặc thậm chí là một nền tảng hoặc hệ sinh thái.\n\n**12:53** Ví dụ, trong một ứng dụng, AI có thể đóng vai trò một tính năng nhỏ, giúp người dùng thao tác nhanh hơn thay vì phải làm thủ công. Hoặc AI có thể là toàn bộ một ứng dụng như ChatGPT, nơi toàn bộ ứng dụng được xây dựng trên nền tảng AI, phục vụ cho một mục đích nhất định. Hoặc AI có thể là một nền tảng như Rewind AI, với nhiều tính năng hỗ trợ AI cho nhiều công việc khác nhau trong cùng một ứng dụng. Đây là phạm vi của AI trong các sản phẩm hiện nay.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=nx8a1rNN4wSuuPBo\u0026amp;start=688\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**13:39** Tiếp theo, về \"Spatial Relationship,\" phần này giúp chúng ta hiểu về cách tính năng AI được bố trí và sắp xếp trong giao diện người dùng (UI). Có nhiều cách để tích hợp AI vào thiết kế, và quan trọng là làm sao để bố trí chúng sao cho hợp lý, tối ưu trải nghiệm người dùng mà không gây rối mắt hay phức tạp giao diện. Spatial Relationship ảnh hưởng trực tiếp đến trải nghiệm người dùng. Ví dụ, AI có thể hoạt động độc lập hoặc song song với các tính năng khác, nhưng vẫn giữ không gian riêng của mình. Khi hiểu được các mối quan hệ này, chúng ta có thể chọn cách sử dụng và sắp xếp tính năng AI một cách tối ưu, không gây phân tâm cho người dùng.\n\n**15:11** Có sáu cách để trình bày tính năng AI, bao gồm:\n\n1. **Separate**: AI hoạt động độc lập.\n2. **Alongside**: AI được đặt bên cạnh các tính năng khác.\n3. **Layer**: AI hoạt động dưới dạng lớp phủ.\n4. **Integrated Parent**: AI đóng vai trò chính trong điều hướng hoặc quản lý nội dung chính.\n5. **Integrated Child**: AI đóng vai trò nhỏ hơn, bổ trợ cho tính năng chính.\n6. **Point**: AI chỉ xuất hiện như một biểu tượng nhỏ, giúp người dùng hiểu thêm về cách nó hoạt động.\n\n**16:41** Tiếp theo là \"Functional Relationship,\" phần này mô tả các mối quan hệ chức năng giữa AI và các tính năng khác trong hệ thống. AI có thể tồn tại độc lập nhưng vẫn adapt (thích nghi) với các nội dung và tính năng của hệ thống ở mức cao hơn. AI có thể tích hợp với các tính năng hiện có để cải thiện hiệu suất, thay vì người dùng phải thao tác thủ công. Khi hiểu rõ cách hoạt động chức năng của AI, chúng ta sẽ xác định rõ vai trò của nó trong ứng dụng và thiết kế để các hành động chức năng của nó không bị xung đột, cũng như không làm gián đoạn luồng sử dụng của người dùng.\n\n**17:28** Có sáu cách để mô tả mối quan hệ chức năng của AI:\n\n1. **Separate**: AI hoạt động riêng biệt.\n2. **Aware Of**: AI tách biệt nhưng có khả năng nhận biết các thay đổi trong tính năng chính.\n3. **Acting Up**: AI tương tác qua lại giữa các tính năng.\n4. **Feature Incorporate**: AI được tích hợp như một phần của một tính năng hiện có.\n5. **Usage**: AI được sử dụng theo cách mà nó tương tác với các phần khác trong ứng dụng.\n6. **Usage Conventionally**: AI tương tác hai chiều với các tính năng khác một cách trực tiếp.\n\n**18:14** Nó sẽ không ảnh hưởng trực tiếp đến tính năng chính, nhưng nó sẽ có tác động qua lại với AI và từ đó giúp cải thiện tính năng chính. Đây là một ví dụ cụ thể hơn về cách sử dụng của nó, chẳng hạn như trong code này có thể generate một panel bên phải.\n\nTiếp theo là **Acting Up**, nghĩa là hai bên sẽ có tác động qua lại, có thể trao đổi dữ liệu qua lại với nhau. Ví dụ, tính năng A có thể hiểu được dữ liệu từ tính năng B và ngược lại. Các dữ liệu này sẽ được trao đổi qua lại liên tục để cải thiện sự tương tác.\n\nTiếp theo là **Feature Incorporate**, nghĩa là AI được tích hợp trực tiếp vào các tính năng hiện có của ứng dụng. Cuối cùng là **Usage Conventionally**, nghĩa là AI sẽ tương tác theo cách thông thường với các tính năng khác, giống như cách các ứng dụng truyền thống hoạt động.\n\nVí dụ như khi bạn dùng một ứng dụng và có nhiều tính năng khác nhau, AI sẽ đóng vai trò trong các phần như feature, nhưng không phải lúc nào cũng là phần chính, mà sẽ đóng vai trò bổ trợ.\n\n**19:06** Ví dụ khác là ứng dụng Quora hay các ứng dụng khác, AI sẽ có nhiều tính năng nhỏ được tích hợp vào, như kiểu gợi ý trả lời câu hỏi, giúp người dùng thực hiện các tác vụ dễ dàng hơn. Vậy là nãy giờ em đã đi qua ba phần chính:\n\n1. **System Scope**: Giới thiệu cách AI tích hợp vào sản phẩm.\n2. **Spatial Relationship**: Giới thiệu cách sắp xếp AI trong giao diện người dùng.\n3. **Functional Relationship**: Giới thiệu các mối quan hệ chức năng giữa AI và các tính năng khác.\n\nNhững phần này giúp tối ưu hóa sản phẩm, cải thiện trải nghiệm người dùng và nâng cao hiệu quả cho ứng dụng AI.\n\n**19:57** Điều này rất quan trọng bởi vì nếu mình hiểu rõ cách áp dụng AI, tính năng mình làm sẽ mang lại nhiều giá trị hơn cho người dùng. Ví dụ mà em quên chưa nhắc đến là phần \"separate.\" Em đã đưa ra một số ví dụ, nhưng để quay lại một chút về \"separate\" – tính năng AI hoạt động độc lập. Mình có thể xem xét trường hợp Microsoft có một cái slider để generate hình ảnh song song với tính năng khác. Hoặc với một ứng dụng như Shopee, AI sẽ đóng vai trò hỗ trợ bên cạnh tính năng chính của ứng dụng.\n\n**20:53** Đó là những ví dụ minh họa cho việc sắp xếp và bố trí AI trong giao diện và sản phẩm. Anh Thành có thấy phần này như thế nào? Em thấy nó giống với các patterns thông thường trong thiết kế.\n\n**22:01** Anh Thành: Đúng rồi, những cái này là các mẫu patterns mình hay dùng trong việc thiết kế ứng dụng AI, hoặc khi tích hợp AI vào một ứng dụng hoặc sản phẩm riêng biệt. Về cơ bản, nó là những cấu trúc quen thuộc để mình hiểu rõ hơn về cách áp dụng AI. Em có thể phân loại, chia nhỏ chúng ra thành những tính năng nhỏ hơn. Phần này rất rõ ràng.\n\n**23:31** Cảm ơn Nam. Ok, tiếp theo là bài của Hoàng và Đạt nhé.\n\nHôm nay, em sẽ giới thiệu một bài gọi là \"AI Button trong các ứng dụng LLM.\" Trước khi vào bài, em sẽ nói qua về nội dung và agenda. Đầu tiên là chúng ta sẽ tìm hiểu về các design patterns liên quan đến AI Button. Những cái pattern này được áp dụng trong nhiều ứng dụng khác nhau. Em sẽ lấy ra những cái phổ biến và dễ hiểu nhất để giới thiệu cho mọi người.\n\n**24:35** Bài này sẽ xoay quanh việc sử dụng ứng dụng AI trong các sản phẩm số. Ứng dụng này tận dụng sức mạnh của các mô hình AI để giải quyết các bài toán cụ thể hoặc hỗ trợ người dùng trong các tác vụ. Khi sử dụng LLM, nhiều người có thể gặp vấn đề là mô hình không đưa ra đúng kết quả như mong đợi. Điều này là do bản chất của các mô hình này chỉ dựa trên khả năng phản hồi dựa trên chuỗi dữ liệu. Có nhiều cách để giải quyết vấn đề này. Một trong những cách tốn kém nhất là phải điều chỉnh lại toàn bộ mô hình từ đầu. Điều này có thể mất nhiều thời gian và nguồn lực.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=Jrnm_7QXsbImTctp\u0026amp;start=1435\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**25:15** Mình có một cách gọi là **in-context learning**, có nghĩa là AI có thể học trực tiếp ngay trong ngữ cảnh hiện tại khi bạn đang sử dụng nó. Đây là một kỹ thuật như là few-shot learning hoặc zero-shot learning, giúp AI tự học mà không cần phải được huấn luyện lại từ đầu. Ví dụ, bạn chỉ cần cho AI một vài ví dụ nhỏ trong ngữ cảnh và nó sẽ tự điều chỉnh cách hoạt động của mình dựa trên những gì được cung cấp. Thay vì phải retrain toàn bộ mô hình, cách này giúp tiết kiệm thời gian và tài nguyên rất nhiều, và nó vẫn đảm bảo AI có thể học từ ngữ cảnh cụ thể mà bạn cung cấp.\n\n**25:52** Với trường hợp này, **in-context learning** được sử dụng rất nhiều trong **prompt engineering**. Mọi người sẽ cung cấp các ví dụ có sẵn trực tiếp vào prompt và mô hình sẽ học từ những ví dụ đó để tạo ra các kết quả tiếp theo. Đó là ý tưởng chính của in-context learning. Về cơ bản, thiết kế sẽ hoạt động như thế này: bạn có một truy vấn, sau đó bạn xây dựng prompt với các ví dụ cần thiết và dữ liệu few-shot learning, rồi bạn đưa nó qua mô hình, mô hình sẽ trả về kết quả dựa trên các ví dụ đó. Tuy nhiên, nó không chỉ dừng lại ở các ví dụ, mà còn bao gồm rất nhiều yếu tố khác.\n\n**26:37** Nhìn rộng hơn, in-context learning liên quan đến việc cung cấp ngữ cảnh vào prompt bằng cách truyền vào các thông tin mà mô hình không có sẵn. Vì đây là một mô hình được huấn luyện trước, kiến thức của nó bị giới hạn, vì vậy bạn truyền thêm thông tin vào ngữ cảnh và prompt để mô hình học trong quá trình tạo ra kết quả. Ví dụ, trong chẩn đoán hình ảnh y khoa, mô hình có thể không có đủ kiến thức chuyên môn. Vì vậy, bạn cung cấp kiến thức đó vào ngữ cảnh và prompt để mô hình học trong quá trình tạo ra kết quả. Đó là cốt lõi của in-context learning.\n\nTiếp theo là nút thiết kế thứ hai quan trọng, được gọi là **data preprocessing/ editing**.\n\n**27:54** Phần này miêu tả quy trình chuẩn bị dữ liệu cho mô hình ngôn ngữ (LM). Như mọi người biết, LM hoạt động dựa trên các cơ sở dữ liệu vector, sử dụng so sánh vector để tìm các điểm dữ liệu tương tự. Quy trình này thường liên quan đến việc xử lý dữ liệu đa phương tiện và các loại thông tin khác nhau. Để đảm bảo đầu ra là tối ưu, việc áp dụng các bước xử lý trước dữ liệu là rất quan trọng. Ví dụ, bạn có thể xử lý trước văn bản bằng cách lọc ra các chi tiết không cần thiết để làm ngắn lại, hoặc với hình ảnh và âm thanh, bạn có thể loại bỏ nhiễu hoặc nén dữ liệu để giảm kích thước trước khi đưa qua mô hình ngôn ngữ.\n\n**29:19** Việc xử lý trước hoặc chỉnh sửa dữ liệu giúp mô hình hoạt động hiệu quả hơn. Có nhiều cách để xử lý trước, tuỳ thuộc vào loại dữ liệu hoặc ngữ cảnh. Bạn sẽ thực hiện điều này dựa trên các yêu cầu cụ thể. Nút thiết kế tiếp theo mà tôi muốn đề cập đến là một thiết kế thường được sử dụng, mặc dù có nhiều tên gọi khác nhau. Tôi gọi nó là **example agent**. Đây là một thiết kế thường thấy khi bạn muốn truy vấn của mình đi qua nhiều ngữ cảnh khác nhau. Ví dụ, nếu bạn có một ứng dụng đánh giá bài viết, bạn có thể cho bài viết đó đi qua một đường ống nơi mỗi agent đánh giá bài viết từ một góc độ khác nhau.\n\n**30:11** Một agent có thể đánh giá bài viết từ góc nhìn của một nhà văn, một agent khác có thể từ một góc nhìn khác. Sau khi đi qua tất cả các agent này, sẽ có một lớp tổng hợp cuối cùng để kết hợp hoặc xử lý các kết quả đó, và cuối cùng cung cấp cho người dùng một kết quả tổng hợp. Thiết kế này thường thấy trong các hệ thống đánh giá, nơi bạn đánh giá kết quả từ các mô hình khác nhau và chọn ra kết quả tốt nhất dựa trên các điều kiện đã được thiết lập trước.\n\n**30:55** Nút thiết kế tiếp theo, gọi là **agentic button**. Vậy agentic có nghĩa là gì? Trong ngữ cảnh của các mô hình ngôn ngữ (LMs), **agentic LMs** ám chỉ việc nâng cấp khả năng của mô hình. Vì mô hình chỉ biết những gì nằm trong dữ liệu huấn luyện của nó, chúng ta sẽ nâng cấp nó để tăng cường sức mạnh của nó và giảm thiểu sự can thiệp của con người. Thiết kế này giúp hệ thống tự động hoá nhiều hơn, cho phép nó hoạt động với ít sự can thiệp của con người hơn.\n\n**32:24** Thiết kế này có một số thành phần chính giúp bạn đạt được mức độ tự động hóa này. Có bốn thành phần chính: **reflection**, **planning**, **execution**, và **multi-collaboration**. Mỗi thành phần này đều giúp hệ thống của bạn trở nên tự động hóa hơn. Đầu tiên, chúng ta hãy nói về **reflection**. Reflection liên quan đến việc đánh giá kết quả ban đầu của mô hình dựa trên một tiêu chí hoặc một chỉ số cụ thể để xác định xem kết quả đó đã được tối ưu hóa chưa. Nếu chưa, hệ thống sẽ điều chỉnh và lặp lại quá trình này, tiếp tục tạo ra kết quả cho đến khi đạt được kết quả tối ưu.\n\n**33:06** Reflection giúp giảm thiểu sự can thiệp của con người vì thay vì tạo ra một kết quả ban đầu không đáp ứng mong đợi của bạn, hệ thống sẽ tinh chỉnh dựa trên các tiêu chí đã được thiết lập trước, cuối cùng đưa ra một kết quả chính xác hơn mà không cần điều chỉnh thủ công.\n\nReflection button này có nghĩa là nó sẽ đánh giá cái output ban đầu của một con AI, rồi nó sẽ đánh giá dựa theo một tiêu chuẩn nào đó hoặc là một cái chỉ số nào đó để xem là cái kết quả này đã tối ưu chưa. Nếu chưa tối ưu nó sẽ thêm thắt một chút và nó sẽ chạy vòng lại con AI đó để nó tạo ra kết quả khác cho tới khi nào đạt được kết quả tối ưu nó sẽ trả cho mình cái kết quả cuối cùng. cái này nó sẽ giúp giảm thiểu việc con người phải can thiệp vào quá trình làm việc, bởi vì nếu mà output đầu tiên không đúng ý mình, mình không cần phải tự chỉnh lại nữa mà nó sẽ tự tối ưu.\n\n**33:42** Button thứ hai là tool. Tool có thể là external, nó có thể là external API hoặc là những cái function mà mọi người code. Những cái tool này được sử dụng để cho model có thể lấy được những knowledge từ thế giới bên ngoài, những real-time knowledge, những external resource mà nó không được train sẵn. Như OpenAI hay là Claude đều có hỗ trợ. Khi đó, con model có thể tự biết khi nào cần gọi tool dựa vào cái description mà mọi người viết trên cái tool đó. Model sẽ tự biết cách lấy và extract thông tin từ tool, rồi trả về cho con LM để nó generate ra output.\n\n**34:30** Kế tiếp là planning. Planning button có nghĩa là mọi người cho con LM có khả năng lập kế hoạch, để tránh việc phải prompt đi prompt lại nhiều lần. Ví dụ, nếu có một task phức tạp, mình sẽ có một cái prompt lớn cho nó plan ra tất cả các step mà nó cần làm theo kiểu step by step. Cách này sẽ cho nó làm những việc nhỏ trước, rồi cuối cùng kết hợp lại thành một cái task lớn. Cái kiểu planning design này có nhiều biến thể, và đây là biến thể đơn giản nhất: lập kế hoạch xong rồi làm từng bước một.\n\n**35:10 C**uối cùng là multi-collaboration. Cái này em đã present cách đây một tháng rồi. Nói chung, nó giống như kiểu là AI giỏi việc nào làm việc đó. Mình có một cái context đúng không? mình chia nó ra, rồi đưa qua từng người. Người nào giỏi việc đó nó sẽ giải quyết việc đó, xong rồi pass qua con agent tiếp theo. Cứ thế, cuối cùng nó sẽ complete được cái requirement. Cái design này sử dụng tính chất divide and conquer khá nhiều. Chia việc lớn thành việc nhỏ, rồi đưa việc nhỏ cho người giỏi chuyên môn. Đây là một cái design button mà em thấy khá nhiều nơi bên ngoài sử dụng.\n\n**36:24** Đó là những design button mà em thấy nhiều nơi sử dụng và hiểu nhất. Em đã trình bày xong. Mọi người có câu hỏi gì không?\n\n**37:10** Hoàng, em nói lại cái phần planning, để confirm lại cái comment của anh Bảo. Nó giống như là kiểu đọc cái prompt đúng không? Nó sẽ hiểu cái prompt của anh trước, xong rồi nó sẽ chia cái prompt ra thành những cái nhỏ hơn, xong rồi nó sẽ có những con worker, có thể là những IDE worker hoặc là những cái prompt nhỏ để nó hoàn thành task đó. Đúng không?\n\n**37:40** Đúng rồi, anh có thể hiểu như vậy. Mình có thể chia prompt ra, ví dụ như là một task phức tạp, nó sẽ chia ra nhiều cái plan nhỏ. Những cái plan nhỏ này sẽ làm step by step. Ví dụ nó làm plan 1 trước, rồi làm plan 2, rồi làm plan 3. Sau khi hoàn thành tất cả các plan, nó sẽ tổng hợp lại ở một cái chỗ nào đó, hoặc là một cái component cuối cùng để nó ra được câu trả lời cuối cùng.\n\n**38:06** Ý là nó giống như cái con Zero mà hôm trước anh Tom present ấy. Con worker sẽ có thể làm một số task như đọc file, xóa file, sửa file, hay là talk với Internet, gửi email các thể loại. basically, agent các thứ như vậy.\n\n**38:52** Đúng rồi, bản chất của nó là thay vì làm một cục rất lớn để giải quyết hết cái task đó, mình phải đi prompt đi prompt lại nhiều lần để nó cho ra kết quả. Mình có một cái prompt trước, để chia nhỏ thành các task nhỏ, rồi sau đó có một cái pipeline để nó đi qua từng con worker, làm những việc nhỏ nhỏ cho mình.\n\n**39:23** Ok, kéo lên slide 14 đi Hoàng, slide 14. Anh cũng thấy là kiểu con này giống giống con Mule Automation mà Tom setup đúng không? Con Mule button mà Tom setup ấy. Em đã code xong rồi nhưng nhìn cái design này với cả cái button giống hệ nhau này.\n\n**39:46** Ừ, cái này là thằng Tpm nó chạy loop rồi, nhìn ra giống giống một tí. Nó giống planning mà anh Tom vừa nói, là nó break task ra từng phần, rồi xử lý từng phần một. Nó có iteration trong đó, giống như là nó có một list các step mà em đã mô tả ở trên. Back lại cái của em, chính là chỗ mà agent đang thấy. Cái của anh thấy nó giống planning hơn, là nó chia plan ra trước, rồi làm step by step từng plan một, đi qua mỗi vòng làm từng cái một. Còn cái này nó giống như là làm song song với nhau, nó parallel với nhau, để ra output xong rồi đánh giá lại output đó, rồi đưa ra kết quả cuối cùng. Chắc anh nhầm cái work rồi, đã correct lại.\n\n**42:28** Đúng rồi, thử đi. Nó là kiểu như vậy đó, nó chia ra thành nhiều việc khác nhau. Nó giống như là classify, nó chạy qua từng cái. Cái này giống multi-collaboration hơn, vì nó giống như question classifier, chỉ chạy một trong mấy cái này thôi. Mỗi agent làm việc đúng chuyên môn của nó, rồi combine lại.\n\n**43:33** Nhưng mà anh thấy mấy phần như reason với input analysis có đúng không? Của Tom, phần expert ấy. Riêng vụ pick domain ấy, nó có classifier ở đó, nhưng mà mấy phần reason với input analyzer là những agent khác nhau. Bên group đó là expert thôi, mình consider nó như là một group expert đúng không? Và nó combine với năm cái agent mình phía dưới.\n\n**45:33** Nếu mà làm tất cả mọi thứ trong cùng một cái prompt, em chắc chắn nó sẽ không ra được kết quả mình mong muốn đâu. Vì context quá nhiều và không có example cụ thể. Đầu tiên là accuracy chắc chắn sẽ giảm vì quá nhiều dữ liệu cùng lúc. Cái chính là phải chia ra nhiều layer, từng bước một. Thực tế mình cần output từ con LM, chứ không thể hardcode từ trước được. Mình chỉ muốn một cái prompt đơn giản nhất, để nó làm ra các câu trả lời nhỏ, rồi từ đó có một câu trả lời lớn.\n\n**46:59** Đúng rồi, khi làm nhỏ ra, mình sẽ biết vấn đề nằm ở đâu để debug. Như anh đã nói, specify kỹ, chia ra từng layer, nếu thấy sai ở đâu mình sửa ở đó. Còn nếu quăng một cục, mình sẽ không biết nó sai chỗ nào, rồi phải sửa rất nhiều lần.\n\n**48:43** Đúng rồi anh. Ví dụ như tạo một cái event trong calendar vào ngày mai, nếu không có sự kiện trong giờ đó tạo event, còn nếu có rồi thông báo. Nếu mình quăng một cục request đảm bảo nó sẽ rối ngay, vì nó phải thực hiện theo step by step. Nếu chia thành từng layer, test từng bước sẽ ổn hơn.\n\n**49:23** Nó sẽ em chắc là 99% là nó sẽ mù luôn á. Nếu mà còn nếu mình chia cái thành layer cơ, thành nhiều lớp layer á, làm test bài test nó sẽ ok hơn. Rồi, hô nào nên nữ, nên văn phòng là có Tôm ở đấy người chửi nhau. Anh không có hỏi nào chắc là cảm ơn Hoàng trước. À, đến Đạt nhé. Đạt nhờ. À, em không thị xem màn hình. Ok rồi, mọi người thấy màn hình của em chưa? Ừ, thấy rồi.\n\n**50:38** Hôm nay em nói về Yelp use cases. Từ từ Đạt, để anh giới thiệu context một chút. đợt này team mấy bạn sẽ focus vô đâu đó và đi search thử mấy cái phần use case ấy. Use case ở đây có nhiều dạng. Cái dạng mà Đạt đang sharing nó sẽ là mình xem thử các bên startup hay enterprise nó đang apply vào để giải quyết vấn đề gì. Là có thể là những cái green field, tức là những cái hoàn toàn mới. Hoặc là những cái mà nó optimize cho cái phần current workflow của chúng đó, kiểu vậy. Nó sẽ viết những use case và report lại hàng tháng, những cái phần update. Ngoài ra có một cái phần dạng use case khác nữa đó là những cái phần tuning mà để boost phần development của bên phía bên phía là tech các thứ. nó sẽ có những cái technique hay là có những cái phần editor mới, hay là mấy cái tool mới các thứ. đ cũng sẽ report cái phần đấy đâu đó trong tech. Đang testing thử trong khoảng hai tuần một đấy. đây là một cái bài đầu tiên chắc con Yelp này, nó đang dạng là con start-up phải không, chắc là. tiếp tục giới thiệu cho anh em một tí về cách mà bọn này đang apply AI là như thế nào?\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=l1ZUsZApjB78hPcD\u0026amp;start=3018\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**52:01** Yelp là cái đơn vị nó đưa ra cái software, nó cung cấp cái software cho các store, các bên mà doanh nghiệp muốn làm các đơn vị nhỏ lẻ như kiểu là giao hàng nhanh, hay là nhà hàng, rồi các bánh dụng cụ cơ bản, kiểu như vậy. Yelp này nó bán cái software cho mọi người làm việc đó. em sẽ chia sẻ chút về thằng này, nó sử dụng AI vào trong cái tooling của nó như thế nào.\n\n**53:00** trước đó chúng nó có một cái machine learning system rồi, bây giờ nó app thêm AI vào để giúp cho cái việc recommendation nó đúng hơn. bọn Yelp này nó có trên hệ thống của chúng nó, nó có nhiều cái thể loại đánh giá như kiểu đánh giá nhà hàng nó không bị tốt chẳng hạn. dựa trên những cái review đó, chúng nó có làm cái trò là text editing để so sánh được những cái kết quả mà spam hay không á. nó sẽ sử dụng AI vào trong cái việc gì. Thứ nhất là chúng nó sẽ tạo, chúng nó sử dụng AI để làm cái việc làm dataset, để train được cái model đánh giá là nó đang spam hay nó đang review tốt hay xấu như thế nào á. nó sẽ sử dụng AI để tạo ra cái dataset dựa trên LinkedIn. ở trong đây, em đọc có thấy bảo là chúng có sử dụng số tính như Zero-shot và Few-shot để làm dataset. chúng nó chỉ sử dụng một số cái model ở trên Hugging Face, rồi xong chúng nó làm classify để đánh giá được là review tốt hay xấu. đây là một cái use case cho cái việc AI dùng để làm text editing.\n\n**54:18** À, sang cái use case thứ hai của chúng nó, là chúng nó có sử dụng Clip Model. Clip Model bản chất của nó là xử lý hình ảnh. Xử lý hình ảnh có nghĩa là sao? Có nghĩa là dựa trên review, dựa trên review... đợi em chút để em kiếm nè. À, Clip á, nó sẽ xử lý hai thứ. Một là cái caption của cái ảnh, và cái ảnh nó như thế nào. qua Clip này á, nó sẽ hiểu được cái context của cái ảnh là cái gì. chúng nó sử dụng Clip vào trong những cái công việc như là những cái người ta đi vào trong một quán ăn hay một cái quán nhậu á, chúng nó sẽ review, chụp ảnh để capture lại những cái thứ này. Và ví dụ như hình ảnh của một cái món sản phẩm đi, trước khi apply Clip nó không đánh giá được, nó không đánh giá được là nó có bánh quế không, nó chỉ đánh giá được mỗi gà rán thôi chẳng hạn. Sau khi apply Clip vào á, nó sẽ biết được là có gà rán và có bánh quế. bản chất, nó sử dụng cái Clip này là một phần của AI, là nó xử lý ảnh, xử lý ảnh và caption của ảnh, và hình ảnh thành vector để nó so sánh với nhau. đây là hai use case của nó. những cái use case này được áp dụng cho cái gì?\n\n**55:38** Hai cái use case trên nó sẽ áp dụng trong cái tình huống là khi mà mình có nhiều review á, mình có thể summarize nó lại thành một cái highlight review ở trên đây. dựa trên những cái thứ mà nó chuyển thành vector được á, nó có thể annotation được cái việc là những cái hình ảnh đang nói cái gì, nó support cho mình được cái gì ở trong đây. Đợi một chút, nó sẽ highlight cho mình luôn. nó sẽ biết được cho mình cái context của cái ảnh là gì, nó có thể annotation được cái việc này. đó là cái use case của cái việc mà AI dùng để làm image summarization.\n\n**56:15** Đầu năm nay nó có release thêm cái là Yelp Assistant. Dựa trên những cái nền tảng cũ của chúng nó, chúng nó có thể tạo ra chatbot rồi, xong nó có thể review lại cái highlight như thế này, mình cứ hỏi nó xong nó recommendation cho mình cái gì thôi. Đơn giản là như vậy. Ngoài ra em có thấy một cái use case cũng khá đặc biệt, có nghĩa là trong cái giai đoạn từ 2020 á, nó nổ ra cái câu chuyện là làm clip ngắn review các thứ á. chúng nó có một cái nguồn dataset nhất định cho cái việc đó. em thấy chúng nó bảo chúng nó sắp release một cái như anh Tom có đề cập, cái bọn đó có thể chuyển văn bản thành giọng nói á. dựa trên cái nguồn dataset review này á, có lẽ chúng nó support review thêm cái việc mà làm video clip ngắn để mô tả cái nhà hàng.\n\n**57:45** Dựa trên những cái review, những cái video mà người ta tới người ta review á, mình có thể tạo ra được một cái đoạn script, xong cho nó chạy qua AI, nó tự động làm ra một cái video về một cái nhà hàng như mình. đây là use case của bọn này, đơn giản nó có thế thôi. Ok, quay lại cái câu hỏi đầu tiên, cái này nó sẽ dạng là dùng AI để label data, đúng không?\n\n**58:35** Ok, vậy là check xem là cái comment là negative hay positive, đúng không? Kể kiểu đấy là một ví dụ. Cái thứ hai nữa là nó sử dụng cái clip model, đúng không? Chắc là sẽ dạng giống như Vision, nhưng mà live hơn, cũng để dán nhãn, đúng không? Để dán nhãn giống như cái của bên phía Plot, dán nhãn cho ảnh. hai cái use case đó, nó sẽ được ứng dụng trong cái việc gì?\n\n**59:18** Em nghĩ có một cái ý khá hay mà nó chưa nói tới, là câu chuyện là nó có nguồn dataset sẵn. Như là ai tới review, ai tới đánh giá các thứ, dựa trên những cái clip ngắn như thế này, nó có thể tạo ra được một cái video intro về cái nhà hàng đó. Nó sử dụng AI. Em nghĩ là nó sử dụng AI để viết kịch bản, rồi sau đó đưa kịch bản đó cho một con AI voice để nói. Nhưng mà hình ảnh nó lấy ở đâu? Như kiểu là video nó sẽ lấy từ đâu ra?\n\n**59:57** Từ trong cái review, ai tới review họ sẽ có một cái video để review. Ok, tự động tạo advertisement, đúng không? Dạ vâng, cho TikTok hay những nền tảng như TikTok các thứ, kiểu summarize từ review của user. Nghe cũng có vẻ sáng tạo đấy. Ừ, chắc anh em confirm mấy cái của anh bảo làm rồi đúng không?\n\n**01:00:07** Đang vậy, cái này ok là cái caption. Ok, đúng hầu như là đúng anh. Bạn nói đúng, là chúng nó sẽ, em nghĩ em nghĩ cái use case này bọn này ban đầu á, cái mục đích ban đầu của bọn này là làm recommendation. trước đó, trước khi có AI chúng nó đã có một cái hybrid recommendation model trước. Căn bản là nó sẽ... Em nghĩ là khi mà có cái này á, nó dẹp gần hết cái model cũ này luôn. Em nghĩ có một cái khá hay là cái business messaging mà chúng nó không có đề cập nhiều. Có nghĩa là em nghĩ là nó sẽ dựa trên là có review top 50 review chẳng hạn. Xong top 50 cái interaction, kiểu như rating như thế nào. Thứ nhất là review tốt, n rating tốt, cái business messaging của nó sẽ tốt. Mà Yelp không đề cập vấn đề này, mình không trách nó được.\n\n**01:00:51** Ok, anh em có câu hỏi cho Đạt không? Bài đầu tiên đấy. Đạt bảo đang thêm mấy cái, mình phải enterprise nữa, nhưng mà thầy thấy đang Viettel với cả FPT, với cả VNG các thứ, đang chưa biết thấy chúng nó thế nào. Đạt kêu mấy cái tool, cái tool gì coding của bên phía FPT hả, đang kêu cùi.\n\n**01:01:41** Hì, một bản for, một bản for của của continue à? Nó thế, nó thế không tốt. Nó hơi cùi, thô. Hai, chị hết rồi à? Chắc vậy. Đạt nhé. Hôm nay mấy bài về Yelp và Tech Linh chắc tuần sau, tuần sau, tuần sau nữa, nếu kịp.\n\n**01:02:01** Tí demo luôn đi, Đạt luôn. Để Đạt demo một tí cái gì nhỉ? Cá đang là một con bot, để có thể question với cả question một cái short code dưới dạng kiểu developer mà hiểu rõ hơn về code, hay là test kiểu như là một vai trò auditor đi kiểm tra chất lượng của code. Đạt đang demo dev cái workflow hay con bot dựa trên diff đó cho anh em xem thử nào. Mình bật hình rồi Đạt ơi.\n\n**01:03:01** đây là một cái project để em xin vào club ai nha. Trình em gọi là hơi 'newb' nên project này mà có lem quá mọi người thông cảm. Workflow cơ bản là em sẽ lấy query, rồi trích xuất ra được cái URL của repo. Ở đây em có dùng lại cái scrapper của anh Tom, nhưng mà nó chưa đúng ý em, nên em có tạo một con scrapper ở local nó sẽ lấy được tất cả content của repo luôn. Nhưng mà cái đó nó quá lâu với quá lớn. Ờ, default hiện tại em chưa thấy làm cách nào mà bỏ vào con context được, trừ khi dùng cái knowledge retrieval, mà dùng knowledge retrieval em không có gọi là trực tiếp được mà phải bỏ vào trước. Mình không có chọn, không có chọn repo được.\n\n**01:05:45** Cái scraper này của anh Tom nó không có lấy content của file, cho nên em chưa vẽ diagram được. Vẽ diagram có thể em dùng, tí nữa em test thử. Cái này là em lấy được content của những file nè, ở root, ở những file doc. Những file đó không chắc câu hỏi của Huy vừa đưa ra chắc là cũng không trả lời được. Để em thử, em có sẵn cái full của em vô đây rồi, offline nhỉ. Bên phía in sẵn content rồi, chứ không online. Cái này em generate bằng luôn, cũng không có.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=DGxSbJrTJjDCyUp5\u0026amp;start=3798\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:07:07** Cái này nó sẽ scrap full content, nó sẽ đầy đủ hơn. Để em thử đặt câu hỏi của bên Huy hay của Hoàng các em thử nào. Mình thử BC chat lên rồi đặt câu hỏi xem. Anh có không? À không. Maybe là cái context này quá lớn, cái phần knowledge retrieval này em chưa tìm được cách mà cho nó vào context tốt được.\n\n**01:09:19** Retrieve tối ưu lắm. Cái file text này cũng mấy chục ngàn dòng, mấy chục ngàn dòng á. Mở lên xem thử nà, đ đang dùng mini hả, đổi sang máy đ xịn hơn xem có ok hơn không. Đồ mini hơi cùi. 2 triệu từ như thế, từ làm sao mà nó còn xong được ta? Em nghĩ là phải có một cái server, cái dedicated server luôn nó mới ok. Anh đang tò mò tại sao nó chạy được ấy, bởi vì 29U word à, như vừa thấy à. Nhân với cả 4 này là số to. Kích cỡ đấy, Follow up xem thử. Ok, tức là em vẫn là từ cái context thôi đúng không, là mình cũng chỉ dạng là query kiểu query vb đúng không, chứ không phải mình nhập hết tất cả cái đấy vào context.\n\n**01:10:57** Ok, đúng rồi anh. em chưa nắm được là cái retrieval của thằng dify nó sẽ chạy như thế nào. Không biết nó chạy có đúng không, nó retrieve có đúng không. Em chưa tracing được nó mà em có cái tool tracing ở phía trước nữa, có thể test lại thử xem như thế nào. Nhưng mà ý là nếu mà kiểu retrieval như này chắc là kết quả nó sẽ không đúng được đâu anh. Anh cũng đang chưa biết là nó sẽ run bao nhiêu data ấy. Kiểu nó chỉ prefer 2-300 thôi, kiểu data không thể nào đủ mà để làm mấy cái task kiểu này. Cái này ít nhất cũng phải vài trăm tương đối data ấy. Dạ cái này còn work in progress.\n\n**01:11:35** Đùa đấy, cứ lên công ty là có AI Club rồi. À, là của full version hay là fix được cái vụ này demo với bọn anh ở trên office nhé, mà try em để lại cho anh. OK, để em xem nó vẫn không build ra chắc mọi người coi đỡ. Cái chắc build bị gì đó, mọi người thấy màn hình không ạ?\n\n**01:13:08** Dạ tuần này như em nói tuần trước em sẽ up cái bài sync.Map này. Em thấy nó hay với chi tiết để mọi người mà xài Go có cái nhìn tổng quan hơn về map nói chung. Và cái thằng sync map này đi qua trước là phần context. khi mọi người viết map đúng không, mà mình nếu mà mình viết concurrent map hay operation đó, mình làm concurrent á, về bản 1.16 trước nó sẽ không báo đâu, nhưng mà nó vẫn không safe nha. Còn bản từ 1.16 trở đi á nó sẽ error như thế này đó. Cho nên là để mà solve được problem này bình thường mọi người có thể viết map kèm với tại package sync, viết manual đó được.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=0nH3Rjv-OZ5FAoAP\u0026amp;start=4394\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:13:56** Bên cạnh đó nó có một cái option khác đó là thằng sync.Map này. chút nữa đến cuối mình sẽ sẽ nói tại sao nó lại được đề ra xài và cái usecase của nó như thế nào. thằng này nó được đề ra để mà mình không cần quan tâm lắm về cái việc mà mình phải xài mutex để lock lại cho việc synchronize. Tức là mình chỉ có việc xài thôi. Xài nó trông đơn giản như thế này nha, nó friendly như là mình viết map kiểm tra value vậy. Ví dụ như mình load một cái key lên có value ok nó sẽ giống như là việc map value bình thường thôi. Trong như này, nếu có là ok true, còn nếu không có false của y chang.\n\n**01:14:38** Còn có một số cái function mà mình có thể xài rất handy. Đây là bảng 12.23 sẽ có clear, clear hết. Ví dụ như load là để lấy value, store là để update hoặc store cái key. Update vậy. Delete các thứ. ngoài cái việc mà mình viết concurrent đó đi đó, bên cạnh đó khi mà mình range, tức là mình loop một cái map nó cũng bị race condition nữa. thằng sync map này nó có cái hàm range này, mình xài mình sẽ không quan tâm nó là ấy, nó sẽ không bị nhưng mà như hàm range bình thường thôi. nó sẽ không cho mình cái cái snapshot mà gọi là consistent nhất, là khi mà mình vừa mới vô cái snapshot nó không được update là.\n\n**01:15:28** Mà mình range, tức là mình loop một cái map, nó cũng bị race condition nữa. thằng `sync.Map` này nó có cái hàm `Range`, mình xài, mình sẽ không quan tâm nó là cái gì, nó sẽ không bị như bình thường đâu. Nhưng mà như hàm `Range` bình thường thôi, nó sẽ không cho mình cái snapshot mà gọi là consistent nhất, là khi mà mình vừa mới vào cái snapshot nó không được update. trong lúc đó mình sẽ phải thay đổi cách viết, nhưng ít nhất là nó sẽ không bị phải error như thế này.\n\n**01:16:06** Đến cái phần bên dưới nó work như thế nào á. mọi người, nếu mà mọi người viết khi mà xem `CH` và `definition` cái `map` nó được cấu trúc như thế này: nó sẽ bao gồm hai cái `map`. Đó, nghe đến đây là mọi người sẽ thấy hơi kinh, nghe hơi thốn `RAM` với `memory`. Nó có một cái `Read Only map` và một cái `Dirty map`. nghe như thế mọi người có thể đoán được là nó sẽ làm việc theo kiểu là những cái value mà nếu mà được `write` nó sẽ được viết vào cái thằng `Dirty map` này hết. Cứ viết `update` vào đây, `update` vào đây, con này nó sẽ giống như là.\n\n**01:16:46** Cái `Read Only map` này nó sẽ là những cái khi mà mình đọc vào á, mình sẽ luôn đọc ở đây. Còn `write` sẽ luôn `write` mới vào thằng `Dirty map`. Còn cái flow bên dưới nó làm việc như thế nào chút xíu nữa mình sẽ nhìn cái chart flow mình sẽ thấy. À, cả hai cái `map` này có một điểm chung: nó đều có một cái con trỏ `entry` nha mọi người, để ý để dễ hiểu cái flow. Ví dụ, ở đây mình thêm một cái `entry` mới, đúng không? nó sẽ thêm vào `Dirty map` và nó đều trỏ đến `entry` này. Cái này nó sẽ giống như là một cái `flag` để đánh dấu rằng là cái `map` này đã được thay đổi rồi. Tức là cái thằng `Read Only map` này nó không phải là mới nhất nữa. Khi này bên dưới nó sẽ nhìn và hiểu rằng là thằng `Dirty map` mới là cái nên đọc vào.\n\n**01:17:27** Hình này thể hiện rằng là ví dụ như mình `update` một cái value nào đó, do bên dưới nó là con trỏ đúng không, mình chỉ việc `update` cái con trỏ đó thôi, không cần phải `update` từng cái value như là mình làm với `map` truyền thống. để làm được điều này á, bên dưới nó để ra một cơ chế là ba cái trạng thái (`state`) cho cái con trỏ `entry` này. `State` thứ nhất là `normal state`, đúng không? `Normal state` tức là những cái value cũ của `map`, nó đang có đủ và có thể xài được, không có bị gì hết. Còn trạng thái `amended` là khi mà `entry` đã bị sửa lại. Còn `delete state` là khi một `entry` nào đó đã được `delete` khỏi `map`, nhưng nó chưa được remove hoàn toàn nha. Tức là nó sẽ được...\n\n**01:18:59** ...assign cái con trỏ `entry` vào `new entry`, chứ chưa remove ra. Còn cái `expired state` là xóa hoàn toàn, giống như là `hard delete` là mất khỏi `map` luôn. Để hình dung rõ hơn, mọi người có thể nhìn cái flow như thế này nha: ví dụ ban đầu cái `map` của mình đang có một cái `key1` và `value1` đúng không? bên `Dirty map` chưa có gì cả, tức là chưa được thêm bớt gì. Sau đó, mình thêm một cái `key2` nào đó, đúng không? nó sẽ được thêm vào `Dirty map`, và khúc này là thằng `map` đã `amended` rồi, nó đã có một cái `flag` `amended` ở đây.\n\n**01:19:40** Sau đó, khi mình xóa (`delete`) một cái `key`, `map` này sẽ bị gán `new entry`, đúng không? Bên này cũng sẽ được tương tự gán `new entry`, giống như cái hình trước. Tức là mình chỉ cần cập nhật con trỏ thôi, không cần phải cập nhật value. Rồi, sau khi `delete` xong, đúng không, để `promote` được cái `Dirty map` này, mình phải cập nhật lại qua bên `Read Only map`, để `Dirty map` trở về `new state`, giống như đưa về trạng thái ban đầu.\n\n**01:20:18** Tương tự, thêm một `key3` nữa, khi thêm cái `key3` này á, cái `state` này nè, sau khi nó đã trở về `new state` rồi đúng không, mình thêm `key3` vào á, nó xác định rằng thằng này đã được `delete` rồi, nó sẽ là `delete` hoàn toàn. Điều này có nghĩa là lần sau, khi nó so sánh với `Dirty map`, nó biết rằng bên này cái `value1` đã bị xóa rồi, không còn nữa. cái `Read Only map` lúc này chỉ còn lại `key2` và `key3`.\n\n**01:20:51** Cho nên chính vì lý do này, `sync.Map` không có hàm `len` cho mọi người xài. Tại vì nếu như mọi người dùng hàm `len` ở đây, sẽ không biết được `value` của nó, tại vì lúc đó nó sẽ đếm cả những cái `value` đã `expired` hay `deleted`. Mọi người có thể thấy, chính vì cái cấu trúc của `sync.Map` được build như thế này, use case của nó được recommended là nên dùng cho những use case mà đọc (`read`) nhiều hơn ghi (`write`). Tức là nếu mà `write` hoặc `delete` nhiều, mọi người tưởng tượng chỗ này nó xài con trỏ liên tục, và có một cái issue bên Go team đã report là thằng này không bao giờ được garbage collected.\n\n**01:21:36** Sau đó Go team họ confirm rằng cái `sync.Map` này được sinh ra chủ yếu để support mấy cái bên trong Go Library thôi. Nếu mọi người thấy nó `handy` vì có những function dễ xài có thể xài, nhưng nếu use case của mọi người mà cần lưu trữ (`store`), hoặc là `update`, `delete` nhiều không nên xài, vì nó sẽ làm chậm hệ thống.\n\n**01:22:24** Dạ chắc chỉ vậy thôi ạ. Em có code lại cái bài bên này là cái bác này hay share mấy bài cũng khá chi tiết, mọi người có thể follow theo dõi. Ủa, cái này là topic gì Phát? Cho anh coi lại cái bài kịch bản đúp đầu r. À, cái sync map à? Ừ, sync map á. Ủa, nó có khác gì với lại cái anh vừa pass vô không vậy? Khác ở cái gì? Hình như là khác á anh. Ý là cái này em nhớ không nhầm là kiểu như cộng đồng tùy. Anh ví dụ use case họ muốn viết một cái gì đó mà họ thấy. Đó, anh nhìn thấy, họ ghi cái trong cái bên đấy, link mà nhìn thấy cách nó chạy mà lý do tụi nó làm thêm cái gì ấy nhỉ?\n\n**01:23:30** Anh nhìn thấy nè, họ thêm một cái lớp nữa để họ xài. Ví dụ như là họ sẽ có những cái use case đúng không? Ví dụ như họ muốn implement generic trên `sync.Map` đó. Cái này cũng có ảnh hưởng do cái vụ link nãy em nói, do thằng này nó không được garbage collected nè. Đó, kiểu vậy. ví dụ như bên này Go team ở dưới, họ đã confirm chốt xong cái này là cái `sync.Map` này họ kêu là cái này là `intended`, intentional choice rồi, cho nên họ sẽ không sửa. Họ sẽ không đổi đúng không? Bây giờ cộng đồng làm gì mình chỉ biết là họ tự xài thôi. Ý là họ thích cái việc `sync.Map` này được để ra dễ xài, có mấy cái function ngon lành, họ ráng thêm một tầng nữa, rồi chế những cái mà họ cảm thấy là ok, mình có thể xài được. Kiểu vậy.\n\n**01:24:07** Ủa mà sao cái clip này cũng lâu mà bữa nay lại chọn à? Ờ, thế kiểu insight thôi, insight cho mọi người xài. Ý là cái use case này cũng có thể được apply cho bên mình. Ví dụ như bên enterprise đúng không? ví dụ mình xài `map`, mà mình xài concurrency đúng không? mọi người sẽ tự viết một cái `struct`, xong rồi mọi người sẽ nhét một cái `mutex` vào, rồi tùy người sẽ ngồi bắt đầu viết lại. Đủ các kiểu. Trong khi đó thằng `sync.Map` rất handy, như nãy em show anh, là mấy cái function này là nó luôn follow cái chuẩn, là anh muốn `load` anh phải gọi hàm này. Kiểu vậy, nó chuẩn hơn.\n\n**01:24:50 M**ọi người sẽ tự viết một cái `struct` như thế, xong rồi mọi người sẽ nhét một cái mutex vào, tùy người sẽ ngồi bắt đầu viết rồi đú các kiểu. Trong khi đó `sync.Map` rất là handy. `Sync.Map` này như em show anh nãy đó, những cái function của nó, nó luôn follow cái chuẩn này hết. Anh muốn load anh phải gọi hàm này, kiểu vậy nó sẽ chuẩn hơn. Nhưng mà như cái bài này là mình phải để ý những cái trade-off của nó, xài cho đúng quy. Ok, hiểu rồi, tức là quy chuẩn cái cách mà sử dụng `map` hả? Với lại workflow hả?\n\n**01:25:26 C**ảm ơn Phát. Rồi để tranh thủ, mấy cái Thành ơi, nhất là xin anh em thêm 10 phút nữa nhé. Nó sẽ hơi tốn thời gian thêm xíu. Nhất là anh nhận được tổng cộng 11 cái submission cho cái bài test của mình. Có ít bài hình ở trên, mấy anh em nhìn ở trên tí. Deadline của mình là đến ngày 20, tức là tuần sau nhé. Bữa trước anh thông báo như là 27 ha, phải không? 26, 27 gì đó là deadline, mấy anh em coi tranh thủ còn một tuần nhìn bài đó rồi làm ha. Cái bài đó nó sẽ quan trọng, có một số cái mà chi tiết của từng bài đó anh chưa có nhìn kỹ. Chỉ có bài của Tôm bữa trước, Tôm nó quăng nhanh lên trên lobby quá, thành ra là có nhìn sơ qua xíu. Nhưng mà còn của mấy anh em chưa nhìn rồi. Nhưng mà cái ý chính là mọi người xem thử nha, cái chất lượng bài của mình á, tập trung ở chuyện là đợt này khi mà market nó thay đổi nhiều vậy, cái demand của thị trường cho cái nghề làm software nó có sự thay đổi lớn á.\n\n**01:26:15** Tất nhiên những cái nhu cầu nó vẫn sẽ còn ở đó thôi, nhưng mà cái số lượng đó nó giảm xuống. Thành ra đó anh gọi là cái sự thay đổi về cái nhu cầu thị trường gần như với góc nhìn của anh trải qua nó là giống như 2014, nhưng mà on-over-again, vậy là sự thay đổi công nghệ mới ra, mọi thứ mới ra, thị trường mới rồi những cái tiềm năng mới nó sẽ xuất hiện trên đó. cái bài test nó sẽ quan trọng với việc là giúp cho mình, nhất là test về văn hóa, nhìn lại trong cái lúc mà tụi anh muốn check lại cái team á, muốn là hai cái đội: đội làm research study với cả đội làm consulting nó có một cái sự phân hóa rõ ràng.\n\n**01:27:36** Nó có một cái sự phân hóa rõ ràng. Như trong cái bài viết anh post lên notion cách đây khoảng hai tuần hả, sẽ có sự phân hóa rõ ràng. Tương lai nó sẽ có thêm một số những cái policy mới cho chính sách về lợi ích khác nhau giữa hai đội nữa. Nhưng mà hiện nay là, như mình thấy đó, mọi người thấy OGIF dần dần nó được chuyển qua gần như thành cái buổi là report lại tất cả những cái study. Cái phần mà anh em đang coi mới và report lại trên này. Có thể những bài đó do được add, có thể những bài đó là do mọi người bắt đầu anh nhìn thấy, có một vài thành viên trong team mình thật sự là thấy cái kiến thức mới đó, xong rồi pick up những kiến thức mới đó để mà coi.\n\nTừ từ thấy rõ ràng là tụi anh muốn cái sự phân hóa đó nó diễn ra càng ngày càng rõ hơn. Và cũng có chính sách rõ ràng cho cái chuyện đó. Tức là ai mà thích coi mấy cái phần topic nhiều hơn, xong rồi ra ứng dụng ở tới mức là MVP, hay là ứng dụng vô những cái dự án nếu có, hoặc là đi deep dive thêm về kiến thức á, sẽ có một cái benefit khác. Những anh em nào mà không nhất thiết để phải ngồi coi những cái phần liên quan tới phần study như vậy, cứ ngồi làm dự án bình thường thôi. Nhưng mà nó sẽ có một số vấn đề khác đi kèm mà anh cũng có list ra trong cái link notion cách đây hai tuần. mọi người xem nhìn lại cái link đó một tí, để biết là vì cái định hướng như vậy nên là cái bài test này nó mang ý nghĩa là xem thử coi là cái mức độ của mọi người trong chuyện bắt kịp kiến thức mới, hoặc là cái độ tương thích với lại văn hóa trong cái giai đoạn mà tất cả mọi thứ nó thay đổi như vậy tới mức nào ha.\n\n**01:29:20** Để hiểu vì cái mục tiêu là như vậy, nên là cái lúc mà chấm cái bài á, anh sẽ là người duy nhất chấm cái bài đó. Team mấy anh chị khác không có chấm đâu. Tất cả mọi người sẽ phải làm mà, nên là anh nghĩ rằng anh set cái standard cho chuyện đó. Nên là mấy bạn chịu khó làm bài đó tự làm là một chuyện. Thứ hai nữa là bài nào mà chất lượng thấp thật ra cũng không có vấn đề gì hết, chấm điểm thấp một xíu thôi, nhưng mà vừa làm hết vẫn sẽ được đủ điểm để mà coi như là pass cái đó. Chỉ là sau đó cái kết quả trước mắt thể hiện được á, là anh sẽ phân cụm thành hai cụm khác nhau.\n\nĐội Foundation hay là đội Lab á, vẫn là đội core của mình từ năm nay, ha. Đó là cái thông báo chính. Nên là trên 11 cái bài này, nếu bạn nào làm xong rồi mà cảm thấy là mình có thể làm tốt hơn được cho cái chuyện mà anh vừa mới nói đó, đội mình thật ra là cái team Foundation và cái team Lab á vẫn sẽ được ưu tiên nhiều hơn trong những vấn đề khác nhau. Được ha. Nên là nếu mà anh em cái bài đó mà đang kiểu làm qua loa á, tập trung ngồi làm kỹ lại tí. Check hai thứ ha: văn hóa trên đó là một, thứ hai nữa là kiến thức.\n\n**01:29:56** Sau đó cái kết quả trước mắt thể thấy được á, là anh sẽ ân cụm thành hai cụm khác nhau, cái đội Foundation hay là đội Lab á vẫn là sẽ đội core của mình từ từ từ 8-9 năm nay ha. đó là vậy, đó là cái thông báo chính. Nên là trên 11 cái bài này, nếu bạn nào làm xong rồi mà cảm thấy là mình có thể làm tốt hơn được cho cái chuyện là anh vừa mới stay ra, là đội mình thiệt ra là cái team Foundation, cái team Lab á vẫn sẽ được ưu tiên nhiều hơn trong những vấn đề khác nhau. Được ha. Nên là nếu mà anh em cái bài đó mà đang kiểu làm qua loa á, tập trung ngồi làm kỹ lại tí, check hai thứ ha: văn hóa trên đó là một, thứ hai nữa là kiến thức cho cái cụm thông tin cái cụm gần nhất mà nó đang có vẻ hot nhất là LLM thôi.\n\nNhưng thực ra team mình vẫn cover rất là nhiều mảng khác nhau, vẫn đang có xem về design, mấy bạn cũng đang xem đúng không. Vẫn có đội đang xem đúng không. Go vẫn đang xem. Blockchain có vẻ nó qua trend tí rồi, thị trường nó đang sideways thôi, nhưng mà về demand của consulting nó vẫn yêu cầu những cái đó rất là nhiều.\n\n**01:31:46** Mấy cái mini app cho telegram, họ mua về rồi clone nhanh lên, thấy góc nhìn của mấy bạn làm business logic (BL) và tech (TCH) bây giờ nó khác một xíu rồi, không còn như ngày đầu nữa. Nhưng mà với consulting mình vẫn có thể sử dụng thôi, bình thường. Hoặc là mình có thể nhìn theo một góc nhìn khác, theo dạng là nó như một cái asset class mới xuất hiện. Với vai trò là developer, mình phải nhìn nó theo góc nhìn làm sao để nó ảnh hưởng đến cái workflow của mình như thế nào, quản lý tài sản ra sao.\n\n**01:32:29** Đó là vấn đề về bài test nhé. Mấy anh em chú ý cái đó. Thứ hai, nãy có nhắc tới cái định hướng về team và số lượng nhân sự. Trong đó có nhắc lại cái link notion hôm trước anh có gửi nhé. Đội Foundation, đội chính khi start lại lần nữa như vậy. Lúc trước team tụi anh bắt đầu chỉ có ba người thôi, sau đó dần dần tăng lên bốn người, rồi lên năm người. Có thêm Quan, có thêm Hiếu, có thêm mấy bạn khác. Nhưng mà ban đầu start với ba người, giờ đội hình xịn hơn rồi. Bây giờ 40 người toàn là thứ dữ, chắc chắn sẽ đi nhanh hơn. Câu chuyện chung là vậy, đánh giá chung cũng là như thế, nên mấy anh em nắm tình hình nha.\n\n**01:33:12** Cái thứ ba nữa có liên quan là Huy Nguyễn, nếu mà xong rồi, chắc tuần sau xem lại thống kê con số về ICY giùm anh nha. Hôm trước em cũng báo là số lượng bắt đầu chạy hơi nhiều, nên mình phải xem lại, cân lại con số cho nó hợp lý. Riêng phần này nhờ Huy và Thành chủ động làm giùm, xử lý giùm anh, xem lại cân số cho nó hợp lý. Thành có một công việc phụ là phần benefit cho thành viên team Lab, xem thử đề xuất như thế nào. Nó có thể được coi là một cái payon, nhưng mình sẽ không trả qua kênh bình thường, mà sẽ có cái cơ chế khác.\n\n**01:33:52** Nhưng mà mấy thành viên team Lab sẽ có cái đó, mọi người quen với cái đó rồi. Cuối cùng là, riêng phần về LLM hiện tại, trong cái list câu hỏi có một câu hỏi quan trọng là làm sao để sử dụng, tìm hiểu bên ngoài sử dụng LLM như thế nào và adapt ra sao. Nhấn mạnh lại câu đó, vì nó là một câu mang ý nghĩa trong việc làm knowledge discovery. Câu hỏi này liên quan đến việc test là không chỉ đơn thuần là dùng, mà là tất cả các công cụ mà mấy anh em thấy được trong team hiện tại. Khi có người sử dụng hiệu quả, có người sử dụng kém hiệu quả hơn, RT (retrieval technology) nó thành một spectrum rất rõ ràng, những người thấp là thấp, những người cao rất cao.\n\n**01:34:38** Tụi anh muốn nâng cái standard đó lên. Spectrum đó tụi anh muốn rút ngắn lại, càng cô động lại càng tốt. Bây giờ nó đang rất dài. Câu này ngoài việc dùng tool để làm discovery, nó còn mang ý nghĩa xem ngành nghề của mình sẽ như thế nào trong việc ứng dụng đó để nâng cao competency của mình, làm việc có năng suất hơn. Đó là toàn bộ vấn đề, và mọi người xác nhận lại xem cái mình làm có đúng chưa, nó có tầng ý nghĩa sâu xa hơn vậy.\n\n**01:35:20** Cuối cùng để kết thúc buổi này, Thành ơi, mấy buổi OGIF sau, những phần mà Tom đã làm liên quan đến việc xây dựng structure của một cái LLM app, có thể lấy cái đó ra phân tích thử nhé. Phân tích lấy cái đó để làm sâu hơn luôn nhé.\n\n**01:35:56**Toàn bộ mọi người hy vọng là tất cả anh em đều pass hết để đi chơi cho nó vui vẻ. Tuần sau sẽ có một cái bài khác. Tuần sau request là bên chỗ của Minh L. Minh ơi, chắc là lên làm một cái demo nha, tiếp tục về cái finite state machine, FSM á. Vì trong định hướng những công nghệ nền tảng như blockchain, AI, nhưng phần chính vẫn sẽ là các anh em làm engineer sẽ có một ngách khác để đi, đó là hiểu rõ các hệ thống lớn vận hành thế nào. Tương lai, nếu mình không phải là người sinh ra để làm data manipulation AI sẽ làm giùm mình, mình không cần tự thiết kế hay làm mấy việc của junior nữa.\n\n**01:37:35** Cách duy nhất để lên senior là hiểu rõ vấn đề và làm kiến trúc thôi. Phần finite state machine đóng vai trò tương đối quan trọng, liên quan đến chuyện scale mà trước giờ tụi mình đã nói nhiều. Trước đó Minh có đọc và hiểu đúng góc nhìn mà anh đang muốn hướng tới. Nên là xem thử làm bài phân biệt các loại general server của nó nhé. Server state machine và event-based server. Rồi làm một cái sample để biểu diễn và implement nó luôn bằng Erlang nha. Erlang có sẵn hết các framework rồi.\n\n**01:39:01** Bài này chắc là khi nào Minh Lưu. ready, nếu tuần sau không kịp có thể là hai tuần. Đề nghị mấy bạn backend và mấy bạn sen team mình gom lại, có gì confirm trước nhé. Vì bài này rất quan trọng trong chuyện phân tích thiết kế phần mềm. Bài này rất quan trọng. Trước giờ mọi người chỉ nói tới modeling và làm C4 thôi, nhưng Erlang là ngôn ngữ đi sát cái này nhất rồi, thường mọi người sẽ không biết hết. Chúng ta không nhất thiết phải học Erlang nhưng có thể nhìn cách thiết kế và build của họ để làm phần đó rất chuẩn, giống như là họ có framework sẵn, mình chỉ cần gắn vào để sử dụng thôi.\n\n**01:39:37 T**ranh thủ, ngày 20 tháng 10 là chủ nhật, Mỹ với Ngọc và Giang có post rồi. Hôm đó là các chị em đi chơi, còn không ở Sài Gòn đại diện team sẽ chúc mọi người phát tài. Chúc mọi người phát tài chắc hợp lý nhất trong trường hợp này. Một chút chúc khác có vẻ không liên quan lắm. Rồi vậy nha, anh em tham gia được đăng ký với Mỹ để book bàn và đi cho hợp lý.\n\n**01:41:19 N**hờ Thành những buổi sau cấu trúc lại thành mấy cái talk nhé. Rồi làm goal đó, team mình có thêm Builder-club nữa, đội đó chắc để xem mấy anh em lúc trước làm Super Bit ổn định lại hoặc làm console ổn định lại anh sẽ cấu trúc lại sau nhé. Đợt này chắc là nghỉ ngơi đầy đủ rồi. Rồi ok, anh em có câu hỏi gì cho bài test không kết thúc ở đây nhé. Rồi tạm biệt mấy anh em, hẹn gặp lại tuần sau. Cảm ơn Thành, cảm ơn tất cả mọi người.\n\n---\n\n**English Transcript**\n\n**0:28** The topic still includes Go Weekly, and Nam is currently testing the weekly design commentary. Let's see how it goes over the next few weeks.\n\n**11:19** Nam will continue to present to the team, and there are a few topics from Hoang, Cat, and Dat. We’re currently researching various use cases that other companies are applying and some of the tools being used by developers. There will likely be a presentation this week or next about these findings. The focus will be on generating a UX design button. In the past, there have been questions about where AI is applied and how it plays a role, whether it serves as a small, standalone component or as part of a broader application for digital products. Today, I will address how AI contributes and how it functions.\n\n**12:11** First, I will talk about system scope relationships. This diagram illustrates how AI is integrated into systems at different levels, from a small component to a comprehensive ecosystem. AI can be a small part of a component or evolve into a larger function, automating features to improve user experience (UX). Here, AI plays a crucial role in digital products, and when integrated, it can fit into various parts, from components to flows, to features, or even as an entire application. It can be part of a platform or ecosystem.\n\n**12:53** For example, as a feature within an app, AI can help users interact with the app more easily, saving time by automating tasks that would otherwise be done manually. As a standalone application, there are many examples like ChatGPT, which serves a specific purpose, or as a platform like Rewind AI, which offers multiple features supporting AI in different tasks within the same app. These are examples of the scope of AI's current operations.\n\n**13:39** Next, regarding the spatial relationship, this helps us understand how AI features are placed and organized within the user interface (UI). There are several ways to integrate AI into design, and it's important to know how to position them in the app so that they optimize user experience without causing confusion or making the interface too complex. Spatial relationships directly affect user experience. For example, AI can operate independently or alongside other features while still maintaining its own space. When you understand these relationships, you can choose how to place and use AI features in a way that enhances usability without overwhelming the user.\n\n**15:11** There are six different methods for presenting AI: it can be entirely separate, alongside other features, layered, integrated with the parent feature, or in small points such as icons. These methods include:\n\n- Separate: AI operates as a separate feature.\n- Alongside: AI is placed next to other features.\n- Layer: AI overlays with another feature.\n- Integrated Parent: AI serves a major role in navigating and managing core content.\n- Integrated Child: AI operates as a secondary, smaller feature.\n- Point: AI is a small icon or widget that helps the user understand its function.\n\n**16:41** Moving on to the functional relationship, this describes the functional interactions between AI and other features in the system. AI can exist separately but still adapt to the overall content and functionality of the app at a higher level. AI can integrate with existing features to improve performance, replacing manual tasks. Understanding how AI works functionally allows us to define its role clearly in the app and design in a way that ensures the functional actions don’t conflict with one another and don't disrupt the user flow.\n\n**17:28** There are six methods to describe this functional relationship, which are similar to the spatial relationships I mentioned earlier:\n\n1. Separate: AI operates independently.\n2. Aware Of: AI exists separately but is aware of how it affects the main feature.\n3. Acting Up: AI interacts back and forth with other features, adapting data between them.\n4. Feature Incorporate: AI is incorporated as a part of an existing feature.\n5. Usage: AI adapts based on how it's used within the app.\n6. Usage Conventionally: AI communicates directly with other features in a two-way interaction.\n\nI will provide an example of this functional relationship in the code I am about to show, where AI generates a panel on the right side of the screen.\n\n**19:06** For example, the acting-up relationship means AI can be aware of and react to changes made by other features, like data syncing between two systems. In contrast, feature incorporation would mean AI is integrated as part of the overall functionality of a specific feature.\n\n**19:57** That covers the main aspects I’ve discussed so far, with three key elements for integrating AI into product design: optimizing product features, improving user functionality, and enhancing the overall effectiveness of the AI-powered system. It’s important to understand how to apply AI properly to provide clear value to the user. If we understand how to apply AI effectively, it becomes easier to design a system that brings value to the user by integrating AI in a meaningful way.\n\n**20:53** I realized I missed an example earlier, so let me go back and explain. I’ll share a few examples that I think will clarify the functional relationships we discussed. For instance, in Microsoft, there’s a tool that generates images, this operates alongside other features in a parallel fashion. There’s also a feature that sits beside the main functions of the app but doesn’t serve as a core part of the experience.\n\n**22:01** Yes, that's a good example. The functional actions and spatial relationships you presented seem to be similar to common patterns. These are just standard patterns for AI design, how to integrate an AI feature into an app or design an AI-driven app, depending on how it’s categorized.\n\n**22:31** Yes, these are patterns we often use when designing AI applications or integrating AI into a separate application or product. Essentially, they are familiar structures to help us better understand how to apply AI. You can categorize and break them down into smaller features. This part is very clear.\n\n**23:31** Thank you, Nam. Ok, next will be Hoàng and Đạt’s presentation.\n\nToday, I will introduce a topic called \"AI Button in LLM Applications.\" Before diving in, let me briefly cover the content and agenda. First, we will explore design patterns related to the AI Button. These patterns are applied in various applications. I’ll pick out the most common and understandable ones to introduce to everyone.\n\n**24:35** This presentation will revolve around using AI in digital products. These applications leverage the power of AI models to solve specific problems or assist users in tasks. When using LLMs, many may encounter the issue where the model does not provide the expected result. This happens because the model operates based on its ability to respond using the data it has been trained on. There are multiple ways to address this issue. One of the most expensive ways is to retrain the entire model from scratch, which can take a lot of time and resources.\n\n**25:15** We have a technique called **in-context learning**, which means AI can learn directly within the current context while you are using it. This technique includes few-shot learning or zero-shot learning, allowing the AI to learn without needing to be retrained from scratch. For example, you only need to provide the AI with a few small examples in the context, and it will adjust its behavior based on what is provided. Instead of retraining the entire model, this method saves a lot of time and resources while still ensuring the AI can learn from the specific context you give it.\n\n**25:52** In this case, **in-context learning** is widely used in **prompt engineering**. People provide available examples directly into the prompt, and the model learns from those examples to generate subsequent results. That's the main idea of in-context learning. Essentially, the design works like this: you have a query, then you build a prompt with the necessary examples and few-shot learning data, and you pass it through the model, which returns a result based on those examples. However, it doesn’t stop at just examples; many other factors are involved as well.\n\n**26:37** Broadly speaking, in-context learning involves feeding the context into the prompt by providing information that the model doesn’t inherently have. Since this is a pre-trained model, its knowledge is limited, so you provide additional information in the context and prompt for the model to learn during the result generation process. For instance, in medical image diagnosis, the model may not have enough specialized knowledge. Therefore, you provide that expertise into the context and prompt so the model can learn during the result generation process. That’s the core of in-context learning.\n\nNext, we have another important design button, which is **data preprocessing/editing**.\n\n**27:54** This section describes the process of preparing data for the language model (LM). As you know, LMs operate based on vector databases, using vector comparisons to find similar data points. This process often involves handling multimedia data and various types of information. To ensure optimal output, applying data preprocessing steps is crucial. For example, you can preprocess text by filtering out unnecessary details to shorten it, or with images and audio, you can remove noise or compress the data to reduce size before passing it through the language model.\n\n**29:19** Data preprocessing or editing helps the model operate more efficiently. There are many ways to preprocess, depending on the type of data or context. You perform this based on specific requirements. The next design button I want to mention is a commonly used one, though it goes by different names. I call it the **example agent**. This design is commonly seen when you want your query to pass through multiple contexts. For example, if you have a content review application, you can let that content pass through a pipeline where each agent evaluates the content from a different perspective.\n\n**30:11** One agent might evaluate the content from a writer's perspective, and another agent might do so from a different angle. After going through all these agents, there will be a final synthesis layer to combine or process those results, ultimately providing the user with a comprehensive output. This design is often seen in evaluation systems where results from different models are evaluated, and the best outcome is chosen based on predefined conditions.\n\n**30:55** The next design button is called **agentic button**. So, what does agentic mean? In the context of language models (LMs), **agentic LMs** refer to enhancing the model's capabilities. Since the model only knows what’s in its training data, we upgrade it to increase its power and minimize human intervention. This design helps the system become more automated, allowing it to operate with less human interference.\n\n**32:24** This design has several key components that help you achieve this level of automation. There are four main components: **reflection**, **planning**, **execution**, and **multi-collaboration**. Each of these components helps make your system more automated. First, let’s talk about **reflection**. Reflection involves evaluating the initial results of the model based on a specific criterion or metric to determine if the result has been optimized. If it hasn’t, the system adjusts and repeats the process, continuing to generate results until it reaches an optimal outcome.\n\n**33:06** Reflection helps reduce human intervention because, instead of producing an initial result that doesn’t meet your expectations, the system refines itself based on pre-established criteria, eventually delivering a more accurate result without manual adjustment.\n\nThe Reflection button means that it will evaluate the initial output of an AI, then assess it according to a certain standard or metric to see if the result has been optimized. If not, it will adjust slightly and run the AI again to generate another result until the optimal result is achieved. This helps reduce the need for human intervention, as if the first output is not what you expected, you don’t need to manually adjust it, the system will optimize itself.\n\n**33:42** The second button is the tool. Tools can be external, such as external APIs or functions that people code. These tools are used to allow the model to access knowledge from the outside world, real-time knowledge, or external resources that it hasn’t been pre-trained on. For example, OpenAI or Claude both support this. The model can know when to call the tool based on the description you write for the tool. The model will know how to retrieve and extract information from the tool and then return it to the LM to generate an output.\n\n**34:30** Next is planning. The planning button means that you give the LM the ability to plan, preventing the need to prompt multiple times. For example, if you have a complex task, you provide a large prompt for the LM to plan out all the steps it needs to take in a step-by-step manner. This allows it to perform smaller tasks first, which are eventually combined into a larger task. This planning design has many variations, and this is the simplest version: planning and then executing step by step.\n\n**35:10** Finally, multi-collaboration. I presented this about a month ago. Essentially, it's like having the AI excel at a particular task. You have a context, right? You divide it and pass it through to different agents. Each agent is good at its specific task, and after they complete their tasks, it passes on to the next agent. In this way, it can complete the requirement. This design heavily utilizes the divide-and-conquer principle, breaking a large task into smaller tasks and assigning each to a specialized agent. This is a design button I’ve seen being used in many places.\n\n**36:24** Those are the design buttons that I’ve seen used in many places and understand the most. I’ve finished my presentation. Does anyone have any questions?\n\n**37:10** Hoàng, can you repeat the part about planning to confirm Bảo’s comment? It’s like it reads the prompt, right? It understands your prompt first, then breaks it down into smaller tasks, and then there are workers, perhaps IDE workers or smaller prompts, to complete the task. Is that correct?\n\n**37:40** Yes, you can think of it that way. You can split the prompt, for example, in a complex task, into several smaller plans. These smaller plans will be done step by step. For instance, it executes plan 1 first, then plan 2, then plan 3. Once all the plans are completed, they are compiled somewhere or in a final component to produce the final answer.\n\n**38:06** It’s like the Zero you presented last time, right? The worker can do tasks like reading files, deleting files, modifying files, or interacting with the Internet, sending emails, and so on. So basically, agents work in this way.\n\n**38:52** Exactly. Instead of handling a massive task all at once, which requires repeated prompting, you start with a prompt that breaks the task into smaller tasks, and then a pipeline runs through each worker, handling small tasks for you.\n\n**39:23** Ok, pull up slide 14, Hoàng. Slide 14. I also see this is kind of like the Mule Automation setup that Tom created, right? The Mule button that Tom set up. I’ve finished the code, but this design and the button look exactly the same.\n\n**39:46** Yes, this is a looping process with Tom, which looks somewhat similar. It’s like planning, as Tom mentioned, where it breaks down the task into parts and handles each part. It has iterations within it, like a list of steps you described earlier. Referring back to yours, the agents can see that. What I’m seeing looks more like planning: it splits the plan upfront and then works step by step on each plan, moving through each round one by one. This one, though, works more in parallel, where they run simultaneously, produce the output, evaluate it, and then return the final result. I think I got the workflow mixed up; it’s now corrected.\n\n**42:28** Exactly, give it a try. It works like that, breaking down into different tasks. It’s more like a classification, running through each one. This is closer to multi-collaboration because it’s like a question classifier, where only one agent runs for each task. Each agent works on its specific expertise, then combines everything.\n\n**43:33** But do you think the parts like reasoning and input analysis are correct? Tom’s expert part. Specifically, for picking domains, there’s a classifier, but reasoning and input analyzers are separate agents. In that group, they’re experts, right? We consider them a group of experts, and they combine with the five agents underneath.\n\n**45:33** If we try to do everything within a single prompt, I’m certain it won’t give us the desired result. The context is too large and lacks specific examples. The main issue is that accuracy will definitely decrease because there’s too much data at once. The key is to split it into multiple layers, step by step. In reality, we need the output from the LM; we can’t hardcode it all in advance. We just want the simplest prompt so it can generate small answers that ultimately lead to a large answer.\n\n**46:59** Exactly, by breaking it down, we can identify where the problem lies and debug it. Like you mentioned, specify clearly and break it down into layers. If something goes wrong, we can fix that part. If you throw everything in at once, you won’t know where the error is, and you’ll have to fix it repeatedly.\n\n**48:43** Exactly. For example, creating an event in the calendar for tomorrow, if there’s no event at that time, it creates the event, but if there is already one, it sends a notification. If we throw in a large request at once, it will get confusing because it has to execute step by step. Breaking it into layers and testing each step will make it work better.\n\n**49:23** I'm almost certain that 99% of the time, it will get lost if it’s done in one go. However, if we split it into layers, into multiple layers, and do the tests, it will work much better. Ok, let's go. If anyone's at the office, Tom’s probably there to argue with. If no one has any more questions, thanks to Hoàng first. Now, Đạt, you’re up. Đạt, are you sharing your screen? Ok, can everyone see my screen? Yes, we can.\n\n**50:38** Today, I’m going to talk about Yelp use cases. Wait a second, Đạt, let me introduce some context first. So this time, the team will focus somewhere and search for some use cases. There are different types of use cases. The type Đạt is sharing is where we look at how startups or enterprises are applying AI to solve specific problems. It could be something completely new, like a greenfield, or it could be optimizing the current workflow of their system. They will write use cases and report updates monthly. In addition, there’s another type of use case, which involves tuning to boost the development on the tech side. They will also report that part somewhere in tech. We’re testing this for about two weeks. This is the first report, and it’s about Yelp. Yelp is a startup, right? Now, Đạt, introduce how they are applying AI.\n\n**52:01** Yelp is a company that provides software to stores and businesses that want to offer services like fast delivery, restaurants, or basic utilities. Yelp sells the software for those tasks. I’ll share a bit about how they use AI in their tools.\n\n**53:00** Before this, they had a machine learning system, but now they’ve added AI to improve the accuracy of their recommendations. Yelp has many types of reviews on its system, like restaurant reviews, which may not always be good. Based on those reviews, they do some text editing to compare whether the results are spam or legitimate. AI is used here in several ways. First, they use AI to create datasets to train a model to assess whether a review is spam or a good/bad review. They use AI to generate datasets based on LinkedIn. From what I’ve read, they use techniques like Zero-shot and Few-shot learning to create these datasets. They use some models from Hugging Face and then classify the reviews as good or bad. This is one use case where AI is applied in text editing.\n\n**54:18** Now onto the second use case, they use the Clip Model. The Clip Model primarily processes images. What does that mean? It means that based on reviews... wait a minute, let me find the reference... Ah, Clip processes two things: one is the caption of the image, and the other is the image itself. Through Clip, it can understand the context of the image. Yelp uses Clip for tasks such as when someone goes into a restaurant or pub and posts reviews or captures images of the place. For example, before applying Clip, it couldn’t identify if there were waffles in a dish; it could only identify fried chicken. After applying Clip, it can now recognize both fried chicken and waffles. Essentially, it uses Clip as part of AI to process images, captions, and convert images into vectors to compare them. So, these are the two use cases for Yelp.\n\n**55:38** These two use cases are applied in situations where you have many reviews, and you can summarize them into a highlight review. Based on the information converted into vectors, it can annotate what the images are conveying, and what they are supporting. Just give it a moment, it will highlight it for you. It understands the context of the image and can annotate it accordingly. This is the use case for how AI is used in image summarization.\n\n**56:15** Earlier this year, Yelp released the Yelp Assistant. Based on their existing platform, they were able to create a chatbot that reviews highlights like this. You simply ask, and it recommends something for you. It's as simple as that. Additionally, I noticed a use case from 2020 when the trend of short review clips started becoming popular. Yelp had a dataset specifically for that purpose. They mentioned that they are about to release something, as Tom referred to, that can convert text to speech. Based on the review dataset, they might support creating short video clips to describe a restaurant.\n\n**57:45** Based on reviews or videos posted by people, Yelp could generate a script and run it through AI to automatically create a video about a restaurant. That’s the use case. It’s simple as that. Ok, going back to the first question, this use case is essentially using AI to label data, right?\n\n**58:35** Ok, so it checks whether the comment is negative or positive, right? That’s one example. The second one is using the Clip Model, correct? It’s similar to Vision but more live, also for labeling, right? Like with Plot, labeling for images. So, these two use cases are applied for what?\n\n**59:18** I think there's an interesting point that hasn't been mentioned yet, which is the story about having a ready-made dataset. For instance, when someone leaves a review or gives a rating, based on these short clips, Yelp could generate an intro video for the restaurant. It uses AI for that. I think they use AI to write the script and then pass that script to an AI voice to narrate. But where do they get the images from? How do they get the video content?\n\n**59:57** From the review, when someone comes to review, they will have a video to review. Ok, so it's automatically generating an advertisement, right? Yes, for TikTok or similar platforms, summarizing user reviews. Sounds pretty creative. Yeah, I guess you guys have confirmed what Bảo mentioned, right?\n\n**01:00:07** Yeah, this one is about the caption, and it's mostly correct. You're right, I think the initial purpose of this use case was for recommendation. Before they had AI, they already had a hybrid recommendation model in place. Basically... I think with this new AI, they will likely replace the old model. One interesting point that wasn't mentioned much is business messaging. I think it’s based on, say, the top 50 reviews or top 50 interactions, how are the ratings, and if the reviews are good and the ratings are good, then the business messaging will also be good. But Yelp didn’t bring up this topic, and we can’t blame them for that.\n\n**01:00:51** Does anyone have any questions for Đạt? This is his first presentation. Đạt mentioned he’s working on adding more, probably for enterprise too. But I’ve seen Viettel, FPT, and VNG, and I’m still not sure how they are doing things. Đạt said some of FPT's coding tools are kind of lame.\n\n**01:01:41** Haha, is it just a continuation of a previous version? Yeah, it’s not great. It’s a bit rough and underdeveloped. Are we done with that? I guess so. Ok, Đạt. Today we’ve covered Yelp and Tech Linh, so maybe next week or the week after that, if time permits.\n\n**01:02:01** Let's do a demo real quick, Đạt. Could you demo something for us? What about a bot that can handle questions or understand short code from a developer’s perspective? Or something like an auditor checking the code quality? Could you demo the workflow or the bot you’re working on with that diff you mentioned? Please turn on the screen, Đạt.\n\n**01:03:01** So this is a project I’m working on for joining the AI Club. I’m pretty new at this, so if the project looks rough, please bear with me. The basic workflow is that I take a query and extract the URL of a repository. Here, I reused Tom’s scraper, but it didn’t fully meet my needs, so I created my own local scraper to fetch all the content from the repo. However, that takes too long and generates too much data. As of now, I haven't found a way to add it to the context unless I use knowledge retrieval. But to use knowledge retrieval, I have to prepare it in advance; I can’t select the repo directly.\n\n**01:05:45** Tom’s scraper doesn’t capture the content of the files, so I haven’t been able to draw a diagram yet. I might use it for the diagram later, I’ll test it out. This scraper only fetches the content from the root directory and some doc files. Those files might not answer Huy’s question accurately. Let me try it; I have my full setup ready offline. The content is already prepared, not online. This was generated directly, so it doesn’t have it either.\n\n**01:07:07** This scraper fetches the full content, so it’s more complete. Let me try asking questions like Huy’s or Hoàng’s. Let’s try BC chat and ask a question there. Do you have it? Ah no. Maybe the context is too large, and I haven’t figured out how to integrate it properly into the knowledge retrieval part.\n\n**01:09:19** The retrieval process is very optimized. This text file has tens of thousands of lines, tens of thousands! Let’s open it and see. Are you using a mini machine? Try switching to a more powerful machine to see if it runs better. The mini machine is a bit weak. Two million words... how is it even handling that? I think you’d need a dedicated server to run it efficiently. I’m curious how it's even running; we’re talking about 29U words, as we saw. Multiply that by 4, and the number is huge. The size... Let's follow up and see. Ok, so you’re working directly from the context, right? You’re querying like a typical query vb, rather than feeding all the data into the context.\n\n**01:10:57** Right, exactly. I’m not sure how the retrieval in this diffy system works. I don’t know if it’s retrieving the correct data or if it’s retrieving at all. I haven’t been able to trace it, but I have a tracing tool that I can test later to see how it works. But the idea is that if the retrieval works like this, it probably won’t give accurate results. You’re unsure about how much data it's running, right? It seems to only prefer 2-300 items, and that’s not enough data for these kinds of tasks. This requires at least several hundred data points. So yeah, this is still a work in progress.\n\n**01:11:35** Just joking, there’s always the AI Club at the company! Oh, so is this the full version, or is it the fixed one? If it’s fixed, demo it for us in the office, and try to leave it for me. OK, let me see. It still hasn’t built, so people are just watching for now. The build seems to have some issues, can everyone see the screen?\n\n**01:13:08** So, this week, as I mentioned last week, I will upload the sync.Map article. I think it's really useful, with details that give people using Go a general overview of maps. Regarding sync.Map, let's first go over the context. When writing maps, especially concurrent maps or concurrent operations, before version 1.16, it wouldn’t show any errors, but it wasn’t safe either. From version 1.16 onward, it throws an error like this. So to solve this issue, people usually write maps with a sync package, like using manual sync.RWMutex.\n\n**01:13:56** Besides that, there’s another option called sync.Map. Later, I’ll explain why this option exists and what its use case is. This sync.Map was created so that you don’t have to worry much about using mutexes to lock data for synchronization. You just use it. It’s as simple as this, and it’s friendly, just like using a map to check values. For example, when you load a key with a value, if it's available, it returns true; otherwise, it returns false, just like a regular map.\n\n**01:14:38** Additionally, it has several handy functions. For example, version 12.23 has `clear` to clear everything, `load` to get a value, `store` to update or store a key, and so on. Besides writing concurrently, when you range (loop) over a map, race conditions can also occur. However, with sync.Map’s range function, it handles that, so you don’t have to worry about it. It doesn’t behave like a typical range function. However, it doesn’t give you a fully consistent snapshot. When you first enter, the snapshot may not be updated. So, during this, you have to change your writing method, but at least it won’t error like this.\n\n**01:16:06** Now, let’s go over how it works. When you write and define the map, it’s structured with two maps. At this point, you might be thinking, “Wow, this sounds like it’s heavy on RAM and memory!” There’s a Read-Only map and a Dirty map. From this, you can infer that values, when written, will be updated in the Dirty map. It just keeps updating there, while the Read-Only map.\n\n**01:16:46** The Read-Only map will always be used when you're reading. Meanwhile, the writes will always be made to the Dirty map. As for the underlying flow, we’ll look at the chart in a moment to better understand it. Both of these maps have a common point: they both use a pointer called an entry. Pay attention to this part to make the flow easier to follow. For example, when you add a new entry, it will be added to the Dirty map, and both will point to this entry. This works as a flag that indicates the map has been changed. So, at this point, the Read-Only map is no longer the most up-to-date version. The system will know that the Dirty map is the one to read from.\n\n**01:17:27** This diagram shows that, for example, when you update a value, since it’s using a pointer underneath, you only need to update the pointer itself, not each value as you would in a traditional map. To achieve this, the system implements a mechanism that defines three states for the entry pointer. The first state is the **normal state**, meaning that the old values in the map are still intact and can be used, without any issues. The second state is **amended**, meaning that the entry has been modified. And the third state is the **delete state**, where an entry has been deleted from the map, but it hasn’t been completely removed. It’s still held in a transitional state, and the entry pointer is moved to a new position, but it hasn’t been fully removed yet.\n\n**01:18:59** The pointer `entry` is assigned to the `new entry`, but it hasn’t been removed yet. The `expired state` refers to complete deletion, like a hard delete, meaning the entry is completely removed from the map. To help visualize this, you can refer to this flow: for example, at the beginning, the map has a `key1` and `value1`, and at this point, the `Dirty map` has nothing, meaning nothing has been added or changed yet. Then, if you add a `key2`, it will be added to the `Dirty map`, and at this point, the map is marked as `amended` because a `flag` indicating `amended` is set here.\n\n**01:19:40** Afterward, when you delete a `key`, the map will be assigned a `new entry`, right? The same thing happens on the other side, as it is also assigned a `new entry`, similar to the previous diagram. In essence, you’re only updating the pointer without having to update the values themselves. Then, after completing the deletion, to promote the `Dirty map`, you must update it through the `Read Only map` so that the `Dirty map` returns to a `new state`, like resetting it to the original state.\n\n**01:20:18** Similarly, when adding a new `key3`, after the state has returned to the `new state`, and you add `key3`, the system identifies that the previous entry has been deleted entirely. This means that next time when it compares with the `Dirty map`, it knows that the `value1` has been deleted and no longer exists. At this point, the `Read Only map` will only contain `key2` and `key3`.\n\n**01:20:51** Due to this, `sync.Map` does not have a `len` function for users to utilize. This is because, if you used the `len` function here, it would not account for the actual values, as it would count even those that have been expired or deleted. As you can see, because `sync.Map` is structured this way, its use case is recommended for scenarios that require more reading (`read`) than writing (`write`). If you perform a lot of `write` or `delete` operations, just imagine the pointer being used continuously, and there’s even an issue reported by the Go team that this map is never garbage collected.\n\n**01:21:36** Later, the Go team confirmed that this `sync.Map` was designed primarily to support some of the internal Go Library processes. If you find it handy because of its user-friendly functions, you can use it, but for use cases that involve storing (`store`), updating, or deleting often, it’s not recommended, as it may slow down the system.\n\n**01:22:24** That’s about it. I’ve written some code based on this topic, and there’s a blogger who shares detailed posts on this subject, so you might want to follow them. Oh, what’s the topic, Phát? Show me the post again. Ah, `sync.Map`, right? `Sync.Map`. Does it differ from what you just passed in? What’s different? I think it does. The point is that I remember, depending on the community, people might have different use cases. For instance, if someone wants to implement something specific, they can adapt it as needed.\n\n**01:23:30** You can see that some people add an extra layer on top for their own use cases. For example, some want to implement generics on `sync.Map`. This is partly because of the linking issue I mentioned earlier, where the map isn’t garbage collected. That’s the problem. The Go team has already confirmed that this behavior is intentional, and they won’t fix it. They’re not going to change it, right? So now, what the community does is figure out how to work around it. They like how easy `sync.Map` is to use, with those nice functions, so they just add another layer to customize it further, making it usable for their specific needs.\n\n**01:24:07** Why are we discussing this old clip now? Oh, it’s just an insight for people to use. This use case can be applied to enterprise environments too. For example, in enterprise projects, if we use `map` and need concurrency, typically, people would write a custom `struct`, add a `mutex`, and then write everything themselves. But `sync.Map` is handy, as I showed earlier, with functions that follow certain standards. If you want to `load`, you have to call a specific function, and everything is structured that way, making it more reliable.\n\n**01:24:50** People usually write a custom `struct`, then add a `mutex`, and start writing all the necessary logic themselves. Meanwhile, `sync.Map` is really handy. As I showed you earlier, it has several functions that adhere to a specific standard. If you want to `load`, you must call a specific function. This structure ensures consistency. However, you still need to be aware of the trade-offs when using `sync.Map`, making sure to apply it correctly in the right context. Right, so you have to understand the proper workflow and `map` usage.\n\n**01:25:26** Phát: Yes, about `Map`. Ok, thanks, Phát. Now, let’s quickly get through a few things. Thành, I need about 10 more minutes from the team. It’ll take a bit more time because I’ve received a total of 11 submissions for the test. Not many have attached images, so some of you can review them. Our deadline is set for the 20th, which is next week. I think earlier, I mentioned the 27th, or was it 26th or 27th? Anyway, that's the deadline. So, please review everything this week and get the submissions ready. This test is important because the market is shifting significantly, and there’s a big change in the demand for software roles.\n\n**01:26:15** Of course, the demand is still there, but the volume has decreased. That's why I refer to it as a shift in the market demand, similar to what happened around 2014, where it’s like things are changing all over again. New technology is coming out, new opportunities, new markets, and emerging potentials. So, this test will be essential in helping us assess, especially when it comes to team culture. We’re taking this opportunity to evaluate the team, particularly to see how the research study team and the consulting team are becoming more distinct.\n\n**01:27:36** There’s a clear distinction between the two teams now. Like I mentioned in the post on Notion about two weeks ago, this distinction is becoming more pronounced. In the future, there will be more specific policies related to different benefits between these two teams. But for now, as you can see, OGIF is gradually becoming a session where we report on all the studies that the team has been reviewing and reporting back on. Some of those reports might be added later, and you can see that some team members have been picking up new knowledge and sharing it.\n\n**01:27:36** Gradually, it's becoming clearer that we want this differentiation to become more distinct over time. And there will be clear policies around this. So, those who enjoy diving deep into topics and taking them to the level of MVP, or applying them in actual projects, or going deeper into knowledge, they will get different benefits. Those who don't necessarily want to focus on study-related topics can continue working on projects as usual, but there will be other issues involved, which I've listed in the Notion link from two weeks ago. Everyone should review that link to understand the direction we're going in. This test is designed to assess how well you can keep up with new knowledge and how aligned you are with the culture during this time of significant changes.\n\n**01:29:20** Because of these goals, I’ll be the only one grading this test. None of the other team members will be grading. Everyone has to do it, and I’ve set the standard for this. So, the important thing is that everyone does the test themselves. Even if the quality isn’t the best, it’s fine. I’ll just give it a lower score, but as long as it’s completed, you’ll pass. The immediate outcome I see from this is that I’ll group the results into two clusters.\n\nThe Foundation team and the Lab team, they’re still the core teams we’ve had for the past eight or nine years. This is the main announcement. If you’ve finished your test and feel like you can improve based on what I’ve just said, the Foundation team and the Lab team will still be prioritized in various aspects. So, if you feel like you did the test carelessly, please take some time to do it thoroughly. Focus on two things: the culture aspect and the knowledge.\n\n**01:29:56** The immediate result you’ll see is that I’ll group the results into two clusters. The Foundation team and the Lab team will remain the core of our team from now until the next eight or nine years. That’s the main announcement. Out of these 11 submissions, if anyone feels they can improve after hearing what I’ve said, please focus on making it better, especially since the Foundation and Lab teams will be prioritized more in different areas. So, if you feel like you’ve done it hastily, take the time to refine it. Check two things: the culture aspect and the latest, hottest topic cluster, which right now is LLM.\n\nBut in reality, our team still covers many different areas. We still have people focusing on design, and others still working on Go, right? Blockchain might have moved a bit out of the spotlight, and the market is going sideways, but consulting still demands a lot of expertise in those areas.\n\n**01:31:46** Regarding mini apps for Telegram, they quickly clone them, and now the business logic (BL) and tech (TCH) approaches have shifted a bit from the early days. But for consulting, we can still use them as usual, or we can view them from a different angle, where they become a new asset class. As developers, we should look at how these affect our workflow and how we manage assets.\n\n**01:32:29** That’s the matter concerning the test. Pay attention to that. Second, as mentioned earlier, regarding team direction and numbers, I mentioned the Notion link I sent earlier. The Foundation team, which started over again, initially had just three people, and then gradually it grew to four, then five. We added Quan, Hiếu, and others. Initially, it was just three of us, but now the team is much stronger. With 40 people, all highly skilled, we’ll certainly move faster. That’s the general overview, so everyone should be aware of the current situation.\n\n**01:33:12** Third, Huy Nguyễn, once you’re done, next week please take a look at the ICY numbers. Earlier, you mentioned the numbers were starting to grow, so we’ll need to review and balance those out. For this task, Huy and Thành, please take charge and ensure it’s handled properly. Thành also has an additional task, which is to review benefits for the Lab team members and propose something. It could be considered as a payon, but it won’t go through the normal channels, as there will be a different mechanism for this.\n\n**01:33:52** But the Lab team members will have that, and everyone’s familiar with it. Lastly, regarding the LLM, in the current question list, there’s an important question about how to use LLM externally and how to adapt it. Emphasize that question, as it’s about knowledge discovery. The test question is not only about using it but about all the tools our team currently uses. When some people use them effectively and others less so, it creates a very clear spectrum, those who are weaker remain weaker, and those who are stronger stand out much more.\n\n**01:34:38** We want to raise the standard. We want to shorten that spectrum, to make it as compact as possible. Right now, the gap is too wide. Beyond using tools for discovery, this question also asks us to look at how our field of work can apply these tools to elevate our competencies and make us more productive. That’s the whole issue, so everyone should confirm whether what they’ve done is correct or not. It has a deeper meaning than it seems.\n\n**01:35:20** Lastly, to wrap up today’s session, Thành, for the next OGIF meetings, apart from diving deeper into use cases, there are things Tom has done related to building the structure of an LLM app. We could take that and analyze it. Let’s break it down and dive deeper into it.\n\n**01:35:56** Hopefully, everyone passes the test so we can all have a good time. Next week, there will be another test. Next week, Minh L., can you do a demo? Continue with the finite state machine, FSM. As part of our focus on foundational technologies like blockchain and AI, the key point is that engineers will have a different path forward. The goal is to understand how large systems operate. In the future, if you’re not the one handling data manipulation, AI will do that for us, we won’t need to design things ourselves or do junior-level tasks anymore.\n\n**01:37:35** The only way to become senior is to understand the issues and work on architecture. The finite state machine plays an important role, especially in scaling, something we’ve talked about a lot. Minh has read through it and understood the direction we’re aiming for. So, we need to do a comparison between the types of general servers it covers. State machine-based servers versus event-based servers. Then create a sample to show how it’s modeled, implemented using Erlang. Erlang already has the frameworks for it.\n\n**01:39:01** This topic will proceed when Minh Lưu is ready. If it’s not next week, it could be in two weeks. I suggest that the backend team and the senior team gather together, and if there’s anything, confirm it beforehand. This topic is critical for software analysis and design. It’s a very important session. Up until now, we’ve only talked about modeling and doing C4 diagrams, but Erlang is the language that goes deepest into this area. Most people don’t know it entirely. We don’t necessarily need to learn Erlang, but we can look at how they design and build systems to handle this area properly, as they already have frameworks available. We just need to plug them in and use them.\n\n**01:39:37** Speaking of which, October 20th is a Sunday, and Mỹ, Ngọc, and Giang have already posted about it. The ladies are going out on that day, and for those not in Saigon, the team representatives will wish everyone prosperity. It seems like wishing prosperity is the most appropriate thing to say in this situation. Any other wishes might not fit as well. Alright, so if anyone wants to join, register with Mỹ to book a table and plan accordingly.\n\n**01:41:19** Thành, in the upcoming meetings, structure things into talks. Then set the goal for that. Our team now has a Builder Club as well. I’ll look into how the team members who used to work on Super Bit and console are stabilizing things, and I’ll restructure afterward. This time, it seems like we’ve had a good rest. Alright, does anyone have any questions about the test? If not, we’ll wrap up here. Alright, goodbye everyone, see you next week. Thanks, Thành, and thanks to everyone.\n\n---\n","title":"OGIF Office Hours #28 - Golang sync.Map, Generative AI UX design patterns, Yelp's AI use cases, Design patterns in LLM application, and Dify github analyzer","short_title":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","description":"OGIF Office Hours #28 covered Go Weekly #16 by Phat on sync.Map concurrency, Nam's Product Commentary #4 on Generative AI UX design patterns, Dat Nguyen's presentation on Yelp's AI use cases including recommendation systems, Hoang's discussion on LLM application design patterns, and Cat's demonstration of a Dify-based Git repository analysis tool.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Mon Oct 21 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/28-20241018.md","slugArray":["updates","ogif","28-20241018"]}],"newMemos":[{"content":"\nAt [Company/Community Name], we’re obsessed with working smarter, not harder. This month, we’re tapping into the power of AI to speed up our processes—and we need YOUR ideas to make it happen! Whether it’s automating writing, deploying AI agents, or streamlining workflows, share your proposal and earn *15 icy* for every approved idea.\n\nHere’s How It Works:\n\n- **What We’re Looking For**: Creative ways to use AI to accelerate tasks—like writing, research, or operations. We want proposals, guidelines, or practices that save time and boost efficiency. (See an example below!)\n- **How to Submit**: Send your idea to [submission link/form/channel] with a brief explanation. No idea’s too small!\n- **Reward**: Earn *15 icy* per approved submission added to our AI-powered playbook. Standout ideas could score up to 25 icy!\n- **Review Process**: Our team reviews weekly and will let you know if your idea’s a go.\n- **What’s Next**: Winning ideas get implemented, and you’ll be credited community-wide!\n\nWhy Join In?\n\n- Earn *icy* tokens to [use in community perks, trade, etc.—add token value context].\n- Help us harness AI to work faster and smarter.\n- Open to everyone—bring your A(I)-game!\n\nExample Submission:\n“Use an AI writing assistant (like Grok) to draft initial posts, memos, or emails in seconds. Team members can edit the output, cutting writing time by 50%.”\n\nReady to speed things up? Submit your AI idea at [link] and start earning icy today!\n\n---\n\nproductivity is one of our study focus.\n\nintro: ref to handbook \u003e how-we-work \u003e ## Pitching ideas\n\naccepted content\n\n- write a rfc\n- build a supporting tool\n-\n\nhow to submit\n\n- show a workflow image\n\nreward\n\nList of proposal\n\nList of articles\n","title":"Productivity","short_title":"","description":"","tags":["earn","productivity"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/000-productivity.md","slugArray":["earn","000-productivity"]},{"content":"","title":"Software Quality","short_title":"","description":"","tags":["earn","quality"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/001-quality.md","slugArray":["earn","001-quality"]},{"content":"","title":"Open Source","short_title":"","description":"","tags":["earn","open-source"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/002-open-source.md","slugArray":["earn","002-open-source"]}],"teamMemos":[{"content":"\n## Record \u0026 reward sharing at Dwarves\nIf you're part of the Dwarves community, you'll notice our culture of continuous learning and knowledge sharing. This culture isn’t just about individual growth; it’s also about building a strong, connected community.\n\nWe believe learning is key to success and our goal is to create an environment where it’s prioritized, valued, and rewarded, allowing everyone to grow. \n\nTo support this, we’ve organized a variety of engaging events and activities that encourage and recognize knowledge sharing. Additionally, we allocate a portion of our profits to acknowledge these contributions with awards.\n\n## Our approach to learning and growth\nLearning goes hand in hand with a growth mindset. In our fast-moving industry, staying ahead means picking up new knowledge all the time. The more knowledge you share, the more rewards you earn. It’s that simple.\n\n## Monthly pool of up to 2500 ICY for recognized contributions\nWe’ve set aside a monthly pool of **2500 ICY (around $4000)** to reward those who contribute valuable insights. **70%** of this pool is dedicated to those who actively share knowledge across the community. \n\nAdditionally, we place extra value on contributions in key areas like **AI/LLM**, **Golang**, **Software Architecture**, and **Blockchain**. If you’ve got expertise in these fields, your insights will be especially appreciated and may earn you more ICY.\n\nCheck out the [**🧊・earn-icy**](https://discord.com/channels/462663954813157376/1006198672486309908/1239502938918096960) channel to see how we distribute our ICY and get involved.\n\n## How you can participate and make a contribution\nIf you have something worth sharing? Drop useful links in our research channels such as [**💻・tech**](https://discord.com/channels/462663954813157376/810481888619135046/1281086341995565057), [**💡・til**](https://discord.com/channels/462663954813157376/1001883339046797342/1281097209072320615) to get recognized. Got a topic in mind? Present it in our OGIFs or submit a note to our [**memo**](https://memo.d.foundation/). Community members are welcome to participate too.\n\nWe also love open-source work, especially building tools that boost our productivity. If you’re into that, join the force at [**🦄・build**](https://discord.com/channels/462663954813157376/1280726623414390805/1280791483280261161) and start cooking up something great.\n\nWe hope this push will keep our learning culture strong and moving forward.\n\nHappy coding and sharing!","title":"Record and reward sharing at Dwarves","short_title":"","description":"Discover how the Dwarves community fosters a culture of continuous learning and knowledge sharing. With a monthly reward pool of up to 2500 ICY, contributors are recognized for sharing valuable insights, especially in areas like AI/LLM, Golang, and more. Get involved and grow with us.","tags":["reward","team","community"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_","thanh","monotykamary"],"date":"Thu Sep 05 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/01_literature/record-reward-sharing-culture.md","slugArray":["playground","01_literature","record-reward-sharing-culture"]},{"content":"\n**A Junior Backend Engineer recounts her journey at Dwarves Foundation, from being attracted by their distinctive logo to finding satisfaction in remote work and learning the important lesson that seeking help from supportive team members is key to professional growth.**\n\n![Cat Nguyen - Junior Backend Engineer at Dwarves](assets/notion-image-1744012268919-ye4mt.webp)\n\nMy journey with Dwarves began with a glance at their logo during my senior year at Bach Khoa University. While browsing through tech job listings on the university's website, Dwarves' distinctive red logo caught my eye amidst a sea of white logo backgrounds. I read the JD, found it aligned with my aspirations, aced the test, sailed through interviews, and here I am.\n\nThough I've only been with Dwarves for just over a year, my job satisfaction is a solid 10/10. Thanks to remote work, I save 2-3 hours on commuting, granting me more time for fitness, knowledge updates, and quality moments with my parents.\n\nIn my year with Dwarves, I've worked on two projects: Console Labs and another client project. Starting at Console, I received big support from **Khoi** and **Tuan Dao**. Even with seemingly simple questions, they patiently explained and provided me with reading materials. I did appreciate it.\n\nAnd the most memorable experience was tackling the client project, where the difficulty level soared, delving into domains I hadn't touched before. For two weeks in a row, I worked tirelessly until 11PM to complete difficult tasks. As a newcomer, I hesitated to seek guidance initially, only turning to **Bien Vo** for the toughest queries. Then **Thanh Pham** caught up and assigned **Hieu Phan** as my mentor. Lucky me!\n\nDespite the steep learning curve, I realized a crucial lesson: when in doubt, reach out to Dwarves Team for support; their unwavering support propels both personal and professional growth.\n","title":"#22 Cat Nguyen on team support","short_title":"Cat Nguyen","description":"Cat Nguyen shares her experience as a Junior Backend Engineer at Dwarves, highlighting the supportive team culture and how asking for help accelerated her growth","tags":["life-at-dwarves, backend-engineer, teamwork"],"pinned":false,"draft":false,"hiring":false,"authors":[],"date":"2023-11-27","filePath":"careers/life/2023-11-27-22-cat-nguyen.md","slugArray":["careers","life","2023-11-27-22-cat-nguyen"]},{"content":"These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.\n\n**Negative attitude**\n\nThe employee may exhibit a negative attitude towards their work, colleagues, or the company. They may also show signs of disengagement, lack of motivation, lack of integrity or unwillingness to learn.\n\n**Incompatibility**\n\nThe employee may **not fit** in with the company culture or may have a personality clash with other team members.\n\n**Lack of professionalism**\n\nThe employee may display **unprofessional** behavior such as ⚠️ being consistently late, no show up during working hours, missing deadlines, or failing to communicate effectively.\n\n**Poor performance**\n\nThe employee may not be meeting the expected performance standards and may be **struggling to complete tasks on time**.\n\n**Low morale**\n\nThe employee's presence may be **causing low morale** in the team or even leading to a toxic work environment.\n\n**High turnover rate**\n\nIf the employee has a high turnover rate, it could be a sign of a bad hire.\n\n![](assets/red-flags_8e2d26f28c0d107f0b2dba0b99c0da5e_md5.webp)\n","title":"Red Flags","short_title":"","description":"These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.","tags":["people","teamwork","performance"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Feb 16 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playbook/operations/red-flags.md","slugArray":["playbook","operations","red-flags"]}],"changelogMemos":[{"content":"\nWe kicked off our gathering after Tết, bringing the community back together in true Dwarves style - dropping some SOL and ICY tokens into everyone's wallets, bringing the community back together in true Dwarves style. Not a bad way to start the year.\n\n![](assets/15-new-year-gathering-airdrop.png)\n\n![](assets/15-new-year-gathering-airdrop-icy.png)\n\nOn the first day back, the team flooded Discord with Tết stories and updates. Between the usual tech talk and project discussions, conversations shifted to lucky money tales, family gatherings, and the inevitable food coma from too many bánh chưng and bánh tét.\n\nSome bragged about their winning streak (or admitted their losses) in traditional New Year games, both the triumphs and the mishaps. Others shared their first sips of spring wine, late-night card games, and that one cousin who somehow always wins.\n\n![](assets/15-new-year-gathering-convo.png)\n\nPhotos and stories kept rolling in. From pristine beaches and mountain retreats to hometown reunions and family feasts, each snapshot captured a different way the team spent the break. Some took the chance to travel, exploring new places. Others returned to familiar spots, embracing the warmth of home, reconnecting with loved ones over shared meals and old traditions.\n\n![](assets/15-new-year-gathering-moments-1.png)\n\n![](assets/15-new-year-gathering-2.png)\n\nAnd of course, there were those who simply recharged ,  sleeping in, catching up on games, and enjoying the rare quiet before diving back into the grind. Now, we’re back at it, picking up where we left off. The Year of the Snake has just begun.\n","title":"Weekly Digest #15: New year Gathering: Sharing Tết, starting strong","short_title":"#15 New year gathering","description":"Tết break came to an end, and the Dwarves team reunited to share stories, reconnect, and kick off the Year of the Snake in style. We brought it all back to Discord, along with a little SOL \u0026 ICY drop to start the year right.","tags":["weekly-digest","team","community"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Tue Feb 04 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/15-new-year-gathering.md","slugArray":["updates","digest","15-new-year-gathering"]},{"content":"\nSince the start, Dwarves has valued flexibility, allowing our team to work where inspiration strikes. But there's something special about being together, learning, sharing knowledge, and working side by side.\n\nThat's where our hybrid working model steps in.  \n\nWe didn’t aim to fill desks but to create a space where you can learn, connect, and rediscover the energy that comes from working alongside others, even if it’s just for a day or two.\n\n### What makes the Dwarves office different\n\nAs a borderless software team, we know not everyone finds comfort or focus working remotely all the time. Even local cafes have their charm, but let’s be honest, it’s not always smooth sailing. That’s why our engineers have the full support they need to perform at their best.\n\n**1. Work in comfort**\n\nFrom **Apple Studio Displays** to **Herman Miller chairs**, we’ve set up workspaces adapt to you. Whether you need a quiet corner or an open area for brainstorming, you’ll find a spot that suits your style.\n\n**2. Productivity perks**\n\nDistractions? Not here. With high-speed internet, tranquil meeting rooms, subsidized meals, and 24/7 access, you have everything you need to stay in the zone.\n\n**3. A supportive environment that fuels growth**\n\nEvery interaction, every conversation, is a chance to pick up something new or share what you know.\n\n![](assets/14-back-to-the-office-team.png)\n\n### Why our office check-in is worth it\n\nWe’ve added a little extra motivation to make coming into the office more rewarding. Every time you check in at **🏢・lobby**, you’ll earn **5 ICY** tokens as part of our daily team perks. It’s our way of encouraging you to take advantage of everything the office offers and to make every visit count.\n\n- **Simple and quick:** Check in, earn your ICY. Kick-off your day with a little boost.\n- **Earn while you work:** Whether you're here for a focused work or to catch up with teammates. You’re rewarded just for showing up.\n- **Stay connected:** These perks are little a reminder that we’re building something together, day by day.\n\n![](assets/14-back-to-the-office-checkin.png)\n\n### Shaping a place for real learning\n\nWhy leave your home office? What makes this place worth the trip? It’s not about fancy setups. Team members who hadn't visited in a while began dropping by, finding new ways to connect.\n\nWhen @tom picks up a new skill, he shares it with everyone in person. It was him, standing by a desk, showing others in real time. These face-to-face exchanges spread knowledge throughout the team effortlessly. We didn’t force people back; they started coming in because something changed.\n\nYou see, there are things you can’t capture in a digital thread. A quick tip, a shared screen, a face that lights up when they finally get it. That’s learning.\n\nWe didn’t aim for a typical office with rigid desks. Instead, we created a space where reaching out feels natural, and where every conversation might teach you something new, where knowledge isn’t just passed around, it’s lived and experienced.\n\n![](assets/14-back-to-the-office-teamwork.png)\n\n### Real voices: What our team says\n\nDon’t just take our word for it, here’s what some of our team members have to say about coming back to the office:\n\n- “The quick chats turn into real learning moments. In an environment where mentors and seniors are always learning, newbies feel encouraged to do the same. It’s all rooted in Dwarves' mentorship culture.” - @datnguyen\n- \"Working from the office helps me stay focused, and it's easy to reach out when I need a hand. It feels good to have that balance.\" – @vincent\n- \"I appreciate the mix of working remotely and coming in. The face-to-face chats and shared meals make it feel more connected. It truly embodies the spirit of engineers.\" – @nhuthuynh\n- \"It’s not just about working harder; it’s about working smarter. I’ve shared a lot of knowledge by bouncing ideas off my teammates\" – @tom\n- \"Being at the office a couple of days a week has made it easier to separate work and home life. Plus, it's good to see everyone now and then.\" – @nam\n\n### Where work meets growth\n\nThough this is still in the early stages, some of us have already reached the hub and made it their go-to spot for getting into the zone. We’d be glad to have you as one of them.\n\nAlong with everything, we do everything we can to level up our team. In the end, it’s not just about work; it’s about how we grow, together.\n","title":"Weekly Digest #14: Embracing hybrid work - the best of both worlds","short_title":"#14 Hybrid work harmony","description":"Discover how Dwarves embraces hybrid working, blending flexibility with in-person connections. Learn how our office space fosters productivity, learning, and real collaboration, even if it’s just for a day or two a week.","tags":["weekly-digest","team","hybrid-working"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Wed Sep 25 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/14-back-to-the-office.md","slugArray":["updates","digest","14-back-to-the-office"]},{"content":"\nWelcome to this week's edition of our roundup, where we highlight the moments that make us more than just lines of code. This week, we're jet-setting around the world with our team's summer adventures, unveiling new memo features, and cheering for a beloved teammate's new chapter in the US. From beachside workspaces to anime figurine collections, we've got it all.\n\n### Memo submission and boosting team contributions\n\nIn the upcoming phase, the team will focus on streamlining processes and developing tools to make memo submission easier for everyone. Here are two ways we're helping you stay up to speed:\n\n- Use commands like `?memo pr` and `?memo list` to check out the latest memos. (Shoutout to @ohagi for this!)\n- Receive webhook notifications in the Discord server for new PRs, speeding up memo operations.\n\nWe're also introducing tooling for submitting fleet notes directly on Discord and automating the distribution of ICY to everyone. However, the amount of ICY distributed is still below the team's limit, so there's plenty of room for more contributions. Everyone can join hands.\n\n![](assets/13-more-than-lines-of-code-icy-updates.webp)\n\n![](assets/13-more-than-lines-of-code-memo.webp)\n\n### Summer snapshots: where our remote team works\n\nThis summer, our team's \"Summer Times, Where Are You Working From?\" updates took us on a virtual journey around the world. We saw it all: Swiss mountains, Vietnamese beaches, killer nail art, desks so neat they spark joy, anime havens, and even our COO chilling beachside (while \"working,\" of course).\n\nThese snapshots of our team's lives, filled with personal flair, with a mix of personalities and passions that make our team so rad, no matter where we're working from.\n\n[Check out the full recap of our team's summer escapades.](https://memo.d.foundation/updates/digest/12-where-are-you-working-from-this-summer/)\n\n![](assets/13-more-than-lines-of-code-summer.webp)\n\n![](assets/13-more-than-lines-of-code-summer-moments.webp)\n\n### A fond farewell and best wishes to our teammate Hieu Phan\n\nAs you all saw in the lobby last Friday, we had our final team dinner with @hieuthu1 before his move to the US. The memories we made together in Vietnam will always stay with us, even as he embarks on his new journey under American skies.\n\nHe was always a caring presence at Hado, a fantastic cook, and a friend who joined us on afternoon walks after work, always connecting everyone on the team. We wish you all the best on your new adventure and don't forget to call us.\n\n![](assets/13-more-than-lines-of-code-farewell.webp)\n\n![](assets/13-more-than-lines-of-code-farewell.png)\n\n### Celebrating the newborn\n\nCongratulations are in order for @thanh, who recently welcomed a new addition to his family. The whole team is overjoyed for him and his partner as they embark on this exciting new chapter. There's been plenty of well-wishing and baby-related chatter around the team.  \n\nOf course, with such happy news, the playful banter has already begun: who's going to be the next to share a baby announcement? We'll have to wait and see.\n\nIn the meantime, we're sending Thanh and his family all the best as they navigate the joys (and sleepless nights!) of parenthood.\n\n![](assets/13-more-than-lines-of-code_13-more-than-line-code-new-born.webp)\n\nFrom tech tips to tiny toes, this week's digest reminds us that we're more than just a team. So, whether you're sharing a memo, a vacation snapshot, or a life-changing announcement, remember: that we're here to celebrate it with you.  \n","title":"Weekly Digest #13: More than lines of code","short_title":"#13 More than lines of code","description":"Welcome to this week's edition of our roundup, where we highlight the moments that make us more than just lines of code. This week, we're jet-setting around the world with our team's summer adventures, introducing new memo features, cheering for a beloved teammate's new chapter in the US, and celebrating new born. From beachside workspaces to anime figurine collections, we've got it all.","tags":["memo","team","remote","weekly-digest"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sat Jul 20 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/13-more-than-lines-of-code.md","slugArray":["updates","digest","13-more-than-lines-of-code"]}],"hiringMemos":[]},"__N_SSG":true},"page":"/","query":{},"buildId":"uBOsB0SlboeWG4_OQtqeb","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>