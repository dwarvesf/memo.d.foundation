<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Dwarves Memo - Home</title><meta property="title" content="Dwarves Memo - Home" data-next-head=""/><meta property="og:title" content="Dwarves Memo - Home" data-next-head=""/><meta name="description" content="Knowledge sharing platform for Dwarves Foundation" data-next-head=""/><meta property="og:description" content="Knowledge sharing platform for Dwarves Foundation" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:site_name" content="Dwarves Memo" data-next-head=""/><link rel="icon" type="image/x-icon" href="{{ $favicon.Permalink }}" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><link rel="alternate" type="application/rss+xml" title="Dwarves Memo - Home - RSS Feed" href="/feed.xml" data-next-head=""/><link rel="alternate" type="application/atom+xml" title="Dwarves Memo - Home - Atom Feed" href="/atom.xml" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com" data-next-head=""/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/ae718064f8e72aa8.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=Public+Sans:ital,wght@0,100..900;1,100..900&amp;family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&amp;display=swap" rel="stylesheet" data-next-head=""/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><link rel="stylesheet" href="/_next/static/css/ae718064f8e72aa8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8134a28a6715c612.js" defer=""></script><script src="/_next/static/chunks/framework-e0f92c31faeb46ff.js" defer=""></script><script src="/_next/static/chunks/main-32230c4a3df1fe38.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b5faca5fa691a273.js" defer=""></script><script src="/_next/static/chunks/09244f9f-613169846494a3db.js" defer=""></script><script src="/_next/static/chunks/973-126675c95a554240.js" defer=""></script><script src="/_next/static/chunks/827-2584a42247c79fb9.js" defer=""></script><script src="/_next/static/chunks/pages/index-3583017ce87d421b.js" defer=""></script><script src="/_next/static/bGFyqUhs6KXRgmgPoIo-F/_buildManifest.js" defer=""></script><script src="/_next/static/bGFyqUhs6KXRgmgPoIo-F/_ssgManifest.js" defer=""></script></head><body class="min-h-screen antialiased"><script>
              (function() {
                // Get saved theme or default to system preference
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const savedTheme = localStorage.getItem('theme');
                
                // Default to system preference if no saved preference
                const theme = (savedTheme === 'light' || savedTheme === 'dark') 
                  ? savedTheme 
                  : (prefersDark ? 'dark' : 'light');
                
                // Apply theme
                if (theme === 'dark') {
                  document.documentElement.classList.add('dark');
                  document.documentElement.setAttribute('data-theme', 'dark');
                } else {
                  document.documentElement.classList.remove('dark');
                  document.documentElement.setAttribute('data-theme', 'light');
                }
              })();
            </script><link rel="preload" as="image" href="/assets/home_cover.webp"/><div id="__next"><div class="bg-background border-border w-sidebar-mobile xl:w-sidebar fixed top-0 left-0 z-40 flex h-full flex-col border-r pt-4 pb-12 font-sans transition-transform duration-300 ease-in-out translate-x-[-100%] xl:translate-x-0 "><a class="mx-4 flex h-10 items-center gap-2 px-2 xl:mx-0 xl:justify-center" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6.25 w-6 min-w-6"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="inline-block font-sans text-xs leading-tight font-bold tracking-tight uppercase xl:hidden">Dwarves<br/>Memo</span></a><nav class="flex flex-1 flex-col p-4 xl:items-center xl:px-2"><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start text-primary" id="sidebar-item-0" data-state="closed" data-slot="tooltip-trigger" href="/"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/home.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/home.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Home</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-1" data-state="closed" data-slot="tooltip-trigger" href="/consulting"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/consulting.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/consulting.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Consulting</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-2" data-state="closed" data-slot="tooltip-trigger" href="/earn"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/earn.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/earn.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Earn</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-3" data-state="closed" data-slot="tooltip-trigger" href="/careers/hiring"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/hiring.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/hiring.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Hiring</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-4" data-state="closed" data-slot="tooltip-trigger" href="/updates/digest"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/digest.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/digest.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">Digest</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-5" data-state="closed" data-slot="tooltip-trigger" href="/updates/ogif"><div class="p-2"><div class="h-6 w-6" style="mask-image:url(&#x27;/assets/img/ogifs.svg&#x27;);-webkit-mask-image:url(&#x27;/assets/img/ogifs.svg&#x27;);background-color:currentColor;mask-repeat:no-repeat;-webkit-mask-repeat:no-repeat;mask-size:contain;-webkit-mask-size:contain;mask-position:center;-webkit-mask-position:center"></div></div><span class="ml-3 inline-block xl:hidden">OGIFs</span></a></nav><div class="mx-4 border-t pt-1 xl:mx-2"><div class="flex items-center justify-between gap-3 p-2 xl:justify-center"><button class="flex cursor-pointer items-center justify-center hover:opacity-80"><svg viewBox="0 0 20 20" width="24" height="24"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></button><span class="inline-block flex-1 shrink-0 text-sm leading-6 font-medium xl:hidden">Night mode</span><button class="bg-border flex h-5 w-9 cursor-pointer items-center justify-center rounded-full py-0.5 pr-4.5 pl-0.5 hover:opacity-95 xl:hidden"><div class="text-foreground-light rounded-full bg-white p-0.5"><svg viewBox="0 0 20 20" width="12" height="12"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></div></button></div></div></div><div class="bg-background text-foreground relative flex h-screen font-sans transition-colors "><div id="sidebar" class="bg-background-secondary xl:w-directory-width h-[calc(100svh-32px)] w-0 flex-col pt-10 pb-2 text-sm leading-normal xl:pr-3 xl:ml-sidebar translate-0 transition duration-100 ease-in-out z-2 overflow-y-auto reading:opacity-0 reading:translate-x-[-10%] reading:pr-0 reading:w-0 reading:ml-0"><div class=""><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/pinned"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Pinned Notes</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium text-muted-foreground pl-2" href="/playbook/operations/ogif"><span>OGIF - Oh God It&#x27;s Friday</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Home</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/careers"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Careers</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/consulting"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Consulting</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/earn"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Earn</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/fund"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Fund</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/handbook"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Handbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/opensource"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Opensource</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playbook"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playground"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playground</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/rfc"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>RFC</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/updates"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Updates</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/tags"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Popular Tags</span></a></div></div></div><div class="relative flex flex-1 flex-col overflow-y-auto"><header class="bg-background/95 supports-[backdrop-filter]:bg-background/60 top-0 w-full shrink-0 font-sans backdrop-blur"><div class="mx-auto flex h-full items-center justify-between border-b p-2 xl:border-none xl:px-5"><div class="flex items-center gap-2.5"><button id="sidebar-toggle" aria-label="Toggle sidebar" class="flex h-10 w-10 cursor-pointer items-center justify-center focus:outline-none xl:hidden"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="text-current"><path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><a class="flex items-center gap-2 xl:hidden" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-[32px] w-[29px] min-w-6 shrink-0"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="font-ibm-sans text-xs text-[11px] leading-[14.849px] font-bold -tracking-[0.157px] uppercase">Dwarves<br/>Memo</span></a></div><div class="ml-auto flex items-center gap-3.5"><div class="command-palette relative z-50"><button class="hidden w-50 cursor-pointer justify-between rounded-md border bg-transparent px-3 py-1.5 transition-all duration-100 ease-in-out hover:shadow-md md:flex" aria-label="Open command palette"><div class="flex items-center gap-0.5"><span class="text-muted-foreground text-sm filter-[opacity(50%)]">🔍 Search...</span></div><div class="text-muted-foreground flex items-center gap-0.5 text-xs"><kbd class="dark:bg-border rounded bg-[#f9fafb] px-1.5 py-0.5">⌘</kbd><kbd class="dark:bg-border rounded bg-[#f9fafb] px-1.5 py-0.5">K</kbd></div></button><button class="text-foreground flex h-10 w-10 items-center justify-center border-none bg-transparent p-0 md:hidden" aria-label="Open search"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 16 16" fill="none" class="text-foreground" aria-hidden="true"><circle cx="6.88881" cy="6.8889" r="5.55556" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></circle><path d="M11.3333 11.3333L14.6666 14.6667" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><button class="hidden cursor-pointer items-center justify-center border-0 bg-transparent outline-none hover:opacity-95 active:opacity-100 xl:flex" aria-label="Toggle reading mode" data-reading-mode="false" data-state="closed" data-slot="tooltip-trigger"><svg width="48" height="28" viewBox="0 0 62 34" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-7 w-12 xl:w-14"><g><rect width="62" height="34" rx="17" class="fill-border dark:fill-border"></rect><g class="transition-transform duration-150 ease-in-out translate-x-0"><circle cx="17" cy="17" r="14" class="fill-white"></circle><path d="M17 23.898V18.3265C17 17.9747 17.1398 17.6373 17.3885 17.3885C17.6373 17.1398 17.9747 17 18.3265 17C18.6783 17 19.0158 17.1398 19.2645 17.3885C19.5133 17.6373 19.6531 17.9747 19.6531 18.3265V21.2449H21.7755C22.3384 21.2449 22.8782 21.4685 23.2763 21.8666C23.6744 22.2646 23.898 22.8045 23.898 23.3673V23.898" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2119 12.8561C14.8891 11.4004 13.114 10.4334 11.1736 10.1113C11.0416 10.0926 10.9071 10.1022 10.7791 10.1395C10.6511 10.1768 10.5324 10.2409 10.4311 10.3275C10.3279 10.4158 10.245 10.5253 10.1883 10.6487C10.1315 10.772 10.1021 10.9062 10.1021 11.0419V18.6088C10.1007 18.8411 10.1854 19.0658 10.3399 19.2394C10.4944 19.413 10.7077 19.5232 10.9386 19.5487C12.4542 19.7543 13.8794 20.354 15.0774 21.276" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2124 15.7885V12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M21.4852 19.5487C21.7161 19.5232 21.9295 19.413 22.084 19.2394C22.2385 19.0658 22.3232 18.8411 22.3218 18.6088V11.0419C22.3218 10.9062 22.2924 10.772 22.2356 10.6487C22.1788 10.5253 22.096 10.4158 21.9928 10.3275C21.8915 10.2409 21.7728 10.1768 21.6447 10.1395C21.5167 10.1022 21.3823 10.0926 21.2502 10.1113C19.3098 10.4334 17.5347 11.4004 16.2119 12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path></g></g></svg></button></div></div></header><div class="main-grid relative w-full flex-1 flex-col"><div class="right-sidebar leading-[140% xl:w-right-sidebar-width hidden font-sans text-sm font-medium xl:flex transition-[transform,opacity,visibility] duration-100 ease-in-out visible w-0 translate-x-0 transform opacity-100 reading:opacity-0 reading:xl:translate-x-[10px] reading:invisible reading:fixed reading:right-[calc((100vw-var(--container-max-width)-var(--nav-sidebar-width))/2-var(--right-sidebar-width)-var(--column-gap))]"><div class="sticky top-[60px] right-0 flex w-full flex-col gap-y-8 pt-4 pb-10 transition-[top] duration-200 ease-in-out"></div></div><main class="main-content mx-auto max-w-[var(--container-max-width)] min-w-0 flex-1 p-[var(--main-padding-mobile)] font-serif xl:p-[var(--main-padding)]"><img alt="" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" class="yggdrasil-tree no-zoom pointer-events-none object-contain opacity-[0.03] md:w-[20vw] xl:w-[20vw] dark:opacity-100 absolute bottom-8 left-1/2 w-[50vw] max-w-xs -translate-x-1/2 xl:translate-x-[80%]" style="color:transparent" src="/assets/img/footer-bg.svg"/><div class="memo-content pb-8"><div class="font-serif"><img src="/assets/home_cover.webp" class="no-zoom max-h-[500px] rounded-sm"/><p class="mt-[var(--element-margin)]">Welcome to the Dwarves Memo.</p><p class="mt-[var(--element-margin)]">This site is a part of our continuous learning engine, where we want to build up the 1% improvement habit, learning in public.</p><p class="mt-[var(--element-margin)]">Written by Dwarves for product craftsmen.</p><p class="mt-[var(--element-margin)]">Learned by engineers. Experimented by engineers.</p><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">💡 OGIFs</h2><div id="changelog" class="link-v-list mt-2.5 flex flex-col gap-1.5" data-placement="vertical"><div id="memo-1" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/41-20250314">#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing</a></div><div id="memo-2" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/39-20250207">##39 Frontend report, DB Scaling, AI Workflow</a></div><div id="memo-3" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/37-20241227">##37 AI Fine-tuning, Data archiving, Datalakes</a></div><div id="memo-4" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/28-20241018">#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis</a></div><div id="memo-5" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline decoration-neutral-100" href="/updates/ogif/27-20241011">#27 Go weekly, Frontend, AI UX, Finite Automata</a></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">✨ New memos</h2><div class="v-list" data-placement="vertical"><div id="memo-1" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Productivity" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/000-productivity">Productivity</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao">tieubao</a></div></div></div><div id="memo-2" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Software Quality" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/001-quality">Software Quality</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao">tieubao</a></div></div></div><div id="memo-3" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b"><div class="v-list-item-image xs:w-3/10 xs:pb-3 xs:pr-6 pt-3 pb-0"><img class="no-zoom h-[130px] w-full rounded-sm object-cover" src="/assets/home_cover.webp" alt="Cover image for Open Source" height="130" loading="lazy"/></div><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/earn/002-open-source">Open Source</a><div class="text-foreground line-clamp-2 text-sm font-normal"></div><div class="dark:text-secondary-light font-ibm-sans text-sm font-normal"><a class="text-secondary-foreground hover:text-primary underline" href="/contributor/tieubao">tieubao</a></div></div></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">🧑‍💻 Life at Dwarves</h2><div class="v-list" data-placement="vertical"><div id="memo-1" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/playground/01_literature/record-reward-sharing-culture">Record and reward sharing at Dwarves</a><div class="text-foreground line-clamp-2 text-sm font-normal">Discover how the Dwarves community fosters a culture of continuous learning and knowledge sharing. With a monthly reward pool of up to 2500 ICY, contributors are recognized for sharing valuable insights, especially in areas like AI/LLM, Golang, and more. Get involved and grow with us.</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">September 05, 2024</div></div></div><div id="memo-2" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/careers/life/2023-11-27-22-cat-nguyen">Cat Nguyen</a><div class="text-foreground line-clamp-2 text-sm font-normal">Cat Nguyen shares her experience as a Junior Backend Engineer at Dwarves, highlighting the supportive team culture and how asking for help accelerated her growth</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">November 27, 2023</div></div></div><div id="memo-3" class="v-list-item xs:flex-row flex w-full flex-col not-last:border-b no-image first:*:pt-0"><div class="flex flex-1 flex-col gap-1 py-3"><a class="hover:text-primary hover:decoration-primary mt-0 text-[17px] font-semibold underline decoration-neutral-100" href="/playbook/operations/red-flags">Red Flags</a><div class="text-foreground line-clamp-2 text-sm font-normal">These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.</div><div class="text-secondary-foreground dark:text-secondary-light font-ibm-sans mt-1 text-sm font-normal">February 16, 2023</div></div></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">📝 Changelog</h2><div id="changelog" class="link-v-list mt-2.5 flex flex-col gap-1.5" data-placement="vertical"><div id="memo-1" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/15-new-year-gathering">#15 New year gathering</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->February 04, 2025</span></div><div id="memo-2" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/14-back-to-the-office">#14 Hybrid work harmony</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->September 25, 2024</span></div><div id="memo-3" class="link-v-list-item"><a class="link-v-list-item-title hover:text-primary hover:decoration-primary inline text-[17px] underline text-primary decoration-primary" href="/updates/digest/13-more-than-lines-of-code">#13 More than lines of code</a><span class="link-v-list-item-time text-secondary-foreground dark:text-secondary-light font-ibm-sans ml-1 inline text-sm font-normal"> - <!-- -->July 20, 2024</span></div></div><h2 class="-track-[0.0125] mt-8 mb-2.5 text-[26px] leading-[140%] font-semibold">🤝 Open positions</h2><div class="v-list" data-placement="vertical"></div><div class="font-sans"><h2 class="font-ibm-sans mt-6 text-[10px] font-medium uppercase">Love what we are doing?</h2><ul class="xs:grid-cols-2 mt-2.5 grid list-none gap-2.5 pl-0"><li><a href="https://discord.gg/dfoundation" class="text-primary text-sm">🩷 Join our Discord Network →</a></li><li><a href="https://github.com/dwarvesf/playground" class="text-primary text-sm">🔥 Contribute to our Memo →</a></li><li><a href="https://careers.d.foundation/" class="text-primary text-sm">🤝 Join us, we are hiring →</a></li><li><a href="http://memo.d.foundation/earn/" class="text-primary text-sm">🙋 Give us a helping hand →</a></li></ul></div></div></div></main><div class="toc-space"></div></div></div><footer class="border-t-border bg-background fixed right-0 bottom-0 left-0 z-40 flex h-8 items-stretch overflow-hidden border-t px-3 py-0 text-[0.875rem] leading-[140%] font-normal tracking-[-0.0125rem]"><div class="socials flex items-center gap-x-[10px] pr-3"><a href="https://github.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Plus/Github"><path id="Vector" d="M9 1.5C8.01509 1.5 7.03982 1.69399 6.12987 2.0709C5.21993 2.44781 4.39314 3.00026 3.6967 3.6967C2.29018 5.10322 1.5 7.01088 1.5 9C1.5 12.315 3.6525 15.1275 6.63 16.125C7.005 16.185 7.125 15.9525 7.125 15.75V14.4825C5.0475 14.9325 4.605 13.4775 4.605 13.4775C4.26 12.6075 3.7725 12.375 3.7725 12.375C3.09 11.91 3.825 11.925 3.825 11.925C4.575 11.9775 4.9725 12.6975 4.9725 12.6975C5.625 13.8375 6.7275 13.5 7.155 13.32C7.2225 12.8325 7.4175 12.5025 7.6275 12.315C5.9625 12.1275 4.215 11.4825 4.215 8.625C4.215 7.7925 4.5 7.125 4.9875 6.5925C4.9125 6.405 4.65 5.625 5.0625 4.6125C5.0625 4.6125 5.6925 4.41 7.125 5.3775C7.7175 5.2125 8.3625 5.13 9 5.13C9.6375 5.13 10.2825 5.2125 10.875 5.3775C12.3075 4.41 12.9375 4.6125 12.9375 4.6125C13.35 5.625 13.0875 6.405 13.0125 6.5925C13.5 7.125 13.785 7.7925 13.785 8.625C13.785 11.49 12.03 12.12 10.3575 12.3075C10.6275 12.54 10.875 12.9975 10.875 13.695V15.75C10.875 15.9525 10.995 16.1925 11.3775 16.125C14.355 15.12 16.5 12.315 16.5 9C16.5 8.01509 16.306 7.03982 15.9291 6.12987C15.5522 5.21993 14.9997 4.39314 14.3033 3.6967C13.6069 3.00026 12.7801 2.44781 11.8701 2.0709C10.9602 1.69399 9.98491 1.5 9 1.5Z" fill="#9B9B9B"></path></g></svg></a><a href="https://discord.gg/dfoundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Discord"><path id="Union" d="M14.1919 3.95003C13.2419 3.50716 12.2133 3.18572 11.1418 3C11.1324 2.9997 11.1231 3.00146 11.1144 3.00517C11.1058 3.00887 11.0981 3.01442 11.0918 3.02143C10.9632 3.25715 10.8132 3.5643 10.7132 3.80002C9.57677 3.62859 8.42102 3.62859 7.28456 3.80002C7.18456 3.55716 7.03455 3.25715 6.89884 3.02143C6.89169 3.00715 6.87026 3 6.84883 3C5.77738 3.18572 4.75592 3.50716 3.79875 3.95003C3.79161 3.95003 3.78447 3.95717 3.77732 3.96431C1.83441 6.87154 1.29868 9.70019 1.56298 12.5003C1.56298 12.5145 1.57012 12.5288 1.58441 12.536C2.87015 13.4789 4.1059 14.0503 5.32737 14.4289C5.34879 14.436 5.37022 14.4289 5.37737 14.4146C5.66309 14.0217 5.92024 13.6074 6.14167 13.1717C6.15596 13.1431 6.14167 13.1146 6.1131 13.1074C5.70595 12.9503 5.32022 12.7646 4.94164 12.5503C4.91307 12.536 4.91307 12.4931 4.9345 12.4717C5.01307 12.4145 5.09164 12.3503 5.17022 12.2931C5.1845 12.2788 5.20593 12.2788 5.22022 12.286C7.67743 13.4074 10.3275 13.4074 12.7561 12.286C12.7704 12.2788 12.7919 12.2788 12.8061 12.2931C12.8847 12.3574 12.9633 12.4145 13.0419 12.4788C13.0704 12.5003 13.0704 12.5431 13.0347 12.5574C12.6633 12.7788 12.2704 12.9574 11.8633 13.1146C11.8347 13.1217 11.8275 13.1574 11.8347 13.1789C12.0633 13.6146 12.3204 14.0289 12.599 14.4217C12.6204 14.4289 12.6419 14.436 12.6633 14.4289C13.8919 14.0503 15.1276 13.4789 16.4134 12.536C16.4277 12.5288 16.4348 12.5145 16.4348 12.5003C16.7491 9.26446 15.9134 6.45724 14.2205 3.96431C14.2133 3.95717 14.2062 3.95003 14.1919 3.95003ZM6.51311 10.7931C5.77738 10.7931 5.16307 10.1145 5.16307 9.27875C5.16307 8.44301 5.76309 7.76442 6.51311 7.76442C7.27028 7.76442 7.87029 8.45015 7.86315 9.27875C7.86315 10.1145 7.26313 10.7931 6.51311 10.7931ZM11.4918 10.7931C10.7561 10.7931 10.1418 10.1145 10.1418 9.27875C10.1418 8.44301 10.7418 7.76442 11.4918 7.76442C12.249 7.76442 12.849 8.45015 12.8419 9.27875C12.8419 10.1145 12.249 10.7931 11.4918 10.7931Z" fill="#9B9B9B"></path></g></svg></a><a href="https://www.facebook.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55 0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95"></path></svg></a><a href="https://dwarves.foundation/" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M17.9 17.39c-.26-.8-1.01-1.39-1.9-1.39h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41a7.984 7.984 0 0 1 2.9 12.8M11 19.93c-3.95-.49-7-3.85-7-7.93c0-.62.08-1.22.21-1.79L9 15v1a2 2 0 0 0 2 2m1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10a10 10 0 0 0 10-10A10 10 0 0 0 12 2"></path></svg></a><a href="mailto:team@dwarves.foundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 36 36"><path fill="#9b9b9b" d="M32.33 6a2 2 0 0 0-.41 0h-28a2 2 0 0 0-.53.08l14.45 14.39Z" class="clr-i-solid clr-i-solid-path-1"></path><path fill="#9b9b9b" d="m33.81 7.39l-14.56 14.5a2 2 0 0 1-2.82 0L2 7.5a2 2 0 0 0-.07.5v20a2 2 0 0 0 2 2h28a2 2 0 0 0 2-2V8a2 2 0 0 0-.12-.61M5.3 28H3.91v-1.43l7.27-7.21l1.41 1.41Zm26.61 0h-1.4l-7.29-7.23l1.41-1.41l7.27 7.21Z" class="clr-i-solid clr-i-solid-path-2"></path><path fill="none" d="M0 0h36v36H0z"></path></svg></a></div><div class="authors !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Dwarves Foundation</span></div><div class="filename !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Memo</span></div><div class="last-updated hidden items-center px-3 text-[#9b9b9b]"><span class="text-[var(--secondary-font-color-light-2)]">© 2025</span></div></footer></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"directoryTree":{"/pinned":{"label":"Pinned Notes","children":{"/playbook/operations/ogif":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/":{"label":"Home","children":{"/earn":{"label":"Earn","children":{"/earn/000-productivity":{"label":"productivity","children":{}},"/earn/001-quality":{"label":"software quality","children":{}},"/earn/002-open-source":{"label":"open source","children":{}},"/earn/003-liquidity":{"label":"liquidity","children":{}},"/earn/readme":{"label":"👾 open bounties","children":{}}}},"/consulting":{"label":"Consulting","children":{"/consulting/case-study":{"label":"Case Study","children":{"/consulting/case-study/screenz-ai":{"label":"screenz.ai","children":{}},"/consulting/case-study/kafi":{"label":"kafi","children":{}},"/consulting/case-study/droppii":{"label":"droppii","children":{}},"/consulting/case-study/konvoy":{"label":"konvoy","children":{}},"/consulting/case-study/cimb":{"label":"cimb","children":{}},"/consulting/case-study/swift":{"label":"swift","children":{}},"/consulting/case-study/startupvn":{"label":"startupvn","children":{}},"/consulting/case-study/open-fabric":{"label":"open fabric","children":{}},"/consulting/case-study/icrosschain":{"label":"icrosschain","children":{}},"/consulting/case-study/hedge-foundation":{"label":"hedge foundation","children":{}},"/consulting/case-study/searchio":{"label":"search.io","children":{}},"/consulting/case-study/tokenomy":{"label":"tokenomy","children":{}},"/consulting/case-study/basehq":{"label":"basehq","children":{}},"/consulting/case-study/momos":{"label":"momos","children":{}},"/consulting/case-study/attrace":{"label":"attrace","children":{}},"/consulting/case-study/setel":{"label":"setel","children":{}},"/consulting/case-study/joinpara":{"label":"joinpara","children":{}},"/consulting/case-study/relay":{"label":"relay","children":{}},"/consulting/case-study/naru":{"label":"naru","children":{}},"/consulting/case-study/mudah":{"label":"mudah","children":{}},"/consulting/case-study/reapit":{"label":"reapit","children":{}},"/consulting/case-study/aharooms":{"label":"aharooms","children":{}},"/consulting/case-study/begroup":{"label":"begroup","children":{}},"/consulting/case-study/airwatt":{"label":"airwatt","children":{}},"/consulting/case-study/voconic":{"label":"voconic","children":{}},"/consulting/case-study/sol":{"label":"sol","children":{}},"/consulting/case-study/dental-marketplace":{"label":"dental marketplace","children":{}},"/consulting/case-study/bhd":{"label":"bhd cinema","children":{}}}},"/consulting/partners-network":{"label":"partners network","children":{}},"/consulting/readme":{"label":"💼 consulting team","children":{}}}},"/handbook":{"label":"Handbook","children":{"/handbook/navigate-changes":{"label":"navigate changes","children":{}},"/handbook/community":{"label":"Community","children":{"/handbook/community/icy-worth":{"label":"how much is your icy worth","children":{}},"/handbook/community/icy-swap":{"label":"how to swap icy to btc","children":{}},"/handbook/community/icy":{"label":"icy","children":{}},"/handbook/community/discord":{"label":"discord","children":{}},"/handbook/community/earn":{"label":"earn","children":{}},"/handbook/community/radar":{"label":"radar","children":{}},"/handbook/community/sharing":{"label":"sharing knowledge","children":{}},"/handbook/community/showcase":{"label":"showcase","children":{}},"/handbook/community/memo":{"label":"memo","children":{}}}},"/handbook/guides":{"label":"Guides","children":{"/handbook/guides/check-in-at-office":{"label":"office check-in process for earning icy","children":{}},"/handbook/guides/leave-request":{"label":"leave request","children":{}},"/handbook/guides/nda":{"label":"NDA \u0026 agreements","children":{}},"/handbook/guides/configure-the-company-email":{"label":"configure your company email","children":{}},"/handbook/guides/one-on-one-meeting":{"label":"1-on-1 meetings","children":{}},"/handbook/guides/continuing-education-allowance":{"label":"continuing education allowance","children":{}},"/handbook/guides/reimbursement":{"label":"reimbursement","children":{}},"/handbook/guides/email-communication-and-use":{"label":"email use","children":{}},"/handbook/guides/password-sharing":{"label":"password sharing","children":{}},"/handbook/guides/asset-request":{"label":"request an asset","children":{}},"/handbook/guides/effective-meeting":{"label":"effective meetings","children":{}},"/handbook/guides/conduct-a-meeting":{"label":"how to conduct a meeting","children":{}}}},"/handbook/making-a-career":{"label":"making a career","children":{}},"/handbook/as-a-community":{"label":"as a community","children":{}},"/handbook/knowledge-base":{"label":"knowledge base","children":{}},"/handbook/stock-option-plan":{"label":"stock option plan","children":{}},"/handbook/readme":{"label":"📔 handbook","children":{}},"/handbook/compliance":{"label":"compliance","children":{}},"/handbook/mma":{"label":"mma","children":{}},"/handbook/hybrid-working":{"label":"hybrid working","children":{}},"/handbook/routine":{"label":"work routine","children":{}},"/handbook/ventures":{"label":"ventures arm","children":{}},"/handbook/purpose":{"label":"purpose","children":{}},"/handbook/benefits-and-perks":{"label":"benefits \u0026 perks","children":{}},"/handbook/dwarves-foundation-is-you":{"label":"you are dwarves foundation","children":{}},"/handbook/getting-started":{"label":"💎 getting started","children":{}},"/handbook/how-we-hire":{"label":"how we hire","children":{}},"/handbook/how-we-spend-money":{"label":"how we spend money","children":{}},"/handbook/misc":{"label":"Misc","children":{"/handbook/misc/marketing-assets":{"label":"marketing assets","children":{}}}},"/handbook/moonlighting":{"label":"moonlighting","children":{}},"/handbook/places-to-work":{"label":"places to work","children":{}},"/handbook/security-rules":{"label":"security rules","children":{}},"/handbook/tools-and-systems":{"label":"tools and systems","children":{}},"/handbook/what-we-stand-for":{"label":"what we stand for","children":{}},"/handbook/what-we-value":{"label":"what we value","children":{}},"/handbook/where-we-work":{"label":"where we work","children":{}},"/handbook/who-does-what":{"label":"who does what","children":{}},"/handbook/faq":{"label":"faq","children":{}},"/handbook/how-we-work":{"label":"how we work","children":{}}}},"/playground":{"label":"Playground","children":{"/playground/01_literature":{"label":"01_literature","children":{"/playground/01_literature/evolutionary-database-design":{"label":"evolutionary database design: managing change and scaling with the system","children":{}},"/playground/01_literature/design":{"label":"Design","children":{"/playground/01_literature/design/product-design-commentary-20241122":{"label":"product design commentary #7: hyper-personalization - how AI improves user experience personalization","children":{}},"/playground/01_literature/design/product-design-commentary-20241115":{"label":"product design commentary #6: AI in design - cool ideas and how to make them happen","children":{}},"/playground/01_literature/design/product-design-commentary-20241101":{"label":"product design commentary #5: figma to swiftui (functional code) with claude AI","children":{}},"/playground/01_literature/design/product-design-commentary-20241018":{"label":"product design commentary #4: generative AI UX design patterns","children":{}},"/playground/01_literature/design/product-design-commentary-20241011":{"label":"product design commentary #3: the art of prompting in ai-human interaction","children":{}},"/playground/01_literature/design/product-design-commentary-20241004":{"label":"product design commentary #2: unpacking the sparkles icon and AI onboarding challenges","children":{}},"/playground/01_literature/design/product-design-commentary-20240927":{"label":"product design commentary #1: new technologies changing ux/ui and product design","children":{}}}},"/playground/01_literature/giving-a-talk-checklist":{"label":"giving a talk","children":{}},"/playground/01_literature/database-design-circular":{"label":"database design circular","children":{}},"/playground/01_literature/a-lens-to-modern-data-engineering":{"label":"a lens to modern data engineering","children":{}},"/playground/01_literature/security":{"label":"Security","children":{"/playground/01_literature/security/a-holistic-guide-to-security":{"label":"a holistic guide to security","children":{}},"/playground/01_literature/security/how-i-came-up-with-our-security-standard":{"label":"how i came up with our security standard","children":{}}}},"/playground/01_literature/record-reward-sharing-culture":{"label":"record and reward sharing at dwarves","children":{}},"/playground/01_literature/designing-for-forgiveness":{"label":"designing for forgiveness: creating error-tolerant interfaces","children":{}},"/playground/01_literature/design-file-sharing-system-part-2-permission-and-password":{"label":"design file-sharing system - part 2: permission \u0026 password","children":{}},"/playground/01_literature/designing-a-model-with-dynamic-properties":{"label":"designing a model with dynamic properties","children":{}},"/playground/01_literature/hybrid-search":{"label":"evaluating search engine in RAG systems","children":{}},"/playground/01_literature/design-file-sharing-system-part-1-directory-structure":{"label":"design file-sharing system - part 1: directory structure","children":{}},"/playground/01_literature/using-foundry-for-evm-smart-contract-developement":{"label":"using foundry for evm smart contract development","children":{}},"/playground/01_literature/creating-a-fully-local-search-engine-on-memo":{"label":"building a local search engine for our memo website","children":{}},"/playground/01_literature/observer-pattern":{"label":"introduce the observer pattern and its use cases","children":{}},"/playground/01_literature/visitor-design-pattern":{"label":"visitor design pattern, the concept, problem solution and use cases","children":{}},"/playground/01_literature/strategy-design-pattern":{"label":"strategy design pattern, the concept, use cases and difference with the state design pattern","children":{}},"/playground/01_literature/vietnam-tech-ecosystem-report":{"label":"vietnam tech ecosystem 2024 report","children":{}},"/playground/01_literature/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing":{"label":"how we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","children":{}},"/playground/01_literature/feedback-mechanism":{"label":"design feedback mechanism for LLM applications","children":{}},"/playground/01_literature/local-first-software":{"label":"local-first software","children":{}},"/playground/01_literature/error-handling-in-rust":{"label":"error handling on rust","children":{}},"/playground/01_literature/engineering":{"label":"Engineering","children":{"/playground/01_literature/engineering/backend":{"label":"Backend","children":{"/playground/01_literature/engineering/backend/bloom-filter":{"label":"bloom filter","children":{}},"/playground/01_literature/engineering/backend/introduction-to-crdt":{"label":"introduction to crdt","children":{}},"/playground/01_literature/engineering/backend/sql-sargable-queries-and-their-impact-on-database-performance":{"label":"sql saragable queries and their impact on database performance","children":{}},"/playground/01_literature/engineering/backend/the-removal-of-apache-kafkas-dependency-on-zookeeper":{"label":"the removal of apache kafka's dependency on zookeeper","children":{}},"/playground/01_literature/engineering/backend/sql-and-how-it-relates-to-disk-reads-and-writes":{"label":"sql and how it relates to disk reads and writes","children":{}}}},"/playground/01_literature/engineering/data":{"label":"Data","children":{"/playground/01_literature/engineering/data/data-pipeline-design-framework":{"label":"data pipeline design framework","children":{}},"/playground/01_literature/engineering/data/quick-learning-vector-database":{"label":"quick learning vector database","children":{}},"/playground/01_literature/engineering/data/mapreduce":{"label":"mapreduce","children":{}}}},"/playground/01_literature/engineering/google-data-fusion":{"label":"google data fusion","children":{}},"/playground/01_literature/engineering/google-dataproc":{"label":"google dataproc","children":{}},"/playground/01_literature/engineering/introducing-htmx-navigating-the-advantages-and-concerns":{"label":"introducing htmx - navigating the advantages and concerns","children":{}},"/playground/01_literature/engineering/typesafe-client-server":{"label":"typesafe client server","children":{}},"/playground/01_literature/engineering/url-redirect-vs-rewrite":{"label":"url redirect vs. rewrite; what’s the difference?","children":{}}}},"/playground/01_literature/template-method-design-pattern":{"label":"a tour of template method pattern with golang","children":{}},"/playground/01_literature/command-pattern":{"label":"command pattern","children":{}},"/playground/01_literature/radix-sort":{"label":"radix sort","children":{}},"/playground/01_literature/state-pattern":{"label":"state pattern","children":{}},"/playground/01_literature/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana":{"label":"dynamic liquidity market maker - a new form of concentrated liquidity amm on solana","children":{}},"/playground/01_literature/memo-knowledge-base-meeting":{"label":"memo knowledge base meeting","children":{}},"/playground/01_literature/peep-nft":{"label":"claim your peeps nft","children":{}},"/playground/01_literature/recording-flow":{"label":"how we set up a recording workflow for dwarves office hours","children":{}},"/playground/01_literature/memo-publication-workflow":{"label":"memo publication workflow","children":{}},"/playground/01_literature/history-of-structured-output-for-llms":{"label":"history of structured outputs for llms","children":{}},"/playground/01_literature/builder-design-pattern":{"label":"introduce the builder pattern and its use cases","children":{}},"/playground/01_literature/how-to-make-a-moc":{"label":"how to make a moc","children":{}},"/playground/01_literature/prototype-design-pattern":{"label":"going through use cases of the prototype design pattern and it place among the creational patterns","children":{}},"/playground/01_literature/singleton-design-pattern":{"label":"a tour of singleton design pattern with golang","children":{}},"/playground/01_literature/echelon-x-singapore-2024-where-innovations-meet-inspiration":{"label":"echelon x singapore 2024: where innovations meet inspiration","children":{}},"/playground/01_literature/c4-modelling":{"label":"breaking down complexity: the role of abstractions and uml in c4 modelling","children":{}},"/playground/01_literature/dollar-cost-averaging":{"label":"dollar cost averaging (dca)","children":{}},"/playground/01_literature/how-i-create-content-for-multiple-platforms-at-dwarves":{"label":"how i create content for multiple platforms at dwarves","children":{}},"/playground/01_literature/understanding-saving-investing-and-speculating-key-differences-and-strategies":{"label":"understanding saving, investing, and speculating: key differences and strategies","children":{}},"/playground/01_literature/writing-content-for-multimedia-guidelines":{"label":"writing content for multimedia guidelines","children":{}},"/playground/01_literature/how-to-earn-reward-from-staking-dfg":{"label":"how to earn reward from staking dfg","children":{}},"/playground/01_literature/how-to-transfer-dfg-from-eth-to-base-for-staking":{"label":"how to bridge $dfg from ethereum mainnet to base network for staking","children":{}},"/playground/01_literature/design-less-present-more-with-deckset":{"label":"design less, present more with deckset","children":{}},"/playground/01_literature/level-up-your-markdown-memos":{"label":"level up your markdown memos: avoiding common pitfalls","children":{}},"/playground/01_literature/tech-canvas":{"label":"tech canvas","children":{}},"/playground/01_literature/how-to-recap-a-publication":{"label":"recapping a publication","children":{}},"/playground/01_literature/lifecycle-of-a-publication":{"label":"life cycle of a publication","children":{}},"/playground/01_literature/how-to-set-up-environment-for-editing-memo":{"label":"how to set up environment to edit memo","children":{}},"/playground/01_literature/_how-to-setup-crypto-wallet-to-withdraw-icy":{"label":"how to set up crypto wallet to withdraw icy","children":{}},"/playground/01_literature/_how-to-withdraw-icy":{"label":"how to withdraw icy","children":{}},"/playground/01_literature/how-to-take-better-screenshots-on-mac":{"label":"how to take better screenshots on mac","children":{}},"/playground/01_literature/how-to-push-content-on-note-d":{"label":"how to push content on memo.d.foundation","children":{}},"/playground/01_literature/labs-weekly-catchup-5":{"label":"labs weekly catchup #5","children":{}},"/playground/01_literature/labs-weekly-catchup-4":{"label":"labs weekly catchup #4","children":{}},"/playground/01_literature/labs-weekly-catchup-3":{"label":"labs weekly catchup #3","children":{}},"/playground/01_literature/labs-weekly-catchup-2":{"label":"labs weekly catchup #2","children":{}},"/playground/01_literature/labs-weekly-catchup-1":{"label":"labs weekly catchup #1","children":{}},"/playground/01_literature/labs-who-we-are":{"label":"labs - who we are","children":{}},"/playground/01_literature/readme":{"label":"dwarves memo","children":{}},"/playground/01_literature/duckdb-demo-and-showcase":{"label":"duckdb demo and showcase","children":{}},"/playground/01_literature/salary-advance":{"label":"$icy salary advance","children":{}},"/playground/01_literature/how-rd-contributes-to-performance-review":{"label":"how r\u0026d contributes to performance review","children":{}},"/playground/01_literature/knowledge-journey":{"label":"knowledge journey","children":{}},"/playground/01_literature/labs-new-member-onboarding":{"label":"labs - new member onboarding","children":{}},"/playground/01_literature/labs-roadmap-nov-23-update":{"label":"labs roadmap (nov 23 update)","children":{}},"/playground/01_literature/labs-topic-proposal-progress-tracking":{"label":"labs - topic proposal \u0026 progress tracking","children":{}},"/playground/01_literature/labs-x-consulting-workflow":{"label":"labs x consulting workflow","children":{}},"/playground/01_literature/reward-model-nomination":{"label":"reward model \u0026 nomination","children":{}},"/playground/01_literature/our-view-on-fullstack-engineering":{"label":"our view on fullstack engineering","children":{}},"/playground/01_literature/adoption-of-pnpm":{"label":"adoption of pnpm","children":{}},"/playground/01_literature/working-on-a-project-interview-assessment-at-dwarves":{"label":"working on a project interview assessment at dwarves","children":{}},"/playground/01_literature/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt":{"label":"how we created an AI powered interview system using openais chatgpt","children":{}},"/playground/01_literature/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"easy prompt engineering for business use and mitigating risks in llms","children":{}},"/playground/01_literature/exploring-machine-learning-approaches-for-fine-tuning-llama-models":{"label":"exploring machine learning approaches for fine tuning llama models","children":{}},"/playground/01_literature/managing-dataflow-and-sql-database-with-concurrency-control":{"label":"managing dataflow and sql database with concurrency control","children":{}},"/playground/01_literature/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue":{"label":"choosing the right javascript framework a deep dive into react vs angular vs vue","children":{}},"/playground/01_literature/design-system-for-layer-2-using-zk-rollup":{"label":"design system for layer 2 using zk rollup","children":{}},"/playground/01_literature/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation":{"label":"lessons learned from being a part of corporate micro frontend implementation","children":{}},"/playground/01_literature/cost-of-react-native":{"label":"cost of react native","children":{}},"/playground/01_literature/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"lessons learned from concurrency practices in blockchain projects","children":{}},"/playground/01_literature/database-designs-for-multilingual-apps":{"label":"database designs for multilingual apps","children":{}},"/playground/01_literature/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"accelerate project initiation with advanced nextjs boilerplate react toolkit","children":{}},"/playground/01_literature/how-blue-green-deployment-helped-mochi":{"label":"how blue green deployment helped mochi","children":{}},"/playground/01_literature/i18n-frontend-guideline":{"label":"i18n frontend guideline","children":{}},"/playground/01_literature/radio-talk-61-monorepo":{"label":"radio talk 61 monorepo","children":{}},"/playground/01_literature/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo":{"label":"from multi repo to monorepo a case study with nghenhan turbo monorepo","children":{}},"/playground/01_literature/radio-talk-60-blue-green-deployment":{"label":"radio talk 60 blue green deployment","children":{}},"/playground/01_literature/growth-is-our-universal-language":{"label":"growth is our universal language","children":{}},"/playground/01_literature/the-key-of-security-mechanisms-in-tackling-cyber-threats":{"label":"the key of security mechanisms in tackling cyber threats","children":{}},"/playground/01_literature/responsibility":{"label":"responsibility","children":{}},"/playground/01_literature/configure-the-company-email":{"label":"configure the company email","children":{}},"/playground/01_literature/tech-event-in-the-latest-transforming-healthcare-with-technology":{"label":"tech event in the latest transforming healthcare with technology","children":{}},"/playground/01_literature/data-analyst-in-retail-trading":{"label":"data analyst in retail trading","children":{}},"/playground/01_literature/passing-the-probation-get-3-upvotes":{"label":"passing the probation get 3 upvotes","children":{}},"/playground/01_literature/react-native-new-architecture":{"label":"react native new architecture","children":{}},"/playground/01_literature/writing":{"label":"Writing","children":{"/playground/01_literature/writing/state-explain-link":{"label":"state, explain, link - an all-purpose writing technique","children":{}}}},"/playground/01_literature/dwarves-radio-talk-17-conduct-a-1-1-session":{"label":"dwarves radio talk 17 conduct a 1 1 session","children":{}},"/playground/01_literature/dwarves-radio-talk-16-run-an-effective-performance-review":{"label":"dwarves radio talk 16 run an effective performance review","children":{}},"/playground/01_literature/understanding-an-application-design":{"label":"understanding an application design","children":{}},"/playground/01_literature/sql-practices-orm-vs-plain-sql":{"label":"sql practices orm vs plain sql","children":{}},"/playground/01_literature/what-i-learned-on-design-thinking-and-software-development":{"label":"what i learned on design thinking and software development","children":{}},"/playground/01_literature/six-things-i-extracted-from-design-thinking":{"label":"six things i extracted from design thinking","children":{}},"/playground/01_literature/gitflow-pull-request":{"label":"gitflow pull request","children":{}},"/playground/01_literature/git-commit-message-convention":{"label":"git commit message convention","children":{}},"/playground/01_literature/are-we-really-engineers":{"label":"are we really engineers","children":{}},"/playground/01_literature/how-we-setup-cicd":{"label":"how we setup cicd","children":{}},"/playground/01_literature/getting-started-with-webflow":{"label":"getting started with webflow","children":{}},"/playground/01_literature/ui-design-best-practices-dwarves":{"label":"UI design best practices dwarves","children":{}},"/playground/01_literature/xpc-services-on-macos-app-using-swift":{"label":"xpc services on macos app using swift","children":{}},"/playground/01_literature/the-correct-way-to-build-kpi":{"label":"the correct way to build kpi","children":{}},"/playground/01_literature/domain-insight-research-framework":{"label":"domain insight research framework","children":{}},"/playground/01_literature/asking-as-a-junior":{"label":"asking as a junior","children":{}},"/playground/01_literature/infinite-image-gallery-with-r3f-an-approach":{"label":"infinite image gallery with r3f an approach","children":{}},"/playground/01_literature/market":{"label":"Market","children":{"/playground/01_literature/market/an-overview-of-micro-investment-in-real-estate":{"label":"an overview of micro investment in real estate","children":{}}}},"/playground/01_literature/grid-and-layout":{"label":"grid and layout","children":{}},"/playground/01_literature/startups-vs-junior-designers":{"label":"startups vs junior designers","children":{}},"/playground/01_literature/gestalt-principles-in-ui-design":{"label":"gestalt principles in UI design","children":{}},"/playground/01_literature/aarrr-framework-in-a-nutshell":{"label":"AARRR framework in a nutshell","children":{}},"/playground/01_literature/a-quick-intro-to-webassembly":{"label":"a quick intro to webassembly","children":{}},"/playground/01_literature/sdk-event-sourcing":{"label":"sdk event sourcing","children":{}},"/playground/01_literature/software-development-life-cycle-101":{"label":"software development life cycle 101","children":{}},"/playground/01_literature/introduce-to-dwarves-memo":{"label":"introduce to dwarves memo","children":{}},"/playground/01_literature/daemons-and-services-programming-guide":{"label":"daemons and services programming guide","children":{}},"/playground/01_literature/remote-moderated-usability-testing":{"label":"remote moderated usability testing","children":{}},"/playground/01_literature/an-alternative-to-tm":{"label":"an alternative to tm","children":{}},"/playground/01_literature/how-a-design-system-work":{"label":"how a design system work","children":{}},"/playground/01_literature/software-modeling":{"label":"software modeling","children":{}},"/playground/01_literature/reusability-in-software-development":{"label":"reusability in software development","children":{}},"/playground/01_literature/blockchain-for-designers":{"label":"blockchain for designers","children":{}},"/playground/01_literature/design-better-mobile-application":{"label":"design better mobile application","children":{}},"/playground/01_literature/introduction-to-software-craftsmanship":{"label":"introduction to software craftsmanship","children":{}},"/playground/01_literature/domain-glossary":{"label":"domain glossary","children":{}},"/playground/01_literature/architecture-decision-record":{"label":"architecture decision record","children":{}},"/playground/01_literature/build-an-assistant-on-the-terminal":{"label":"build an assistant on the terminal","children":{}},"/playground/01_literature/create-circular-text-using-swiftui":{"label":"create circular text using swiftui","children":{}},"/playground/01_literature/draw-watch-face-using-swiftui":{"label":"draw watch face using swiftui","children":{}},"/playground/01_literature/applied-security-basis":{"label":"applied security basis","children":{}},"/playground/01_literature/swiftui":{"label":"swiftui","children":{}},"/playground/01_literature/bunk-license-check":{"label":"bunk license check","children":{}},"/playground/01_literature/well-crafted-software":{"label":"well crafted software","children":{}},"/playground/01_literature/objective":{"label":"objective","children":{}},"/playground/01_literature/project-management":{"label":"project management","children":{}},"/playground/01_literature/kubernetes-helm-101":{"label":"kubernetes helm 101","children":{}},"/playground/01_literature/what-is-kubernetes":{"label":"what is kubernetes","children":{}},"/playground/01_literature/traits-to-assess-during-an-interview":{"label":"traits to assess during an interview","children":{}},"/playground/01_literature/recursively-export-file-pattern-in-javascript-es6-application":{"label":"recursively export file pattern in javascript es6 application","children":{}},"/playground/01_literature/playaround-with-clojure":{"label":"playaround with clojure","children":{}},"/playground/01_literature/playaround-with-rust":{"label":"playaround with rust","children":{}},"/playground/01_literature/overview-on-broker-pattern-in-distributed-system":{"label":"overview on broker pattern in distributed system","children":{}},"/playground/01_literature/fundamental-end-to-end-frontend-testing-with-cypress":{"label":"fundamental end to end frontend testing with cypress","children":{}},"/playground/01_literature/uidynamicanimator":{"label":"uidynamicanimator","children":{}},"/playground/01_literature/reproduce-apple-find-me-bottom-menu-view":{"label":"reproduce apple find me bottom menu view","children":{}},"/playground/01_literature/build-a-passcode-view-with-swift":{"label":"build a passcode view with swift","children":{}},"/playground/01_literature/istio":{"label":"istio","children":{}},"/playground/01_literature/different-ways-to-test-react-application":{"label":"different ways to test react application","children":{}},"/playground/01_literature/federated-byzantine":{"label":"federated byzantine","children":{}},"/playground/01_literature/fabric-hyperledger-architecture-explanation":{"label":"fabric hyperledger architecture explanation","children":{}},"/playground/01_literature/setup-react-project-with-webpack-and-babel":{"label":"setup react project with webpack and babel","children":{}},"/playground/01_literature/split-and-reuse-code-in-react-application":{"label":"split and reuse code in react application","children":{}},"/playground/01_literature/hoc-renderprops-and-hook-in-reactjs":{"label":"hoc renderprops and hook in reactjs","children":{}},"/playground/01_literature/resource-assignment":{"label":"resource assignment","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-2":{"label":"the principle of spacing in UI design part 2","children":{}},"/playground/01_literature/finite-state-machine":{"label":"finite state machine","children":{}},"/playground/01_literature/card-sorting-and-a-glimpse-at-experimental-sorting-session":{"label":"card sorting and a glimpse at experimental sorting session","children":{}},"/playground/01_literature/about-devops":{"label":"about devops","children":{}},"/playground/01_literature/our-daily-standup-format":{"label":"our daily standup format","children":{}},"/playground/01_literature/good-design-understanding":{"label":"good design understanding","children":{}},"/playground/01_literature/competency-mapping":{"label":"competency mapping","children":{}},"/playground/01_literature/design-resourcestools":{"label":"design resourcestools","children":{}},"/playground/01_literature/design-tips-tricks":{"label":"design tips tricks","children":{}},"/playground/01_literature/design-system":{"label":"design system","children":{}},"/playground/01_literature/design-workflow":{"label":"design workflow","children":{}},"/playground/01_literature/three-levels-of-design":{"label":"three levels of design","children":{}},"/playground/01_literature/ui-design-fundamental":{"label":"UI design fundamental","children":{}},"/playground/01_literature/ux-model":{"label":"UX model","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-1":{"label":"the principle of spacing in UI design part 1","children":{}},"/playground/01_literature/be-careful-with-your-code-splitting-setup":{"label":"be careful with your code splitting setup","children":{}},"/playground/01_literature/qc-onboarding":{"label":"qc onboarding","children":{}},"/playground/01_literature/dcos-series-part-5-gitlab":{"label":"dcos series part 5 gitlab","children":{}},"/playground/01_literature/dcos-series-part-4-deploy-simple-application-with-backend-database":{"label":"dcos series part 4 deploy simple application with backend database","children":{}},"/playground/01_literature/dcos-series-part-3-service-discovery-and-load-balancing":{"label":"dcos series part 3 service discovery and load balancing","children":{}},"/playground/01_literature/dcos-series-part-2-deploy-simple-applications":{"label":"dcos series part 2 deploy simple applications","children":{}},"/playground/01_literature/dcos-series-part-1-quick-look-installation":{"label":"dcos series part 1 quick look installation","children":{}},"/playground/01_literature/skill-of-software-engineer":{"label":"skill of software engineer","children":{}},"/playground/01_literature/docker-registry":{"label":"docker registry","children":{}},"/playground/01_literature/agile-using-clickup-as-agile-management-tool":{"label":"agile using clickup as agile management tool","children":{}},"/playground/01_literature/agile-how-to-create-clickup-tickets":{"label":"agile how to create clickup tickets","children":{}},"/playground/01_literature/considering-factors-for-performance-evaluating":{"label":"considering factors for performance evaluating","children":{}},"/playground/01_literature/how-we-contribute-to-homebrew":{"label":"how we contribute to homebrew","children":{}},"/playground/01_literature/the-10x-engineer":{"label":"the 10x engineer","children":{}},"/playground/01_literature/definition-of-done":{"label":"definition of done","children":{}},"/playground/01_literature/estimation-in-agile":{"label":"estimation in agile","children":{}},"/playground/01_literature/sprint-lifecycle":{"label":"sprint lifecycle","children":{}},"/playground/01_literature/remote-prepare-and-get-going":{"label":"remote prepare and get going","children":{}},"/playground/01_literature/docker-microcontainers":{"label":"docker microcontainers","children":{}}}},"/playground/00_fleeting":{"label":"00_fleeting","children":{"/playground/00_fleeting/automata":{"label":"automata","children":{}},"/playground/00_fleeting/error-handling-patterns":{"label":"error handling patterns","children":{}},"/playground/00_fleeting/founder-liquidity":{"label":"founder liquidity","children":{}},"/playground/00_fleeting/why-hollywood-and-gaming-struggle-with-ai":{"label":"why hollywood and gaming struggle with AI","children":{}},"/playground/00_fleeting/subscription-pricing-models":{"label":"subscription pricing models","children":{}},"/playground/00_fleeting/erlang-fsm":{"label":"erlang finite state machine","children":{}},"/playground/00_fleeting/rust-trait":{"label":"rust trait","children":{}},"/playground/00_fleeting/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy":{"label":"explaining gradient descent in machine learning with a simple analogy","children":{}},"/playground/00_fleeting/organize-team-know-how-with-zettelkasten-method":{"label":"organize team know-how with zettelkasten method","children":{}},"/playground/00_fleeting/how-to-talk-to-chatgpt-effectively":{"label":"how to talk to chatgpt effectively","children":{}},"/playground/00_fleeting/icy-in-2024":{"label":"$icy in 2024","children":{}},"/playground/00_fleeting/icy-dfg":{"label":"💠 df protocol, $icy and $dfg","children":{}},"/playground/00_fleeting/202302281019-case-study-write-heavy-scalable-and-reliable-inventory-platform":{"label":"case study: write-heavy scalable and reliable inventory platform","children":{}},"/playground/00_fleeting/202301191192-multi-column-index-in-db":{"label":"multi-column index in db","children":{}},"/playground/00_fleeting/202301091379-invoking-component-functions-in-react":{"label":"invoking component functions in react","children":{}},"/playground/00_fleeting/202212131609-how-to-deal-with-technical-debt-in-scrum":{"label":"how to deal with technical debt in scrum","children":{}},"/playground/00_fleeting/202211141287-go-json-parsing":{"label":"go json parser: number \u003c-\u003e interface","children":{}},"/playground/00_fleeting/202211141513-materialized-view-pattern":{"label":"materialized view pattern","children":{}},"/playground/00_fleeting/202211081111-error-messaging":{"label":"error messaging","children":{}},"/playground/00_fleeting/202210172128-sign-in-form-best-practices":{"label":"sign-in form best practices","children":{}},"/playground/00_fleeting/202210162154-the-best-of-css-tldr":{"label":"the best of css tldr","children":{}},"/playground/00_fleeting/202210150019-migration-planning":{"label":"migration planning","children":{}},"/playground/00_fleeting/202210131000-behavior-driven-development":{"label":"behavior driven development","children":{}},"/playground/00_fleeting/202210131516-react-fiber":{"label":"react fiber","children":{}},"/playground/00_fleeting/202210122014-forward-proxy":{"label":"forward proxy","children":{}}}},"/playground/_radar":{"label":"_radar","children":{"/playground/_radar/readme":{"label":"tech radar","children":{}},"/playground/_radar/codecept":{"label":"codecept","children":{}},"/playground/_radar/apache-spark":{"label":"apache spark","children":{}},"/playground/_radar/ant-design":{"label":"ant design","children":{}},"/playground/_radar/apache-kafka":{"label":"apache kafka","children":{}},"/playground/_radar/argocd":{"label":"argocd","children":{}},"/playground/_radar/astro":{"label":"astro","children":{}},"/playground/_radar/backstage":{"label":"backstage","children":{}},"/playground/_radar/blue-green-deployment":{"label":"blue green deployment","children":{}},"/playground/_radar/browserstack":{"label":"browserstack","children":{}},"/playground/_radar/carbon":{"label":"carbon","children":{}},"/playground/_radar/chatgpt-assistance":{"label":"chatgpt assistance","children":{}},"/playground/_radar/chromatic":{"label":"chromatic","children":{}},"/playground/_radar/clickhouse":{"label":"clickhouse","children":{}},"/playground/_radar/cloudflare-workers":{"label":"cloudflare workers","children":{}},"/playground/_radar/commitlint":{"label":"commitlint","children":{}},"/playground/_radar/copilot":{"label":"copilot","children":{}},"/playground/_radar/cucumber":{"label":"cucumber","children":{}},"/playground/_radar/cypress":{"label":"cypress","children":{}},"/playground/_radar/dapr":{"label":"dapr","children":{}},"/playground/_radar/deno":{"label":"deno","children":{}},"/playground/_radar/detox":{"label":"detox","children":{}},"/playground/_radar/devcontainers":{"label":"devcontainers","children":{}},"/playground/_radar/devpod":{"label":"devpod","children":{}},"/playground/_radar/dora-metrics":{"label":"dora metrics","children":{}},"/playground/_radar/duckdb":{"label":"duckdb","children":{}},"/playground/_radar/earthly":{"label":"earthly","children":{}},"/playground/_radar/elixir-umbrella-project":{"label":"elixir umbrella project","children":{}},"/playground/_radar/elixir":{"label":"elixir","children":{}},"/playground/_radar/erlang":{"label":"erlang","children":{}},"/playground/_radar/error-logging-convention":{"label":"error logging convention","children":{}},"/playground/_radar/eslint":{"label":"eslint","children":{}},"/playground/_radar/event-sourcing":{"label":"event sourcing","children":{}},"/playground/_radar/excalidraw":{"label":"excalidraw","children":{}},"/playground/_radar/expo":{"label":"expo","children":{}},"/playground/_radar/figma":{"label":"figma","children":{}},"/playground/_radar/formal-verification":{"label":"formal verification","children":{}},"/playground/_radar/fullstack-tracing":{"label":"fullstack tracing","children":{}},"/playground/_radar/gestalt-principle":{"label":"gestalt principle","children":{}},"/playground/_radar/github-actions":{"label":"github actions","children":{}},"/playground/_radar/golang":{"label":"golang","children":{}},"/playground/_radar/grafana":{"label":"grafana","children":{}},"/playground/_radar/graylog":{"label":"graylog","children":{}},"/playground/_radar/headless-ui":{"label":"headless UI","children":{}},"/playground/_radar/hoppscotch":{"label":"hoppscotch","children":{}},"/playground/_radar/ipfs":{"label":"ipfs","children":{}},"/playground/_radar/jotai":{"label":"jotai","children":{}},"/playground/_radar/k6":{"label":"k6","children":{}},"/playground/_radar/k9s":{"label":"k9s","children":{}},"/playground/_radar/kaniko":{"label":"kaniko","children":{}},"/playground/_radar/kotlin":{"label":"kotlin","children":{}},"/playground/_radar/kubeseal-sops":{"label":"kubeseal sops","children":{}},"/playground/_radar/ladle":{"label":"ladle","children":{}},"/playground/_radar/langchain":{"label":"langchain","children":{}},"/playground/_radar/large-language-model-llm":{"label":"large language model LLM","children":{}},"/playground/_radar/loki":{"label":"loki","children":{}},"/playground/_radar/makefile":{"label":"makefile","children":{}},"/playground/_radar/micro-frontend":{"label":"micro frontend","children":{}},"/playground/_radar/monorepo":{"label":"monorepo","children":{}},"/playground/_radar/msw":{"label":"msw","children":{}},"/playground/_radar/n6n":{"label":"n6n","children":{}},"/playground/_radar/nestjs":{"label":"nestjs","children":{}},"/playground/_radar/netlify":{"label":"netlify","children":{}},"/playground/_radar/newrelic":{"label":"newrelic","children":{}},"/playground/_radar/nextjs":{"label":"nextjs","children":{}},"/playground/_radar/nodejs":{"label":"nodejs","children":{}},"/playground/_radar/nostrum":{"label":"nostrum","children":{}},"/playground/_radar/nx":{"label":"nx","children":{}},"/playground/_radar/orval":{"label":"orval","children":{}},"/playground/_radar/page-object-model":{"label":"page object model","children":{}},"/playground/_radar/partytown":{"label":"partytown","children":{}},"/playground/_radar/phaser":{"label":"phaser","children":{}},"/playground/_radar/phoenix":{"label":"phoenix","children":{}},"/playground/_radar/playwright":{"label":"playwright","children":{}},"/playground/_radar/pnpm":{"label":"pnpm","children":{}},"/playground/_radar/progressive-delivery":{"label":"progressive delivery","children":{}},"/playground/_radar/prometheus":{"label":"prometheus","children":{}},"/playground/_radar/prompt-engineering":{"label":"prompt engineering","children":{}},"/playground/_radar/qwik":{"label":"qwik","children":{}},"/playground/_radar/radix-ui":{"label":"radix UI","children":{}},"/playground/_radar/react-hook-form":{"label":"react hook form","children":{}},"/playground/_radar/react-llm":{"label":"react LLM","children":{}},"/playground/_radar/react-native":{"label":"react native","children":{}},"/playground/_radar/react-query":{"label":"react query","children":{}},"/playground/_radar/react-server-component":{"label":"react server component","children":{}},"/playground/_radar/react-testing-library":{"label":"react testing library","children":{}},"/playground/_radar/react":{"label":"react","children":{}},"/playground/_radar/reinforcement-learning-from-human-feedback":{"label":"reinforcement learning from human feedback","children":{}},"/playground/_radar/remix":{"label":"remix","children":{}},"/playground/_radar/replayio":{"label":"replayio","children":{}},"/playground/_radar/reverse-engineering":{"label":"reverse engineering","children":{}},"/playground/_radar/rust":{"label":"rust","children":{}},"/playground/_radar/selenium":{"label":"selenium","children":{}},"/playground/_radar/semantic-release-auto-release":{"label":"semantic release auto release","children":{}},"/playground/_radar/sentry":{"label":"sentry","children":{}},"/playground/_radar/serverlessq":{"label":"serverlessq","children":{}},"/playground/_radar/solidity":{"label":"solidity","children":{}},"/playground/_radar/solidjs":{"label":"solidjs","children":{}},"/playground/_radar/stern":{"label":"stern","children":{}},"/playground/_radar/svelte":{"label":"svelte","children":{}},"/playground/_radar/swagger":{"label":"swagger","children":{}},"/playground/_radar/swift-ui":{"label":"swift UI","children":{}},"/playground/_radar/swift":{"label":"swift","children":{}},"/playground/_radar/swr":{"label":"swr","children":{}},"/playground/_radar/tailwindcss":{"label":"tailwindcss","children":{}},"/playground/_radar/tauri":{"label":"tauri","children":{}},"/playground/_radar/team-topologies":{"label":"team topologies","children":{}},"/playground/_radar/timeline":{"label":"Timeline","children":{"/playground/_radar/timeline/create-working-devcontainer-for-nextjs-boilerplate":{"label":"create working devcontainer for nextjs boilerplate","children":{}},"/playground/_radar/timeline/open-source-devpod-paperspace-provider":{"label":"open source devpod paperspace provider","children":{}},"/playground/_radar/timeline/create-working-devcontainer-for-go-api":{"label":"create working devcontainer for go api","children":{}},"/playground/_radar/timeline/fe-23-training-type-safe-client-server":{"label":"fe 23 training type safe client server","children":{}},"/playground/_radar/timeline/first-introduced-use-of-duckdb-in-consolelabs-logconsoleso":{"label":"first introduced use of duckdb in consolelabs logconsoleso","children":{}},"/playground/_radar/timeline/add-type-safe-client-server-support-for-next-boilerplate":{"label":"add type safe client server support for next boilerplate","children":{}},"/playground/_radar/timeline/building-reliable-apps-sentry-and-distributed-tracing-for-effective-monitoring":{"label":"building reliable apps sentry and distributed tracing for effective monitoring","children":{}},"/playground/_radar/timeline/an-engineering-story-map-for-llms":{"label":"an engineering story map for llms","children":{}},"/playground/_radar/timeline/exploring-resumable-server-side-rendering-with-qwik":{"label":"exploring resumable server side rendering with qwik","children":{}},"/playground/_radar/timeline/challenge-faced-when-researching-rlhf-with-open-assistant":{"label":"challenge faced when researching rlhf with open assistant","children":{}},"/playground/_radar/timeline/embracing-go-1210s-slog-a-unified-logging-interface-with-benchmarks-against-zerolog-and-zap":{"label":"embracing go 1210s slog a unified logging interface with benchmarks against zerolog and zap","children":{}},"/playground/_radar/timeline/adoption-of-pnpm":{"label":"adoption of pnpm","children":{}},"/playground/_radar/timeline/diagnosing-and-resolving-performance-issues-with-pprof-and-trace-in-go":{"label":"diagnosing and resolving performance issues with pprof and trace in go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-fortress":{"label":"migrate yarn to pnpm in fortress","children":{}},"/playground/_radar/timeline/level-up-your-testing-game-harnessing-gomock-for-unbeatable-unit-testing-in-go":{"label":"level up your testing game harnessing gomock for unbeatable unit testing in go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nghe-nhan-droppii":{"label":"migrate yarn to pnpm in nghe nhan droppii","children":{}},"/playground/_radar/timeline/common-design-patterns-in-golang-part-1":{"label":"common design patterns in golang part 1","children":{}},"/playground/_radar/timeline/go-training-2023-from-basic-to-advanced":{"label":"go training 2023 from basic to advanced","children":{}},"/playground/_radar/timeline/llms-accuracy-self-refinement":{"label":"llms accuracy self refinement","children":{}},"/playground/_radar/timeline/adversarial-prompting":{"label":"adversarial prompting","children":{}},"/playground/_radar/timeline/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"chunking strategies to overcome context limitation in LLM","children":{}},"/playground/_radar/timeline/dealing-with-long-term-memory-of-chatbot":{"label":"dealing with long term memory of chatbot","children":{}},"/playground/_radar/timeline/error-handling-and-failure-management-in-a-go-system":{"label":"error handling and failure management in a go system","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nextjs-boilerplate":{"label":"migrate yarn to pnpm in nextjs boilerplate","children":{}},"/playground/_radar/timeline/lessons-learned-building-an-llm-chatbot-a-case-study":{"label":"lessons learned building an LLM chatbot a case study","children":{}},"/playground/_radar/timeline/q-learning":{"label":"q learning","children":{}},"/playground/_radar/timeline/foundation-model":{"label":"foundation model","children":{}},"/playground/_radar/timeline/integrate-zod-to-nextjs-boilerplate":{"label":"integrate zod to nextjs boilerplate","children":{}},"/playground/_radar/timeline/llm-query-caching":{"label":"LLM query caching","children":{}},"/playground/_radar/timeline/build-your-chatbot-with-open-source-large-language-models":{"label":"build your chatbot with open source large language models","children":{}},"/playground/_radar/timeline/integrate-playwright-x-codecept-with-discord":{"label":"integrate playwright x codecept with discord","children":{}},"/playground/_radar/timeline/overcoming-distributed-system-challenges-using-golang":{"label":"overcoming distributed system challenges using golang","children":{}},"/playground/_radar/timeline/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"easy prompt engineering for business use and mitigating risks in llms","children":{}},"/playground/_radar/timeline/migrate-headlessui-to-radixui":{"label":"migrate headlessui to radixui","children":{}},"/playground/_radar/timeline/llm-101-enhance-developer-productivity":{"label":"LLM 101 enhance developer productivity","children":{}},"/playground/_radar/timeline/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"approaches to manage concurrent workloads like worker pools and pipelines","children":{}},"/playground/_radar/timeline/lessons-learned-from-being-a-part-of-corporate-microfrontend-implementation":{"label":"lessons learned from being a part of corporate microfrontend implementation","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-react-toolkit":{"label":"migrate yarn to pnpm in react toolkit","children":{}},"/playground/_radar/timeline/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"lessons learned from concurrency practices in blockchain projects","children":{}},"/playground/_radar/timeline/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"applying mock service worker msw for seamless web development","children":{}},"/playground/_radar/timeline/integrate-playwright-to-run-e2e-test-with-fortress":{"label":"integrate playwright to run e2e test with fortress","children":{}},"/playground/_radar/timeline/from-multi-repo-to-monorepo-a-case-study-with-nghenhan":{"label":"from multi repo to monorepo a case study with nghenhan","children":{}},"/playground/_radar/timeline/case-study-how-blue-green-deployment-help-mochi":{"label":"case study how blue green deployment help mochi","children":{}},"/playground/_radar/timeline/develop-codecept-to-integrate-with-fortress":{"label":"develop codecept to integrate with fortress","children":{}},"/playground/_radar/timeline/case-study-from-multiple-repo-to-monorepo-at-nghe-nhan":{"label":"case study from multiple repo to monorepo at nghe nhan","children":{}},"/playground/_radar/timeline/apply-blue-green-deployment-to-mochi":{"label":"apply blue green deployment to mochi","children":{}},"/playground/_radar/timeline/memo-blue-green-deployment":{"label":"memo blue green deployment","children":{}},"/playground/_radar/timeline/brainery-blue-green-deployment":{"label":"brainery blue green deployment","children":{}},"/playground/_radar/timeline/brainery-validation-with-zod":{"label":"brainery validation with zod","children":{}},"/playground/_radar/timeline/brainery-progressive-delivery":{"label":"brainery progressive delivery","children":{}},"/playground/_radar/timeline/memo-react-native-new-architecture":{"label":"memo react native new architecture","children":{}},"/playground/_radar/timeline/backend-for-call-requests-to-binance-and-get-data-from-multiple-platforms":{"label":"backend for call requests to binance and get data from multiple platforms","children":{}},"/playground/_radar/timeline/create-backend-monorepo-to-share-code-and-manage-multiple-services-in-one-repo":{"label":"create backend monorepo to share code and manage multiple services in one repo","children":{}},"/playground/_radar/timeline/nextjs-boilerplate":{"label":"nextjs boilerplate","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-wego":{"label":"apply page object model structure to wego","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-aharooms":{"label":"apply page object model structure to aharooms","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-artzy":{"label":"apply page object model structure to artzy","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-sci":{"label":"apply page object model structure to sci","children":{}},"/playground/_radar/timeline/build-automation-for-sci":{"label":"build automation for sci","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-basehq":{"label":"apply page object model structure to basehq","children":{}},"/playground/_radar/timeline/practice-and-using-selenium-in-setel-project":{"label":"practice and using selenium in setel project","children":{}},"/playground/_radar/timeline/mdx-document-for":{"label":"mdx document for","children":{}},"/playground/_radar/timeline/develop":{"label":"develop","children":{}},"/playground/_radar/timeline/apply-monorepos-to-repit-to-resolve-the-problem-of-consistency":{"label":"apply monorepos to repit to resolve the problem of consistency","children":{}},"/playground/_radar/timeline/learn-typescript-as-a-mandatory-to-develop-reapit-foundation":{"label":"learn typescript as a mandatory to develop reapit foundation","children":{}},"/playground/_radar/timeline/develop-sdk-integration-demo-for-sajari":{"label":"develop sdk integration demo for sajari","children":{}},"/playground/_radar/timeline/live-view":{"label":"live view","children":{}},"/playground/_radar/timeline/migrate-aharooms-pms-to-typescript":{"label":"migrate aharooms pms to typescript","children":{}},"/playground/_radar/timeline/create-api-service-for-urbox-to-sync-orders-from-3rd-parties-and-manage-shipment":{"label":"create api service for urbox to sync orders from 3rd parties and manage shipment","children":{}},"/playground/_radar/timeline/nghenhan-microservices":{"label":"nghenhan microservices","children":{}},"/playground/_radar/timeline/radio-talk-65-fullstack-type-safe-with-trpc":{"label":"radio talk 65 fullstack type safe with trpc","children":{}},"/playground/_radar/timeline/understanding-test-doubles-an-in-depth-look":{"label":"understanding test doubles an in depth look","children":{}},"/playground/_radar/timeline/radio-talk-64-coding-best-practice-that-optimizing-go-compiler":{"label":"radio talk 64 coding best practice that optimizing go compiler","children":{}},"/playground/_radar/timeline/reward-model":{"label":"reward model","children":{}},"/playground/_radar/timeline/sum-command":{"label":"sum command","children":{}},"/playground/_radar/timeline/reinforcement-learning":{"label":"reinforcement learning","children":{}},"/playground/_radar/timeline/react-server-component":{"label":"react server component","children":{}},"/playground/_radar/timeline/select-vector-database-for-llm":{"label":"select vector database for LLM","children":{}},"/playground/_radar/timeline/workaround-with-openais-token-limit-with-langchain":{"label":"workaround with openais token limit with langchain","children":{}},"/playground/_radar/timeline/working-with-langchain-document-loaders":{"label":"working with langchain document loaders","children":{}},"/playground/_radar/timeline/the-cost-of-react-native":{"label":"the cost of react native","children":{}},"/playground/_radar/timeline/state-of-frontend-2023-react-vs-angular-vs-vue":{"label":"state of frontend 2023 react vs angular vs vue","children":{}},"/playground/_radar/timeline/unit-testing-best-practices-in-golang":{"label":"unit testing best practices in golang","children":{}},"/playground/_radar/timeline/what-is-pnpm":{"label":"what is pnpm","children":{}},"/playground/_radar/timeline/tackling-server-state-complexity-in-frontend-development":{"label":"tackling server state complexity in frontend development","children":{}},"/playground/_radar/timeline/why-we-chose-our-tech-stack":{"label":"why we chose our tech stack","children":{}},"/playground/_radar/timeline/why-micro-frontend":{"label":"why micro frontend","children":{}},"/playground/_radar/timeline/radio-talk-monorepo":{"label":"radio talk monorepo","children":{}},"/playground/_radar/timeline/radio-talk-blue-green-deployment":{"label":"radio talk blue green deployment","children":{}},"/playground/_radar/timeline/radio-talk-a-demo-of-query-engine-postgresql-vs-apache-spark":{"label":"radio talk a demo of query engine postgresql vs apache spark","children":{}},"/playground/_radar/timeline/rnd-team-mentioned-apache-spark-as-a-solution-to-handle-query-big-data":{"label":"rnd team mentioned apache spark as a solution to handle query big data","children":{}},"/playground/_radar/timeline/radio-talk-engineering-health-metrics":{"label":"radio talk engineering health metrics","children":{}},"/playground/_radar/timeline/radio-talk-nextjs-13":{"label":"radio talk nextjs 13","children":{}},"/playground/_radar/timeline/radio-talk-using-nextjs-as-a-fullstack-framework":{"label":"radio talk using nextjs as a fullstack framework","children":{}},"/playground/_radar/timeline/use-yup-to-validate-form-values-in-droppii":{"label":"use yup to validate form values in droppii","children":{}},"/playground/_radar/timeline/vitejs-native-modules":{"label":"vitejs native modules","children":{}},"/playground/_radar/timeline/radio-talk-introduction-to-apache-spark":{"label":"radio talk introduction to apache spark","children":{}},"/playground/_radar/timeline/vercel-switching-their-packages-from-yarn-to-pnpm-caught-our-attention":{"label":"vercel switching their packages from yarn to pnpm caught our attention","children":{}},"/playground/_radar/timeline/radio-talk-remix-vs-nextjs":{"label":"radio talk remix vs nextjs","children":{}},"/playground/_radar/timeline/radio-talk-turborepo":{"label":"radio talk turborepo","children":{}},"/playground/_radar/timeline/react-toolkit-migrate-from-lerna-to-turporepo":{"label":"react toolkit migrate from lerna to turporepo","children":{}},"/playground/_radar/timeline/react-toolkit":{"label":"react toolkit","children":{}},"/playground/_radar/timeline/urbox-backend-api":{"label":"urbox backend api","children":{}},"/playground/_radar/timeline/use-monorepos-to-build-v3-of-react-sdk-for-searchio":{"label":"use monorepos to build v3 of react sdk for searchio","children":{}},"/playground/_radar/timeline/use-nx-for-managing-basehq-frontend-monorepos":{"label":"use nx for managing basehq frontend monorepos","children":{}},"/playground/_radar/timeline/using-k6-in-setel":{"label":"using k6 in setel","children":{}},"/playground/_radar/timeline/use-monorepos-to-resolve-the-problem-of-sharing-ui-components-in-aharoom":{"label":"use monorepos to resolve the problem of sharing UI components in aharoom","children":{}},"/playground/_radar/timeline/a-case-study-interview-into-micro-frontends-building-design-system-for-e-commerce-platform":{"label":"a case study interview into micro frontends building design system for e commerce platform","children":{}},"/playground/_radar/timeline/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"accelerate project initiation with advanced nextjs boilerplate react toolkit","children":{}},"/playground/_radar/timeline/adapt-cucumber-as-a-bdd-for-wego":{"label":"adapt cucumber as a bdd for wego","children":{}}}},"/playground/_radar/timescaledb":{"label":"timescaledb","children":{}},"/playground/_radar/tla":{"label":"tla","children":{}},"/playground/_radar/trunk-based-development":{"label":"trunk based development","children":{}},"/playground/_radar/turborepo":{"label":"turborepo","children":{}},"/playground/_radar/type-safe-client-server":{"label":"type safe client server","children":{}},"/playground/_radar/typescript":{"label":"typescript","children":{}},"/playground/_radar/ui-documentation":{"label":"UI documentation","children":{}},"/playground/_radar/uno-css":{"label":"uno css","children":{}},"/playground/_radar/upptime":{"label":"upptime","children":{}},"/playground/_radar/v-model":{"label":"v model","children":{}},"/playground/_radar/vector-database":{"label":"vector database","children":{}},"/playground/_radar/vercel":{"label":"vercel","children":{}},"/playground/_radar/vitejs":{"label":"vitejs","children":{}},"/playground/_radar/volta":{"label":"volta","children":{}},"/playground/_radar/wasm":{"label":"wasm","children":{}},"/playground/_radar/webdriverio":{"label":"webdriverio","children":{}},"/playground/_radar/webflow":{"label":"webflow","children":{}},"/playground/_radar/yup":{"label":"yup","children":{}},"/playground/_radar/zod":{"label":"zod","children":{}},"/playground/_radar/zustand":{"label":"zustand","children":{}}}},"/playground/blockchain":{"label":"Blockchain","children":{"/playground/blockchain/build-custom-ai-agent-with-elizaos":{"label":"build custom AI agent with elizaos","children":{}},"/playground/blockchain/web3-development-with-foundry":{"label":"web3 development with foundry","children":{}},"/playground/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin":{"label":"implement a token swap from the base chain to bitcoin for cross-chain transactions","children":{}},"/playground/blockchain/ton_core_concept":{"label":"ton's base concepts","children":{}},"/playground/blockchain/ton_blockchain_of_blockchains":{"label":"ton: blockchain of blockchains","children":{}},"/playground/blockchain/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana":{"label":"introduce to solana token 2022 - new standard to create a token in solana","children":{}},"/playground/blockchain/solana-core-concept":{"label":"solana core concepts","children":{}},"/playground/blockchain/metaplex-nft-compression":{"label":"metaplex nft compression","children":{}},"/playground/blockchain/plonky2":{"label":"plonky2","children":{}},"/playground/blockchain/polygon-zkevm-architecture":{"label":"polygon zkevm architecture","children":{}},"/playground/blockchain/starknet-architecture":{"label":"starknet architecture","children":{}},"/playground/blockchain/zk-snarks":{"label":"zk-snarks","children":{}},"/playground/blockchain/layer-2":{"label":"layer 2: scaling solutions for ethereum","children":{}},"/playground/blockchain/solana-account":{"label":"solana account","children":{}},"/playground/blockchain/foundational-topics":{"label":"Foundational Topics","children":{"/playground/blockchain/foundational-topics/zero-knowledge-proofs":{"label":"zero-knowledge proofs","children":{}},"/playground/blockchain/foundational-topics/blocks":{"label":"blocks","children":{}},"/playground/blockchain/foundational-topics/distributed-systems":{"label":"distributed systems","children":{}},"/playground/blockchain/foundational-topics/pos":{"label":"pos","children":{}},"/playground/blockchain/foundational-topics/smart-contract":{"label":"smart contract","children":{}},"/playground/blockchain/foundational-topics/topics":{"label":"topics","children":{}}}},"/playground/blockchain/multisign-wallet":{"label":"multisign wallet","children":{}},"/playground/blockchain/anchor-framework":{"label":"anchor framework","children":{}},"/playground/blockchain/blockchain-bridge":{"label":"blockchain bridge","children":{}},"/playground/blockchain/nft-fractionalization":{"label":"nft fractionalization","children":{}},"/playground/blockchain/how-tokens-work-on-solana":{"label":"how tokens work on solana","children":{}},"/playground/blockchain/liquidity-pool":{"label":"liquidity pool","children":{}}}},"/playground/frontend":{"label":"Frontend","children":{"/playground/frontend/report":{"label":"Report","children":{"/playground/frontend/report/frontend-report-march-2025":{"label":"march 2025","children":{}},"/playground/frontend/report/frontend-report-february-2025":{"label":"february 2025","children":{}},"/playground/frontend/report/frontend-report-january-2025":{"label":"january 2025","children":{}},"/playground/frontend/report/frontend-report-second-half-of-november-2024":{"label":"nov 2024 (second half)","children":{}},"/playground/frontend/report/frontend-report-first-half-of-november-2024":{"label":"nov 2024 (first half)","children":{}},"/playground/frontend/report/frontend-report-october-2024":{"label":"october 2024","children":{}},"/playground/frontend/report/frontend-report-september-2024":{"label":"september 2024","children":{}},"/playground/frontend/report/frontend-report-august-2024":{"label":"august 2024","children":{}},"/playground/frontend/report/frontend-report-july-2024":{"label":"july 2024","children":{}}}},"/playground/frontend/react":{"label":"React","children":{"/playground/frontend/react/code-splitting":{"label":"code splitting","children":{}},"/playground/frontend/react/component-composition-patterns":{"label":"component composition patterns","children":{}},"/playground/frontend/react/design-system-integration":{"label":"design system integration","children":{}},"/playground/frontend/react/hook-architecture":{"label":"hook architecture","children":{}},"/playground/frontend/react/rendering-strategies":{"label":"rendering strategies","children":{}},"/playground/frontend/react/state-management-strategy":{"label":"state management strategy","children":{}},"/playground/frontend/react/testing-strategies":{"label":"testing strategies","children":{}}}},"/playground/frontend/websockets":{"label":"websockets","children":{}},"/playground/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree":{"label":"from markup to pixels - a look inside the dom, cssom, and render tree","children":{}},"/playground/frontend/window-and-iframe-communication":{"label":"window and iframe communication","children":{}},"/playground/frontend/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"applying mock service worker (msw) for seamless web development","children":{}},"/playground/frontend/render-optimization-in-data-fetching-libraries":{"label":"render optimization in data-fetching libraries","children":{}},"/playground/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql":{"label":"a fragment colocation pattern with react \u0026 apollo graphql","children":{}},"/playground/frontend/scroll-driven-animations":{"label":"scroll-driven animations","children":{}},"/playground/frontend/react-server-component":{"label":"react server components, nextjs route and data fetching","children":{}},"/playground/frontend/url-formats-for-sharing-via-social-networks":{"label":"url formats for sharing via social networks","children":{}},"/playground/frontend/shadow-dom":{"label":"shadow dom","children":{}},"/playground/frontend/retain-scroll-position-in-infinite-scroll":{"label":"retain scroll position in infinite scroll","children":{}},"/playground/frontend/continuous-translation":{"label":"continuous translation","children":{}},"/playground/frontend/what-is-pnpm-compare-to-npmyarn":{"label":"what is pnpm compare to npm/yarn","children":{}},"/playground/frontend/why-micro-frontend":{"label":"why micro frontend","children":{}},"/playground/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution":{"label":"why we chose our tech stack accelerating development with a robust frontend solution","children":{}},"/playground/frontend/tackling-server-state-complexity-in-frontend-development":{"label":"tackling server state complexity in frontend development","children":{}},"/playground/frontend/variable-fonts":{"label":"variable fonts","children":{}},"/playground/frontend/when-should-we-use-usereducer-instead-of-usestate":{"label":"when should we use usereducer instead of usestate?","children":{}},"/playground/frontend/preserving-and-resetting-state-in-react":{"label":"preserving and resetting state in react","children":{}},"/playground/frontend/mixpanel":{"label":"mixpanel","children":{}},"/playground/frontend/validation-with-zod":{"label":"validation with zod","children":{}},"/playground/frontend/parse-dont-validate-in-typescript":{"label":"parse, don't validate in typescript","children":{}},"/playground/frontend/webassembly":{"label":"webassembly","children":{}},"/playground/frontend/singleton-design-pattern-in-javascript":{"label":"singleton design pattern in javascript","children":{}},"/playground/frontend/an-introduction-to-atomic-css":{"label":"an introduction to atomic css","children":{}},"/playground/frontend/intro-to-indexeddb":{"label":"intro to indexeddb","children":{}},"/playground/frontend/the-fundamental-of-web-performance":{"label":"the fundamental of web performance","children":{}},"/playground/frontend/wai-aria":{"label":"wai-aria","children":{}},"/playground/frontend/build-polymorphic-react-components-with-typescript":{"label":"build polymorphic react components with typescript","children":{}},"/playground/frontend/threejs":{"label":"Threejs","children":{"/playground/frontend/threejs/cameras-in-threejs":{"label":"cameras in threejs","children":{}}}},"/playground/frontend/prevent-layout-thrashing":{"label":"prevent layout thrashing","children":{}},"/playground/frontend/pure-css-parallax":{"label":"pure css parallax","children":{}},"/playground/frontend/css-container-queries":{"label":"css container queries","children":{}},"/playground/frontend/hsl-color":{"label":"hsl color","children":{}},"/playground/frontend/mitigate-blocking-the-main-thread":{"label":"mitigate blocking the main thread","children":{}},"/playground/frontend/css-in-js":{"label":"css in js","children":{}},"/playground/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second":{"label":"dark mode flickers a white background for a fraction of a second","children":{}},"/playground/frontend/why-dom-manipulation-is-slow":{"label":"why dom manipulation is slow?","children":{}},"/playground/frontend/why-virtual-dom-is-fast":{"label":"why virtual dom is fast?","children":{}},"/playground/frontend/vitejs-native-modules":{"label":"vitejs native modules","children":{}},"/playground/frontend/javascript-modules":{"label":"javascript modules","children":{}},"/playground/frontend/atomic-design-pattern":{"label":"atomic design pattern","children":{}},"/playground/frontend/focus-trap":{"label":"focus trap","children":{}},"/playground/frontend/html-inert":{"label":"html inert","children":{}},"/playground/frontend/useeffect-double-calls-in-react-18":{"label":"useeffect double calls in react 18","children":{}},"/playground/frontend/react-18":{"label":"react 18","children":{}},"/playground/frontend/remix-versus-nextjs":{"label":"remix versus nextjs","children":{}},"/playground/frontend/zaplib-post-mortem":{"label":"zaplib post-mortem","children":{}},"/playground/frontend/parallelism-in-javascript":{"label":"parallelism in javascript","children":{}},"/playground/frontend/mpa-spa-and-partial-hydration":{"label":"mpa, spa and partial hydration","children":{}},"/playground/frontend/micro-frontends-microservices-for-frontend-development":{"label":"micro frontends microservices for frontend development","children":{}},"/playground/frontend/using-correct-html-element-to-increase-website-accessibility":{"label":"using correct html element to increase website accessibility","children":{}},"/playground/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss":{"label":"remove unused css styles from bootstrap using purgecss","children":{}}}},"/playground/use-cases":{"label":"Use Cases","children":{"/playground/use-cases/service_monitoring_with_upptime":{"label":"secure and transparent uptime monitoring with upptime and github secrets","children":{}},"/playground/use-cases/create-slides-with-overleaf":{"label":"create slides with overleaf and chatgpt","children":{}},"/playground/use-cases/optimize-init-load-time-for-trading-platform":{"label":"optimizing initial load time for a trading platform","children":{}},"/playground/use-cases/ai-interview-platform-mvp":{"label":"building mvp for ai-driven interview platform","children":{}},"/playground/use-cases/optimizing-ui-for-effective-investment-experience":{"label":"hedge foundation - optimizing UI for effective investment experience","children":{}},"/playground/use-cases/implement-binance-future-pnl-analysis-page":{"label":"implement binance futures pnl analysis page by phoenix liveview","children":{}},"/playground/use-cases/migrate-normal-table-to-timescale-table":{"label":"migrate regular tables into timescaledb hypertables to improve query performance","children":{}},"/playground/use-cases/bitcoin-alt-performance-tracking":{"label":"tracking bitcoin-altcoin performance indicators in btc hedging strategy","children":{}},"/playground/use-cases/database-hardening-for-trading-platform":{"label":"database hardening for a trading platform","children":{}},"/playground/use-cases/data-archive-and-recovery":{"label":"building a data archive and recovery strategy for high-volume trading system","children":{}},"/playground/use-cases/persist-history-using-data-snapshot-pattern":{"label":"implementing data snapshot pattern to persist historical data","children":{}},"/playground/use-cases/ai-ruby-travel-assistant-chatbot":{"label":"ai-powered ruby travel assistant","children":{}},"/playground/use-cases/building-chatbot-agent-for-project-management-tool":{"label":"building chatbot agent to streamline project management","children":{}},"/playground/use-cases/building-data-pipeline-ogif-transcriber":{"label":"building data pipeline for OGIF transcriber","children":{}},"/playground/use-cases/centralized-monitoring-setup-for-trading-platform":{"label":"setup centralized monitoring system for hedge foundation trading platform","children":{}},"/playground/use-cases/binance-transfer-matching":{"label":"building better binance transfer tracking","children":{}},"/playground/use-cases/crypto-market-outperform-chart-rendering":{"label":"visualizing crypto market performance: btc-alt dynamic indicators in golang","children":{}},"/playground/use-cases/enhancing-cryptocurrency-transfer-logger":{"label":"transfer mapping: enhancing loggers for better transparency","children":{}},"/playground/use-cases/reconstructing_trading_pnl_data_pipeline_approach":{"label":"reconstructing historical trading pnl: a data pipeline approach","children":{}},"/playground/use-cases/ai-powered-monthly-project-reports":{"label":"project reports system: a case study","children":{}}}},"/playground/ai":{"label":"AI","children":{"/playground/ai/securing-your-remote-mcp-servers":{"label":"securing your remote MCP servers","children":{}},"/playground/ai/tool-level-security-for-remote-mcp-servers":{"label":"tool-level security for remote MCP servers","children":{}},"/playground/ai/model-context-protocol":{"label":"intro to model context protocol","children":{}},"/playground/ai/building-llm-system":{"label":"Building LLM System","children":{"/playground/ai/building-llm-system/quantization-in-llm":{"label":"quantization for large language models","children":{}},"/playground/ai/building-llm-system/graphrag":{"label":"graphrag - building a knowledge graph for RAG system","children":{}},"/playground/ai/building-llm-system/guardrails-in-llm":{"label":"guardrails in LLM","children":{}},"/playground/ai/building-llm-system/react-in-llm":{"label":"react(reason + act) in LLM","children":{}},"/playground/ai/building-llm-system/rewoo-in-llm":{"label":"rewoo: reasoning without observation - a deeper look","children":{}},"/playground/ai/building-llm-system/model-selection":{"label":"model selection","children":{}},"/playground/ai/building-llm-system/logs-pillar":{"label":"logging","children":{}},"/playground/ai/building-llm-system/metric-pillar":{"label":"metrics","children":{}},"/playground/ai/building-llm-system/observability-in-ai-platforms":{"label":"observability in AI platforms","children":{}},"/playground/ai/building-llm-system/trace-pillar":{"label":"tracing","children":{}},"/playground/ai/building-llm-system/intent-classification-by-llm":{"label":"intent classification by LLM","children":{}},"/playground/ai/building-llm-system/llm-as-a-judge":{"label":"LLM as a judge","children":{}},"/playground/ai/building-llm-system/use-cases-for-llm-applications":{"label":"use cases for LLM applications","children":{}},"/playground/ai/building-llm-system/the-rise-of-ai-applications-with-llm":{"label":"the rise of AI applications with LLM","children":{}},"/playground/ai/building-llm-system/evaluation-guideline-for-llm-application":{"label":"evaluation guidelines for LLM applications","children":{}},"/playground/ai/building-llm-system/prevent-prompt-injection":{"label":"prevent prompt injection","children":{}},"/playground/ai/building-llm-system/building-llm-system":{"label":"§ building LLM system","children":{}},"/playground/ai/building-llm-system/multi-agent-collaboration-for-task-completion":{"label":"multi-agent collaboration for task completion","children":{}},"/playground/ai/building-llm-system/multimodal-in-rag":{"label":"multimodal in RAG","children":{}}}},"/playground/ai/digest":{"label":"Digest","children":{"/playground/ai/digest/ai-digest-02":{"label":"AI digest #2 new command aider, openhands, qwen2.5 coder 32b, predicted output","children":{}},"/playground/ai/digest/ai-digest-01":{"label":"AI digest #1 aider reasoning, openai realtime api, cline - pre claude-dev ","children":{}}}},"/playground/ai/copilots":{"label":"Copilots","children":{"/playground/ai/copilots/projects-operations":{"label":"project operations copilots","children":{}},"/playground/ai/copilots/team-copilots":{"label":"team copilots","children":{}}}},"/playground/ai/text-to-mongodb":{"label":"natural language to database queries: text-to-mongodb","children":{}},"/playground/ai/use-cases":{"label":"Use Cases","children":{"/playground/ai/use-cases/salesforce":{"label":"salesforce use cases","children":{}},"/playground/ai/use-cases/yelp":{"label":"yelp use cases","children":{}}}},"/playground/ai/evaluate-chatbot-agent-by-simulated-user":{"label":"evaluate chatbot agent by user simulation","children":{}},"/playground/ai/journey-of-thought-prompting":{"label":"journey of thought prompting: harnessing AI to craft better prompts","children":{}},"/playground/ai/llm-tracing-in-ai-system":{"label":"LLM tracing in AI system","children":{}},"/playground/ai/caching-with-rag-system":{"label":"evaluating caching in RAG systems","children":{}},"/playground/ai/generative-ui":{"label":"what is generative ui?","children":{}},"/playground/ai/re-ranking-in-rag":{"label":"re-ranking in RAG","children":{}},"/playground/ai/function-calling":{"label":"function calling in AI agents","children":{}},"/playground/ai/building-llm-powered-tools-with-dify":{"label":"streamlining internal tool development with managed llmops: a dify case study","children":{}},"/playground/ai/thumbs-up-and-thumbs-down-pattern":{"label":"thumbs up and thumbs down pattern","children":{}},"/playground/ai/supervisor-ai-agents":{"label":"building agent supervisors to generate insights","children":{}},"/playground/ai/raptor-llm-retrieval":{"label":"raptor: tree-based retrieval for language models","children":{}},"/playground/ai/proximal-policy-optimization":{"label":"proximal policy optimization","children":{}},"/playground/ai/a-grand-unified-theory-of-the-ai-hype-cycle":{"label":"a grand unified theory of the AI hype cycle","children":{}},"/playground/ai/developing-rapidly-with-generative-ai":{"label":"developing rapidly with generative AI","children":{}},"/playground/ai/rlhf-with-open-assistant":{"label":"rlhf with open assistant","children":{}},"/playground/ai/story-map-for-llms":{"label":"story map for llms","children":{}},"/playground/ai/adversarial-prompting":{"label":"adversarial prompting in prompt engineering","children":{}},"/playground/ai/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"chunking strategies to overcome context limitation in LLM","children":{}},"/playground/ai/llms-accuracy-self-refinement":{"label":"llm's accuracy - self refinement","children":{}},"/playground/ai/llm-query-caching":{"label":"query caching for large language models","children":{}},"/playground/ai/reinforcement-learning":{"label":"introduction to reinforcement learning and its application with llms","children":{}},"/playground/ai/foundation-model":{"label":"foundation models: the latest advancement in AI","children":{}},"/playground/ai/select-vector-database-for-llm":{"label":"select vector database for LLM","children":{}},"/playground/ai/build-your-chatbot-with-open-source-large-language-models":{"label":"build your chatbot with open source large language models","children":{}},"/playground/ai/workaround-with-openais-token-limit-with-langchain":{"label":"workaround with openai's token limit with langchain","children":{}},"/playground/ai/working-with-langchain-document-loaders":{"label":"working with langchain document loaders","children":{}}}},"/playground/market-commentary":{"label":"Market Commentary","children":{"/playground/market-commentary/event-takeaways-2nd":{"label":"2nd talks and takeaways","children":{}},"/playground/market-commentary/event-takeaways-1st":{"label":"1st talks and takeaways","children":{}},"/playground/market-commentary/2025-28th-feb":{"label":"#9: bybit loses $1.5b in hack, claude 3.7 sonnet drops, and openart designs characters","children":{}},"/playground/market-commentary/2025-21th-feb":{"label":"#8: r1 1776 goes open-source, cardex gets hacked, and grok-3 debuts","children":{}},"/playground/market-commentary/2025-14th-feb":{"label":"#7: 10x AI cost reduction, lyft’s 2026 robotaxi milestone, and solana etf buzz","children":{}},"/playground/market-commentary/2025-7th-feb":{"label":"#6 trending products, deepseek wave, and ethereum predictions","children":{}},"/playground/market-commentary/2025-17th-jan":{"label":"#5 vc trends, blockchain breakthroughs, and AI innovations","children":{}},"/playground/market-commentary/2025-10th-jan":{"label":"#4 AI supercomputers, mini AI pcs, sea vc","children":{}},"/playground/market-commentary/2025-3rd-jan":{"label":"#3 AI at ces, wall street boom, blockchain trends","children":{}},"/playground/market-commentary/2024-27th-dec":{"label":"#2 AI talent wars, openai’s new models, hyperliquid","children":{}},"/playground/market-commentary/2024-13th-dec":{"label":"#1 gemini 2.0, openai’s sora,  a16z’s predictions","children":{}}}},"/playground/forward-engineering":{"label":"Forward Engineering","children":{"/playground/forward-engineering/2025-02":{"label":"20242025","children":{}},"/playground/forward-engineering/2024-09":{"label":"september 2024","children":{}},"/playground/forward-engineering/2023-11":{"label":"november 2023","children":{}},"/playground/forward-engineering/2023-10":{"label":"october 2023","children":{}},"/playground/forward-engineering/2023-08":{"label":"august 2023","children":{}},"/playground/forward-engineering/2023-06":{"label":"june 2023","children":{}},"/playground/forward-engineering/2023-05":{"label":"may 2023","children":{}},"/playground/forward-engineering/2023-03":{"label":"march 2023","children":{}},"/playground/forward-engineering/2023-12":{"label":"december 2023","children":{}},"/playground/forward-engineering/2022":{"label":"2022","children":{}},"/playground/forward-engineering/volume-03":{"label":"tech radar volume 03","children":{}},"/playground/forward-engineering/volume-02":{"label":"tech radar volume 02","children":{}},"/playground/forward-engineering/volume-01":{"label":"tech radar volume 01","children":{}},"/playground/forward-engineering/readme":{"label":"📡 tech radar","children":{}}}},"/playground/go":{"label":"Go","children":{"/playground/go/weekly":{"label":"Weekly","children":{"/playground/go/weekly/dec-13":{"label":"#24 go 1.24 testing/synctest experiment for time and concurrency testing","children":{}},"/playground/go/weekly/dec-06":{"label":"#23 draft release notes for go 1.24 and weak pointers in go","children":{}},"/playground/go/weekly/nov-29":{"label":"#22 gomlx: ml in go without python","children":{}},"/playground/go/weekly/nov-22":{"label":"#21 go sync.once is simple","children":{}},"/playground/go/weekly/nov-15":{"label":"#20 go turns 15","children":{}},"/playground/go/weekly/nov-08":{"label":"#19 writing secure go code","children":{}},"/playground/go/weekly/nov-01":{"label":"#18 fuzz testing go http services","children":{}},"/playground/go/weekly/oct-25":{"label":"#17 leveraging benchstat projects in go benchmark and go plan9 memo on 450% speeding up calculations","children":{}},"/playground/go/weekly/oct-18":{"label":"#16 understand sync.map","children":{}},"/playground/go/weekly/oct-11":{"label":"#15 go embed and reflect","children":{}},"/playground/go/weekly/oct-04":{"label":"#14 compile-time eval \u0026 sqlite with wazero","children":{}},"/playground/go/weekly/sep-27":{"label":"#13 compiler quests and vector vexations","children":{}},"/playground/go/weekly/sep-20":{"label":"#12 cli tools for k8s, rest, and terminals","children":{}},"/playground/go/weekly/sep-13":{"label":"#11 actors, frameworks, and the future of go","children":{}},"/playground/go/weekly/sep-06":{"label":"#10 script, telemetry","children":{}},"/playground/go/weekly/aug-30":{"label":"#9 tinygo, sqlite vector search, and permify","children":{}},"/playground/go/weekly/aug-23":{"label":"#8 gonb, kubetrim, and gophercon uk 2024","children":{}},"/playground/go/weekly/aug-16":{"label":"#7 go 1.23, websockets, and structs","children":{}},"/playground/go/weekly/aug-09":{"label":"#6 cogent core, russ cox stepping down","children":{}},"/playground/go/weekly/aug-02":{"label":"#5 go 1.23 features, memory, minecraft, and more","children":{}},"/playground/go/weekly/jul-26":{"label":"#4 ethical hacking, http requests, mac app development","children":{}},"/playground/go/weekly/jul-12":{"label":"#3 generic collections, generics constraints, AI bot","children":{}},"/playground/go/weekly/jul-05":{"label":"#2 go 1.23 iterators","children":{}},"/playground/go/weekly/june-27":{"label":"#1 ebpf and pgo optimization techniques","children":{}}}},"/playground/go/extension-interface-pattern":{"label":"go extension interface pattern","children":{}},"/playground/go/go-import":{"label":"go import design: using git repo path","children":{}},"/playground/go/go-package":{"label":"package first design","children":{}},"/playground/go/go-generics-type-safety":{"label":"how does go achieve type safety when it enables generics?","children":{}},"/playground/go/go-for-enterprise":{"label":"Go For Enterprise","children":{"/playground/go/go-for-enterprise/who-using-golang-in-enterprise":{"label":"who is using go in enterprise?","children":{}},"/playground/go/go-for-enterprise/enterprise-standard-language":{"label":"go as an enterprise standard language","children":{}},"/playground/go/go-for-enterprise/how-to-use-go-in-enterprise":{"label":"how to use go in the enterprise","children":{}},"/playground/go/go-for-enterprise/when-to-use-golang-in-enterprise":{"label":"when to use go in the enterprise","children":{}},"/playground/go/go-for-enterprise/why-enterprise-chose-java":{"label":"why enterprise chose java","children":{}},"/playground/go/go-for-enterprise/why-go":{"label":"why go?","children":{}}}},"/playground/go/compute-union-2-finite-automata":{"label":"efficient union of finite automata in golang: a practical approach","children":{}},"/playground/go/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"approaches to manage concurrent workloads like worker pools and pipelines","children":{}},"/playground/go/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq":{"label":"message queues and streaming platforms eg kafka nats rabbitmq","children":{}},"/playground/go/unit-testing-best-practices-in-golang":{"label":"unit testing best practices in golang","children":{}},"/playground/go/profiling-in-go":{"label":"profiling in go","children":{}},"/playground/go/go-in-software-engineering":{"label":"go in software engineering","children":{}},"/playground/go/go-concurrency":{"label":"go concurrency","children":{}},"/playground/go/slice-and-array-in-golang":{"label":"slice and array in golang","children":{}},"/playground/go/use-go-selenium-to-crawl-data":{"label":"use go selenium to crawl data","children":{}},"/playground/go/connecting-vim-with-golang":{"label":"connecting vim with golang","children":{}}}},"/playground/market-report":{"label":"Market Report","children":{"/playground/market-report/2024-october":{"label":"october 2024","children":{}},"/playground/market-report/2024-september":{"label":"september 2024","children":{}},"/playground/market-report/2024-august":{"label":"august 2024","children":{}},"/playground/market-report/2024-july":{"label":"july 2024","children":{}},"/playground/market-report/2024-may":{"label":"may 2024","children":{}},"/playground/market-report/2024-april":{"label":"april 2024","children":{}},"/playground/market-report/2024-march":{"label":"march 2024","children":{}},"/playground/market-report/2024-february":{"label":"february 2024","children":{}},"/playground/market-report/2024-january":{"label":"january 2024","children":{}},"/playground/market-report/2023-december":{"label":"december 2023","children":{}}}},"/playground/devbox":{"label":"Devbox","children":{"/playground/devbox/devbox":{"label":"§ devbox","children":{}},"/playground/devbox/story":{"label":"Story","children":{"/playground/devbox/story/devbox-production-success-story":{"label":"devbox in production: our success story","children":{}},"/playground/devbox/story/devbox-local-development-env":{"label":"using devbox to setup local development environment","children":{}},"/playground/devbox/story/devbox-nix-and-our-devbox-adoption":{"label":"the overview into nix \u0026 how we use devbox @ dwarves","children":{}},"/playground/devbox/story/devbox-docker-adoption-and-challenges":{"label":"our docker adoption and its challenges","children":{}},"/playground/devbox/story/devbox-a-world-before-docker":{"label":"the world before docker","children":{}}}},"/playground/devbox/guide":{"label":"Guide","children":{"/playground/devbox/guide/containerless":{"label":"ditch the containers: go containerless with devbox","children":{}},"/playground/devbox/guide/devboxjson":{"label":"devbox.json: your project's dna","children":{}},"/playground/devbox/guide/run-your-own-shell":{"label":"devbox shell: your dev environment, your rules","children":{}}}},"/playground/devbox/introduction":{"label":"Introduction","children":{"/playground/devbox/introduction/the-reason-for-being":{"label":"the reason for being","children":{}},"/playground/devbox/introduction/why-devbox-but-not-nix":{"label":"devbox vs nix: why we chose simplicity","children":{}}}},"/playground/devbox/research":{"label":"Research","children":{"/playground/devbox/research/content-addressable-storage-in-docker":{"label":"devbox vs nix: why we chose simplicity","children":{}},"/playground/devbox/research/fixed-output-derivation":{"label":"fixed-output derivation in nix","children":{}},"/playground/devbox/research/nix-is-faster-than-docker-build":{"label":"nix is faster than docker build","children":{}},"/playground/devbox/research/pinning-nixpkgs":{"label":"pinning nixpkgs in nix","children":{}},"/playground/devbox/research/shadow-copies":{"label":"shadow copies in docker builds","children":{}},"/playground/devbox/research/unstable-package-installation":{"label":"unstable package installation in docker","children":{}}}}}}}},"/careers":{"label":"Careers","children":{"/careers/archived":{"label":"Archived","children":{"/careers/archived/full-stack-engineer":{"label":"full-stack engineer","children":{}},"/careers/archived/executive-assistant":{"label":"executive assistant","children":{}},"/careers/archived/technical-recruiter":{"label":"technical recruiter","children":{}},"/careers/archived/backend-engineer-go-elixir-rust":{"label":"backend engineer, go/elixir/rust","children":{}},"/careers/archived/react-native-developer":{"label":"react native developer","children":{}},"/careers/archived/android-developer":{"label":"mobile engineer, android","children":{}},"/careers/archived/community-executive":{"label":"community executive","children":{}},"/careers/archived/data-engineering":{"label":"energy - data engineering","children":{}},"/careers/archived/devops":{"label":"devops engineer - fintech","children":{}},"/careers/archived/frontend-developer-junior":{"label":"junior frontend developer","children":{}},"/careers/archived/frontend":{"label":"frontend","children":{}},"/careers/archived/ios-developer":{"label":"ios developer - energytech","children":{}},"/careers/archived/macos-developer":{"label":"software engineer, macos","children":{}},"/careers/archived/product-designer-new-grad":{"label":"product designer, new grad","children":{}},"/careers/archived/product-designer":{"label":"product designer","children":{}},"/careers/archived/qc-automation":{"label":"qc engineer, automation - logistics","children":{}},"/careers/archived/qc-manual":{"label":"fintech - qc engineer, manual","children":{}},"/careers/archived/reactjs-web-engineer":{"label":"web engineer, react.js","children":{}},"/careers/archived/visual-designer":{"label":"visual designer","children":{}},"/careers/archived/android":{"label":"android","children":{}},"/careers/archived/golang":{"label":"golang","children":{}},"/careers/archived/intern":{"label":"intern","children":{}},"/careers/archived/ios":{"label":"ios developer","children":{}},"/careers/archived/qa":{"label":"qa engineer","children":{}}}},"/careers/open-positions":{"label":"Open Positions","children":{"/careers/open-positions/business-manager":{"label":"business development manager","children":{}},"/careers/open-positions/growth-lead":{"label":"growth lead","children":{}}}},"/careers/life":{"label":"Life","children":{"/careers/life/2024-09-26-29-dat-nguyen":{"label":"dat nguyen","children":{}},"/careers/life/2024-02-19-28-duyen-tran":{"label":"duyen tran","children":{}},"/careers/life/2024-01-22-27-tri-tran":{"label":"tri tran","children":{}},"/careers/life/2024-01-03-25-khoi-nguyen":{"label":"khoi nguyen","children":{}},"/careers/life/2023-12-13-24-tai-pham":{"label":"tai pham","children":{}},"/careers/life/2023-12-12-23-hieu-nghia":{"label":"hieu nghia","children":{}},"/careers/life/2023-11-27-22-cat-nguyen":{"label":"cat nguyen","children":{}},"/careers/life/2023-11-20-21-minh-cloud":{"label":"minh cloud","children":{}},"/careers/life/2023-11-13-20-hoai-khang":{"label":"hoai khang","children":{}},"/careers/life/2023-11-03-19-vi-tran":{"label":"vi tran","children":{}},"/careers/life/2023-10-30-18-tuan-tran":{"label":"tuan tran","children":{}},"/careers/life/2023-10-16-16-kim-ngan":{"label":"kim ngan","children":{}},"/careers/life/2023-10-13-17-hoang-nguyen":{"label":"hoang nguyen","children":{}},"/careers/life/2023-10-09-15-khoi-ngo":{"label":"khoi ngo","children":{}},"/careers/life/2023-10-02-14-dat-pham":{"label":"dat pham","children":{}},"/careers/life/2023-09-29-13-bien-vo":{"label":"bien vo","children":{}},"/careers/life/2023-09-18-12-toan-ho":{"label":"toan ho","children":{}},"/careers/life/2023-09-05-11-dinh-nam":{"label":"dinh nam","children":{}},"/careers/life/2023-08-17-10-cuong-mai":{"label":"cuong mai","children":{}},"/careers/life/2023-08-07-9-hoang-anh":{"label":"hoang anh","children":{}},"/careers/life/2023-06-30-7-khac-vy":{"label":"khac vy","children":{}},"/careers/life/group":{"label":"Group","children":{"/careers/life/group/2023-06-01-software-design-group":{"label":"software design group","children":{}}}},"/careers/life/2022-09-21-7-my-anh":{"label":"my anh","children":{}},"/careers/life/2022-08-11-6-hieu-vu":{"label":"hieu vu","children":{}},"/careers/life/2022-08-04-6-duy-nguyen":{"label":"duy nguyen","children":{}},"/careers/life/2022-08-03-5-nam-nguyen":{"label":"nam nguyen","children":{}},"/careers/life/2022-07-22-4-an-tran":{"label":"an tran","children":{}},"/careers/life/2022-03-17-3-tom-nguyen":{"label":"tom nguyen","children":{}},"/careers/life/2022-02-25-2-anh-tran":{"label":"anh tran","children":{}},"/careers/life/2022-02-14-1-thanh-pham":{"label":"thanh pham","children":{}},"/careers/life/2021-03-31-0-tuan-dao":{"label":"tuan dao","children":{}},"/careers/life/2021-03-11-0-phat-nguyen-career":{"label":"phat nguyen","children":{}},"/careers/life/2020-05-08-0-thanh-pham":{"label":"thanh pham","children":{}},"/careers/life/2020-04-10-0-huy-nguyen":{"label":"huy nguyen","children":{}}}},"/careers/culture":{"label":"culture","children":{}},"/careers/manifesto":{"label":"manifesto","children":{}},"/careers/internship":{"label":"Internship","children":{"/careers/internship/2019":{"label":"2019","children":{"/careers/internship/2019/2019":{"label":"spring internship 2019","children":{}}}}}},"/careers/apprentice":{"label":"Apprentice","children":{"/careers/apprentice/2022":{"label":"2022","children":{"/careers/apprentice/2022/batch-of-2022":{"label":"batch of 2022","children":{}},"/careers/apprentice/2022/2022-meet-ngoc-thanh-pham":{"label":"thanh pham","children":{}},"/careers/apprentice/2022/2022-meet-tuan-dao":{"label":"tuan dao","children":{}}}},"/careers/apprentice/apprentice":{"label":"apprentice program","children":{}}}},"/careers/readme":{"label":"👋 join the dwarves","children":{}}}},"/opensource":{"label":"Opensource","children":{"/opensource/readme":{"label":"☀️ open source","children":{}}}},"/playbook":{"label":"Playbook","children":{"/playbook/operations":{"label":"Operations","children":{"/playbook/operations/culture-test":{"label":"culture test","children":{}},"/playbook/operations/checklists":{"label":"Checklists","children":{"/playbook/operations/checklists/leave-and-request-checklist":{"label":"leave request","children":{}},"/playbook/operations/checklists/offboarding-checklist":{"label":"offboarding","children":{}},"/playbook/operations/checklists/artifact-checklist":{"label":"back up artifact","children":{}},"/playbook/operations/checklists/project-archive":{"label":"project archive","children":{}},"/playbook/operations/checklists/project-case-study":{"label":"project case study","children":{}},"/playbook/operations/checklists/project-communication":{"label":"project communication","children":{}},"/playbook/operations/checklists/project-handover":{"label":"project handover","children":{}},"/playbook/operations/checklists/project-initialization":{"label":"project initialization","children":{}},"/playbook/operations/checklists/assets-checklist":{"label":"assets","children":{}},"/playbook/operations/checklists/billing-checklist":{"label":"billing","children":{}},"/playbook/operations/checklists/candidate-checklist":{"label":"candidate","children":{}},"/playbook/operations/checklists/consulting-contract-checklist":{"label":"consulting contract","children":{}},"/playbook/operations/checklists/hiring-checklist":{"label":"hiring","children":{}},"/playbook/operations/checklists/onboarding-checklist":{"label":"onboarding","children":{}},"/playbook/operations/checklists/unemployment-social-health-insurance":{"label":"unemployment, social, health insurance","children":{}},"/playbook/operations/checklists/vietnam-invoice-checklist":{"label":"vietnam invoice","children":{}}}},"/playbook/operations/how-to-conduct-delivery-reports":{"label":"how to conduct delivery reports","children":{}},"/playbook/operations/how-we-do-effective-planning-and-reporting":{"label":"how we do effective planning and reporting","children":{}},"/playbook/operations/project-schedule-delivery-guidelines":{"label":"project delivery schedule and guidelines","children":{}},"/playbook/operations/ogif":{"label":"OGIF - oh god it's friday","children":{}},"/playbook/operations/red-flags":{"label":"red flags","children":{}},"/playbook/operations/focus-on-software-delivery":{"label":"focus on software delivery","children":{}},"/playbook/operations/are-you-helping":{"label":"are you helping","children":{}},"/playbook/operations/the-inner-circle":{"label":"the inner circle","children":{}},"/playbook/operations/mbti-type-intj":{"label":"mbti type intj","children":{}},"/playbook/operations/mbti-type-istp":{"label":"mbti type istp","children":{}},"/playbook/operations/mbti-type-estj":{"label":"mbti type estj","children":{}},"/playbook/operations/mbti-type-istj":{"label":"mbti type istj","children":{}},"/playbook/operations/applying-myersbriggs-type-indicator-in-hr":{"label":"applying myersbriggs type indicator in hiring","children":{}},"/playbook/operations/the-four-preferences":{"label":"the four preferences","children":{}},"/playbook/operations/making-decision-as-a-team-member":{"label":"making decision as a team member","children":{}},"/playbook/operations/adjust-the-way-we-work-in-basecamp-style":{"label":"adjust the way we work in basecamp style","children":{}},"/playbook/operations/beyond-the-title":{"label":"beyond the title","children":{}},"/playbook/operations/go-the-extra-mile":{"label":"go the extra mile","children":{}},"/playbook/operations/the-dwarves-runs-by-ideas":{"label":"the dwarves runs by ideas","children":{}},"/playbook/operations/a-tips-of-hiring-dont":{"label":"a tips of hiring - do \u0026 don't","children":{}},"/playbook/operations/the-dwarves-culture-handbook":{"label":"the dwarves culture handbook","children":{}},"/playbook/operations/how-people-matter-should-work":{"label":"how people matter should work","children":{}},"/playbook/operations/delegation-and-believe-it-will-work":{"label":"delegation and believe it will work","children":{}},"/playbook/operations/constructive-feedback":{"label":"constructive feedback","children":{}},"/playbook/operations/transparency":{"label":"transparency","children":{}},"/playbook/operations/bric-a-brac":{"label":"bric a brac","children":{}},"/playbook/operations/account":{"label":"account","children":{}},"/playbook/operations/avoid-burn-out":{"label":"avoid burn out","children":{}},"/playbook/operations/writing-management-objectives-in-smart":{"label":"writing management objectives in smart","children":{}},"/playbook/operations/building-a-solid-high-performing-team":{"label":"building a solid high performing team","children":{}},"/playbook/operations/hiring-for-operations-team":{"label":"hiring for operations team","children":{}},"/playbook/operations/annual-bonus-for-sales":{"label":"annual bonus for sales","children":{}},"/playbook/operations/bunk-license-check":{"label":"bunk license check","children":{}},"/playbook/operations/collaboration-guidelines":{"label":"collaboration guidelines","children":{}},"/playbook/operations/compliance-check-process":{"label":"compliance check process","children":{}},"/playbook/operations/email-template":{"label":"Email Template","children":{"/playbook/operations/email-template/assignment-invitation-2":{"label":"assignment inviation (skip pre-assessment)","children":{}},"/playbook/operations/email-template/assignment-invitation":{"label":"assignment inviation","children":{}},"/playbook/operations/email-template/confirm-resume-date":{"label":"confirm employee's resume date day","children":{}},"/playbook/operations/email-template/farewell":{"label":"farewell letter","children":{}},"/playbook/operations/email-template/follow-up-onboarding-items":{"label":"follow-up onboarding items","children":{}},"/playbook/operations/email-template/hung-king-commemoration-day":{"label":"hung king commemoration day","children":{}},"/playbook/operations/email-template/information-about-resource-change":{"label":"inform about resource change","children":{}},"/playbook/operations/email-template/international-labour-day":{"label":"international labour day","children":{}},"/playbook/operations/email-template/interview-invitation":{"label":"interview invitation","children":{}},"/playbook/operations/email-template/milestone-sign-off":{"label":"milestone sign-off","children":{}},"/playbook/operations/email-template/national-day":{"label":"national day","children":{}},"/playbook/operations/email-template/new-year-day":{"label":"new year day","children":{}},"/playbook/operations/email-template/offer-letter":{"label":"offer letter","children":{}},"/playbook/operations/email-template/referral-bonus-confirmation-note":{"label":"referral bonus confirmation note","children":{}},"/playbook/operations/email-template/rejection-email":{"label":"rejection","children":{}},"/playbook/operations/email-template/salary-increment":{"label":"salary increment announcement","children":{}},"/playbook/operations/email-template/tet-holiday":{"label":"tet holiday","children":{}},"/playbook/operations/email-template/thank-you-letter":{"label":"thank you letter","children":{}},"/playbook/operations/email-template/welcome-onboard":{"label":"welcome onboard","children":{}},"/playbook/operations/email-template/welcome-to-dwarves-update":{"label":"welcome to dwarves updates","children":{}}}},"/playbook/operations/naming-convention":{"label":"naming convention","children":{}},"/playbook/operations/setup-email-template":{"label":"setup email template in gmail","children":{}},"/playbook/operations/delegate-work-not-responsibility":{"label":"delegate work not responsibility","children":{}},"/playbook/operations/types-of-employees":{"label":"types of employees","children":{}},"/playbook/operations/hiring-approach":{"label":"hiring approach","children":{}},"/playbook/operations/the-okr":{"label":"the okr","children":{}},"/playbook/operations/our-metrics-for-performance-review":{"label":"our metrics for performance review","children":{}},"/playbook/operations/make-remote-working-works":{"label":"make remote working works","children":{}},"/playbook/operations/blocking-distraction":{"label":"blocking distraction","children":{}},"/playbook/operations/effective-meeting":{"label":"effective meeting","children":{}},"/playbook/operations/our-policy-for-remote-working":{"label":"our policy for remote working","children":{}}}},"/playbook/business":{"label":"Business","children":{"/playbook/business/pricing-model-bill-by-hours":{"label":"pricing model: bill by hours","children":{}},"/playbook/business/invoice":{"label":"invoice","children":{}},"/playbook/business/nda":{"label":"NDA","children":{}},"/playbook/business/collaboration-guideline":{"label":"collaboration guideline","children":{}},"/playbook/business/df-workflow":{"label":"dwarves workflow","children":{}},"/playbook/business/fbsc":{"label":"fbsc","children":{}},"/playbook/business/how-to-work-with-clients":{"label":"how to work with clients","children":{}},"/playbook/business/service-feedbacks":{"label":"service feedbacks","children":{}},"/playbook/business/setting-the-budget":{"label":"setting the budget","children":{}},"/playbook/business/fixed-budget-scope-controlled":{"label":"fixed budget scope controlled","children":{}},"/playbook/business/the-adjacent-possible":{"label":"the adjacent possible","children":{}}}},"/playbook/engineering":{"label":"Engineering","children":{"/playbook/engineering/estimation-guidelines":{"label":"estimation guidelines","children":{}},"/playbook/engineering/presentation":{"label":"monitoring","children":{}},"/playbook/engineering/repo-icon":{"label":"release","children":{}}}},"/playbook/design":{"label":"Design","children":{"/playbook/design/design-system":{"label":"lean-canvas","children":{}},"/playbook/design/ia":{"label":"NDA","children":{}},"/playbook/design/ix":{"label":"ia","children":{}},"/playbook/design/aarrr":{"label":"AARRR","children":{}},"/playbook/design/design-sprint":{"label":"design sprint","children":{}},"/playbook/design/lean-canvas":{"label":"lean canvas","children":{}},"/playbook/design/prototype":{"label":"low-fidelity prototype: UI design","children":{}},"/playbook/design/ui":{"label":"UI","children":{}},"/playbook/design/ux":{"label":"UX","children":{}},"/playbook/design/wireframe":{"label":"wireframe","children":{}}}},"/playbook/readme":{"label":"playbook","children":{}}}},"/fund":{"label":"Fund","children":{"/fund/ventures-fund-1":{"label":"dwarves ventures fund 1","children":{}},"/fund/ventures-fund-0":{"label":"dwarves ventures fund 0","children":{}}}},"/rfc":{"label":"RFC","children":{"/rfc/readme":{"label":"rfcs","children":{}},"/rfc/000-template":{"label":"000 RFC template","children":{}}}},"/updates":{"label":"Updates","children":{"/updates/digest":{"label":"Digest","children":{"/updates/digest/174-2025-whats-new-march":{"label":"what's new in march 2025","children":{}},"/updates/digest/173-2025-whats-new-february":{"label":"what's new in february 2025","children":{}},"/updates/digest/15-new-year-gathering":{"label":"#15 new year gathering","children":{}},"/updates/digest/2024-in-review":{"label":"2024 in review","children":{}},"/updates/digest/172-2024-whats-new-december":{"label":"what's new in december 2024","children":{}},"/updates/digest/2024-summit-building-bonds-our-way":{"label":"summit 2024: building bonds our way","children":{}},"/updates/digest/171-2024-whats-new-november":{"label":"what's new in november 2024","children":{}},"/updates/digest/170-2024-whats-new-oct":{"label":"what's new in october 2024","children":{}},"/updates/digest/169-2024-whats-new-september":{"label":"what's new in september 2024","children":{}},"/updates/digest/2024-navigating-changes":{"label":"navigating changes","children":{}},"/updates/digest/14-back-to-the-office":{"label":"#14 hybrid work harmony","children":{}},"/updates/digest/168-2024-whats-new-august":{"label":"what's new in august 2024","children":{}},"/updates/digest/167-2024-whats-new-july":{"label":"what's new in july 2024","children":{}},"/updates/digest/13-more-than-lines-of-code":{"label":"#13 more than lines of code","children":{}},"/updates/digest/12-summer-moments":{"label":"#12 summer moments","children":{}},"/updates/digest/166-2024-whats-new-june":{"label":"what's new in june 2024","children":{}},"/updates/digest/2024-semi-annual-review":{"label":"state of dwarves: 2024 semi-annual review","children":{}},"/updates/digest/11-come-grow-with-us":{"label":"#11 come grow with us","children":{}},"/updates/digest/10-from-lean-to-learner":{"label":"#10 from lean to learner","children":{}},"/updates/digest/165-2024-whats-new-may":{"label":"what's new in may 2024","children":{}},"/updates/digest/2024-community-meet-up":{"label":"dwarves' 2nd community offline meet-up","children":{}},"/updates/digest/9-a-little-more-speed-for-summer":{"label":"#9 a little more speed for summer","children":{}},"/updates/digest/8-then-came-the-last-days-of-may":{"label":"#8 then came the last days of may","children":{}},"/updates/digest/7-a-journey-through-time":{"label":"#7 a journey through time","children":{}},"/updates/digest/164-2024-whats-new-april":{"label":"what's new in april 2024","children":{}},"/updates/digest/6-stay-for-the-culture":{"label":"#6 come for the conversation, stay for the culture","children":{}},"/updates/digest/5-delay-the-gratification":{"label":"#5 endure the hardship, delay the gratification","children":{}},"/updates/digest/4-finding-your-authentic-tribe":{"label":"#4 finding your authentic tribe","children":{}},"/updates/digest/3-we-all-start-somewhere":{"label":"#3 we all start somewhere","children":{}},"/updates/digest/2-walk-around-learn-around":{"label":"#2 walk around learn around","children":{}},"/updates/digest/1-what-do-you-stand-for":{"label":"#1 what do you stand for?","children":{}},"/updates/digest/163-2024-whats-new-march":{"label":"what's new in march 2024","children":{}},"/updates/digest/162-2024-whats-new-february":{"label":"what's new in february 2024","children":{}},"/updates/digest/161-2024-whats-new-january":{"label":"what's new in january 2024","children":{}},"/updates/digest/160-2023-whats-new-december":{"label":"what's new in december 2023","children":{}},"/updates/digest/readme":{"label":"changelog","children":{}},"/updates/digest/digest":{"label":"digest","children":{}},"/updates/digest/159-2023-whats-new-november":{"label":"what's new in november 2023","children":{}},"/updates/digest/158-2023-whats-new-october":{"label":"what's new in october 2023","children":{}},"/updates/digest/2023-happy":{"label":"happy 2023","children":{}},"/updates/digest/2022-dwarves-of-the-year":{"label":"dwarves of the year 2022","children":{}},"/updates/digest/2022-in-review":{"label":"2022 in review","children":{}},"/updates/digest/2022-summit-engineering-a-good-time":{"label":"summit 2022: engineering a good time","children":{}},"/updates/digest/road-to-100":{"label":"road to 100","children":{}},"/updates/digest/2022-whats-new-may":{"label":"what's new in may 2022","children":{}},"/updates/digest/2022-whats-new-january":{"label":"what's new in january 2022","children":{}},"/updates/digest/2021-whats-new-december":{"label":"what's new in december 2021","children":{}},"/updates/digest/2021-dwarves-of-the-year":{"label":"dwarves of the year 2021","children":{}},"/updates/digest/2021-whats-new-july":{"label":"what's new in july 2021","children":{}},"/updates/digest/2020-in-review":{"label":"2020 in review","children":{}},"/updates/digest/2021-in-review":{"label":"2021 in review","children":{}},"/updates/digest/2019-in-review":{"label":"2019 in review","children":{}},"/updates/digest/2018-in-review":{"label":"2018 in review","children":{}}}},"/updates/ogif":{"label":"OGIF","children":{"/updates/ogif/41-20250314":{"label":"#41 icy-btc, github bot, mcp-db, pocket turing","children":{}},"/updates/ogif/39-20250207":{"label":"##39 frontend report, db scaling, AI workflow","children":{}},"/updates/ogif/37-20241227":{"label":"##37 AI fine-tuning, data archiving, datalakes","children":{}},"/updates/ogif/28-20241018":{"label":"#28 go sync.map, AI ux, yelp ai, LLM patterns, git analysis","children":{}},"/updates/ogif/27-20241011":{"label":"#27 go weekly, frontend, AI ux, finite automata","children":{}},"/updates/ogif/26-20241004":{"label":"#26 design insights, go tools, trading app, chatbots, essays","children":{}},"/updates/ogif/25-20240927":{"label":"#25 team updates, hybrid work, AI insights, go weekly","children":{}},"/updates/ogif/24-20240920":{"label":"#24 go weekly, AI workflows, team AI demo, figma-ui with claude","children":{}},"/updates/ogif/23-20240913":{"label":"#23 go weekly, fe report, hybrid work, AI agents","children":{}},"/updates/ogif/22-20240906":{"label":"#22 hybrid work, tech report, go weekly, AI demo","children":{}},"/updates/ogif/21-20240830":{"label":"#21 community engagement, go weekly, journey of thought for prompt engineering","children":{}},"/updates/ogif/20-20240823":{"label":"#20 go weekly, dynamic objects, devbox, LLM tracing, cursor AI","children":{}},"/updates/ogif/19-20240821":{"label":"#19 go weekly, UI design, file sharing, dify AI","children":{}},"/updates/ogif/18-20240809":{"label":"#18 go weekly, rag, ui, fe updates","children":{}},"/updates/ogif/17-20240802":{"label":"#17 community call july, c4 model, interview life in the us","children":{}},"/updates/ogif/16-20240726":{"label":"#16 go weekly, dune query, AI voice clone, RAG re-ranking","children":{}},"/updates/ogif/15-20240719":{"label":"#15 AI supervisors, local-first software, code completion, bot commands","children":{}},"/updates/ogif/14-20240712":{"label":"#14 generic collections, pricing models, and OGIF summarizer","children":{}},"/updates/ogif/13-20240705":{"label":"#13 go weekly updates, radix sort, human feedback mechanism, and effective chatgpt usage","children":{}},"/updates/ogif/12-20240628":{"label":"#12 june updates, go performance, ebpf, pgo, multimodal RAG","children":{}},"/updates/ogif/11-20240621":{"label":"#11 design patterns: template method \u0026 visitor, radix sort, and weekly tech commentary","children":{}},"/updates/ogif/10-20240614":{"label":"#10 behavioral patterns and map content organization","children":{}},"/updates/ogif/9-20240607":{"label":"#9 what's next for june and behavior design patterns","children":{}},"/updates/ogif/7-20240517":{"label":"#7 echelon expo, programming patterns, and moonlighting","children":{}},"/updates/ogif/6-20240510":{"label":"#6 factory pattern, erlang state machines, and trading process","children":{}},"/updates/ogif/5-20240503":{"label":"#5 singapore market report, c4 modelling, memo's nested sidebar","children":{}},"/updates/ogif/4-20240426":{"label":"#4 dca, devbox","children":{}},"/updates/ogif/3-20240419":{"label":"#3 generative ai, tokenomics, and finance talks","children":{}},"/updates/ogif/2-20240412":{"label":"#2 devbox as the new docker, security standards, and understanding liquidity","children":{}},"/updates/ogif/1-20240405":{"label":"#1 markdown presentations, research pipeline, screenshots how-to","children":{}},"/updates/ogif/readme":{"label":"OGIF - oh god it's friday","children":{}}}},"/updates/changelog":{"label":"Changelog","children":{"/updates/changelog/2024-10-25-knowledge-base":{"label":"build your knowledge base","children":{}},"/updates/changelog/2024-09-13-dwarve-updates-ai-llm":{"label":"the stage of AI and LLM at dwarves","children":{}},"/updates/changelog/readme":{"label":"dwarves updates","children":{}},"/updates/changelog/2023-09-12-growth-stages":{"label":"the stage of growth at dwarves","children":{}},"/updates/changelog/2022-08-26-the-next-leading-chairs":{"label":"the next leading chairs","children":{}},"/updates/changelog/2022-06-26-blockchain-and-data":{"label":"the future is blockchain and data","children":{}},"/updates/changelog/2022-03-31-hiring-stages":{"label":"the stages of hiring at dwarves","children":{}},"/updates/changelog/2021-12-30-2021-in-review":{"label":"it's a wrap: 2021 in review","children":{}},"/updates/changelog/2021-12-01-engineering-org-structure":{"label":"engineering organizational structure","children":{}},"/updates/changelog/2021-10-31-path-to-growth":{"label":"the path to growth at dwarves","children":{}},"/updates/changelog/2021-09-29-engineer-performance-review":{"label":"engineer performance review","children":{}},"/updates/changelog/2021-08-23-project-compliance":{"label":"project compliance","children":{}},"/updates/changelog/2021-07-11-dalat-office":{"label":"da lat office","children":{}},"/updates/changelog/2021-06-10-dwarves-updates":{"label":"dwarves updates","children":{}}}},"/updates/wala":{"label":"WALA","children":{"/updates/wala/001-43-factory":{"label":"43 factory","children":{}},"/updates/wala/002-dzs-media":{"label":"dzs media","children":{}},"/updates/wala/003-sp-group":{"label":"sp group","children":{}},"/updates/wala/readme":{"label":"WALA","children":{}}}}}}}},"/tags":{"label":"Popular Tags","children":{"/tags/earn":{"label":"#earn","children":{},"count":8},"/tags/productivity":{"label":"#productivity","children":{},"count":8},"/tags/quality":{"label":"#quality","children":{},"count":1},"/tags/open-source":{"label":"#open-source","children":{},"count":4},"/tags/liquidity":{"label":"#liquidity","children":{},"count":2},"/tags/ai":{"label":"#AI","children":{},"count":58},"/tags/hiring":{"label":"#hiring","children":{},"count":23},"/tags/case-study":{"label":"#case-study","children":{},"count":29},"/tags/handbook":{"label":"#handbook","children":{},"count":46},"/tags/business":{"label":"#business","children":{},"count":8},"/tags/growth":{"label":"#growth","children":{},"count":2},"/tags/software-development":{"label":"#software-development","children":{},"count":1},"/tags/database-management":{"label":"#database-management","children":{},"count":1},"/tags/icy":{"label":"#icy","children":{},"count":14},"/tags/career":{"label":"#career","children":{},"count":39},"/tags/fullstack":{"label":"#fullstack","children":{},"count":2},"/tags/ux-ui":{"label":"#ux-ui","children":{},"count":13},"/tags/product-design":{"label":"#product-design","children":{},"count":7},"/tags/report":{"label":"#report","children":{},"count":8},"/tags/checklist":{"label":"#checklist","children":{},"count":17},"/tags/presentation":{"label":"#presentation","children":{},"count":1},"/tags/database":{"label":"#database","children":{},"count":8},"/tags/sql":{"label":"#sql","children":{},"count":4},"/tags/data-modeling":{"label":"#data-modeling","children":{},"count":1},"/tags/data-engineering":{"label":"#data-engineering","children":{},"count":5},"/tags/system-design":{"label":"#system-design","children":{},"count":2},"/tags/architecture":{"label":"#architecture","children":{},"count":4},"/tags/etl":{"label":"#etl","children":{},"count":3},"/tags/automata":{"label":"#automata","children":{},"count":1},"/tags/fintech":{"label":"#fintech","children":{},"count":16},"/tags/mobile":{"label":"#mobile","children":{},"count":3},"/tags/go":{"label":"#go","children":{},"count":5},"/tags/error":{"label":"#error","children":{},"count":1},"/tags/community":{"label":"#community","children":{},"count":42},"/tags/startup":{"label":"#startup","children":{},"count":9},"/tags/shares":{"label":"#shares","children":{},"count":1},"/tags/founder":{"label":"#founder","children":{},"count":1},"/tags/entertainment":{"label":"#entertainment","children":{},"count":1},"/tags/culture":{"label":"#culture","children":{},"count":10},"/tags/test":{"label":"#test","children":{},"count":1},"/tags/life-at-dwarves, ai-developer, hybrid-work":{"label":"#life-at-dwarves, ai-developer, hybrid-work","children":{},"count":1},"/tags/hybrid-working":{"label":"#hybrid-working","children":{},"count":2},"/tags/guide":{"label":"#guide","children":{},"count":11},"/tags/security":{"label":"#security","children":{},"count":10},"/tags/reward":{"label":"#reward","children":{},"count":3},"/tags/team":{"label":"#team","children":{},"count":38},"/tags/design":{"label":"#design","children":{},"count":31},"/tags/ux":{"label":"#UX","children":{},"count":2},"/tags/directory-structure":{"label":"#directory-structure","children":{},"count":2},"/tags/file-management":{"label":"#file-management","children":{},"count":2},"/tags/file-system":{"label":"#file-system","children":{},"count":2},"/tags/permissions":{"label":"#permissions","children":{},"count":1},"/tags/database-modelling":{"label":"#database-modelling","children":{},"count":1},"/tags/nda":{"label":"#NDA","children":{},"count":1},"/tags/compliance":{"label":"#compliance","children":{},"count":2},"/tags/people":{"label":"#people","children":{},"count":27},"/tags/operations":{"label":"#operations","children":{},"count":73},"/tags/llm":{"label":"#LLM","children":{},"count":76},"/tags/rag":{"label":"#RAG","children":{},"count":5},"/tags/search":{"label":"#search","children":{},"count":1},"/tags/evaluation":{"label":"#evaluation","children":{},"count":3},"/tags/delivery":{"label":"#delivery","children":{},"count":3},"/tags/reporting":{"label":"#reporting","children":{},"count":1},"/tags/project":{"label":"#project","children":{},"count":16},"/tags/billbyhours":{"label":"#billbyhours","children":{},"count":1},"/tags/engineering":{"label":"#engineering","children":{},"count":56},"/tags/subscription":{"label":"#subscription","children":{},"count":1},"/tags/pricing":{"label":"#pricing","children":{},"count":1},"/tags/product":{"label":"#product","children":{},"count":1},"/tags/blockchain":{"label":"#blockchain","children":{},"count":50},"/tags/evm":{"label":"#evm","children":{},"count":5},"/tags/foundry":{"label":"#foundry","children":{},"count":2},"/tags/search-engine":{"label":"#search-engine","children":{},"count":1},"/tags/duckdb":{"label":"#duckdb","children":{},"count":3},"/tags/transformers.js":{"label":"#transformers.js","children":{},"count":1},"/tags/hybrid-search":{"label":"#hybrid-search","children":{},"count":1},"/tags/erlang":{"label":"#erlang","children":{},"count":1},"/tags/elixir":{"label":"#elixir","children":{},"count":5},"/tags/fsm":{"label":"#fsm","children":{},"count":1},"/tags/design-pattern":{"label":"#design-pattern","children":{},"count":9},"/tags/gang-of-four":{"label":"#gang-of-four","children":{},"count":9},"/tags/observer-pattern":{"label":"#observer-pattern","children":{},"count":1},"/tags/behavior-pattern":{"label":"#behavior-pattern","children":{},"count":2},"/tags/visitor-design-pattern":{"label":"#visitor-design-pattern","children":{},"count":1},"/tags/strategy-design-pattern":{"label":"#strategy-design-pattern","children":{},"count":1},"/tags/market-report":{"label":"#market-report","children":{},"count":32},"/tags/ogif":{"label":"#OGIF","children":{},"count":31},"/tags/guidelines":{"label":"#guidelines","children":{},"count":3},"/tags/feedback":{"label":"#feedback","children":{},"count":2},"/tags/mechanism":{"label":"#mechanism","children":{},"count":1},"/tags/local-first":{"label":"#local-first","children":{},"count":1},"/tags/crdt":{"label":"#crdt","children":{},"count":2},"/tags/data-synchronization":{"label":"#data-synchronization","children":{},"count":1},"/tags/data-ownership":{"label":"#data-ownership","children":{},"count":1},"/tags/real-time-collaboration":{"label":"#real-time-collaboration","children":{},"count":1},"/tags/rust":{"label":"#rust","children":{},"count":10},"/tags/trait":{"label":"#trait","children":{},"count":1},"/tags/error-handling":{"label":"#error-handling","children":{},"count":1},"/tags/data-structure":{"label":"#data-structure","children":{},"count":1},"/tags/bloom-filter":{"label":"#bloom-filter","children":{},"count":1},"/tags/big-o":{"label":"#big-o","children":{},"count":1},"/tags/behavioral-pattern":{"label":"#behavioral-pattern","children":{},"count":1},"/tags/golang":{"label":"#golang","children":{},"count":45},"/tags/behavior-patterns":{"label":"#behavior-patterns","children":{},"count":2},"/tags/algorithms":{"label":"#algorithms","children":{},"count":1},"/tags/sorting":{"label":"#sorting","children":{},"count":1},"/tags/network":{"label":"#network","children":{},"count":2},"/tags/machine-learning":{"label":"#machine-learning","children":{},"count":2},"/tags/zettelkasten":{"label":"#zettelkasten","children":{},"count":1},"/tags/prompt":{"label":"#prompt","children":{},"count":1},"/tags/chatgpt":{"label":"#chatgpt","children":{},"count":1},"/tags/solana":{"label":"#solana","children":{},"count":7},"/tags/amm":{"label":"#amm","children":{},"count":1},"/tags/memo":{"label":"#memo","children":{},"count":14},"/tags/instructions":{"label":"#instructions","children":{},"count":10},"/tags/guideline":{"label":"#guideline","children":{},"count":15},"/tags/ops":{"label":"#ops","children":{},"count":2},"/tags/nft":{"label":"#nft","children":{},"count":3},"/tags/workflow":{"label":"#workflow","children":{},"count":5},"/tags/recording":{"label":"#recording","children":{},"count":1},"/tags/history":{"label":"#history","children":{},"count":1},"/tags/creational-design-pattern":{"label":"#creational-design-pattern","children":{},"count":1},"/tags/moc":{"label":"#moc","children":{},"count":3},"/tags/software-design":{"label":"#software-design","children":{},"count":2},"/tags/software-architecture":{"label":"#software-architecture","children":{},"count":3},"/tags/graphical-notation":{"label":"#graphical-notation","children":{},"count":2},"/tags/techecosystem":{"label":"#techecosystem","children":{},"count":1},"/tags/summit":{"label":"#summit","children":{},"count":4},"/tags/crypto":{"label":"#crypto","children":{},"count":1},"/tags/content":{"label":"#content","children":{},"count":6},"/tags/investment":{"label":"#investment","children":{},"count":1},"/tags/personal-finance":{"label":"#personal-finance","children":{},"count":1},"/tags/dfg":{"label":"#dfg","children":{},"count":6},"/tags/tutorial":{"label":"#tutorial","children":{},"count":7},"/tags/standardization":{"label":"#standardization","children":{},"count":1},"/tags/work-adoption":{"label":"#work-adoption","children":{},"count":1},"/tags/code of conduct":{"label":"#code of conduct","children":{},"count":1},"/tags/research":{"label":"#research","children":{},"count":3},"/tags/field-notes":{"label":"#field-notes","children":{},"count":1},"/tags/innovation":{"label":"#innovation","children":{},"count":2},"/tags/radar":{"label":"#radar","children":{},"count":9},"/tags/bounty":{"label":"#bounty","children":{},"count":4},"/tags/communications":{"label":"#communications","children":{},"count":3},"/tags/token":{"label":"#token","children":{},"count":2},"/tags/brain":{"label":"#brain","children":{},"count":1},"/tags/knowledge-base":{"label":"#knowledge-base","children":{},"count":1},"/tags/engineering/data":{"label":"#engineering/data","children":{},"count":5},"/tags/data-pipeline":{"label":"#data-pipeline","children":{},"count":1},"/tags/vector-database":{"label":"#vector-database","children":{},"count":4},"/tags/payment":{"label":"#payment","children":{},"count":2},"/tags/consulting":{"label":"#consulting","children":{},"count":24},"/tags/partners":{"label":"#partners","children":{},"count":1},"/tags/life-at-dwarves, community-contributor, techie-project":{"label":"#life-at-dwarves, community-contributor, techie-project","children":{},"count":1},"/tags/brainery":{"label":"#brainery","children":{},"count":2},"/tags/devops":{"label":"#devops","children":{},"count":6},"/tags/google-cloud":{"label":"#google-cloud","children":{},"count":1},"/tags/google-data-studio":{"label":"#google-data-studio","children":{},"count":1},"/tags/google-data-fusion":{"label":"#google-data-fusion","children":{},"count":1},"/tags/reliability":{"label":"#reliability","children":{},"count":2},"/tags/cdap":{"label":"#cdap","children":{},"count":1},"/tags/data":{"label":"#data","children":{},"count":14},"/tags/google-dataproc":{"label":"#google-dataproc","children":{},"count":1},"/tags/hadoop":{"label":"#hadoop","children":{},"count":2},"/tags/streaming":{"label":"#streaming","children":{},"count":1},"/tags/life-at-dwarves, alumni, career-growth":{"label":"#life-at-dwarves, alumni, career-growth","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, continuous-learning":{"label":"#life-at-dwarves, backend-engineer, continuous-learning","children":{},"count":1},"/tags/ecommerce":{"label":"#ecommerce","children":{},"count":2},"/tags/dropshipping":{"label":"#dropshipping","children":{},"count":1},"/tags/dwarves":{"label":"#dwarves","children":{},"count":22},"/tags/work":{"label":"#work","children":{},"count":18},"/tags/internal":{"label":"#internal","children":{},"count":11},"/tags/discussion":{"label":"#discussion","children":{},"count":6},"/tags/event":{"label":"#event","children":{},"count":7},"/tags/labs":{"label":"#labs","children":{},"count":24},"/tags/catchup":{"label":"#catchup","children":{},"count":5},"/tags/home":{"label":"#home","children":{},"count":2},"/tags/tauri":{"label":"#tauri","children":{},"count":1},"/tags/htmx":{"label":"#htmx","children":{},"count":2},"/tags/frontend":{"label":"#frontend","children":{},"count":68},"/tags/life-at-dwarves, backend-engineer, community-building":{"label":"#life-at-dwarves, backend-engineer, community-building","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, personal-development":{"label":"#life-at-dwarves, backend-engineer, personal-development","children":{},"count":1},"/tags/backend":{"label":"#backend","children":{},"count":5},"/tags/estimation":{"label":"#estimation","children":{},"count":1},"/tags/code-generation":{"label":"#code-generation","children":{},"count":1},"/tags/typesafe":{"label":"#typesafe","children":{},"count":1},"/tags/internship":{"label":"#internship","children":{},"count":3},"/tags/life-at-dwarves, backend-engineer, teamwork":{"label":"#life-at-dwarves, backend-engineer, teamwork","children":{},"count":1},"/tags/discord":{"label":"#discord","children":{},"count":38},"/tags/workshop":{"label":"#workshop","children":{},"count":1},"/tags/demo":{"label":"#demo","children":{},"count":1},"/tags/protocol":{"label":"#protocol","children":{},"count":2},"/tags/performance-review":{"label":"#performance-review","children":{},"count":2},"/tags/assessment":{"label":"#assessment","children":{},"count":1},"/tags/knowledge":{"label":"#knowledge","children":{},"count":2},"/tags/tech-radar":{"label":"#tech-radar","children":{},"count":1},"/tags/evaluating-tech":{"label":"#evaluating-tech","children":{},"count":1},"/tags/process":{"label":"#process","children":{},"count":9},"/tags/updates":{"label":"#updates","children":{},"count":36},"/tags/life-at-dwarves, product-executive, personal-growth":{"label":"#life-at-dwarves, product-executive, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-member":{"label":"#life-at-dwarves, frontend-engineer, community-member","children":{},"count":1},"/tags/distributed-system":{"label":"#distributed-system","children":{},"count":1},"/tags/data-types":{"label":"#data-types","children":{},"count":1},"/tags/data-structures":{"label":"#data-structures","children":{},"count":2},"/tags/life-at-dwarves, communication-specialist, remote-work":{"label":"#life-at-dwarves, communication-specialist, remote-work","children":{},"count":1},"/tags/life-at-dwarves, qa-engineer, mentorship":{"label":"#life-at-dwarves, qa-engineer, mentorship","children":{},"count":1},"/tags/client":{"label":"#client","children":{},"count":6},"/tags/life-at-dwarves, qa-engineer, quality-standards":{"label":"#life-at-dwarves, qa-engineer, quality-standards","children":{},"count":1},"/tags/guidline":{"label":"#guidline","children":{},"count":1},"/tags/performance":{"label":"#performance","children":{},"count":30},"/tags/playbook":{"label":"#playbook","children":{},"count":4},"/tags/software":{"label":"#software","children":{},"count":8},"/tags/framework":{"label":"#framework","children":{},"count":6},"/tags/learning":{"label":"#learning","children":{},"count":4},"/tags/system design":{"label":"#system design","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, learning-culture":{"label":"#life-at-dwarves, backend-engineer, learning-culture","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, mentorship":{"label":"#life-at-dwarves, backend-engineer, mentorship","children":{},"count":3},"/tags/life-at-dwarves":{"label":"#life-at-dwarves","children":{},"count":2},"/tags/backend-engineer":{"label":"#backend-engineer","children":{},"count":1},"/tags/remote-work":{"label":"#remote-work","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-building":{"label":"#life-at-dwarves, frontend-engineer, community-building","children":{},"count":1},"/tags/enterprise":{"label":"#enterprise","children":{},"count":10},"/tags/australia":{"label":"#australia","children":{},"count":1},"/tags/sargable-queries":{"label":"#sargable-queries","children":{},"count":1},"/tags/zookeeper":{"label":"#zookeeper","children":{},"count":1},"/tags/kafka":{"label":"#kafka","children":{},"count":1},"/tags/sequential-reads":{"label":"#sequential-reads","children":{},"count":1},"/tags/sequential-writes":{"label":"#sequential-writes","children":{},"count":1},"/tags/random-reads":{"label":"#random-reads","children":{},"count":1},"/tags/random-writes":{"label":"#random-writes","children":{},"count":1},"/tags/url-redirect":{"label":"#url-redirect","children":{},"count":1},"/tags/url-rewrite":{"label":"#url-rewrite","children":{},"count":1},"/tags/http":{"label":"#http","children":{},"count":1},"/tags/seo":{"label":"#SEO","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-learning":{"label":"#life-at-dwarves, frontend-engineer, community-learning","children":{},"count":1},"/tags/life-at-dwarves, engineer, work-culture":{"label":"#life-at-dwarves, engineer, work-culture","children":{},"count":1},"/tags/dx":{"label":"#dx","children":{},"count":1},"/tags/life-at-dwarves, software-engineer, mentorship":{"label":"#life-at-dwarves, software-engineer, mentorship","children":{},"count":1},"/tags/machine learning":{"label":"#machine learning","children":{},"count":1},"/tags/r\u0026d":{"label":"#r\u0026d","children":{},"count":1},"/tags/web":{"label":"#web","children":{},"count":9},"/tags/micro-frontend":{"label":"#micro-frontend","children":{},"count":3},"/tags/tool":{"label":"#tool","children":{},"count":3},"/tags/technique":{"label":"#technique","children":{},"count":9},"/tags/vietnam":{"label":"#vietnam","children":{},"count":1},"/tags/write-heavy":{"label":"#write-heavy","children":{},"count":1},"/tags/inventory-platform":{"label":"#inventory-platform","children":{},"count":1},"/tags/scalability":{"label":"#scalability","children":{},"count":1},"/tags/doordash":{"label":"#doordash","children":{},"count":1},"/tags/low-latency":{"label":"#low-latency","children":{},"count":1},"/tags/observability":{"label":"#observability","children":{},"count":5},"/tags/engineer":{"label":"#engineer","children":{},"count":2},"/tags/teamwork":{"label":"#teamwork","children":{},"count":2},"/tags/leadership":{"label":"#leadership","children":{},"count":4},"/tags/multi-column-index":{"label":"#multi-column-index","children":{},"count":1},"/tags/index":{"label":"#index","children":{},"count":1},"/tags/composite-index":{"label":"#composite-index","children":{},"count":1},"/tags/react":{"label":"#react","children":{},"count":15},"/tags/hooks":{"label":"#hooks","children":{},"count":2},"/tags/components":{"label":"#components","children":{},"count":1},"/tags/scrum":{"label":"#scrum","children":{},"count":2},"/tags/technicaldebt":{"label":"#technicaldebt","children":{},"count":1},"/tags/projectmanagement":{"label":"#projectmanagement","children":{},"count":1},"/tags/email":{"label":"#email","children":{},"count":22},"/tags/decoder":{"label":"#decoder","children":{},"count":1},"/tags/json":{"label":"#json","children":{},"count":1},"/tags/materialized-view":{"label":"#materialized-view","children":{},"count":1},"/tags/data-warehouse":{"label":"#data-warehouse","children":{},"count":1},"/tags/mapreduce":{"label":"#mapreduce","children":{},"count":1},"/tags/distributed":{"label":"#distributed","children":{},"count":3},"/tags/form":{"label":"#form","children":{},"count":1},"/tags/uilibraries":{"label":"#uilibraries","children":{},"count":1},"/tags/migrations":{"label":"#migrations","children":{},"count":1},"/tags/agile":{"label":"#agile","children":{},"count":6},"/tags/behavior-driven-development":{"label":"#behavior-driven-development","children":{},"count":1},"/tags/testing":{"label":"#testing","children":{},"count":4},"/tags/ubiquitous-language":{"label":"#ubiquitous-language","children":{},"count":1},"/tags/forward-proxy":{"label":"#forward-proxy","children":{},"count":1},"/tags/apprenticeship":{"label":"#apprenticeship","children":{},"count":3},"/tags/life-at-dwarves, apprenticeship, backend-engineer":{"label":"#life-at-dwarves, apprenticeship, backend-engineer","children":{},"count":1},"/tags/remote":{"label":"#remote","children":{},"count":12},"/tags/showcase":{"label":"#showcase","children":{},"count":1},"/tags/practice":{"label":"#practice","children":{},"count":7},"/tags/life-at-dwarves, backend-engineer, golang":{"label":"#life-at-dwarves, backend-engineer, golang","children":{},"count":1},"/tags/life-at-dwarves, operations, techie-story":{"label":"#life-at-dwarves, operations, techie-story","children":{},"count":1},"/tags/life-at-dwarves, devops-engineer, personal-growth":{"label":"#life-at-dwarves, devops-engineer, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, senior-engineer, mentorship":{"label":"#life-at-dwarves, senior-engineer, mentorship","children":{},"count":1},"/tags/swap":{"label":"#swap","children":{},"count":2},"/tags/quant":{"label":"#quant","children":{},"count":1},"/tags/radio":{"label":"#radio","children":{},"count":3},"/tags/writing":{"label":"#writing","children":{},"count":1},"/tags/english":{"label":"#english","children":{},"count":1},"/tags/life-at-dwarves, data-engineer, remote-work":{"label":"#life-at-dwarves, data-engineer, remote-work","children":{},"count":1},"/tags/apprentice":{"label":"#apprentice","children":{},"count":1},"/tags/life-at-dwarves, ui-designer, design-communication":{"label":"#life-at-dwarves, ui-designer, design-communication","children":{},"count":1},"/tags/life-at-dwarves, engineering-manager, mentorship":{"label":"#life-at-dwarves, engineering-manager, mentorship","children":{},"count":1},"/tags/meeting":{"label":"#meeting","children":{},"count":4},"/tags/us":{"label":"#us","children":{},"count":4},"/tags/funding":{"label":"#funding","children":{},"count":2},"/tags/ventures":{"label":"#ventures","children":{},"count":3},"/tags/mbti":{"label":"#mbti","children":{},"count":6},"/tags/intj":{"label":"#intj","children":{},"count":1},"/tags/istp":{"label":"#istp","children":{},"count":1},"/tags/estj":{"label":"#estj","children":{},"count":1},"/tags/istj":{"label":"#istj","children":{},"count":1},"/tags/personalities":{"label":"#personalities","children":{},"count":1},"/tags/management":{"label":"#management","children":{},"count":4},"/tags/fnb":{"label":"#fnb","children":{},"count":2},"/tags/early-stage":{"label":"#early-stage","children":{},"count":3},"/tags/design-thinking":{"label":"#design-thinking","children":{},"count":2},"/tags/healthcare":{"label":"#healthcare","children":{},"count":1},"/tags/browser-extension":{"label":"#browser-extension","children":{},"count":2},"/tags/git":{"label":"#git","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, growth-mindset":{"label":"#life-at-dwarves, software-engineer, growth-mindset","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, career-change":{"label":"#life-at-dwarves, backend-engineer, career-change","children":{},"count":1},"/tags/marketplace":{"label":"#marketplace","children":{},"count":2},"/tags/tips":{"label":"#tips","children":{},"count":10},"/tags/real-estate":{"label":"#real-estate","children":{},"count":1},"/tags/nocode":{"label":"#nocode","children":{},"count":1},"/tags/hospitality":{"label":"#hospitality","children":{},"count":1},"/tags/ride-hailing":{"label":"#ride-hailing","children":{},"count":1},"/tags/iot":{"label":"#iot","children":{},"count":1},"/tags/macos":{"label":"#macos","children":{},"count":3},"/tags/swift":{"label":"#swift","children":{},"count":7},"/tags/partnership":{"label":"#partnership","children":{},"count":1},"/tags/pm":{"label":"#pm","children":{},"count":4},"/tags/travel":{"label":"#travel","children":{},"count":1},"/tags/operation":{"label":"#operation","children":{},"count":7},"/tags/idea":{"label":"#idea","children":{},"count":1},"/tags/purpose":{"label":"#purpose","children":{},"count":2},"/tags/wasm":{"label":"#wasm","children":{},"count":2},"/tags/transparency":{"label":"#transparency","children":{},"count":1},"/tags/event-sourcing":{"label":"#event-sourcing","children":{},"count":1},"/tags/sdlc":{"label":"#sdlc","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, design-engineering":{"label":"#life-at-dwarves, frontend-engineer, design-engineering","children":{},"count":1},"/tags/modeling":{"label":"#modeling","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, engineering-values":{"label":"#life-at-dwarves, software-engineer, engineering-values","children":{},"count":1},"/tags/goal":{"label":"#goal","children":{},"count":2},"/tags/license":{"label":"#license","children":{},"count":1},"/tags/template":{"label":"#template","children":{},"count":20},"/tags/k8s":{"label":"#k8s","children":{},"count":1},"/tags/js":{"label":"#js","children":{},"count":2},"/tags/clojure":{"label":"#clojure","children":{},"count":1},"/tags/react.js":{"label":"#react.js","children":{},"count":2},"/tags/employee":{"label":"#employee","children":{},"count":2},"/tags/onboarding":{"label":"#onboarding","children":{},"count":1},"/tags/assets":{"label":"#assets","children":{},"count":1},"/tags/company":{"label":"#company","children":{},"count":1},"/tags/tooling":{"label":"#tooling","children":{},"count":9},"/tags/human-resource":{"label":"#human-resource","children":{},"count":1},"/tags/dcos":{"label":"#dcos","children":{},"count":5},"/tags/docker":{"label":"#docker","children":{},"count":11},"/tags/okr":{"label":"#okr","children":{},"count":1},"/tags/oss":{"label":"#oss","children":{},"count":1},"/tags/rfc":{"label":"#RFC","children":{},"count":2},"/tags/newsletter":{"label":"#newsletter","children":{},"count":45},"/tags/web3":{"label":"#web3","children":{},"count":4},"/tags/monitoring":{"label":"#monitoring","children":{},"count":2},"/tags/upptime":{"label":"#upptime","children":{},"count":1},"/tags/mcp":{"label":"#MCP","children":{},"count":3},"/tags/tech-report":{"label":"#tech-report","children":{},"count":15},"/tags/overleaf":{"label":"#overleaf","children":{},"count":1},"/tags/slide":{"label":"#slide","children":{},"count":1},"/tags/office-hours":{"label":"#office-hours","children":{},"count":30},"/tags/btc":{"label":"#btc","children":{},"count":1},"/tags/forward-engineering":{"label":"#forward-engineering","children":{},"count":14},"/tags/weekly-digest":{"label":"#weekly-digest","children":{},"count":15},"/tags/wrap-up":{"label":"#wrap-up","children":{},"count":7},"/tags/real-time":{"label":"#real-time","children":{},"count":1},"/tags/phoenix-live-view":{"label":"#phoenix-live-view","children":{},"count":1},"/tags/timescaledb":{"label":"#timescaledb","children":{},"count":1},"/tags/go-weekly":{"label":"#go-weekly","children":{},"count":24},"/tags/finance":{"label":"#finance","children":{},"count":1},"/tags/agents":{"label":"#agents","children":{},"count":4},"/tags/defi":{"label":"#DeFi","children":{},"count":2},"/tags/aider":{"label":"#aider","children":{},"count":2},"/tags/qwen2.5":{"label":"#qwen2.5","children":{},"count":1},"/tags/openhand":{"label":"#openhand","children":{},"count":1},"/tags/predicted output":{"label":"#predicted output","children":{},"count":1},"/tags/project-management":{"label":"#project-management","children":{},"count":1},"/tags/copilots":{"label":"#copilots","children":{},"count":2},"/tags/team-management":{"label":"#team-management","children":{},"count":1},"/tags/mongodb":{"label":"#mongodb","children":{},"count":1},"/tags/salesforce":{"label":"#salesforce","children":{},"count":1},"/tags/use cases":{"label":"#use cases","children":{},"count":2},"/tags/design-system":{"label":"#design-system","children":{},"count":1},"/tags/storybook":{"label":"#storybook","children":{},"count":1},"/tags/hook":{"label":"#hook","children":{},"count":1},"/tags/cline":{"label":"#cline","children":{},"count":1},"/tags/realtime api":{"label":"#realtime api","children":{},"count":1},"/tags/interface":{"label":"#interface","children":{},"count":1},"/tags/import":{"label":"#import","children":{},"count":1},"/tags/package":{"label":"#package","children":{},"count":1},"/tags/yelp":{"label":"#yelp","children":{},"count":1},"/tags/wala":{"label":"#WALA","children":{},"count":4},"/tags/film":{"label":"#film","children":{},"count":1},"/tags/generics":{"label":"#generics","children":{},"count":2},"/tags/log":{"label":"#log","children":{},"count":1},"/tags/pillar":{"label":"#pillar","children":{},"count":3},"/tags/metric":{"label":"#metric","children":{},"count":1},"/tags/tracing":{"label":"#tracing","children":{},"count":1},"/tags/intent-classification":{"label":"#intent-classification","children":{},"count":1},"/tags/prompting":{"label":"#prompting","children":{},"count":1},"/tags/language":{"label":"#language","children":{},"count":5},"/tags/ai-agents":{"label":"#ai-agents","children":{},"count":2},"/tags/ai-evaluation":{"label":"#ai-evaluation","children":{},"count":1},"/tags/prompt-engineering":{"label":"#prompt-engineering","children":{},"count":4},"/tags/ai-integration":{"label":"#ai-integration","children":{},"count":1},"/tags/networking":{"label":"#networking","children":{},"count":7},"/tags/finite-automata":{"label":"#finite-automata","children":{},"count":1},"/tags/pattern-matching":{"label":"#pattern-matching","children":{},"count":1},"/tags/state-machines":{"label":"#state-machines","children":{},"count":1},"/tags/java":{"label":"#java","children":{},"count":1},"/tags/programming":{"label":"#programming","children":{},"count":1},"/tags/caching":{"label":"#caching","children":{},"count":1},"/tags/devbox":{"label":"#devbox","children":{},"count":17},"/tags/nix":{"label":"#nix","children":{},"count":9},"/tags/generative-ui":{"label":"#generative-ui","children":{},"count":1},"/tags/function-calling":{"label":"#function-calling","children":{},"count":1},"/tags/ton":{"label":"#ton","children":{},"count":2},"/tags/ai-powered":{"label":"#ai-powered","children":{},"count":1},"/tags/pattern":{"label":"#pattern","children":{},"count":1},"/tags/supervisor-architecture":{"label":"#supervisor-architecture","children":{},"count":1},"/tags/document-processing":{"label":"#document-processing","children":{},"count":1},"/tags/information-retrieval":{"label":"#information-retrieval","children":{},"count":1},"/tags/iterators":{"label":"#iterators","children":{},"count":1},"/tags/reinforcement-learning":{"label":"#reinforcement-learning","children":{},"count":3},"/tags/kernel-programing":{"label":"#kernel-programing","children":{},"count":1},"/tags/anchor":{"label":"#anchor","children":{},"count":2},"/tags/containerization":{"label":"#containerization","children":{},"count":4},"/tags/virtualization":{"label":"#virtualization","children":{},"count":4},"/tags/meet-up":{"label":"#meet-up","children":{},"count":4},"/tags/meetup":{"label":"#meetup","children":{},"count":2},"/tags/energy":{"label":"#energy","children":{},"count":1},"/tags/motivation":{"label":"#motivation","children":{},"count":1},"/tags/cybersecurity":{"label":"#cybersecurity","children":{},"count":2},"/tags/serverless":{"label":"#serverless","children":{},"count":1},"/tags/doty":{"label":"#doty","children":{},"count":5},"/tags/websocket":{"label":"#websocket","children":{},"count":1},"/tags/protocols":{"label":"#protocols","children":{},"count":1},"/tags/rendering":{"label":"#rendering","children":{},"count":1},"/tags/dom":{"label":"#dom","children":{},"count":3},"/tags/cssom":{"label":"#cssom","children":{},"count":1},"/tags/render-tree":{"label":"#render-tree","children":{},"count":1},"/tags/iframe":{"label":"#iframe","children":{},"count":1},"/tags/postmessage":{"label":"#postmessage","children":{},"count":1},"/tags/mock-service-worker":{"label":"#mock-service-worker","children":{},"count":1},"/tags/api-mocking":{"label":"#api-mocking","children":{},"count":1},"/tags/web-development-tool":{"label":"#web-development-tool","children":{},"count":1},"/tags/data-fetching":{"label":"#data-fetching","children":{},"count":1},"/tags/frontend,":{"label":"#frontend,","children":{},"count":1},"/tags/graphql":{"label":"#graphql","children":{},"count":1},"/tags/reactjs":{"label":"#reactjs","children":{},"count":2},"/tags/scroll-driven-animations":{"label":"#scroll-driven-animations","children":{},"count":1},"/tags/animations":{"label":"#animations","children":{},"count":1},"/tags/intersection-observer":{"label":"#intersection-observer","children":{},"count":1},"/tags/nextjs":{"label":"#nextjs","children":{},"count":1},"/tags/server-component":{"label":"#server-component","children":{},"count":1},"/tags/caching-data":{"label":"#caching-data","children":{},"count":1},"/tags/social-networks":{"label":"#social-networks","children":{},"count":1},"/tags/foundation-model":{"label":"#foundation-model","children":{},"count":1},"/tags/fine-tuning":{"label":"#fine-tuning","children":{},"count":1},"/tags/vector database":{"label":"#vector database","children":{},"count":1},"/tags/shadow-dom":{"label":"#shadow-dom","children":{},"count":1},"/tags/web-api":{"label":"#web-api","children":{},"count":1},"/tags/swr-infinite":{"label":"#swr-infinite","children":{},"count":1},"/tags/web-design":{"label":"#web-design","children":{},"count":1},"/tags/tuning-llm":{"label":"#tuning-llm","children":{},"count":2},"/tags/langchain":{"label":"#langchain","children":{},"count":1},"/tags/translation":{"label":"#translation","children":{},"count":1},"/tags/profiling":{"label":"#profiling","children":{},"count":1},"/tags/state-mangement":{"label":"#state-mangement","children":{},"count":1},"/tags/global-state-management":{"label":"#global-state-management","children":{},"count":1},"/tags/css":{"label":"#css","children":{},"count":5},"/tags/fonts":{"label":"#fonts","children":{},"count":1},"/tags/variable-fonts":{"label":"#variable-fonts","children":{},"count":1},"/tags/state-management":{"label":"#state-management","children":{},"count":2},"/tags/component":{"label":"#component","children":{},"count":1},"/tags/proof-of-knowledge":{"label":"#proof-of-knowledge","children":{},"count":1},"/tags/fronten":{"label":"#fronten","children":{},"count":1},"/tags/typescript":{"label":"#typescript","children":{},"count":4},"/tags/analytics-tools":{"label":"#analytics-tools","children":{},"count":1},"/tags/analytics-platform":{"label":"#analytics-platform","children":{},"count":1},"/tags/software engineer":{"label":"#software engineer","children":{},"count":1},"/tags/parsing":{"label":"#parsing","children":{},"count":1},"/tags/validation":{"label":"#validation","children":{},"count":1},"/tags/webassembly":{"label":"#webassembly","children":{},"count":1},"/tags/sandbox":{"label":"#sandbox","children":{},"count":1},"/tags/zk-rollup":{"label":"#zk-rollup","children":{},"count":2},"/tags/polygon":{"label":"#polygon","children":{},"count":1},"/tags/starknet":{"label":"#starknet","children":{},"count":1},"/tags/ethereum":{"label":"#ethereum","children":{},"count":2},"/tags/zero-knowledge":{"label":"#zero-knowledge","children":{},"count":1},"/tags/atomic-css":{"label":"#atomic-css","children":{},"count":1},"/tags/client-side":{"label":"#client-side","children":{},"count":1},"/tags/storage":{"label":"#storage","children":{},"count":1},"/tags/frontend/performance":{"label":"#frontend/performance","children":{},"count":2},"/tags/wai-aria":{"label":"#wai-aria","children":{},"count":1},"/tags/accessibility":{"label":"#accessibility","children":{},"count":4},"/tags/polymorphic-component":{"label":"#polymorphic-component","children":{},"count":1},"/tags/threejs":{"label":"#threejs","children":{},"count":1},"/tags/web-performance":{"label":"#web-performance","children":{},"count":2},"/tags/html":{"label":"#html","children":{},"count":4},"/tags/animation":{"label":"#animation","children":{},"count":1},"/tags/zk-proof":{"label":"#zk-proof","children":{},"count":1},"/tags/guides":{"label":"#guides","children":{},"count":1},"/tags/responsive-design":{"label":"#responsive-design","children":{},"count":1},"/tags/hsl":{"label":"#hsl","children":{},"count":1},"/tags/javascript":{"label":"#javascript","children":{},"count":4},"/tags/css-in-js":{"label":"#css-in-js","children":{},"count":1},"/tags/tip":{"label":"#tip","children":{},"count":1},"/tags/dark-mode":{"label":"#dark-mode","children":{},"count":1},"/tags/multisign-wallet":{"label":"#multisign-wallet","children":{},"count":1},"/tags/virtual-dom":{"label":"#virtual-dom","children":{},"count":1},"/tags/native-modules":{"label":"#native-modules","children":{},"count":1},"/tags/vitejs":{"label":"#vitejs","children":{},"count":1},"/tags/esm":{"label":"#esm","children":{},"count":1},"/tags/modules":{"label":"#modules","children":{},"count":1},"/tags/blockchain-bridge":{"label":"#blockchain-bridge","children":{},"count":1},"/tags/foundational-topics":{"label":"#foundational-topics","children":{},"count":5},"/tags/distributed-systems":{"label":"#distributed-systems","children":{},"count":1},"/tags/pos":{"label":"#pos","children":{},"count":1},"/tags/smart-contract":{"label":"#smart-contract","children":{},"count":1},"/tags/atomic-design":{"label":"#atomic-design","children":{},"count":1},"/tags/a11y":{"label":"#a11y","children":{},"count":1},"/tags/useeffect":{"label":"#useeffect","children":{},"count":1},"/tags/concurrency":{"label":"#concurrency","children":{},"count":2},"/tags/parallelism":{"label":"#parallelism","children":{},"count":1},"/tags/engineering/frontend":{"label":"#engineering/frontend","children":{},"count":1},"/tags/wfh":{"label":"#wfh","children":{},"count":1},"/tags/tech radar":{"label":"#tech radar","children":{},"count":1},"/tags/policy":{"label":"#policy","children":{},"count":1},"/tags/vim":{"label":"#vim","children":{},"count":1}}}},"ogifMemos":[{"content":"\n### Topics and Highlights\n\n- **Swap ICY-BTC:** Huy shared updates on the ICY-BTC swap mechanism, explaining the current state and adjustments needed to ensure accurate ICY valuation during swaps.\n- **GitHub BotL:** Thanh introduced a GitHub bot to automate PR reviews, aiming to improve processing speed and consistency in code management.\n- **Memo UI:** The team presented improvements to the Memo user interface, focusing on better data access and user experience.\n- **Agentic: MCP-DB:** Huy discussed the MCP-DB system, highlighting how it handles data storage and retrieval to support agents in automated workflows.\n- **Pocket Turning, Recapable:** Vincent shared progress on the Pocket Turning and Recapable, outlining the completion of core gameplay and next steps.\n- **Funding Rate Arbitrage:**  Antran presented a strategy for funding rate arbitrage across multiple exchanges, addressing technical challenges and execution strategies.\n\n### Vietnamese Transcript\n\n**[05:30]** Hôm nay chắc mình bắt đầu sớm nha. Buổi hôm nay chắc kết hợp với lại anh trong buổi meeting một xíu. Một phần là sẽ làm showcase, cái thứ hai là anh tổng kết một số việc mà bữa trước có trao đổi với mấy anh em á. Cái số hai, cái số ba là mình sẽ bắt đầu cho mấy anh em đăng ký công việc. Hiện tại để mà dễ trước, chắc là mình sẽ để cho Huy Nguyễn đi show mấy cái phần bên Huy trước, liên quan tới ICY một tí, xong rồi show một số cái về tech mà team mình đang làm nè. Để mình có một cái snapshot về chuyện là team tech thì hiện nay như thế nào nhé. Rồi sắp tới thì team mình cần gì, với lại mấy anh em xem contribute được gì vào đó ha.\n\n**[06:35]** Huy, Thành đâu? Nhường sân khấu này nè. Rồi ok, nội dung đầu tiên, chắc là bên ICY Swap trước đi. Mình announce đó, hồi tuần trước, tuần này deploy lên rồi thì giờ những cái khác biệt như thế nào, chắc nhờ Huy đi lại hết mấy series đó.\n\n**[07:29]** Alo, rồi rồi, đã xem màn hình rồi. Thì bây giờ mọi người có thể vào trang ICY Swap để mà swap được rồi. Đây, mình chỉ số liệu nha. Nhưng mà ở trên đây thì nó đang ready hết tất cả mọi thứ rồi. Việc làm duy nhất bây giờ là đang ngồi soát lại mấy cái số ICY á. Tại vì lúc trước vận hành á, thì mình vận hành theo kiểu là mình neo cái giá ICY, nên mình cũng không quan tâm cái lượng lưu thông (circulated) lắm. Nên có mấy trường hợp là mình để vô mấy cái ví của team, hoặc là chuyển qua mấy cái Mochi Balance của em hoặc là của anh Bảo. Thì mấy cái đó đang cần rà soát lại để mà nó ra cái số lưu thông đúng. Tại vì giờ mình sẽ ngồi, cái giá của mình nó sẽ dynamic theo cái pool nên cần ngồi check lại cái đó thì cũng gần xong hết rồi.\n\n**[09:09]** Giờ còn mỗi cái account của anh Bảo là cần kiểm tra lại thôi. Nhớ có đợt là chuyển cho anh Bảo, giờ đang ngồi xem lại cái phần đó rồi cộng trừ lại rồi cắt cái phần đó ra khỏi cái circulated thì số này nó sẽ ra đúng. Còn lại hiện tại muốn swap ủng hộ thì cũng có thể swap được ở trên trang này. Lịch là đang vậy. Em show thử cái list Holder của mình hiện tại cho mấy anh em xem chắc cần biết nhiều hơn xíu. Trước giờ mọi người tham gia không quan tâm nhiều lắm nhưng mà chắc lần này thì mình cần để ý hơn.\n\n**[09:51]** ICY của mình mình deploy ở trên Base, đúng không? Nên khi anh em vào trong cái list Holder, mọi người sẽ thấy được một cái list khoảng tất cả những cái ví nào đang được giữ ICY của team mình, thì là CCK Holder ha. Là một. Rồi thì cái link để mà vô đây chắc Huy share nha. Chứ mọi người lên mà search thì chắc không biết được đâu.\n\nĐầu tiên là anh em cần nắm cái này. Quay qua đoạn này rồi. Anh nghĩ mấy anh em cần quan tâm phần này nhiều hơn xíu. Nó trở thành cái norm của thế giới tech luôn rồi, không cần làm gì mới nữa. Nên anh em nắm được thì sẽ ok hơn.\n\n**[10:33]** ICY của mình hiện đã được list. Trong danh sách này có các ví minter, ví dùng để lập ngân sách cho các hoạt động, và một số ví đang nắm giữ lượng ICY lớn. Các hoạt động liên quan đến staking ICY sẽ được triển khai dần dần trong thời gian tới. Đây là thông tin đầu tiên anh em cần nắm rõ.\n\n**[11:15]** Huy, demo thử luồng swap đi. Có ai có địa chỉ Bitcoin với một ít ICY không? Vincent có ở đây không? Ok, giờ thử swap từ ICY sang Bitcoin. Giá hiện tại được tính theo cơ chế động dựa trên lượng ICY đang lưu hành và pool. Chức năng swap rất đơn giản, chỉ cần điền số lượng, bấm swap là xong.\n\n**[12:27]** Khoan đã, đừng nhập địa chỉ ảo. Ok, vậy là ổn rồi. Khung đầu tiên là ICY như bình thường. Ở dưới thì đang hiển thị đơn vị là satoshi, tức là đơn vị nhỏ nhất của Bitcoin. Khi nhập số lượng vào, nó sẽ tự động chuyển đổi. Tuy nhiên, tỷ giá hiện tại đang bị lệch một chút, khoảng 1.2 thay vì 1.5. Đây chắc là lỗi tính toán nhỏ, chỉnh lại là được.\n\n**[13:28]** Cần có số ICY tối thiểu để swap. Thử nhập 30 ICY xem sao. Refresh lại thử xem có được không.\n\n**[14:43]** Hình như không đủ tiền trong ví rồi. Bạn có ETH trên Base không? Chuyển qua Base và kiểm tra lại xem.\n\n**[15:51]** Không phải lỗi đó đâu. Vấn đề là account chưa được đăng ký nên không thể thực hiện giao dịch. Sẽ fix phần đó sau. Mục tiêu ở đây là giúp mọi người hiểu rõ hơn về cơ chế swap và cách định giá token. Nếu nắm rõ thì sau này sẽ dễ dàng hơn trong việc quản lý tokenomics.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=0LryX12wLbTu3i1m\u0026amp;start=806\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[16:47]** Huy, giải thích nhanh lại cơ chế tính giá đi. Lần trước Quan demo chưa nói kỹ phần đó. Giá của ICY được xác định theo cơ chế minting, nghĩa là giá sẽ không thay đổi mạnh nếu có ai đó swap số lượng lớn. Nó không hoạt động theo kiểu cơ chế tạo lập thị trường tự động (AMM) mà giá sẽ được kiểm soát theo cơ chế minting. Cơ chế này giúp giá duy trì ổn định ngay cả khi có giao dịch lớn.\n\n**[17:43]** Hoàn toàn là nó phụ thuộc vào Bitcoin. Nên nếu giá Bitcoin tăng thì lượng ICY mà anh em đang cầm sẽ tăng về giá trị USD. Còn về cơ chế minting, nhờ Huy giải thích thêm một chút. Nói chung là cơ chế chung của mình trước giờ là mình sẽ cố định giá trị của ICY theo USDC. Anh em không cần quan tâm nhiều, cứ hiểu đơn giản là một ICY tương đương với 1.5 USD.\n\n**[18:37]**Phần đảm bảo này là để giúp team vận hành có thể đảm bảo là tới ngày thì sẽ đổi USDC vào trong contract để mọi người swap. Tỷ giá swap trong contract cũ là cố định ở mức 1.5 ICY, nhưng đó là model cũ. Model mới của mình thì linh hoạt hơn. Nếu anh em đã dùng Uniswap hay các AMM (Automatic Market Maker) khác thì nó cũng tương tự một chút. Ở đây, cơ chế hoạt động là bên dưới có một pool thanh khoản (liquidity pool), trong đó chứa cả ETH và USDC. Tùy vào tình hình của pool lúc đó, tỷ giá sẽ được điều chỉnh dựa trên lượng ETH và USDC trong pool.\n\n**[19:18]** Cơ chế của mình cũng tương tự như vậy. Giá ICY sẽ được quyết định bởi lượng Bitcoin trong pool và lượng ICY đang được lưu hành. Công thức đơn giản thôi: mình có lượng ICY (X), có lượng BTC (Y) trong pool, thì X/Y sẽ ra được giá trị của một ICY tính theo BTC. Công thức này là công thức toán học cơ bản, không có gì phức tạp.\n\n**[19:55]** Do cơ chế hoạt động của mình, sẽ có hai thời điểm làm thay đổi thanh khoản:\n\n1. **Thời điểm đầu tiên** là vào mỗi tháng, team vận hành sẽ đổ thêm BTC vào pool để làm chi phí cho các hoạt động của team. Lúc này giá ICY sẽ tăng lên một chút vì lượng BTC trong pool tăng lên.\n2. **Thời điểm thứ hai** là khi team đẩy thêm ICY vào pool (minting thêm). Khi mint thêm ICY, giá ICY trên thị trường sẽ giảm xuống do lượng ICY trong pool tăng lên.\n\n**[20:35]** Hai trường hợp trên sẽ ảnh hưởng trực tiếp đến giá ICY. Còn nếu giá Bitcoin thay đổi thì giá trị USD của ICY có thể thay đổi, nhưng giá ICY tính theo BTC thì không thay đổi. Market impact từ Bitcoin là yếu tố bên ngoài, không ảnh hưởng trực tiếp đến việc minting hoặc giá trị ICY trong pool.\n\n**[21:12]** Anh em có câu hỏi gì thêm thì đặt câu hỏi, tí nữa sẽ trả lời sau. À, có câu hỏi về việc swap ngược từ BTC về ICY đúng không? Hiện tại thì chưa có chức năng đó. Hiện tại chỉ hỗ trợ swap từ ICY sang BTC thôi, không có chức năng swap ngược lại. Tức là mua vào thì được, nhưng bán ra thì chưa hỗ trợ.\n\n**[21:40]** Cảm ơn Huy. Có gì cần lưu ý thêm không? Cần lưu ý là hiện tại vẫn đang trong giai đoạn thử nghiệm nên có thể có một số trường hợp ngoại lệ. Ví dụ như một số tình huống có thể phát sinh khi swap hoặc thanh khoản chưa đủ. Về cơ bản thì luồng hiện tại vẫn đang hoạt động ổn định.\n\n**[22:00]** Như là số lượng ICY tối thiểu để swap. Vì bản chất là team mình đang cover cái phần phí mà để mà làm gas trên ETH, trên Base và cả trên BTC luôn thì nên đang kiểu đang giới hạn cái số ICY nó swap nhiều tí để mà hạn chế với cái việc mà mọi người swap tầm 1-2 ICY để test á thì nó tốn cái chi phí gas nên đang để tầm trên 20 ICY mới cho mọi người swap trên web.\n\nCái thứ hai là ở cái do cái việc mà mình mint thêm ICY thì nó sẽ làm thay đổi giá thị trường, thì nên em đang disable luôn cái phần mà cơ chế cái ứng lương trước của mình.\n\n**[22:37]** Tức là đồng loạt ứng lương thì nó sẽ ảnh hưởng giá đúng không? Vậy cái lesson learn trong cái này đó là sau đợt này làm thì có vài điểm mà anh đang thấy là bắt đầu team mình đang tập trung vô build những cái tool nó hỗ trợ mình hoạt động. Cũng là một số cái thử nghiệm mới, cũng là một số cái mà hỗ trợ hoạt động thiệt sự. Nhưng mà sau khi xong mấy cái bài này thì nó sẽ ra được một số mấy cái article liên quan thì mấy anh em nếu mà trước đó không có tham gia những cái dự án đó có thể tìm lại những cái bài đó để mà coi được cái game, cái knowledge game từ cái đợt đó là cái gì của mấy anh em làm dự án đó ha.\n\n**[23:24]** Rồi thì trong cái vụ ICY Swap đợt này chắc là được hai ba bài phải không? Dạ, như được ba bài. Còn kiểu viết nhiều thêm thì vẫn có nhiều cái để viết. Ừ, thôi đó cứ thong thả từ từ đi.\n\n**[24:02]** Sau phần của Huy, anh cảm ơn Huy rồi chuyển sang nội dung thứ hai liên quan đến những gì team mình đang làm. Anh Bảo ai nói trước cũng được, nhưng chắc là để Thành nói trước. Thành bảo là em nói trước cũng được, em sẽ gom lại hết để anh cho mọi người biết team đang ở giai đoạn nào. Nhưng anh bảo là để Thành nói trước đi, tại vì đang có người bấm chuông. Rồi anh mời Thành bắt đầu.\n\n**[25:00]** Mọi người, Memo của mình là một trong những cái đợt lớn đợt này, có upgrade format lại cho nhìn nó ok hơn tí. Mình luôn muốn mình tạo những cái map content, những cái thứ mà mình đọc được cái mình up lên đây. Nhưng mà hiện tại cái mô hình đó thật ra nó cũng không có còn quá hiệu quả với chuyện là mấy cái model ra đời nó nén dữ liệu lại, rồi mình query trực tiếp từ đó ra thì nó sẽ hiệu quả hơn.\n\nThì cái point của chuyện là đưa những cái kiến thức mà nó bình thường lên trên Memo thì nó cũng không phù hợp lắm ha. Nên đợt này lúc mà làm lại thì có một cái ý chính để mà muốn nói với anh em đó là Memo hiện tại sẽ được dùng chỉ cho mục đích duy nhất thôi ,  đó là cái knowledge gain mà từ dự án.\n\nCái đó là gần như là những cái mới mà nó xuất phát từ chính cái hoạt động của cái team mình. Gần như trên đây sau này nó sẽ gồm là liên quan tới lĩnh vực gì đó, mình đã làm gì đó trong đó. Nó có nhiều hơn, maybe là sau một giai đoạn thì khi tụi nó train lại cái model thì những cái dữ liệu của mình á thì nó sẽ trở thành một phần của kiến thức chung cho cả cộng đồng.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=MhsFuFFQ5NFKTlYS\u0026amp;start=1556\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[25:39]** Và cái phần này anh nghĩ là nó sẽ giúp ích rất nhiều cho cái chuyện mà mọi người làm kiểu training lại cho AI model sau này, hoặc là mấy cái chuyện mà mình muốn nó có cái việc mà suggestion kiểu tự động ấy.\n\n**[26:24]** Nội dung sẽ trở thành một phần trong mô hình đó hoặc nếu có mấy công cụ tìm kiếm trên internet, thì có thể bài của mình chỉ là một phần nhỏ trong nguồn tài liệu được tham khảo vào thôi, giống như là một phần nhỏ trong citation. Điều này cũng không có vấn đề gì lớn. Nhưng nhìn chung, toàn bộ những nội dung này sẽ gần như trở thành spirit của team.\n\nTrong lần nâng cấp lớn này, có một điểm chính mà Tuấn đã hoàn thành chưa nhỉ? Tuấn ơi, phần liên quan đến việc đồng bộ toàn bộ dữ liệu của team, nhất là về phần nội dung, hiện đang được định hướng như vậy để các thành viên nắm rõ hơn.\n\n**[27:00]** Tức là sau đợt này, các thành viên đang tham gia vào các dự án sẽ có xu hướng ngồi lại với nhau để xem xét kỹ hơn từ những dự án đó, và xác định rõ phần **knowledge gain** (kiến thức thu được) từ chính các dự án đó là gì. Sau đó, team sẽ đưa lên Memo làm nguồn tài liệu nội bộ cho team.\n\nPhần thứ hai là ở cuối mỗi bài sẽ có một phần liên quan đến **group of reading**. Hiện tại phần này vẫn chưa hoàn chỉnh, nhưng ý tưởng là sau khi hoàn thiện, sẽ có thêm phần thông tin tổng hợp về bài viết để người đọc có thể tra cứu và học thêm từ bài viết đó.\n\n**[27:47]** Ngoài ra, tất cả dữ liệu của team được viết ra sẽ được gán định danh ví dụ như **GitHub**, **Discord**, hoặc những kênh nội bộ khác. Dữ liệu này sẽ được upload lên dạng **blockchain storage** trên nền tảng **Arweave (AV)** – một nền tảng lưu trữ phi tập trung. Điều này giúp cho nội dung của team có một định danh rõ ràng và minh bạch.\n\nThêm vào đó, người đọc sẽ có thể xem lại bài viết, đánh giá hoặc để lại phản hồi trực tiếp trên bài viết. Đây là một phần của ý tưởng nâng cấp mới cho trang **Memo** của team.\n\n**[28:39]** Trước đây, team đã có ý định sử dụng Obsidian để quản lý nội dung, nhưng có vẻ như một số thành viên gặp khó khăn trong việc làm quen với công cụ đó. Vì vậy, hiện tại để làm cho mọi thứ đơn giản hơn, team sẽ chuyển sang cơ chế trực tiếp hơn. Cụ thể là thay vì phải làm qua Obsidian, các thành viên có thể submit nội dung trực tiếp vào repository của thư viện chung của team.\n\nCác thành viên chỉ cần đưa nội dung vào và submit trực tiếp qua nền tảng này, không cần phải tuân theo workflow bắt buộc của Obsidian nữa. Nếu ai vẫn muốn dùng Obsidian thì không sao, nhưng nếu không dùng thì cũng không ảnh hưởng gì cả. Đây là thay đổi cơ bản nhất trong hệ thống Memo của team.\n\n**[29:24]** Hiện tại team đang làm một số dự án chính, bao gồm:\n\n1. Bitcoin Swap – đã nhắc tới ở phần trước.\n2. Memo – vừa mới trình bày xong.\n3. Hai dự án nhỏ khác:\n\n- **agentic** – nhóm của Quang và Huy đang phát triển.\n- **github bot** – nhóm của Thành đang thực hiện, hiện đang test thử.\n\nGiờ chắc nhường lại cho Thành để chia sẻ thêm về những nội dung này.\n\n**[30:32]** Dự án này đã được khởi động hơn một tuần và đã chính thức chạy code được hơn một tuần. Mục đích chính của nó là tạo ra một hệ thống nhắc nhở (reminder). Trước đây, team thường gặp tình huống khi tạo pull request (PR), mọi người hay để đó và chờ chạy xong rồi quên luôn việc cần review. Tool này sẽ phục vụ cho việc theo dõi và cập nhật thông tin về các hoạt động hàng ngày trên github hoặc hàng tuần trên các kênh giao tiếp nội bộ của team.\n\n**[31:18]** Hệ thống này được thiết kế dưới dạng một tích hợp đơn giản. Luồng hoạt động cơ bản bao gồm một số use case như: thông báo cho người được assign để review, tương tác với GitHub API, và post thông tin vào các kênh nội bộ như Discord hoặc Slack. Hiện tại, team đang test thử trên Discord. Ngoài ra, team cũng đang thử nghiệm với agentic và một framework mới gọi là **Mastra AI**.\n\nFramework này khác với các tool Python thông thường. Một số thành viên trong team không quen làm việc với Python, nên team muốn thử nghiệm xem liệu sử dụng framework mới này có hiệu quả hơn các giải pháp hiện tại hay không. Framework này hỗ trợ các tính năng như setup môi trường, define các trạng thái để quản lý dữ liệu, và cho phép cấu hình lại tùy theo nhu cầu của team.\n\n**[32:19]** Cấu trúc của hệ thống này có hai phần chính:\n\n1. **Agentic App** – Đây là ứng dụng chính để xử lý các hoạt động của hệ thống.\n2. **Discord App** – Hỗ trợ việc gửi thông báo vào Discord.\n\nNgoài ra, hệ thống còn có một vài component phụ, như workflow để xử lý công việc theo lịch trình, kiểm tra và thông báo cho developer nếu có bất kỳ pull request nào đang chờ được review. Nếu pull request vượt quá một khoảng thời gian nhất định, hệ thống sẽ gửi thông báo để nhắc người thực hiện review.\n\n**[33:12]** Agentic App sẽ expose một vài API cho phép chat và theo dõi trạng thái của các pull request. Khi có một pull request được tạo ra, hệ thống sẽ tự động xác định các điều kiện như trạng thái của pull request (work in progress hay chưa), thời gian tạo pull request, và sẽ gửi thông báo cho người review sau khoảng 30 phút kể từ lúc tạo. Ví dụ: nếu có một pull request cần được review nhưng không có ai assigned hoặc đã quá thời gian xử lý, hệ thống sẽ tự động ping lại người phụ trách.\n\n**[35:02]** Thay vì phải theo dõi thủ công, hệ thống sẽ gắn con agent vào để tự động theo dõi và thông báo thông qua endpoint của hệ thống. Trong phần logic, hệ thống sẽ định nghĩa các điều kiện cụ thể, chẳng hạn như chỉ gửi thông báo nếu pull request được tạo trong vòng 30 phút hoặc đang trong trạng thái work in progress. Nếu pull request được cập nhật hoặc chuyển trạng thái, hệ thống sẽ tự động theo dõi và gửi thông báo cho developer để đảm bảo không bị sót.\n\n**[35:39]** Hệ thống sẽ hoạt động dựa trên code filter thông thường. Ngoài ra, nó sẽ có một số workflow khác như việc gửi thông báo vào cuối ngày để tổng hợp tình trạng của các pull request trên Discord. Hệ thống sẽ tự động gửi thông báo về số lượng pull request đang mở, tình trạng của chúng và trạng thái review hiện tại. Đây là chức năng chính của tool này ,  đóng vai trò như một công cụ reminder.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=Zduog0abeAWXIIM4\u0026amp;start=2107\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[36:24]** Hệ thống cũng có thể tích hợp với các công cụ chat khác. Đơn giản là có thể tạo thêm một command và gửi request tới endpoint của hệ thống. Các request này sẽ được định nghĩa dựa trên schema cụ thể, ví dụ như input là **review ID** hoặc các thông tin khác liên quan đến trạng thái của pull request. Hệ thống sẽ lấy dữ liệu này và hiển thị trên giao diện mà người dùng thường xuyên sử dụng.\n\n**[37:04]** Phần xử lý backend của hệ thống được thực hiện thông qua tool Lippia, một công cụ định dạng dữ liệu JSON thành dạng bảng Markdown table hoặc dạng data binding. Hiện tại team đang test thử hai luồng xử lý này trước khi mở rộng thêm các tính năng khác. Khi hệ thống hoạt động ổn định, các workflow này sẽ được mở cho tất cả các thành viên trong team thử nghiệm và phát triển thêm.\n\n**[38:08]** Hệ thống được thiết kế để mở rộng một cách linh hoạt. Các thành viên trong team có thể tự phát triển và đóng góp các workflow khác nhau. Hệ thống này cho phép xây dựng các tool dưới dạng một đơn vị độc lập (**packaging unit**), sau đó kết hợp các đơn vị này lại để tạo ra các workflow phức tạp hơn. Khi muốn phát hành một workflow mới, các thành viên chỉ cần định nghĩa lại đơn vị cơ bản và tích hợp nó vào hệ thống.\n\nViệc mở rộng các workflow sẽ giúp hệ thống phát triển theo chiều ngang (mở rộng số lượng tính năng), thay vì theo chiều dọc (phát triển tính năng hiện tại). Khi số lượng các workflow tăng lên, hệ thống sẽ càng trở nên linh hoạt và mạnh mẽ hơn.\n\n**[38:54]** Về cơ bản, workflow được coi là lớp ứng dụng (application layer) tương tự như các API data trước đây. Hệ thống này sẽ hoạt động ở cấp độ tool, nhưng người dùng cuối sẽ tương tác với nó qua giao diện của workflow. Hiện tại, vẫn chưa có đơn vị nào triển khai thành công mô hình này ở quy mô lớn. Tuy nhiên, GitHub hiện đã mở rộng API cho các developer tạo các extension và tích hợp chúng trực tiếp vào GitHub.\n\n**[39:40]** Dify đang xây dựng một nền tảng để hỗ trợ các developer phát triển và triển khai các tool và workflow này một cách dễ dàng hơn. Mục tiêu là tạo ra một marketplace để các tool và workflow có thể được phân phối và sử dụng bởi nhiều người dùng khác nhau. Hệ thống này tương tự như một nền tảng mở, cho phép các developer bên thứ ba triển khai các tool và workflow của riêng họ.\n\nTrên nền tảng của Dify đã có khoảng 50 tool khác nhau. Một số tool đã từng được phát hành dưới dạng thử nghiệm, nhưng do chưa có định hướng rõ ràng và thiếu sự hỗ trợ từ cộng đồng, nên chúng chưa đạt được thành công như mong đợi.\n\n**[40:17]** Một số nền tảng trước đây đã thử xây dựng mô hình tương tự nhưng chưa đạt được thành công. Lý do là vì các tool này chỉ được xây dựng dưới dạng form, thiếu khả năng tương tác với dữ liệu bên ngoài và chưa có khả năng kết hợp các workflow phức tạp. Tuy nhiên, Dify đang tập trung vào việc giải quyết các vấn đề này để tạo ra một hệ sinh thái hoàn chỉnh cho các workflow và tool.\n\n**[40:59]** Các công cụ này cũng cho phép người dùng đẩy dữ liệu từ các nguồn bên ngoài vào hệ thống. Người dùng có thể gửi dữ liệu từ các ứng dụng bên ngoài qua các Open Form hoặc API. Dify sẽ tự động xử lý và định dạng dữ liệu để sử dụng trong các workflow của hệ thống.\n\n**[41:56]** Team đang tập trung vào hai hướng phát triển chính:\n\n1. Tiếp tục mở rộng và phát triển các workflow hiện có.\n2. Cải tiến và tối ưu hóa các công cụ hiện tại để hỗ trợ việc triển khai và sử dụng dễ dàng hơn.\n\nHệ thống được xây dựng dựa trên các tiêu chuẩn chung về thiết kế tool và workflow. Công cụ Smithery hiện tại đang đóng vai trò như một Agent để quản lý các workflow. Smithery cũng có thể được sử dụng như một Package Manager để cài đặt và quản lý các tool trong hệ thống.\n\n**[42:53]** Workflow sẽ hoạt động theo cơ chế, nếu một workflow nào đó trở nên phổ biến, mọi người có thể lấy nó về và sử dụng dưới dạng tool. Bản chất của các công cụ này là được thiết kế để phục vụ các domain cụ thể. Ví dụ như một công cụ để tạo file, tìm kiếm hoặc lấy file code chẳng hạn. Nó hoạt động giống như một SDK, tức là một bộ thư viện mà bạn chỉ cần import vào để sử dụng.\n\n**[43:37]** Khi đã tích hợp vào SDK, bạn có thể sử dụng các method sẵn có để thao tác với dữ liệu. Điều này cho phép tích hợp dễ dàng vào các công cụ AI. Hiện tại, chỉ có Cross là hỗ trợ trực tiếp cho các thao tác này. Tuy nhiên, trong tương lai, nó sẽ được chuẩn hóa để các công cụ khác cũng có thể dễ dàng tích hợp. Trường hợp của Manus là một ví dụ. Manus sử dụng rất nhiều tool khác nhau, tuy nhiên khi so sánh với hệ thống agent trong Smithery, về cơ bản chúng là hai lớp hoàn toàn khác nhau.\n\n**[44:15]** Trong hệ thống của Manus, các công cụ được kết hợp lại để tạo ra các workflow tổng quát hơn. Các công cụ này hoạt động ở các lớp khác nhau, trong khi các agent trong Smithery được thiết kế để hoạt động độc lập. Câu hỏi đặt ra là làm thế nào để phân biệt rõ ràng sự khác nhau giữa hệ thống của Manus và hệ thống agent trong Smithery. Có một bài tóm tắt về điều này đã được đăng trong kênh AI Club ,  nội dung chính nói về khả năng suy nghĩ (thinking) và khả năng sử dụng máy tính (computer use).\n\n**[45:09]** Cơ chế của hệ thống Manus là một hệ thống service-oriented. Để kết hợp nhiều tool với nhau trong cùng một workflow, cần phải định nghĩa rõ các bước thực hiện. Ví dụ như bước 1 cần sử dụng tool nào, bước 2 cần sử dụng tool nào, v.v. Điều này đòi hỏi các bước phải được cấu hình cụ thể. Tuy nhiên, hệ thống mới có khả năng suy luận để tự động xác định xem cần sử dụng những công cụ nào để hoàn thành tác vụ. Đây chính là điểm khác biệt giữa hệ thống mới và các hệ thống cũ.\n\n**[45:59]** Cụ thể, hệ thống mới có thể nhận biết được một tác vụ cần sử dụng bao nhiêu công cụ, thực hiện qua các bước nào, và có thể điều chỉnh thứ tự thực hiện một cách thông minh. Đây là một cơ chế đặc biệt và khác biệt so với các hệ thống cũ. Nói cách khác, nó hoạt động như một Supervisor ,  có khả năng suy luận và đưa ra quyết định về thứ tự và phương pháp thực hiện các bước trong workflow.\n\n**[46:35]** Hệ thống Supervisor hoạt động ở lớp cao hơn so với các agent trong  Smithery. Các agent trong  Smithery chỉ đơn giản là các công cụ thực thi một tác vụ cụ thể, trong khi Supervisor có khả năng quản lý và điều phối toàn bộ quá trình thực hiện tác vụ. Việc tích hợp Supervisor cho phép hệ thống hoạt động một cách linh hoạt hơn, đồng thời dễ dàng mở rộng và bổ sung thêm các công cụ mới.\n\n**[47:33]** Mục tiêu của team là hiểu rõ cách hoạt động của hệ thống và nắm được cơ chế điều hành của các workflow. Nếu có thể xác định được cách thức triển khai và quản lý các workflow, thì sẽ có thể chọn lọc và sử dụng các công cụ hiệu quả hơn. Đây là điều mà team đang hướng tới ,  xây dựng một hệ thống có khả năng mở rộng và tối ưu hóa quy trình làm việc.\n\n**[48:24]** Tiếp theo, team sẽ tập trung vào việc xây dựng hệ thống **MCP**. Đây là một hệ thống mới được thiết kế để quản lý dữ liệu và workflow. Team đã tiến hành demo hệ thống này cách đây khoảng hai tuần. Bản chất của hệ thống MCP là xây dựng một agent hoạt động trên nền tảng có sẵn. Người dùng có thể nhanh chóng triển khai và kiểm tra hệ thống thông qua MCP.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=KGQZ4rVPmrc9nMq9\u0026amp;start=2935\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[49:10]** MCP sẽ là một hệ thống hoàn chỉnh, bao gồm một cơ sở dữ liệu (**database**) và một máy chủ (**server**). Điều này cho phép hệ thống hoạt động một cách độc lập và có khả năng xử lý dữ liệu lớn. Khác với các hệ thống cũ, MCP sẽ cho phép người dùng điều chỉnh cấu hình và quản lý dữ liệu dễ dàng hơn.\n\n**[49:58]** Bản chất của MCP là một agent, được định nghĩa theo một cấu trúc input và output cụ thể. Điều này cho phép các hệ thống khác nhau có thể kết nối và tương tác với MCP thông qua các giao thức tiêu chuẩn. Nói cách khác, MCP có thể được tích hợp vào bất kỳ hệ thống nào thông qua các giao thức được định nghĩa sẵn.\n\n**[50:35]** MCP cũng cho phép người dùng quản lý dữ liệu thông qua Knowledge Database, bản chất nó là timescale database, dump hết mọi data về hoạt động của team vào trong đó. Đây là một cơ sở dữ liệu dạng time-series, cho phép ghi nhận các sự kiện theo thời gian thực, ai làm backend sẽ quen dạng event sourcing, event log. Ví dụ: ghi nhận thông tin về các thành viên của team, trạng thái hoạt động của hệ thống, hoặc các sự kiện quan trọng khác.\n\n**[51:13]** Knowledge Database sẽ lưu trữ toàn bộ dữ liệu hoạt động của team, bao gồm các thông tin như ai đã thực hiện tác vụ gì, trạng thái của hệ thống vào từng thời điểm cụ thể, và các thông tin khác liên quan đến hoạt động nội bộ của team. Điều này cho phép team theo dõi và phân tích hiệu suất làm việc, từ đó đưa ra các quyết định điều chỉnh hợp lý.\n\n**[51:51]** Concept của hệ thống sẽ có một thành phần gọi là Landing Zone. Landing Zone có nghĩa là mọi dữ liệu mà mình đang có ,  khoảng mười mấy đến hàng chục bộ dữ liệu (database) ,  sẽ được tập kết vào đây. Trước đây, khoảng ba đến năm năm trước, nếu muốn xây dựng một hệ thống lưu trữ dữ liệu mình sẽ tạo một con bot để thu thập mọi hoạt động của team và đưa vào trong cơ sở dữ liệu của mình.\n\nVới mô hình Meta mới, tất cả các dữ liệu lớn (Big Data) sẽ được dump vào một kho lưu trữ tạm thời dưới dạng file .dat trên S3 hoặc GCS (Google Cloud Storage). Con MCP này sẽ có khả năng đọc trực tiếp từ Landing Zone. Nếu hệ thống thấy rằng dữ liệu trong Landing Zone có giá trị và cần thiết, nó có thể tự động chuyển đổi dữ liệu đó sang dạng Time Series Database (TSDB) để sử dụng lâu dài. Đây chính là end game (kết quả cuối cùng) của hệ thống này.\n\nCòn lại, vấn đề sẽ là xây dựng các Use Case (trường hợp sử dụng) dựa trên các dữ liệu đã được tổ chức trong hệ thống ,  theo hướng mà team mong muốn. Đây là định hướng phát triển quan trọng của hệ thống MCP trong thời gian tới.\n\n**[52:25]** Vậy là hiện tại team sẽ có một hệ thống cơ sở dữ liệu cũ ,  đó là cơ sở dữ liệu dạng table kiểu cũ, nằm ở phần bên dưới của hệ thống (có thể thấy trên diagram với các khối màu xanh dương). Giờ đây, team đang bổ sung thêm hai thành phần mới:\n\n- Thành phần **Landing Zone** ,  nằm trong khối màu vàng phía trên của hệ thống.\n- Thành phần **Time Series Database (TSDB)** ,  được kết nối trực tiếp với các thành phần trong hệ thống cũ để phân tích và khai thác dữ liệu.\n\nTeam đang lưu trữ các dữ liệu thô trong Landing Zone. Về bản chất, việc tập kết dữ liệu trong Landing Zone giống như việc gom quân ,  tập trung tất cả dữ liệu về một chỗ, sau đó mới quyết định cách phân tích và xử lý. Đây là cơ chế giúp hệ thống vận hành linh hoạt hơn và dễ dàng mở rộng khi có thêm dữ liệu mới.\n\n**[53:11]** Điểm đặc biệt của hệ thống này là khả năng tự động chuyển đổi dữ liệu từ Landing Zone sang Time Series Database. Cơ chế này xuất phát từ nhu cầu ngày càng tăng về phân tích dữ liệu cục bộ (local analytics). Đây là xu hướng đang nổi lên trong bối cảnh sự phát triển của AI (Trí tuệ nhân tạo).\n\nSự trỗi dậy của AI đã làm gia tăng nhu cầu về các hệ thống phân tích dữ liệu theo thời gian thực. Khi các dữ liệu thô được tập kết vào Landing Zone, hệ thống sẽ tự động nhận diện dữ liệu có giá trị và chuyển chúng sang TSDB để phân tích chi tiết hơn. Đây là một bước tiến quan trọng trong việc xây dựng hệ thống phân tích dữ liệu hiệu quả và có khả năng thích ứng với những thay đổi của thị trường.\n\n**[53:45]** Hiện tại team đã có thể chạy analytic trực tiếp cho phần dữ liệu được lưu trữ trên local. Hệ thống này cho phép chạy analytic ngay trên dữ liệu Data Lake mà không cần phải chuyển dữ liệu đi xa. Đối với phần dữ liệu trong Landing Zone ,  tức là phần file packet mà Huy đang show trên màn hình ,  đây là phần mà team cần tập trung nghiên cứu thêm. Vấn đề này có liên quan đến text processing, nên mấy anh em cần phải pick up (nắm bắt) chủ đề này. Cái này cũng không khó lắm, chắc học trong vòng nửa ngày là có thể nắm được cơ bản.\n\nPhần Prompt để tìm kiếm và khai thác dữ liệu cũng khá nhanh và đơn giản, không phức tạp. Đây là phần rất đáng để thử nghiệm vì nó liên quan đến cơ chế knowledge discovery (khám phá tri thức) trong hệ thống. Đây là một trong những phần nâng cấp mới mà Huy vừa nhắc tới.\n\n**[54:22]**  Điểm nổi bật nhất của hệ thống trong đợt nâng cấp này chính là **Knowledge Hub**. Đây là nơi mà team sẽ tập trung toàn bộ dữ liệu để phục vụ cho việc phân tích và khai thác tri thức. Knowledge Hub sẽ trở thành một dạng **data pool** chung của toàn team. Bất kỳ ai cũng có thể thêm dữ liệu vào đây, và hệ thống sẽ xử lý, chuyển đổi dữ liệu theo format tiêu chuẩn.\n\nĐiều quan trọng là khi hệ thống đã được thiết lập xong, mọi người trong team sẽ có chung một **protocol** để sử dụng. Các module hoặc component khác nhau sẽ có thể **share (chia sẻ)** chung một cấu trúc dữ liệu và truy cập trực tiếp vào Knowledge Hub. Đây sẽ là nền tảng chung để đồng bộ dữ liệu và xử lý dữ liệu trong nội bộ team.\n\n**[54:58]** Về phần cơ sở dữ liệu (DB), hệ thống sẽ có hai lớp:\n\n- **DB cũ:** Dùng để hỗ trợ các nghiệp vụ hiện có và xử lý các dữ liệu có cấu trúc sẵn.\n- **DB mới:** Được thiết kế để kết nối trực tiếp với **Knowledge Hub** và hỗ trợ phân tích dữ liệu theo thời gian thực.\n\nĐiểm đặc biệt là phần **MCP** sẽ đóng vai trò như một **protocol** để các module khác nhau có thể giao tiếp với nhau. Điều này có nghĩa là bất kỳ dữ liệu nào cần được truy cập hoặc xử lý, chỉ cần đưa vào đúng đường dẫn của hệ thống thì nó sẽ tự động được xử lý theo cấu trúc tiêu chuẩn. Đây là cách để hệ thống đồng nhất dữ liệu và tránh xung đột khi có nhiều nguồn dữ liệu cùng được xử lý.\n\n**[55:43]** Từ giờ, team sẽ cần làm quen với các cơ chế xử lý dữ liệu mới. Mọi người nên dành thời gian để tìm hiểu thêm về các thành phần trong hệ thống mới. Khi các thành phần này hoạt động ổn định, các dự án mới của team sẽ tận dụng các công cụ này để triển khai nhanh hơn và hiệu quả hơn. Đây sẽ là bộ công cụ chính để phục vụ cho các dự án trong tương lai.\n\nHệ thống này có tiềm năng trở thành **requirement** bắt buộc trong các dự án tiếp theo. Nếu bạn muốn bắt kịp với hệ thống mới, hãy bắt đầu từ việc tìm hiểu các nguyên lý cơ bản về MCB và các protocol liên quan.\n\n**[56:40]** Trước đây, khi team triển khai hệ thống trên S3 hoặc GCS (Google Cloud Storage), việc xử lý dữ liệu khá mất thời gian. Tuy nhiên, với cơ chế mới, dữ liệu từ Landing Zone sẽ được xử lý nhanh hơn và dễ dàng hơn.\n\nHệ thống đã được thử nghiệm trên nhiều nền tảng khác nhau, bao gồm **S3** và **GCS**. Tuy nhiên, vì hạ tầng hiện tại của team đang chạy trên **GCS**, nên các dữ liệu từ Landing Zone sẽ được xử lý trên GCS trước. Mặc dù vậy, về mặt kỹ thuật, hệ thống này có thể mở rộng sang các nền tảng khác mà không gặp trở ngại lớn.\n\n**[57:45]** Cơ chế hoạt động của Landing Zone khá đơn giản:\n\n- Các dữ liệu từ nhiều nguồn khác nhau sẽ được tập trung vào Landing Zone.\n- Các dữ liệu này sẽ được lưu dưới dạng **file Parquet** theo từng ngày.\n- Hệ thống có khả năng đọc lại các file này thông qua cơ chế **Time Series Database** (TSDB).\n\nHiện tại, một số file **Parquet** mẫu đã được tạo và đang trong quá trình kiểm tra. Nếu cần, team có thể chạy thử demo trên các dữ liệu mẫu này để kiểm tra tính nhất quán của hệ thống.\n\n**[58:24]** Những hoạt động của team giống như kiểu **AI sub** hoặc **Memo** thì nó cũng được đẩy hết lên đây. Nhiệm vụ của **Landing Zone** là lưu trữ mọi dữ liệu mà team muốn, ai muốn lưu trữ gì thì cứ đẩy hết vào đây rồi sau đó hệ thống sẽ quyết định xử lý dữ liệu đó như thế nào. Hệ thống cũng đã cung cấp một số công cụ để mọi người có thể đẩy dữ liệu lên, ví dụ như là các **API proxy** để forward các sự kiện. Mọi người muốn push thông tin lên Landing Zone thì chỉ cần gọi API là được.\n\nMemo hiện tại đang sử dụng cơ chế này để lấy dữ liệu từ các **nền tảng xã hội** và đồng bộ vào hệ thống. Cơ chế này cũng đã được thử nghiệm thành công. Còn đối với những loại dữ liệu có tính đặc thù như là **Discord messages** hoặc **data từ Basecamp**, team cần phải xây dựng các **crawler** hoặc các **connector** để thu thập dữ liệu. Hiện tại, team đã có một số template sẵn cho những loại dữ liệu này.\n\n**[58:59]** Về hướng phát triển tiếp theo, team sẽ tập trung vào việc khai thác dữ liệu từ Landing Zone. Nếu bạn muốn tham gia vào dự án này, lời khuyên là hãy bắt đầu từ một **vertical cụ thể**. Ví dụ:\n\n- Xác định một **use case** rõ ràng.\n- Tìm hiểu xem **dữ liệu nào** cần cho use case đó.\n- Định nghĩa lại cơ chế khai thác dữ liệu theo hướng **từ trên xuống dưới**.\n\nThay vì kiểu thấy dữ liệu nào hay thì lưu lại, team nên nghĩ theo hướng là **xác định use case trước** rồi mới quyết định lưu trữ dữ liệu. Điều này giúp hệ thống hoạt động một cách có tổ chức và dễ dàng quản lý hơn.\n\nVí dụ cụ thể là nếu có một use case về **Project Nghệ Nhân** thì team sẽ cần tạo một **Git Agent** để thu thập dữ liệu từ Git, sau đó đẩy dữ liệu đó vào **Knowledge Hub** thông qua MCP. Từ đó, hệ thống sẽ định nghĩa các công cụ khai thác dữ liệu cho use case này.\n\n**[1:00:16]** Ngoài ra, team đang phát triển một MCP Server nhỏ. MCP Server này thực chất là một server cơ bản, sử dụng các thành phần kỹ thuật thông thường của hệ thống internet hiện tại. Nó định nghĩa các input và output rõ ràng, cho phép kết nối với nhiều loại giao diện khác nhau.\n\nVí dụ:\n\n- Nếu có một MCP để xử lý dữ liệu từ Slack, team sẽ định nghĩa các API cho từng loại dữ liệu.\n- Nếu cần có các công cụ để đọc dữ liệu từ Google Sheets hoặc phân tích dữ liệu về tình trạng check-in trong tuần, team có thể tạo các MCP tool để xử lý những dữ liệu đó.\n\nMCP sẽ là một thành phần trung gian để đồng bộ và xử lý dữ liệu từ nhiều nguồn khác nhau. Mọi người có thể truy cập các công cụ này từ Editor, Command Line, hoặc bất kỳ giao diện nào khác.\n\n**[1:01:07]** Bản chất của MCP là nó sẽ đóng vai trò như một **API Gateway** để kết nối các công cụ. Nếu bạn cần theo dõi việc check-in hàng tuần của mọi người trong team, bạn có thể tạo một MCP để thu thập dữ liệu từ **Knowledge Hub** và Google Sheets, sau đó so sánh dữ liệu để xem ai đã check-in và ai chưa check-in.\n\nHệ thống hiện tại đang dừng ở mức độ triển khai MCP Server cơ bản. Giao diện hiện tại sử dụng **Command Line** để gọi MCP, nhưng về cơ bản team có thể mở rộng để kết nối với các công cụ khác nhau.\n\n**[1:01:43]** Hệ thống đang tập trung vào việc triển khai cơ chế xác thực (authentication) và phân quyền (authorization).\n\n- Authentication – Xác thực người dùng để truy cập vào hệ thống.\n- Authorization – Phân quyền cho các hoạt động xử lý dữ liệu.\n\nHệ thống đang được sử dụng nội bộ trong team, chưa công khai ra bên ngoài. Nếu bạn muốn sử dụng MCP, bạn sẽ cần nhập vào **private key** để xác thực quyền truy cập.\n\n**[1:02:23]** Về mặt kỹ thuật, MCP có thể mở rộng ra các thành phần khác nhau trong hệ thống. Mọi người có thể tích hợp MCP vào các ứng dụng hiện tại hoặc các công cụ hiện có mà không cần phải viết lại quá nhiều code.\n\nTeam vẫn đang thử nghiệm tính năng này và tập trung vào việc hoàn thiện các phần về bảo mật và quản lý quyền truy cập. Khi hệ thống đã ổn định, mọi người có thể tích hợp MCBP vào các quy trình xử lý dữ liệu hiện có.\n\n**[1:03:00]** Chỉ là đang dừng lại ở đây thôi, chưa xử lý được các bài toán phức tạp về authorization. Sau khi hoàn thành các bước hiện tại thì mới đến việc xử lý các bài toán phức tạp hơn liên quan đến authorization và quyền sử dụng hệ thống. Mọi người có thể tập trung vào các vấn đề cơ bản trước đã.\n\nRồi, cảm ơn Huy nhé. Đây là một trong những phần phát triển kỹ thuật quan trọng của team. Nếu theo dõi các hoạt động trên tech và AI Club, mọi người sẽ nhận ra team đang tiến tới các bước tiếp theo trong quá trình phát triển. Về mặt kỹ thuật, mọi người nên chú ý vào các từ khóa quan trọng mà Huy vừa đề cập. Nếu chưa hiểu rõ thì có thể xem lại bản ghi để nắm được đầy đủ thông tin.\n\n**[1:03:45]** Team core vẫn đang tiếp tục phát triển hệ thống. Yêu cầu tất cả các thành viên tham gia vào dự án để có thể **transfer knowledge** hiệu quả hơn. Dự án này là môi trường để mọi người học hỏi và thực hành.\n\nĐây là cơ hội để các thành viên mới trong team tiếp cận và nắm bắt các khía cạnh kỹ thuật quan trọng. Nếu cảm thấy chưa sẵn sàng thì có thể tham khảo các phần hướng dẫn và tài liệu nội bộ để bắt kịp. Việc training sẽ được thực hiện trong quá trình làm việc chứ không có các buổi training riêng. Đây là môi trường thực hành trực tiếp để vừa làm vừa học.\n\n**[1:04:29]** Bên cạnh việc phát triển hệ thống, team cũng đang thực hiện knowledge transfer từ các dự án đã hoàn thành. Dự kiến cuối tháng sẽ có một buổi tổng hợp lại các bài học rút ra từ các dự án này. Nếu ai chưa thực sự hiểu rõ thì có thể tham khảo hoặc hỏi các thành viên đã làm qua để nắm thêm thông tin.\n\nNếu cảm thấy chưa sẵn sàng hoặc cần thêm thông tin thì có thể hỏi trực tiếp các thành viên trong team. Mọi người có thể ping các thành viên có kinh nghiệm hơn để nhận được sự hỗ trợ.\n\n**[1:05:07]** Team có hai nhóm khác nhau đang hoạt động song song:\n\n- **Team của Tuấn** đang phát triển một số game và ứng dụng nhỏ.\n- **Team build** đang làm việc trên các ứng dụng thử nghiệm để kiểm tra tính khả thi của hệ thống.\n\nCác hoạt động này tương tự với các nhóm **Build Club** và **AI Club** trong team Foundation. Một số sản phẩm đã bắt đầu có **output** tốt. Tuấn và team đang phát triển một trò chơi dựa trên **Turing Machine**.\n\n**[1:06:38]** Trò chơi **Turing Machine** mà team Tuấn phát triển được chuyển thể từ phiên bản board game thành phiên bản trên thiết bị di động. Mục tiêu của trò chơi là đoán một chuỗi gồm **ba số**. Để đoán đúng chuỗi số này, người chơi sẽ nhận được các **clue** (gợi ý).\n\nVí dụ:\n\n- Nếu gợi ý nói rằng “một trong ba số phải lớn hơn 1” → Người chơi có thể nhập số vào và hệ thống sẽ xác định xem đáp án có đúng hay không.\n- Nếu hai số sai nhưng một số đúng thì hệ thống sẽ phản hồi ngay để người chơi có thể tiếp tục điều chỉnh.\n\nLuật chơi khá phức tạp nên có thể gây khó khăn cho người chơi mới. Tuấn và team đang tiếp tục điều chỉnh để trò chơi trở nên dễ tiếp cận hơn mà không mất đi tính thử thách.\n\n**[1:07:23]** Tên trò chơi là [**Pocket Turing**](https://pocket-turing.vercel.app/) bởi vì phiên bản board game gốc của nó liên quan đến các thẻ đục lỗ – giống như cơ chế hoạt động của Turing Machine trong lập trình máy tính. Tuy nhiên, mình đã điều chỉnh và phát triển thêm các yếu tố mới để phù hợp hơn với phiên bản di động.\n\nMÌnh có kế hoạch tinh chỉnh và mở rộng trò chơi trong các phiên bản tiếp theo. Ngoài ra, cũng đang kiểm tra xem có thể triển khai thêm các tính năng thu phí hoặc các tùy chọn nâng cao để tăng khả năng monetize.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=bP3ZjI3af1fVijle\u0026amp;start=3997\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:08:16]** Mình đang thử nghiệm phiên bản beta của trò chơi. Trò chơi đã hoàn thiện về mặt gameplay và người chơi có thể trải nghiệm trọn vẹn các tính năng. Bước tiếp theo là thử nghiệm với nhóm người dùng rộng hơn để thu thập phản hồi và cải thiện sản phẩm.\n\n**[1:09:15]** Mục tiêu tiếp theo là đưa trò chơi vào App Store và Google Play để tiếp cận nhiều người dùng hơn. Trước mắt, team muốn đảm bảo trò chơi hoạt động ổn định và không phát sinh lỗi nghiêm trọng.\n\nTuấn kỳ vọng trò chơi sẽ thu hút được ít nhất **100 người dùng** trả phí trong giai đoạn thử nghiệm đầu tiên. Nếu nhận được phản hồi tích cực  sẽ mở rộng thêm các tính năng mới và cải thiện trải nghiệm người chơi. Mong nhận được phản hồi từ các thành viên khác để có thể điều chỉnh và hoàn thiện sản phẩm tốt hơn. Tuấn đã chia sẻ link tải trò chơi cho các thành viên trong team để mọi người có thể trải nghiệm và đóng góp ý kiến.\n\n**[1:10:13]** Nếu anh em hứng thú với việc build sản phẩm thì giai đoạn này là thời điểm phù hợp để bắt đầu. Trước đây team đã thử nghiệm nhiều lần nhưng lần này là cơ hội tốt để làm bài bản hơn. Việc phát triển các sản phẩm nội bộ không chỉ giúp cải thiện năng lực kỹ thuật mà còn mở ra cơ hội thương mại hóa trong tương lai.\n\nNgoài game của Tuấn, team đang phát triển thêm các công cụ khác. Nếu có ý tưởng hay, anh em có thể đóng góp để cùng xây dựng và thử nghiệm. Cách bán hoặc thương mại hóa sản phẩm thì tính sau, quan trọng là hoàn thiện các tính năng cốt lõi trước.\n\n**[1:10:58]** Tiếp theo là phần của An. An từng làm một tool gọi là **Rec** để tổng hợp thông tin theo dạng giống với hệ thống của **Apple**. Phiên bản 1 của Rec yêu cầu người dùng tự sắp xếp thông tin, còn phiên bản 2 hiện tại đã được tích hợp AI để hỗ trợ sắp xếp tự động.\n\nTuy nhiên, AI vẫn có một số hạn chế trong việc nhận diện nội dung đầy đủ. Đôi khi AI không thể xác định được toàn bộ ngữ cảnh nên kết quả trả về chưa thực sự hoàn hảo. Tuy nhiên, các nội dung quan trọng vẫn được sắp xếp và hiển thị đầy đủ.\n\n**[1:11:56]** Tool này đang trong giai đoạn hoàn thiện, nhưng các chức năng cốt lõi đã ổn định. Hiện tại, team đang tập trung vào việc cải thiện phần giao diện và tối ưu trải nghiệm người dùng. An dự kiến sẽ tiếp tục phát triển thêm các tính năng bổ sung để hỗ trợ người dùng tốt hơn.\n\n**[1:12:51]** Các dự án của team hiện đang ở giai đoạn thử nghiệm và cải tiến. Nếu ai có thắc mắc hoặc góp ý, có thể trực tiếp trao đổi với An hoặc các thành viên khác trong team. Hiện tại, các dự án đã showcase gần hết. Các phần chi tiết hơn sẽ được đề cập vào buổi sau.\n\n**[1:13:57]** Bên đội mình, anh luôn nói về chuyện kiến thức liên quan tới liquidity và game in general, thì anh em thật sự muốn team mình đẩy theo hướng đó một chút. Vì nó có lợi cho gần như là cái life skill luôn, đúng không? Nên anh muốn team mình đi theo hướng đấy trong đợt này. Mấy anh em, đặc biệt là những người hứng thú với trading, tức là lấy data về để tìm kiếm cái Alpha trên đó, Intel trên đó, để ra được những cái market-making dựa trên điều kiện nào đó.\n\n**[1:14:43]** Nó là một cái, hoặc có thể đi xa hơn để làm một luồng rất tuyệt vời. Hình như hiện tại chỉ là ước mơ của anh thôi. An đã làm được một version, anh thấy khá ok. Đây là cơ hội để cho anh em biết trong team đang có những tiến triển như vậy. Đang chạy ha, mời An. Nói chung là game kiếm tiền thôi. Coi tụi nó kiếm tiền sao thì mình làm vậy. Mấy cái thường thường thì có biết một cái gì để thử, nó cũng là dạng **Delta neutral**, đúng không? Thì mình cũng research những thứ đó. Rồi đi build và research xong để có kiến thức ship.\n\n**[1:15:30]** Chơi cái cột này hết thôi, không nhìn tới đâu nữa. Mọi người thấy màn hình terminal chưa? Có thấy chưa? Có thấy rồi, ok, chạy để chạy thử. Chắc phải zoom lên, zoom lên một hai level, hơi nhỏ, rồi ok rồi. Đây là arbitrage để ăn funding free, thì có nhiều thể loại arbitrage. Cái này chỉ là một trong những loại đó thôi, ăn trên chênh lệch phantom giữa các sàn. Đang tập trung vào ba sàn: Binance, OKX ,  thằng OKX này sàn của nó không có nhiều dữ liệu lắm ,  nên em có cái diagram cho cái đó không, An?\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=IevTgfLbxwcu6MOh\u0026amp;start=4506\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:16:26]** Nghĩ mọi người sẽ hơi khó hình dung. Nhìn cái này chắc không hiểu nó là gì. Có diagram không? Ok, không có vẽ à? Có cái này, to, nhưng là lý thuyết, không cụ thể ra được high level. Không thấy, chắc phải ngồi vẽ lại sơ sơ. Thấy chưa? Chắc nhìn hình của anh đi, hình của anh, biết ngay là cái này luôn. PRL à? Ủa, nó đang chạy lộn, quên, nó đang vào mấy cái socket của…\n\n**[1:17:47]** Tụi nó để lấy real-time data về. Đang lấy dữ liệu từ bao nhiêu account? Ba cái: Binance, Bybit, với cái gì nữa? Ok rồi, init để lấy giá về, đúng không? Lấy giá, lấy funding, lấy phí chưa? Lấy mấy cái data như phí thì đang code theo calculation, chưa xài để lấy về. OKX chắc không có. Mấy cái trade này thì thường chỉ nằm ở hai cái chính: Binance, Bybit. Ừ, setup ok rồi, nó sẽ có mảng thể hiện cái vị nào đang có chênh lệch funding, thì có profit. Em tính được nếu mình vào thì nó sẽ bao nhiêu. PH là số lần thu funding để hòa vốn phí, monitoring cái cao nhất giữa hai sàn. Bước một là lấy chênh lệch funding giữa hai bên, đúng không? Không, em nói là lấy trên ba exchange, so với góc của nó vẫn là exchange net, đúng không? Ừ, exchange net tính sau thôi.\n\n**[1:19:49]** Funding thì giả sử tụi nó thường, trên lập giá thì không có nhiều, kiểu một thằng dương, hai thằng đều dương, hoặc hai thằng đều âm, thì chênh lệch ít hơn. Thật ra mình đặt counter trên cái chênh khác, chỉ là offset giá di chuyển, để không lỗ bởi giá. Trên exchange net, không có chênh lệch đó, mình làm funding lúc nào cũng bằng 0. Vì không có phí swap, thay vì counter trên sàn khác bằng cái đó, mình counter lúc funding bằng 0. Hiện tại chấp nhận ít lợi hơn, nhưng version đầu tiên vậy.\n\n**[1:20:33]** Lấy giá, lấy **funding**, rồi coi có deploy capital thôi, đúng không? Chạy thử chưa? Chưa đổ tiền vô. Ừ, cái này kiểu game scale, cần nhiều tiền mới ăn, vài trăm ngàn thì vô không thấy gì. Hiểu, ok. Về kỹ thuật thì em làm gì? Từ lúc price crash, em làm những gì?\n\n**[1:21:25]** Đầu tiên em research chart trước, coi nó thế nào, có chênh lệch gì không. Xong rồi ship, code hết bằng Cloud 3.7. Phải lấy data sàn trước qua socket, từ API dock của sàn, quăng lên WebSocket client. Sàn nào cũng có dock, lấy về ship lên, tự view được.\n\n**[1:22:15]** Web cho ba sàn xong, có form đầy đủ. Sau đó build chart, giải thích cho nó, build từ từ. Check data, sai thì tự build sample để đảm bảo data đúng. Vì format giữa các sàn khác nhau.\n\n**[1:23:01]** Khác hết, nên cần test data valid mới compare được. Tiếp theo build con để vào lệnh, khi phát hiện thì có thằng đứng ra vào lệnh, watch bot xem lỗ không, làm từ từ, hợp lý. Quá trình hết bao lâu? Một tuần.\n\n**[1:23:58]** Bước tiếp theo của tool này là gì? Em sẽ check data trước, xem sàn nào dễ kiếm tiền, có lời. Quản lý rủi ro, lấy phí structure của sàn. Vì phí ảnh hưởng lớn, phải tính chính xác, đảm bảo lời mới vào lệnh. Có back system không? Có history để backtest không? Có, nhưng không chính xác.\n\n**[1:25:38]** Đây là showcase kỹ thuật, hướng này team tự lập, ok. Anh em showcase game trading, có bước đẩy tiếp, đang trên đường làm cái muốn làm, rất good. Công nghệ, techno house, xài thế nào thôi. Quan trọng nhất là…\n\n**[1:26:19]** Hoạt động team hiện tại vậy nhé. Productivity gần đây bắt đầu sync. Tom comment trước, productivity team giờ bao nhiêu? 2/10 hay 4/10? So với 6-7/10 cần, anh thấy setup tốt rồi.\n\n**[1:27:01]** Bước tiếp theo về mặt catch up cái công nghệ tool link để hỗ trợ mình vận hành đội theo mô hình này, nó đang được improve từ từ lên. Bên thị trường, thị trường **funding** nói chung và những sản phẩm bắt đầu cũng rục rịch quay trở lại. Người ta thấy công nghệ ổn định hơn. Bên **crypto** thì do macro ảnh hưởng nhiều, nhưng cứ có trend nào về tech là sẽ vô cắn thôi, là vậy ha. Anh đang thấy sắp tới tín hiệu để nó resume lại thì đâu đó khoảng 50/50. Trước đó anh nhìn thì cái market rất tệ, kiểu mọi thứ chưa sẵn sàng. Dù có học nhiều, làm nhiều thì cũng không ra kết quả liền.\n\n**[1:27:40]** Nhưng đợt này anh nghĩ mấy anh em sẽ phải có sự yêu cầu về chuyện tham gia mấy cái này ha. Tuần sau chắc nhờ Huy, Tom với Thành thống kê lại, xem ngoại trừ dự án anh em đang làm thì hoạt động tham gia những dự án side project như vậy, anh em nào đang làm gì ha. Đó là thần sau nội dung, thì cũng trao đổi gần hết rồi. Tuần sau còn một số cái core flow tiếp, nhưng chắc cũng không ảnh hưởng quá nhiều tới mọi thứ.\n\n**[1:28:22]** Hôm nay là ngày 14, hy vọng đến cuối tháng này, buổi họp team tiếp theo sẽ show được nhiều progress hơn. Tất cả những thứ mình đang làm rất quan trọng ha. Toàn bộ này đều đang được đưa lên Memo, tụi anh đang sử dụng Memo đó không chỉ để share trên đó không ăn thua.\n\n**[1:29:01]** Shill khắp nơi mấy công ty khác mình biết, bắt đầu mở rộng network ra để xem tìm kiếm user cần thiết. Chuyện là mình biết những thứ này rồi thì làm sao mình mound được khả năng mình profit từ kiến thức của mình, ý là vậy. Ok ha, đó là cái skin mà team đang chạy theo. Tóm lại, thị trường nhận định đang như vậy. Tuần sau mấy anh em sẽ phải đăng ký làm cái registration vô cho những cái phần, với lại Huy, Tom và Thành là những bên mạng bắt buộc.\n\n**[1:29:45]** Còn bên mấy cái hobby club, như kiểu Build hay gì đó, thì anh không yêu cầu cao vào bên đó, ra cái kỹ thuật để apply, nó cũng không quan trọng lắm. Quan trọng là output nhiều hơn. Hai nhóm khác nhau: một nhóm là những phần mà core project mình sẽ làm, tập trung vào làm sao tăng activity, tăng cái **knowledge base** của mọi người; còn cụm kia tập trung vào cái **skill set**, chuyện develop product sao launch, làm sao làm onboarding tốt hơn, user các kiểu. Là một cái nhóm skill set khác.\n\n**[1:30:27]** Đặc biệt là Huy, Huy đang co cho cái việc quay trở lại office để bắt đầu làm shadowing cho chuyện **knowledge transfer**, thì nếu được thì cứ tiếp tục để nó diễn ra. Rồi xem số liệu như thế nào thì report lại cho anh ha. Hopefully khi nào có con số đây thì mấy anh em xem thảo luận tiếp, làm sao setup cái vụ shadowing đó trên mấy cái dự án mà mình, mấy cái site mà mình tham gia, để có cái case share với nhau ha.\n\n**1:31:05** Toàn bộ là vậy. Nếu bây giờ không có gì khác thì chắc mình kết thúc ở đây. Đây có bao nhiêu bạn nhờ? 28 bạn hả? Không, đang bao nhiêu bạn trong con này nhờ? Chuẩn bị spam ICY, có vấn đề để transfer ICY chưa? Để cái này mai mốt lấy acc anh, hoài căng quá. Có vấn đề trên ICY nhờ. Mọi người ra random nha, giật cô hồn nha. Amount 28 thì mình sẽ drop 14 token ICY, entry là 14 rồi. Xin mời, duration là 5 giây. Ok, let’s go. Một ICY hồi nãy là tương đương khoảng 100 Satoshi rồi đó.\n\n**[1:32:09]** Tuần sau lịch vậy nha, mọi người xem phối hợp với nhau để làm việc cho hiệu quả rồi. Bye bye.\n\n---\n\n### English Transcript\n\n**[05:30]** Hello, can you hear me? Oh, okay, it’s fine now. Today, I think we’ll start a bit early. Today’s session will probably combine with the brother in the meeting for a little bit. One part will be to do a showcase, the second part is that the brother will summarize some things that were discussed with the guys previously. The second and third parts are that we’ll start letting the guys register for tasks. For now, to make it easier, I’ll probably let Huy Nguyễn go first to show the parts related to Huy, which involve ICY a little, and then show some tech stuff that our team is currently working on. This will give me a snapshot of how the tech team is doing right now. Then, moving forward, what our team needs and what the guys can contribute to it. Alright, let’s get started.\n\n**[06:35]** Huy, where’s Thành? Let’s give them the stage now. Okay, for the first content, let’s start with ICY Swap. We announced it, last week or this week it was deployed, so now how are the differences, I’ll probably ask Huy to go over that whole series again.\n\n**[07:29]** Hello, alright, I’ve seen the screen already. So now everyone can go to the ICY Swap page to swap. Here, I’ll show the data. But up here, everything is fully ready now. The only thing left to do is that we’re currently reviewing the ICY numbers. Because previously, when we were operating, we operated by pegging the ICY price, so we didn’t really care much about the circulating supply. So there were some cases where we put it into the team’s wallets or transferred it to Mochi Balances for me or for brother Bảo. Those things need to be reviewed again to get the correct circulating supply number. Because now we’ll sit down, and the price will be dynamic based on the pool, so we need to check that again, and it’s almost done.\n\n**[09:09]** Now, the only thing left is brother Bảo’s account that needs to be checked again. I remember there was a time we transferred to brother Bảo, so now we’re reviewing that part, doing the addition and subtraction, and cutting that part out of the circulating supply, then this number will come out correct. For now, if anyone wants to swap to support, they can swap on this page. That’s the current schedule. I’ll show the list of our current Holders so the guys can see, probably need to know a bit more. Up until now, people participated without paying much attention, but this time we need to be more mindful.\n\n**[09:51]** Our ICY is deployed on Base, right? So when the guys go into the Holder list, everyone will see a list of all the wallets currently holding our team’s ICY, which are the CCK Holders. That’s one thing. And then the link to access this, Huy will share it, I guess. Because if people go search for it, they probably won’t find it.\n\nFirst, the guys need to understand this. Moving on to this part now. I think the guys need to pay more attention to this part. It’s become the norm in the tech world already, no need to do anything new anymore. So if the guys grasp this, it’ll be better.\n\n**[10:33]** Our ICY is now listed. In this list, there are minter wallets, wallets used to budget for activities, and some wallets holding large amounts of ICY. Activities related to staking ICY will be rolled out gradually in the coming time. This is the first piece of information the guys need to understand clearly.\n\n**[11:15]** Huy, demo the swap process for us. Does anyone have a Bitcoin address with some ICY? Is Vincent here? Okay, now let’s try swapping from ICY to Bitcoin. The current price is calculated dynamically based on the circulating ICY amount and the pool. The swap function is very simple, just enter the amount, press swap, and it’s done.\n\n**[12:27]** Wait, don’t enter a fake address. Okay, it’s good now. The first frame is ICY as usual. Below it, it’s displaying the unit in satoshi, which is the smallest unit of Bitcoin. When you enter the amount, it will automatically convert. However, the current exchange rate is slightly off, around 1.2 instead of 1.5. This is probably a small calculation error, we can fix it.\n\n**[13:28]** You need a minimum amount of ICY to swap. Try entering 30 ICY and see how it goes. Refresh it and check if it works.\n\n**[14:43]** It seems like there’s not enough money in the wallet. Do you have ETH on Base? Transfer it to Base and check again.\n\n**[15:51]** It’s not that error. The issue is that the account hasn’t been registered, so it can’t perform the transaction. We’ll fix that part later. The goal here is to help everyone understand the swap mechanism and how token pricing works better. If you understand it well, it’ll be easier to manage tokenomics later on.\n\n**[16:47]** Huy, quickly explain the pricing mechanism again. Last time Quan demoed it but didn’t go into detail about that part. The price of ICY is determined by the minting mechanism, meaning the price won’t fluctuate heavily if someone swaps a large amount. It doesn’t operate like an automated market maker (AMM) mechanism; the price will be controlled through the minting mechanism. This mechanism helps keep the price stable even with large transactions.\n\n**[17:43]** It completely depends on Bitcoin. So if Bitcoin’s price goes up, the amount of ICY you guys are holding will increase in USD value. As for the minting mechanism, Huy, explain a bit more. Generally, our overall mechanism so far is that we fix ICY’s value to USDC. You guys don’t need to worry too much, just understand simply that one ICY is equivalent to 1.5 USD.\n\n**[18:37]** This assurance part is to help the operating team ensure that by the deadline, USDC will be added into the contract for everyone to swap. The swap rate in the old contract was fixed at 1.5 ICY, but that was the old model. Our new model is more flexible. If you guys have used Uniswap or other AMMs (Automated Market Makers), it’s somewhat similar. Here, the mechanism works with a liquidity pool underneath, which contains both ETH and USDC. Depending on the pool’s situation at that time, the exchange rate will be adjusted based on the amount of ETH and USDC in the pool.\n\n**[19:18]** Our mechanism works similarly. The price of ICY will be determined by the amount of Bitcoin in the pool and the amount of ICY currently in circulation. The formula is simple: we have the amount of ICY (X), we have the amount of BTC (Y) in the pool, then X/Y will give us the value of one ICY in terms of BTC. This formula is basic mathematics, nothing complicated.\n\n**[19:55]** Due to our operating mechanism, there will be two moments that change liquidity:\n\n1. **The first moment** is every month when the operating team adds more BTC into the pool to cover the costs of the team’s activities. At this point, the price of ICY will increase slightly because the amount of BTC in the pool increases.\n2. **The second moment** is when the team adds more ICY into the pool (minting more). When more ICY is minted, the market price of ICY will decrease because the amount of ICY in the pool increases.\n\n**[20:35]** The two cases above will directly affect the price of ICY. However, if the price of Bitcoin changes, the USD value of ICY might change, but the price of ICY in terms of BTC will not change. The market impact from Bitcoin is an external factor and does not directly affect the minting or the value of ICY in the pool.\n\n**[21:12]** If you guys have any more questions, feel free to ask, and we’ll answer them later. Oh, there’s a question about swapping back from BTC to ICY, right? Currently, that function isn’t available. Right now, we only support swapping from ICY to BTC, not the reverse swap. Meaning you can buy in, but selling out isn’t supported yet.\n\n**[21:40]** Thank you, Huy. Anything else to note? One thing to note is that we’re still in the testing phase, so there might be some exceptional cases. For example, some situations might arise during swaps or when liquidity isn’t sufficient. Fundamentally, though, the current flow is still operating stably.\n\n**[22:00]** Like the minimum ICY amount required to swap. Because essentially, our team is covering the gas fees for transactions on ETH, on Base, and even on BTC, we’re kind of limiting it so that the ICY amount swapped needs to be a bit higher. This is to avoid situations where people swap just 1-2 ICY to test, which would cost gas fees, so we’ve set it at around above 20 ICY to allow swapping on the web.\n\nThe second thing is that since minting more ICY will change the market price, I’ve disabled the part about our previous salary advance mechanism.\n\n**[22:37]** Meaning if everyone advances salaries at the same time, it would affect the price, right? So the lesson learned from this is that after this round, there are a few points I’m noticing. Our team is starting to focus on building tools to support our operations. These are also some new experiments and some things that genuinely support our activities. But after finishing these tasks, we’ll produce some articles related to them. So if any of you didn’t participate in those projects earlier, you can look back at those articles to understand the game, the knowledge gained from that round, and what the guys working on those projects achieved.\n\n**[23:24]** So with this ICY Swap round, we’ll probably get two or three articles, right? Yes, like three articles. And if we want to write more, there’s still plenty to write about. Yeah, alright, take it slow and steady.\n\n**[24:02]** After Huy’s part, I thank Huy and move on to the second topic related to what our team is currently doing. Brother Bảo, whoever wants to go first is fine, but I’ll probably let Thành speak first. Thành said it’s okay for him to go first, he’ll gather everything to let everyone know what stage the team is at. But I said let Thành go first because someone’s ringing the bell. Alright, I invite Thành to start.\n\n**[25:00]** Everyone, our Memo is one of the big things this round, and we’ve upgraded its format to make it look a bit better. We always want to create content maps, things that we can read and upload here. But currently, that model isn’t really that effective anymore because new models compress data, and querying directly from there would be more efficient.\n\nSo the point is that putting ordinary knowledge onto Memo isn’t very suitable anymore. For this round, when reworking it, there’s one main idea I want to tell you all: Memo will now be used for one sole purpose ,  the knowledge gained from projects.\n\nThat’s almost like the new things that come directly from our team’s activities. In the future, it’ll mostly consist of what field it’s related to and what we’ve done in that field. There’s more to it ,  maybe after a period when they retrain the model, our data will become part of the shared knowledge for the whole community.\n\n**[25:39]** And I think this part will be very helpful for things like retraining AI models later or for cases where we want it to provide automatic suggestions.\n\n**[26:24]** The content will become part of that model, or if there are internet search tools, our articles might just be a small part of the referenced materials, like a small piece in a citation. That’s not a big issue. But overall, all this content will pretty much become the spirit of the team.\n\nIn this major upgrade, there’s one key point that Tuấn has completed, right? Tuấn, the part about syncing all the team’s data, especially the content, is currently being directed this way so the members can understand it better.\n\n**[27:00]** Meaning after this round, the members participating in projects will tend to sit down together to review those projects more closely and determine exactly what the **knowledge gain** from those projects is. After that, the team will upload it to Memo as internal reference material for the team.\n\nThe second part is that at the end of each article, there will be a section related to a **group of reading**. This part isn’t fully complete yet, but the idea is that once it’s finished, there will be an additional section summarizing information about the article so readers can look up and learn more from it.\n\n**[27:47]** In addition, all the data written by the team will be tagged with identifiers such as **GitHub**, **Discord**, or other internal channels. This data will be uploaded to a **blockchain storage** form on the **Arweave (AV)** platform ,  a decentralized storage platform. This ensures that the team’s content has a clear and transparent identifier.\n\nOn top of that, readers will be able to review the articles, rate them, or leave feedback directly on the articles. This is part of the new upgrade idea for the team’s **Memo** page.\n\n**[28:39]** Previously, the team intended to use Obsidian to manage content, but it seems some members had difficulty getting familiar with that tool. Therefore, to make things simpler now, the team will switch to a more direct mechanism. Specifically, instead of having to go through Obsidian, members can submit content directly to the repository of the team’s shared library.\n\nMembers just need to input the content and submit it directly through this platform, without having to follow Obsidian’s mandatory workflow anymore. If someone still wants to use Obsidian, that’s fine, but if they don’t, it won’t affect anything. This is the most fundamental change in the team’s Memo system.\n\n**[29:24]** Currently, the team is working on several main projects, including:\n\n1. Bitcoin Swap ,  already mentioned in the previous section.\n2. Memo ,  just presented.\n3. Two smaller projects:\n    - **Agentic** ,  being developed by Quang and Huy’s group.\n    - **GitHub Bot** ,  being worked on by Thành’s group, currently in testing.\n\nNow, I’ll probably hand it over to Thành to share more about these contents.\n\n**[30:32]** This project was started over a week ago and has officially been running code for more than a week. Its main purpose is to create a reminder system. Previously, the team often encountered situations where, after creating a pull request (PR), people would leave it there, wait for it to finish running, and then forget about the need to review it. This tool will serve to track and update information about daily activities on GitHub or weekly activities on the team’s internal communication channels.\n\n**[31:18]** This system is designed as a simple integration. The basic workflow includes several use cases, such as notifying the person assigned to review, interacting with the GitHub API, and posting information to internal channels like Discord or Slack. Currently, the team is testing it on Discord. Additionally, the team is experimenting with Agentic and a new framework called **Mastra AI**.\n\nThis framework is different from typical Python tools. Some team members aren’t familiar with working in Python, so the team wants to test whether using this new framework is more effective than current solutions. The framework supports features like setting up the environment, defining states to manage data, and allowing reconfiguration based on the team’s needs.\n\n**[32:19]** The system’s structure has two main parts:\n\n1. **Agentic App** ,  This is the main application for handling the system’s activities.\n2. **Discord App** ,  This supports sending notifications to Discord.\n\nAdditionally, the system has a few auxiliary components, such as workflows to handle scheduled tasks, check, and notify developers if there are any pull requests waiting for review. If a pull request exceeds a certain amount of time, the system will send a notification to remind the person responsible for reviewing it.\n\n**[33:12]** The Agentic App will expose a few APIs that allow chatting and tracking the status of pull requests. When a pull request is created, the system will automatically identify conditions like the pull request’s status (work in progress or not), the time it was created, and will notify the reviewer after about 30 minutes from the creation time. For example, if a pull request needs review but no one is assigned or it has exceeded the processing time, the system will automatically ping the responsible person again.\n\n**[35:02]** Instead of having to track manually, the system will attach an agent to automatically monitor and notify through the system’s endpoint. In the logic part, the system will define specific conditions, such as only sending notifications if the pull request was created within 30 minutes or is in a work-in-progress state. If the pull request is updated or changes status, the system will automatically track and notify the developer to ensure nothing is missed.\n\n**[35:39]** The system will operate based on standard code filters. Additionally, it will have some other workflows, like sending notifications at the end of the day to summarize the status of pull requests on Discord. The system will automatically send notifications about the number of open pull requests, their statuses, and the current review status. This is the main function of this tool ,  acting as a reminder tool.\n\n**[36:24]** The system can also integrate with other chat tools. It’s simple ,  you can create an additional command and send a request to the system’s endpoint. These requests will be defined based on a specific schema, such as the input being a **review ID** or other information related to the pull request’s status. The system will take this data and display it on the interface that users frequently use.\n\n**[37:04]** The backend processing of the system is handled through the Lippia tool, which formats JSON data into Markdown tables or data-binding formats. Currently, the team is testing these two processing flows before expanding to additional features. Once the system is stable, these workflows will be opened up for all team members to test and further develop.\n\n**[38:08]** The system is designed to scale flexibly. Team members can independently develop and contribute different workflows. This system allows the creation of tools as standalone **packaging units**, which can then be combined to create more complex workflows. When wanting to release a new workflow, members just need to redefine the basic unit and integrate it into the system.\n\nExpanding workflows will help the system grow horizontally (increasing the number of features) rather than vertically (developing existing features). As the number of workflows increases, the system will become more flexible and powerful.\n\n**[38:54]** Fundamentally, workflows are considered the application layer, similar to previous data APIs. This system will operate at the tool level, but end users will interact with it through the workflow interface. Currently, no entity has successfully implemented this model on a large scale. However, GitHub has now expanded its API for developers to create extensions and integrate them directly into GitHub.\n\n**[39:40]** Dify is building a platform to support developers in developing and deploying these tools and workflows more easily. The goal is to create a marketplace where tools and workflows can be distributed and used by various users. This system is similar to an open platform, allowing third-party developers to deploy their own tools and workflows.\n\nOn Dify’s platform, there are already about 50 different tools. Some tools were previously released as experiments, but due to a lack of clear direction and community support, they didn’t achieve the expected success.\n\n**[40:17]** Some platforms in the past tried building similar models but didn’t succeed. The reason is that those tools were only built as forms, lacking the ability to interact with external data and unable to combine complex workflows. However, Dify is focusing on solving these issues to create a complete ecosystem for workflows and tools.\n\n**[40:59]** These tools also allow users to push data from external sources into the system. Users can send data from external applications via Open Forms or APIs. Dify will automatically process and format the data for use in the system’s workflows.\n\n**[41:56]** The team is focusing on two main development directions:\n\n1. Continuing to expand and develop existing workflows.\n2. Improving and optimizing current tools to support easier deployment and use.\n\nThe system is built based on common standards for tool and workflow design. The Smithery tool is currently acting as an Agent to manage workflows. Smithery can also be used as a Package Manager to install and manage tools within the system.\n\n**[42:53]** Workflows will operate on the mechanism that if a workflow becomes popular, people can take it and use it as a tool. The nature of these tools is that they are designed to serve specific domains. For example, a tool for creating files, searching, or retrieving code files. It works like an SDK, meaning a library that you just need to import to use.\n\n**[43:37]** Once integrated into the SDK, you can use the available methods to manipulate data. This allows easy integration into AI tools. Currently, only Cross directly supports these operations. However, in the future, it will be standardized so other tools can also integrate easily. The case of Manus is an example. Manus uses many different tools, but when compared to the agent system in Smithery, they are fundamentally two completely different layers.\n\n**[44:15]** In Manus’s system, tools are combined to create more general workflows. These tools operate at different layers, while agents in Smithery are designed to work independently. The question is how to clearly distinguish the difference between Manus’s system and the agent system in Smithery. There’s a summary article about this posted in the AI Club channel ,  the main content discusses the ability to think (thinking) and the ability to use computers (computer use).\n\n**[45:09]** The mechanism of the Manus system is a service-oriented system. To combine multiple tools into a single workflow, the execution steps need to be clearly defined. For example, step 1 uses which tool, step 2 uses which tool, and so on. This requires the steps to be specifically configured. However, the new system has the ability to reason and automatically determine which tools are needed to complete a task. This is the key difference between the new system and older systems.\n\n**[45:59]** Specifically, the new system can recognize how many tools a task requires, which steps to go through, and can intelligently adjust the execution order. This is a special mechanism and a difference compared to older systems. In other words, it operates like a Supervisor ,  capable of reasoning and making decisions about the order and method of executing steps in a workflow.\n\n**[46:35]** The Supervisor system operates at a higher layer than the agents in Smithery. Agents in Smithery are simply tools that execute a specific task, while the Supervisor has the ability to manage and coordinate the entire task execution process. Integrating the Supervisor allows the system to operate more flexibly while making it easy to expand and add new tools.\n\n**[47:33]** The team’s goal is to understand how the system works and grasp the mechanics of managing workflows. If we can determine how to deploy and manage workflows, we’ll be able to select and use tools more effectively. This is what the team is aiming for ,  building a system capable of scaling and optimizing workflows.\n\n**[48:24]** Next, the team will focus on building the **MCP** system. This is a new system designed to manage data and workflows. The team conducted a demo of this system about two weeks ago. The essence of the MCP system is to build an agent that operates on an existing platform. Users can quickly deploy and test the system through MCP.\n\n**[49:10]** MCP will be a complete system, including a **database** and a **server**. This allows the system to operate independently and handle large amounts of data. Unlike older systems, MCP will allow users to adjust configurations and manage data more easily.\n\n**[49:58]** The essence of MCP is an agent, defined with a specific input and output structure. This allows different systems to connect and interact with MCP through standard protocols. In other words, MCP can be integrated into any system via predefined protocols.\n\n**[50:35]** MCP also allows users to manage data through the Knowledge Database, which is essentially a timescale database where all the team’s activity data is dumped. This is a time-series database that enables recording events in real-time, something backend developers will recognize as event sourcing or event logs. For example, it records information about team members, the system’s operational status, or other significant events.\n\n**[51:13]** The Knowledge Database will store all the team’s activity data, including details like who performed which task, the system’s status at specific times, and other information related to the team’s internal operations. This allows the team to track and analyze work performance, thereby making reasonable adjustment decisions.\n\n**[51:51]** The system’s concept includes a component called the Landing Zone. The Landing Zone means that all the data we currently have ,  about a dozen to tens of datasets (databases) ,  will be centralized here. Three to five years ago, if we wanted to build a data storage system, we’d create a bot to collect all the team’s activities and input them into our database.\n\nWith the new Meta model, all large data (Big Data) will be dumped into a temporary storage in the form of .dat files on S3 or GCS (Google Cloud Storage). The MCP will have the ability to read directly from the Landing Zone. If the system determines that the data in the Landing Zone is valuable and necessary, it can automatically convert that data into a Time Series Database (TSDB) for long-term use. This is the end game (final outcome) of this system.\n\nThe remaining issue will be building Use Cases based on the organized data in the system ,  in the direction the team desires. This is a key development direction for the MCP system in the near future.\n\n**[52:25]** So currently, the team will have an old database system ,  a traditional table-based database located at the bottom of the system (visible in the diagram with blue blocks). Now, the team is adding two new components:\n\n- The **Landing Zone** component ,  located in the yellow block at the top of the system.\n- The **Time Series Database (TSDB)** component ,  directly connected to the old system’s components for data analysis and exploitation.\n\nThe team is storing raw data in the Landing Zone. Essentially, centralizing data in the Landing Zone is like rallying troops ,  gathering all the data in one place before deciding how to analyze and process it. This mechanism makes the system more flexible and easily scalable when new data is added.\n\n**[53:11]** The special feature of this system is its ability to automatically convert data from the Landing Zone to the Time Series Database. This mechanism stems from the growing need for local data analytics. This is an emerging trend in the context of AI (Artificial Intelligence) development.\n\nThe rise of AI has increased the demand for real-time data analysis systems. When raw data is centralized in the Landing Zone, the system will automatically identify valuable data and transfer it to the TSDB for more detailed analysis. This is a significant step forward in building an efficient and adaptable data analysis system to market changes.\n\n**[53:45]** Currently, the team can already run analytics directly on the data stored locally. This system allows running analytics right on the Data Lake without needing to transfer data elsewhere. For the data in the Landing Zone ,  the file packets that Huy is showing on the screen ,  this is the part the team needs to focus on researching further. This issue relates to text processing, so the guys need to pick up this topic. It’s not too difficult; it’ll probably take about half a day to grasp the basics.\n\nThe Prompt for searching and exploiting data is also quite fast and simple, not complicated. This is a part very worth experimenting with because it relates to the knowledge discovery mechanism in the system. This is one of the new upgrades Huy just mentioned.\n\n**[54:22]** The most standout feature of the system in this upgrade is the **Knowledge Hub**. This is where the team will centralize all data to serve analysis and knowledge exploitation. The Knowledge Hub will become a common **data pool** for the entire team. Anyone can add data here, and the system will process and convert the data into a standard format.\n\nThe important thing is that once the system is fully set up, everyone in the team will have a common **protocol** to use. Different modules or components will be able to **share** a common data structure and access the Knowledge Hub directly. This will be the common foundation for syncing and processing data within the team.\n\n**[54:58]** Regarding the database (DB), the system will have two layers:\n\n- **Old DB:** Used to support existing operations and process pre-structured data.\n- **New DB:** Designed to connect directly with the **Knowledge Hub** and support real-time data analysis.\n\nThe special thing is that the **MCP** will act as a **protocol** for different modules to communicate with each other. This means that any data needing access or processing just needs to be fed into the system’s correct pathway, and it will be automatically processed according to the standard structure. This is how the system unifies data and avoids conflicts when multiple data sources are processed simultaneously.\n\n**[55:43]** From now on, the team will need to get familiar with the new data processing mechanisms. Everyone should take the time to learn more about the components in the new system. Once these components are stable, the team’s new projects will leverage these tools to deploy faster and more efficiently. This will be the main toolkit to serve future projects.\n\nThis system has the potential to become a **requirement** for upcoming projects. If you want to keep up with the new system, start by learning the basic principles of MCP and related protocols.\n\n**[56:40]** Previously, when the team deployed systems on S3 or GCS (Google Cloud Storage), data processing took quite a bit of time. However, with the new mechanism, data from the Landing Zone will be processed faster and more easily.\n\nThe system has been tested on various platforms, including **S3** and **GCS**. However, since the team’s current infrastructure runs on **GCS**, the data from the Landing Zone will be processed on GCS first. That said, technically, the system can expand to other platforms without major obstacles.\n\n**[57:45]** The Landing Zone’s operating mechanism is quite simple:\n\n- Data from various sources will be centralized in the Landing Zone.\n- This data will be stored as **Parquet files** by day.\n- The system can read these files back through the **Time Series Database (TSDB)** mechanism.\n\nCurrently, some sample **Parquet** files have been created and are being tested. If needed, the team can run a demo on these sample data sets to check the system’s consistency.\n\n**[58:24]** The team’s activities, like **AI sub** or **Memo**, are also fully pushed up here. The task of the **Landing Zone** is to store all the data the team wants ,  anyone who wants to store something can push it all here, and then the system will decide how to process that data. The system has also provided some tools for people to push data up, such as **API proxies** to forward events. If anyone wants to push information to the Landing Zone, they just need to call the API.\n\nMemo is currently using this mechanism to pull data from **social platforms** and sync it into the system. This mechanism has been successfully tested. For more specific data types like **Discord messages** or **data from Basecamp**, the team needs to build **crawlers** or **connectors** to collect the data. Currently, the team already has some ready-made templates for these data types.\n\n**[58:59]** For the next development direction, the team will focus on exploiting data from the Landing Zone. If you want to join this project, the advice is to start with a specific **vertical**. For example:\n\n- Identify a clear **use case**.\n- Find out **which data** is needed for that use case.\n- Redefine the data exploitation mechanism in a **top-down** approach.\n\nInstead of storing whatever data seems interesting, the team should think in terms of **defining the use case first** and then deciding what data to store. This helps the system operate in an organized and easily manageable way.\n\nA specific example is if there’s a use case about **Project Nghệ Nhân**, the team would need to create a **Git Agent** to collect data from Git, then push that data into the **Knowledge Hub** via MCP. From there, the system would define data exploitation tools for this use case.\n\n**[1:00:16]** Additionally, the team is developing a small MCP Server. This MCP Server is essentially a basic server, using standard technical components of the current internet system. It defines clear inputs and outputs, allowing connection to various interfaces.\n\nFor example:\n\n- If there’s an MCP to process data from Slack, the team will define APIs for each data type.\n- If tools are needed to read data from Google Sheets or analyze weekly check-in status data, the team can create MCP tools to handle that data.\n\nMCP will act as an intermediary component to sync and process data from various sources. Everyone can access these tools from the Editor, Command Line, or any other interface.\n\n**[1:01:07]** The essence of MCP is that it will serve as an **API Gateway** to connect tools. If you need to track everyone’s weekly check-ins in the team, you can create an MCP to collect data from the **Knowledge Hub** and Google Sheets, then compare the data to see who has checked in and who hasn’t.\n\nThe current system is at the stage of deploying a basic MCP Server. The current interface uses the **Command Line** to call MCP, but fundamentally, the team can expand it to connect with various other tools.\n\n**[1:01:43]** The system is focusing on implementing authentication and authorization mechanisms.\n\n- **Authentication** – Verifying users to access the system.\n- **Authorization** – Assigning permissions for data processing activities.\n\nThe system is currently being used internally within the team and has not been made public externally. If you want to use MCP, you’ll need to input a **private key** to authenticate your access rights.\n\n**[1:02:23]** Technically, MCP can expand to different components within the system. Everyone can integrate MCP into existing applications or tools without needing to rewrite too much code.\n\nThe team is still testing this feature and focusing on completing the security and access management parts. Once the system is stable, everyone can integrate MCP into their existing data processing workflows.\n\n**[1:03:00]** It’s just paused here for now; we haven’t tackled the complex authorization problems yet. After completing the current steps, we’ll move on to addressing more complex issues related to authorization and system usage rights. For now, everyone can focus on the basic issues first.\n\nAlright, thank you, Huy. This is one of the important technical development parts for the team. If you follow the activities on the tech and AI Club, you’ll notice the team is moving toward the next steps in the development process. Technically, everyone should pay attention to the key terms Huy just mentioned. If you’re not clear on them, you can review the transcript to get the full information.\n\n**[1:03:45]** The core team is still continuing to develop the system. We request all members to participate in the project so we can **transfer knowledge** more effectively. This project is an environment for everyone to learn and practice.\n\nThis is an opportunity for new team members to get acquainted with and grasp important technical aspects. If you feel unprepared, you can refer to the internal guides and documents to catch up. Training will happen during the work process rather than in separate sessions. This is a hands-on environment where you learn while doing.\n\n**[1:04:29]** Alongside system development, the team is also conducting knowledge transfer from completed projects. We expect to have a session at the end of the month to summarize the lessons learned from these projects. If anyone doesn’t fully understand yet, they can refer to or ask members who’ve worked on them for more information.\n\nIf you feel unprepared or need more details, you can directly ask team members. Everyone can ping more experienced members to get support.\n\n**[1:05:07]** The team has two different groups working in parallel:\n\n- **Tuấn’s team** is developing some games and small applications.\n- **The build team** is working on experimental applications to test the system’s feasibility.\n\nThese activities are similar to the **Build Club** and **AI Club** groups within the Foundation team. Some products have started showing good **output**. Tuấn and his team are developing a game based on the **Turing Machine**.\n\n**[1:06:38]** The **Turing Machine** game that Tuấn’s team is developing is adapted from the board game version into a mobile version. The game’s goal is to guess a sequence of **three numbers**. To guess the correct sequence, players receive **clues**.\n\nFor example:\n\n- If the clue says “one of the three numbers must be greater than 1” → Players can input numbers, and the system will determine if the answer is correct.\n- If two numbers are wrong but one is correct, the system will respond immediately so players can continue adjusting.\n\nThe rules are quite complex, which might be challenging for new players. Tuấn and the team are continuing to tweak it to make the game more accessible without losing its challenge.\n\n**[1:07:23]** The game is called [**Pocket Turing**](https://pocket-turing.vercel.app/) because the original board game version involves punched cards ,  similar to how the Turing Machine works in computer programming. However, I’ve adjusted and added new elements to make it more suitable for the mobile version.\n\nI plan to refine and expand the game in future versions. Additionally, I’m checking if we can implement premium features or advanced options to increase monetization potential.\n\n**[1:08:16]** I’m testing the beta version of the game. The gameplay is complete, and players can fully experience the features. The next step is to test it with a broader user group to gather feedback and improve the product.\n\n**[1:09:15]** The next goal is to bring the game to the App Store and Google Play to reach more users. For now, the team wants to ensure the game runs stably without serious bugs.\n\nTuấn hopes the game will attract at least **100 paying users** in the initial testing phase. If we get positive feedback, we’ll expand with new features and improve the player experience. I’d love to hear feedback from other team members to adjust and perfect the product further. Tuấn has shared the game download link with team members so everyone can try it and provide input.\n\n**[1:10:13]** If you guys are excited about building products, this is a good time to start. The team has experimented many times before, but this is a chance to do it more systematically. Developing internal products not only improves technical skills but also opens up future commercialization opportunities.\n\nBesides Tuấn’s game, the team is working on other tools. If you have any good ideas, feel free to contribute so we can build and test together. How to sell or monetize the products can be figured out later; the priority is completing the core features first.\n\n**[1:10:58]** Next is An’s part. An once made a tool called **Rec** to aggregate information in a format similar to **Apple**’s system. Version 1 of Rec required users to manually organize information, while the current Version 2 has integrated AI to support automatic organization.\n\nHowever, the AI still has some limitations in fully recognizing content. Sometimes it can’t grasp the entire context, so the results aren’t completely perfect. Still, the important content is organized and displayed fully.\n\n**[1:11:56]** This tool is in the refinement stage, but the core functions are stable. Currently, the team is focusing on improving the interface and optimizing the user experience. An plans to continue developing additional features to better support users.\n\n**[1:12:51]** The team’s projects are currently in the testing and improvement phase. If anyone has questions or suggestions, they can directly discuss with An or other team members. For now, we’ve showcased almost all the projects. More detailed parts will be covered in the next session.\n\n**[1:13:57]** On our team’s side, I always talk about the knowledge related to **liquidity** and **game in general**, right? We really want the team to push a little in that direction because it’s beneficial for almost like a **life skill**, you know? So I want our team to head in that direction this time. Especially those of you who are really interested in **trading**, meaning getting data to find the **Alpha** on it, the **Intel** on it, to come up with some **market-making** strategies based on certain conditions or something like that.\n\n**[1:14:43]** It’s one thing, or it could even go further to create a really awesome flow. It feels like it’s just my dream for now. An has already made a version that I think is pretty okay. Just taking this chance to let you guys know that the team has this kind of progress going on. It’s running, right? Let’s invite An. Okay, okay, generally it’s just a money-making game. See how they make money, and we’ll do the same. The usual stuff has a bit of something to test, a bit of it is also in the form of **Delta neutral**, right? So we also research those things there. Then go build and research to have the knowledge to ship it.\n\n**[1:15:30]** Play this column until it’s all used up, that’s it, no looking over there. Do you all see the terminal screen? Do you see it? Yes, okay, let’s run it. Let’s run it. I think we need to zoom in, zoom in one or two levels, it’s still a bit small, okay now. Yeah, this is the arbitrage to eat the **funding free frost**, right? There are many, many types of arbitrage like that. This is just one of those types, which is eating off the difference, the **phantom bin**, between the exchanges. We’re focusing on three exchanges: **Binance**, **OKX** ,  that devil OKX, their exchange doesn’t have too much stuff ,  so, do you have a diagram for that, An?\n\n**[1:16:26]** I think it’ll be a bit hard for everyone to visualize. Looking at this, they won’t understand what it is. Is there one? Okay, no diagram? Oh, there’s this, big one, just theory, nothing concrete comes up at a high level? I guess we’ll have to sketch it roughly again. Do you see it yet? Maybe look at my diagram, yeah, my diagram, so you know it’s this right away. The PRL? Wait, it’s messed up, running wrong, forgot, it’s going into the sockets of…\n\n**[1:17:47]** Those guys to pull **real-time data** back, and it’s currently pulling data from how many accounts? Three accounts ,  **Binance**, **Bybit**, and what? Okay, initialized to get the price back, right? Getting the price, getting the **funding** back, getting the price yet? Getting some data like fees and fee-related data, it’s kind of coded according to that calculation, but it hasn’t been used to fetch yet. OKX probably doesn’t have it. Those trades, okay, don’t have it, so usually it’s just on the two main ones, which are **Binance** and **Bybit**. Yeah, setup is okay, and then it’ll have an array to show which pair has the difference, the difference in **funding**, then it’ll have profit, and I calculate that if we enter, how much it would be. PH is the number, the number of times we collect the **funding** to break even with the fees. It’s monitoring, monitoring the highest between the two exchanges, that’s it. Step one is getting the difference, the difference in **funding** between the two sides, right? Between the two sides that I’m talking about, no. Because I said this is getting from three exchanges, so compared to its angle, it’s still on the exchange net, right? Yeah, exchange net is something calculated later because…\n\n**[1:19:49] Funding**, let’s say normally, on the price setup, they usually don’t have much, like one is positive, two are positive, or two are negative, so the difference is smaller. Actually, we place a counter on the other difference, it’s just the offset, the price movement offset, so it doesn’t lose due to the price. On the exchange net, there won’t be that difference, we make the **funding** always equal to zero, right? Because there’s no swap fee there, and instead of countering on another exchange with that thing, we counter at the moment when the **funding** is also zero. Currently, we accept that it won’t be as profitable, but that’s how the first version is.\n\n**[1:20:33] b**So we get the price, get the top, get the **funding**, then see if it’s just about deploying capital, right? Have you tried running it yet? Not yet, haven’t poured money in. Yeah, this is like a scale game, you need a lot of money to profit, but with just a few hundred or a few thousand, it goes in, and it doesn’t look like much. Got it, got it, okay, understood. But on the technical side, technically, for me to do this, what did I apply from start to finish? From when it crashed, what did I do? Yeah…\n\n**[1:21:25]** First, I researched that chart beforehand, checked how it was, whether it had this or that, all those things. Yeah, got those dots sorted. We’ll ship it for that, yeah, that guy will code everything with code, okay? Using this **Cloud 3.7**, right? First, you have to get the exchange’s data before calculating anything. The main exchanges will pull from sockets, and the setup is, first, on the API docks of the exchanges, right? This one, yeah, then pull their docks back, throw it to this guy, it ships up to the **WebSocket client**. Every exchange has docks, all of them, pull them back, ship them up, and it’ll auto-view.\n\n**[1:22:15]** The web for all three exchanges is done, with forms and everything. After that, we start building it up, yeah, this part, this chart. The first step is probably explaining it to it and stuff, kind of it builds slowly up. Then check the data and all, if it’s wrong, it auto-builds itself. I built it, and it auto, every time there’s something new, it’ll auto-build a sample to check the data before finding it again for us. Cool, to ensure the data is correct or not. Because the thing between the exchanges, the format is different, the data format is…\n\n**[1:23:01]** Different, everything is different, right? So to compare it all into one final form for it to compare, it needs a section to test pulling the data back, ensuring the data is valid, then it starts comparing. That’s the step. Next, it’ll be building things like, next is, yeah, building the guy to place orders. When it detects these, there’ll be a guy standing by to place orders, watching our bot to see if it’s losing or whatever, slowly, reasonably. The whole process took how long? About a week, yeah, cool, huh? So now the next step…\n\n**[1:23:58]** The next step of this tool I’m working on, what’s the next step? I’ll check the data first, check the data to see which one makes money easily, which one is profitable, which one you put money into and it’s all profitable. There’ll be data to check those profits, then manage more of our stuff, like risks and all that, then, yeah, pull all the data back about the fee structure of the exchanges. Because if you use those trusts or whatever, the exchange fees affect the thing a lot, so you need the exact fees, then calculate…\n\n**[1:24:51]** How to ensure it’s profitable in the end before placing orders, right? Next will probably be those steps. Okay, that’s the step, the step of when to place orders, that’s the final thing, right? The rest needs to filter the data first. Is there a back system built? Because I think this data, does it have history or not? Does it? Or is it just at that moment? It does, if you can get the history, it’s backtest history, but I think it’s not accurate, huh? Yeah, not accurate, not there. I don’t think so. The other stuff might have it, but this arbitrage is a bit hard to get accurate.\n\n**[1:25:38]** This is a technical showcase. I think with this direction in the team, independently, our team, regarding this direction, it’s okay. You guys showcasing the trading game have started having steps that the team is pushing forward to do. I think fundamentally, fundamentally, you guys are all on a path, on the way to getting to what you want to do, which is very good. The thing is, with technology, with that tech know-how, how we bring it out and use it, right?\n\n**[1:26:19]** The team’s activities in general are like this, okay? Regarding productivity, it feels like recently everyone has started syncing with each other to a certain degree. But with Tom, Tom is probably out, but Tom had a comment from before when you guys were sitting and chatting. We were thinking, what’s the team’s productivity level right now? How much would Tom rate it? 2/10 or 4/10, huh? If compared to the level we need, you guys at like 6-7/10, the general average, I think we’re in a very good setup right now.\n\n**[1:27:01]** The next step regarding the quality, the technology tool link to support us in operating the team according to this model, it’s being improved slowly but surely. On the market side, the **funding** market in general and the products are starting to stir and come back. People see the technology becoming more stable. In **crypto**, it’s heavily influenced by macro factors, but whenever there’s a tech trend, they’ll jump in and bite, that’s how it is, right? I’m seeing signals for it to resume soon, about 50/50 right now. Before this, I looked at the market, and it was really bad, like everything wasn’t ready yet. Even if you studied a lot and worked a lot, results wouldn’t come immediately.\n\n**[1:27:40]** But this time, I think you guys will need to have some requirements about participating in these things, okay? Next week, I’ll probably ask Huy,Tom and Thành to compile some stats, to see besides the projects you’re working on, what’s the participation in side projects like that, who’s doing what, alright? That’s the follow-up after the content, we’ve discussed almost everything. Next week, there are still some core flow parts left, but they probably won’t affect things too much.\n\n**[1:28:22]** Today is the 14th, I hope by the end of this month, the next team meeting will show more progress. Everything we’re doing is very important, right? Another important thing is we have Sister Minh here, Nicki. Probably past the out time already. All of this is being uploaded to **Memo**, and we’re using that **Memo** not just to share on it ,  that’s not enough ,  but the channels we’re working on are being sent out…\n\n**[1:29:01]** To everywhere, to other companies we know, starting to expand the network to look for necessary users. The thing is, we know these things already, so how do we mound our ability to profit from our knowledge? That’s the idea, okay? That’s the skin the team is following. In summary, the market assessment is like this. Next week, you guys will need to register for those parts, and Huy, Tom, and Thành are the mandatory segments.\n\n**[1:29:45]** As for the hobby clubs, like Build or something, I don’t have high demands there, producing technical stuff to apply, it’s not that important. The output matters more. Two different groups: one group is the core project parts we’ll work on, focusing on how to increase activity, increase everyone’s **knowledge base**; the other group focuses on the **skill set**, how to develop products for launch, how to do onboarding better, users and all that. It’s a different skill set group. You guys next week jump in and start thinking, especially…\n\n**[1:30:27]** Especially Huy, Huy is co-handling the return to the office to start shadowing for **knowledge transfer**. If it works, just keep it going, then see how the numbers look and report back to me, okay? Hopefully, when we have the numbers, you guys discuss further, figure out how to set up that shadowing on the projects, the sites we’re involved in, to have cases to share with each other, right? Like Sister An, finishing this in a week is super solid, doing everything herself, using new workflows and all, it’s great…\n\n**[1:31:05]** Alright, you guys, that’s the whole thing. If there’s nothing else now, we’ll probably end here. How many people are here? 28 people, huh? No, how many in this call right now? Preparing to spam ICY, is there an issue with transferring ICY yet? Everyone go random, grab it like ghosts, okay? Amount is 28, so we’ll drop 14 ICY tokens, entry is 14 already. Go ahead, duration is 5 seconds. Okay, let’s go. One ICY earlier was about 100 Satoshi already.\n\n**[1:32:09]** Now starting, don’t know when the boss updates the multiplier price, just estimate it for now. Today’s early, next time seeing Bitcoin, it looks cool. Happy Weekend, bye bye everyone.\n","title":"OGIF Office Hours #41 - ICY-BTC Swap, GitHub Bot, MCP-DB, Pocket Turing, Recapable, and Arbitrage Strategy","short_title":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","description":"In OGIF 41, the team covered key updates on the ICY-BTC swap, GitHub bot automation, MCP-DB system for agent workflows, and progress on the Pocket Turning and Recapable projects and we also shared insights into funding rate arbitrage strategies.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Thu Mar 20 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/41-20250314.md","slugArray":["updates","ogif","41-20250314"]},{"content":"\n### Topics and Highlights\n\n- **Team check-ins \u0026 workflow**: Kicked off with roll-call vibes, planning speed-run topics, and assigning tasks to Hải, Cường, and Tom. Encouraged quick 10-minute concept pitches.\n- **Frontend updates**: Hải’s January report covered React 19 and Next.js 15.1, spotlighting the View Transition API for smoother stage animations and Deno Deploy’s new server-side rendering support.\n- **Tooling \u0026 libraries**: Explored Transformer for running Python models in JS, Neon’s switch from Webpack to a lighter setup with better hot reloads, and HTMX’s rise with logic-in-HTML simplicity.\n- **Database design practices**: Cường recapped scaling databases with business growth, emphasizing DBA roles, migrations, CI systems, and versioning for managing schema changes and avoiding API breaks.\n- **AI-driven development**: Tom showcased a full-cycle approach, leveraging AI for rapid planning, task breakdowns, and proposals.\n- **Skillset spotlight**: Highlighted team strengths, security/performance (Thành), user/data flow (Tom), and how to align them with proposals, from MVP to real-time app concepts.\n- **Process optimization**: Detailed Tom’s AI-assisted workflow: extracting insights, crafting prompts, validating concepts, and scaling tasks with 90% accuracy, plus burning questions for client rapport.\n- **Q\u0026A \u0026 next steps**: Wrapped with open questions, a nod to future Tom-led sessions, and a promise to refine skills like real-time handling and proposal structuring.\n\n### Vietnamese transcript\n\n**[00:00]** Bắt đầu thôi nào. Chào mấy anh em, cảm ơn đã đợi. Thành với Cường đâu rồi? Cường có lên phòng chưa? Thấy đăng ký thứ Sáu mà giờ lên đây rồi, đúng không? Tuần này Thành đâu rồi? À, lên rồi, đứng đây nè. Tuấn, Tom lên stage luôn nha. \n\n**[04:51]** Đang xem mấy cái bài, tự nhiên cái link này Tom ơi, đẹp chưa? Để anh sửa lại. Ngày hôm nay 186 giao dịch, 1 user, 30 ICY member như cũ, 5 cái inactive, 1482 giả mạo. Hai channel chat nhiều nhất, ba channel chat nhiều nhất, mấy người chat nhiều nhất là ai? Ờ, tiêu rồi! Còn ai nữa không? Hôm nay thiếu ai không? Có hai chủ đề cũ: một cái là \"run and report\". Sáng nay anh post link lên rồi, chắc vậy, để kiểm tra lại. Cái thứ hai là bài design của Cường, anh chưa biết nội dung.\n\n**[06:03]** Bài này là cái gì vậy, ngồi nghe mà chẳng hiểu gì luôn. Bài số ba là nối tiếp cái series hôm trước, mấy anh em viết xong, làm xong, giờ nó thành hình cụ thể rồi. Qua 3 tháng thì team cũng có vài cập nhật mới, hướng đi này rõ ràng hơn chút. Hệ thống thấy cũ rồi, tí anh forward link cho mọi người đọc trước qua email.\n\n**[07:10]** Đăng ký dùng thử đi, tí nữa vào xem. Plan là vậy. Chắc ship bài của Hải trước, rồi tới bài của Cường, rồi tới bài của Tom, mấy phần Tom làm đó. Nội dung hôm nay chắc vậy. Anh em xem thử còn thiếu ai không, hay thấy ngắn quá, có gì liên quan nữa không? Ai thiếu vậy? Thành lên chưa? Ờ, đệ Thành đỉnh quá, hết việc để làm rồi. Anh cũng nghĩ vậy.\n\n**[08:55]**  Đợi chút nha, đợi đủ người rồi tụi mình speed-run mấy chủ đề này. Chủ đề cũng đơn giản thôi. Anh em cố gắng tóm gọn bài của mình, nói concept, idea trong 10 phút thôi, đừng dài quá, để dành thời gian cho buổi kia. Nếu cần hơn 10 phút thì nói dài hơn chút, vậy nha. Tuần sau có lịch lên văn phòng, tuần này check-in bình thường thôi. \n\n**[09:57]** Tuần sau dựa trên danh sách đăng ký, anh sẽ đề xuất với Huy Nguyễn làm trò điểm danh cho đủ mặt. Thành policy luôn rồi. Tuần sau làm điểm danh cho đông đủ. Đoạn tiếp theo thì mấy dự án cũ giờ gần xong hết rồi. Giờ dep blockchain với AI là vua của mọi nghề, anh em nào muốn làm trực tiếp thì phải lên kế hoạch cái đó. Có ai trùng gì không, hay còn ý gì nữa không?\n\n**[10:58]** Chắc bắt đầu với bài của Hải trước nha. Hải ơi, mời em trình bày. Dạ, mọi người thấy hình của em rồi đúng không? Em tóm tắt Frontend report tháng 1. Tháng 12 năm ngoái, React 19 release đi kèm với nó là thằng Next.JS 15.1 cũng tung ra một phiên bản mới.\n\n**[12:07]** Để hỗ trợ cả thằng Next.JS lẫn thằng React 19 luôn. Bên Reactthì em thấy nó đang làm một cái API khá hay, gọi là View Transition. Browser giờ đã có API View Transition này rồi, nhưng trước đây thì React chưa hỗ trợ. Một số thư viện đã viết và dùng cái API của bên kia, nhưng khi đưa lên React thì gặp vài vấn đề về performance. Ờ, tụi nó đang đợi API này từ React để hỗ trợ tốt hơn, giúp giải quyết vấn đề performance rõ ràng hơn. \n\n**[12:48]** API này dùng để làm animation khi chuyển giữa hai stage của trang web. Ví dụ như anh kéo xuống dưới đây, nó sẽ như ví dụ bên dưới này, cái stage đầu tiên là box nằm trên, stage thứ hai thì box nằm dưới. Thay vì chuyển stage mà nó nhảy thẳng xuống luôn, thì View Transition này hỗ trợ mình tạo hiệu ứng animation, nhảy qua nhảy lại các kiểu. Tương tự, với mấy cái như hình ảnh, nó cũng tạo hiệu ứng animation. \n\n**[13:28]** Khi chuyển đổi hình ảnh, thay vì chỉ nhảy sang hình khác ngay lập tức. Dạ, cái API này vẫn đang trong giai đoạn thử nghiệm thôi. Phải dùng phiên bản thử nghiệm thì mình mới xài được. Nhưng nó hứa hẹn sẽ tăng performance khi sử dụng. Vì trước đây, thằng Motion cũng đã hỗ trợ rồi, nhưng chỉ trong môi trường thuần thôi. Còn nếu lên thì nó gặp vài vấn đề performance, tại vì nó phải xử lý cả trước và sau khi set.\n\n**[14:07]** Cho cái phần này, bên SCS thì có mấy thứ như thằng Deno Deploy. Lúc trước nó chỉ hỗ trợ deploy static site thôi, nhưng giờ nó đã hỗ trợ hoàn toàn để deploy cả thằng Next.JS luôn, kể cả server-side rendering. Giờ mình có thể dùng Deno thay thế, hòa chung được, để deploy một ứng dụng NS. Dạ, cái này vẫn chưa có gì để nói hết. Còn cái thư viện Transformer Z này cũng khá hay. Bản chất của nó là đang biến mấy cái model. \n\n**[15:03]** Bản chất của nó là đang biến mấy cái model viết bằng Python lên thành JS, để mình có thể chạy trực tiếp mấy cái model này trên trình duyệt luôn, không cần qua API hay ngôn ngữ Python gì hết. Như trong bài này, nó chạy được cái sentiment testing. Ví dụ như positive hay negative, hoặc là object detection, như phát hiện con mèo. Bản chất thì em nghĩ mấy model khác, mấy cái pipeline khác, vẫn chạy được, miễn là nó được hỗ trợ bởi thư viện này.\n\n**[18:33]** Bọn em buộc phải hỗ trợ kiểu dù có mạng hay không, data vẫn phải lưu được hết. Sau đó chọn cách lưu xuống IndexedDB, rồi khi có kết nối trở lại, mới đẩy data lên server. Kiểu như vậy. Ở dưới đây nó có hướng dẫn step-by-step để xử lý. Làm vậy thì sẽ gặp vài vấn đề, như list data bị fail khi sync chẳng hạn. Nó chỉ ra một số cách để giải quyết mấy vấn đề đó.\n\n**[19:22]** Kiểu như vậy. An mới post link gì đó à? Zero là con gì? An mới bảo gì kìa, có liên quan không? Bữa trước thấy Lập, cũng bảo cái vụ \"local first\", chắc giống vậy đúng không? Mọi người chung bài toán, thi nhau đi giải. Tiếp theo, bên Win thì có nhắc. Bài này có update chút, giờ nó support thằng đó luôn rồi. Lúc trước Node.js thì phải có command line để combine thằng Typescript ra js mới chạy được. Còn giờ nó chạy trực tiếp luôn.\n\n**[20:01]** Như nó chạy bằng cái command line, load file luôn. Theo em thấy, còn một bài nữa về anh dec này, kể về chuyện các dependency ở bên MBM. Nó cứ ra version mới hoài, kiểu mỗi version lại kèm theo mấy cái breaking change. Ổng nói làm vậy khá cực, muốn update version nhưng sợ app không theo kịp. Không phải lúc nào cũng có thời gian để xử lý hết. Nên ổng không thích thằng React lắm, chọn hướng khác. Ổng bảo thằng này sẽ ổn định hơn, ít bị thay đổi như vậy. Ổng ưu tiên thằng này hơn. Thằng HTMX thì cũng nổi lên đang đứng top 1.\n\n**[21:07]** Dạ, còn một bài cuối nhanh về thằng Neon. Thằng này cung cấp dịch vụ về database. Nó vừa chuyển từ Webpack sang cái khác. Trong quá trình đó, nó gặp vài vấn đề, nhận ra một số hạn chế của Webpack. Như là nó không support tốt, có một danh sách dài những khó khăn ngay đây. Nhưng kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái mới ổn hơn Webpack. Thứ nhất, nó ít lỗi hơn, reliable hơn thằng Webpack. Thứ hai, config của nó đơn giản hơn. Như nó nói, chỉ cần mười mấy, hai mươi cái plugin của Webpack là làm cho nó nhẹ hơn nhiều. Em cũng không biết tại sao nó để vậy.\n\n**[22:03]** Nhưng mà cái kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái hot reload của nó ok hơn thằng Webpack. Nó ít kiểm khi bị full reload hơn thằng Webpack. Thứ hai là config của nó, nó simple hơn. Như nó nói là nó cỡ mười mấy, hai mươi cái plugin của Webpack gì đó, nó làm cho cái của nó nhẹ hơn nhiều.\n\n**[23:01]** Bài này nó chủ yếu là nói về những cái khó khăn và những cái kết quả cuối cùng khi mà nó chuyển từ Webpack sang cái kia. Dạ, vậy là cái của mấy anh em đang thay đổi à? Đang chuyển qua từ cái Webpack chuyển qua cái con kia là một đúng không? Cái React ở trên kia thì sao?\n\n**[23:50]** Chuyển qua HTMX hả? Là hai rồi, còn gì khác nữa không? Xài con Deno à? Với lại TP hả? TP thành main framework hả? Ừ, dạ, cho nó rồi. Còn mấy bài khác thì mọi người có thể đọc thêm trong cái này. Dạ, cái gì nhờ Hải post lại cái link nhé? Cảm ơn Hải, cảm ơn mấy anh em đã cho cái reply. HTMX nó là cái gì mà tại sao lại được chọn vậy? HTML nhưng mà có logic trong đó hả? Kiểu nó sẽ thêm một số thằng trực tiếp vô cái HTML, rồi dùng để trực tiếp ông lại chê nhau thôi. Cái trò này từ thời Backbone.js với lại Knockout.js. \n\n**[25:06]** Đây cả chục năm, giờ mới làm y chang vậy mà. Anh em có câu hỏi gì không? Cho một phút comment thêm. Có gì cần update thêm không? Có gì nhờ Hải post link vô, cho vô ngoài random hay vô group chat nhé. Mời bạn tiếp theo. Mời Cường đi nhanh qua chủ đề về database design. Dạ, bắt đầu luôn. Tiết học lịch sử hả? Cái này, cái bài mấy cái practice này là có từ 2017 rồi.\n\n**[26:17]** Em chỉ recap lại thôi à? Tổng kết hả? Tổng kết cái kỹ năng thiết kế dữ liệu, tip entity hả? Dạ, không, không hẳn là quản lý dữ liệu. Kiểu mấy cái practice để mà mình handle mấy cái kiến thức trong quá trình mình phát triển, mình grow cái database của mình lên. Dạ, em xin vô luôn. Database với lại cái hệ thống mà mình phát triển thì lúc nào cũng đi đôi với nhau. Khi mà phần mềm của mình scale up để bắt kịp cái business demand, thì mình bắt buộc phải scale up cái database của mình lên để quản lý số lượng lớn các.\n\n**[26:51]** Dữ liệu trải qua từng năm. Ví dụ như từ 2015, Amazon mới có khoảng 50 triệu dữ liệu, thì bắt đầu tới 2020 đã phát triển lên tới mức phải handle 200 triệu dữ liệu. Vậy tại sao cần phải có những cái practice này? Khi mà cái database của mình có tới cả trăm hoặc cả ngàn schema, thì cái management system như SQL Server hay mấy cái hệ thống quản lý dữ liệu khác, mình nhìn vào sơ đồ schema, table hay data thì không thể biết hết được tất cả.\n\n**[27:27]** Các cái context. Tại sao những cái change này đã được apply vào trong hệ thống? Để đúc kết ra được thì sẽ có một vài practice. Bắt buộc phải có sự kết hợp giữa con người và hệ thống để quản lý các kiến thức này. Tất cả những cái này chỉ là practice, không bao gồm việc lựa chọn hệ thống quản lý database hay thiết kế database schema. Nó bao gồm cách mà mình chia sẻ kiến thức database, lưu trữ những kiến thức này. Và khi những cái database change được boost lên thì sẽ có một hệ thống riêng để quản lý mấy cái change này, như continuous integration và những cái tương tự.\n\n**[28:02]** Đó là những cái change này sẽ bắt buộc phải follow một vài refactoring rules. Về no-sharing thì bình thường trong tổ chức của mình sẽ có một người gọi là DBA. Người này sẽ quản lý cũng như phải chia sẻ tất cả kiến thức và các sự thay đổi của database được apply vào hệ thống. Ví dụ, nếu mình có nhiều team dev, dev 1 khi phát triển phần mềm A, dev 2 quản lý phần mềm B, thì cả hai khi push change lên database của hệ thống sẽ phải hỏi qua người DBA. DBA này sẽ verify từng change xem nó có tác dụng gì, để quyết định cái change đó có make sense với database chính hay không.\n\n**[28:34]** Khi từng dev push cái database của mình lên, thì dev này sẽ verify với hệ thống chính để xem các API gọi đến database có bị ảnh hưởng gì không. Sau đó sẽ đánh giá cái change này có cần thiết không. Nếu cái change này ảnh hưởng quá lớn đến hệ thống, thì người DBA có thể reject cái change đó, bắt người dev phải update, refactor hoặc chỉnh sửa lại cho hợp lý. Khi cái change đã được approve, thì người DBA sẽ phải document lại rằng cái change này có ý nghĩa gì, tại sao cần cái change đó, rồi post một cái migration lên cho database master bắt đầu cập nhật.\n\n**[29:14]** Những cái dữ liệu này còn phải được lưu trữ ở một chỗ nào đó mà tất cả mọi người đều dễ dàng truy cập và tìm kiếm để biết tại sao những thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào một cái repository, giống như một coding project. Cái repository này chứa tất cả database artifact, bao gồm script chạy database, credential login, configuration, và mức độ dung lượng tối đa mà các instance này có thể quản lý, cũng như các documentation của hệ thống. Cái repository này cũng tương tự như một coding project, sẽ được quản lý bởi một version control.\n\n**[29:51]** Cũng như là tìm kiếm để biết được là tại sao những cái thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào trong một cái repository giống như một coding project vậy. Mọi người có gì hỏi thêm không?\n\n**[30:39]** Để mọi người có thể check, cũng như kiểm tra các cái change, context và history của những thay đổi này trong hệ thống, thì mỗi lần thay đổi, người push cái migration này sẽ tạo một cái pull request và thêm description. Description này giải thích tại sao cần cái change này, nó cần thiết ra sao, và những hệ thống nào sẽ bị ảnh hưởng bởi cái change đó. Người review, đa số là các dev của những API mà cái change này tác động trực tiếp tới, sẽ vào xem xét.\n\n**[31:14]** Sau khi những thay đổi này được merge vào nhánh master, sẽ có versioning để mình có thể rollback hoặc deploy các version này vào từng hệ thống để dev, testing, và cuối cùng là đưa lên production. Khi mà mình có nhiều dev instance giữa các version, thì lúc dev từng hệ thống riêng, mình sẽ phải checkout ra từ một instance của master database để sử dụng cho việc development. Như vậy, khi thay đổi gì đó hoặc migration một cái mới, mình không ảnh hưởng trực tiếp tới cái database chính.\n\n**[31:52]** Khi đó, mình cần có một hệ thống CI. Mỗi khi thay đổi gì trong instance mà mình dev, mình có thể dễ dàng verify xem cái change này có break master database hay không. Đồng thời, khi ai đó push một cái change mới lên master database, mình sẽ được thông báo về schema thay đổi hoặc resource conflict trước khi làm chậm tiến độ dev. Khi boost một thay đổi trên database, những thay đổi này bao gồm mấy bước như sau: thay đổi một cái database schema. \n\n**[32:25]** Khi push một thay đổi, mình phải tạo một migration script lên database đó. Sau khi script này được merge, mình phải đổi database access code để API có thể dùng cái change mới đó. Đối với những database change như thêm một column mới, thì có thể không nhất thiết phải thay đổi access layer của API khi change này được push lên. Vì một số API không cần dùng tới cột mới đó. Ví dụ, mình có bảng user với name và address, một service mới cần thêm field birthday vào bảng user, thì các service cũ như service gom nhóm user theo address không cần thay đổi gì trong API để tích hợp cái change mới này.\n\n**[33:07]** Đối với những change ảnh hưởng lớn, như giới thiệu một non-null value hay tách bảng, thì tất cả service phụ thuộc vào nó cần phải đổi data access layer để tránh lỗi. Ví dụ như bảng user vừa nãy, nếu tách bảng user ra, thì service nào dùng bảng đó phải thay đổi toàn bộ access layer để không bị lỗi. Ngoài ra, có thể dùng một cái gọi là transition interface để dần dần apply các thay đổi mới, rồi boost cái change đó mà không làm crash API cũ.\n\n**[33:45]** Sau khi đã refactor và apply change lên master database, mình còn phải notify tất cả các service dùng database này để tránh break mấy cái API đó. Đồng thời, mọi người có thể contact nhau để resolve config khi thay đổi master database. Về phần recap, trong quá trình develop một software, khi phần mềm phát triển thì bắt buộc database của mình cũng phải phát triển theo. Để mọi người đều nắm được thông tin và context của từng cái change trong database này, cần vận dụng tất cả kiến thức để chia sẻ và sắp xếp kiến thức của mình.\n\n**[34:32]** Đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian, mọi người resource conflict giữa các cái database change. Bài này thấy nó có giá trị ở chỗ góc nhìn. Chắc là giống như góc nhìn dev, nhưng mà nó đứng góc nhìn về chuyện là thay đổi đối tượng làm việc chính.\n\n**[35:21]** Thông tin và cũng như context của những từng cái change bên trong database này thì cần phải vận dụng tất cả những kiến thức để mà know sharing cũng như là sắp xếp các cái kiến thức của mình và đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian mọi người resource conflict giữa các cái database change. Hết rồi. Mọi người có gì hỏi thêm không?\n\n**[35:58]** Là không phải codebase mà là cái database đúng không? Theo hướng đó nhiều. Cứ nghe tới đoạn này thấy hơi meta quá, kiểu hệ thống lớn chắc mới quan tâm, còn hệ thống như hiện tại thì hơi khó áp dụng hả? Khoảng hệ thống cỡ 20 table là thấy hơi lâu lâu, nhìn vô cũng hơi chóng mặt rồi. Đúng rồi. Vậy cái này liên quan tới chuyện documentation, quản lý versioning, với cả làm monitoring. Không phải version monitoring, mà là notification cho mấy cái team khác đúng không? Dạ, vậy nó còn ít lắm, nhưng mà đúng rồi. Mấy cái này đưa vô thì hợp lý, vì có góc nhìn.\n\n**[36:50]** Quản trị data trước tới mấy cái kia. Logic thì logic ở đây, mai mốt data chạy rồi. Anh em có hỏi gì Cường không? Không thì sẽ kết thúc ở đây. Bài này có giá trị về góc nhìn. Nghĩ mấy anh em khi làm backend mà muốn làm giàu thì sẽ phải theo dự án suốt đời. Dự án càng lâu thì nghĩa là dự án càng có tiền. Thấy vậy, đi được với dự án càng lâu thì về bản chất nó sẽ ok. Nhưng mà thường dev thì nó sẽ lười. Dev thấy cái gì mà làm lâu quá thì bị chán, hành vi rất là lạ. \n\n**[37:40]** Trước khi qua bài tiếp theo, để đóng góp cho buổi hôm nay, một cái keyword của tuần này, trong quá trình đi ngồi đọng lại, có một keyword mới, mới học được. Từ mới dành cho những bạn chưa biết, giống như anh chưa biết. Đây là kiểu hôm nay lên, có một trường phái tên là Luddism. Luddism là một cái chữ xuất thân từ thế kỷ 19, khi cuộc cách mạng công nghiệp diễn ra. Ngành những ngành liên quan tới dệt may được tự động hóa, thì cái đó tức là những người theo có cửa, họ tên là Luddites sao á, mới đi đốt mấy cái máy đó.\n\n**[38:34]** Mấy cái máy đó cướp việc của mình, cướp chén cơm của mình, nên họ đi phá mấy máy đó. Thành ra cái này trở thành một trường phái Luddism. Tức là tầng lớp working class đi chống lại xu hướng hiện đại hóa. Rồi chữ khóa tiếp theo đi sâu tiếp thì sẽ ra Neo-Luddism, với lại cái thằng Luddism ngay đây. Cái gì anh em đọc thêm nhé, thấy khá là relevant với mình sắp tới. Theo những dự đoán mà hôm trước.\n\n**[39:18]** Mình ngồi nói với nhau á, thì sắp tới chắc sẽ nhiều người dậy lắm. Ở trên Reddit thì nó có một cái bài cách đây 2 năm, có cái làn sóng Neo-Luddism mới sẽ xuất hiện. Giờ lên thấy cũng nhiều lắm ha. Thì cố gắng, góc nhìn anh thì cố gắng, anh em không nên, không nên theo trường phái này. Hồi phát triển sẽ đi tiếp, không nên chống lại bánh xe lịch sử. Rồi có luôn cái subreddit tên là Luddism luôn, nói từ Luddism luôn. Không chỉ nói về automation, mà nói về đủ thứ trả lại công nghệ trên đời. Cảm giác lạc lỏng, cảm giác thế này thế kia. Đây là keyword khá là thú vị, ha, anh em.\n\n**[40:08]** Không bị dính vào đây ha. Rồi cái số hai nữa là có cái liên quan đến cái này. Thằng vừa rồi mới ngồi, mới ngồi tìm ra này, đó là U.S. geopolitical. Có một góc nhìn về chuyện nước Mỹ phát triển như thế nào. Anh nghiên cứu về thị trường vốn, có cái dòng tiền đầu tư nó chảy đâu, nên vô tình lọt vô cái chủ đề này. Đây là chủ đề thứ hai, thấy cũng khá thú vị. Maybe anh em sẽ quan tâm. Chủ đề này liên quan tới macro economy. Thì ra nó được, từ cái trị nó chuyển qua thành macro economy.\n\n**[40:56]** Nước Mỹ sẽ có xu hướng có hai phái thôi. Một là isolationism, tức là cô lập hóa. Một chữ khác thể dùng cái đó, tính làm đây ha. Thì trong cái movement này, nó nói gì? Nước Mỹ sẽ có xu thế là nó co mình lại, không deploy mấy cái resource đi khắp nơi để giao thương nữa, mà gom cái đó về, đứng đó phòng thủ. Đây là cái cụm thứ nhất. Hiện tại, tất cả những tin tức mình thấy được á, thì nó đang trong cái đó, protectionism hoặc là isolationism. Cụm này, hướng thứ hai mình thấy là globalization.\n\n**[41:41]** Globalization thì những cái sáng chế, những cái công việc sẽ tập trung vào chuyện trading với nhau nhiều hơn, giao thương nhiều hơn. Nước này nước nọ quăng những cái đó đi khắp nơi. Mỹ sẽ có xu hướng là out ra ngoài, những anh chị em theo cái phái đó. Những cái nước theo cái phái đó cũng sẽ có xu hướng cởi mở hơn, chạy khắp nơi. Thì nó là cái tình trạng trong trạng thái mà nó diễn ra từ report, từ năm 45 tới gần đây, thì đã đi thành những cái cụm nhỏ. \n\n**[42:20]** Trong giai đoạn sau chiến tranh với Nhật, đi nút cho Nhật hai cú xong rồi, thì giúp Nhật với Đức sau Chiến tranh. Nó sẽ giúp tái thiết lại, thì bắt đầu nó deploy, nó globalization theo hướng đó. Đó là cái phase ban đầu. Nó bắt cái đoạn đó, đến khi mà Nhật mạnh quá rồi phải không, thì bắt đầu sẽ bị nerf lại bằng một số sự kiện nhất định. Ở đây có sự kiện này, với cả sự kiện tên là VIA này, ra thông tin hơn. Nhưng cơ bản là vậy. Thì idea chính là gì? Idea chính là đang có cái xu hướng học từ lịch sử trước đây.\n\n**[43:09]** Từ cái Great Depression năm 1930 cho tới giờ, hiện nay 2020, có một cái nước Mỹ đang trở lại với trường phái protectionism. Sẽ dẫn đến tất cả những nước khác cũng sẽ đi theo cái này. Ai cũng sẽ là dân tộc mình là cái chính. Thì cái chuyện mà mình nhảy khắp nơi sẽ ít lại hơn, so với giai đoạn này. Đường đỏ là đường Trung Quốc nè, đường màu này là đường của Nga nè. Nga sau năm 91 cũng được buff xong rồi, nó đi, nó quất Crimea, cái bị nerf lại. Hiện đang tới Trung Quốc. \n\n**[43:54]** Mà cái này sẽ ảnh hưởng gì? Tới thế sẽ ảnh hưởng là thị trường thì nó sẽ khó khăn. Theo cái hướng nó sẽ favor một số nước nhất định. Không biết Việt Nam, Việt Nam hiện nay trong top 4 mấy cái nước có delta import-export với Mỹ vẫn cao, nhưng mà vẫn được buff. Không biết có được ăn nhậu gì không, nhưng về cơ bản thì mọi người sẽ chạy chậm với tiền mình hơn. Thì hai cái hướng chính nè. Một hướng là công nghệ nó ra, nó replay liên tục để trường hợp mà cái cụm từ này lại được gọi tên lần nữa. \n\n**[44:35]** Với cả cái xu thế về kinh tế toàn cầu đang dày, anh đáng là nó sẽ đi kèm với cái gì mình từng nói với nhau. Thị trường càng ngày càng khó tính, được proven qua cái này ha. Dễ dàng thấy với mình thì mình sẽ phải behave như thế nào. Mời Tom lên show hàng những kỹ năng của Tom. Anh nghĩ là mấy anh em trong team sẽ cần đấy. Anh em trong team mình sẽ cần những kỹ năng mà Tom nó được từ cái làm việc với ai ha. \n\n**[45:31]** Trong team mình hiện tại á, có một số cái mình không nói về hướng phát triển của software nữa nha. Cái đó thì nói với nhau suốt rồi. Nhưng mà trong quá trình làm việc với Tôm, anh nhận ra Tôm có một kỹ năng rất hay. Đó là gần như nguyên cái life cycle của chuyện làm phần mềm, một mình Tôm gần như dùng khả năng viết code, tự động hóa bằng tool, tự mình viết agent luôn. Thì gần như cả quá trìn h từ dev ban đầu, capture cái insight dự án, xong rồi lên planning.\n\n**[46:07]** Cả các thứ, Tom xử lý rất OK. Nên hiện tại anh muốn em show một tí [âm nhạc] về cái approach của em trong quá trình làm việc. Khi em nhận được đề bài cho tới lúc em đến cái planning của em, nó như thế nào, em đã làm ra sao? À, OK, chắc để em share screen. Hy vọng không có gì nhạy cảm. Anh nghĩ là mình lấy luôn cái đề bài mà tí nữa mình sẽ đi sâu, sẵn đó. Ô, đề bài chơi cái đó luôn đó. Mình đang không biết là cái gì đó, mình cũng chưa đi sâu luôn. Thì giờ cái phong hợp nhất đang gần như là zero.\n\n**[46:55]** Nó có một cái ví dụ về đề bài thôi đấy, cho tới lúc mà em đến cái kia như nào. OK, để em share screen, tìm lại cái chỗ đó, đúng không? OK, thông thường, logic phía em là như thế nào? Mình có data, mình muốn gỡ ra những ý của cái data này. Nếu mình có mấy cái ảnh này, thì ví dụ em sẽ cởi hết chỗ này, sau đó extract ra. Cái app này nó có những cái gì mình sẽ phải để ý. OK, sau đó là những direction mình muốn cho nó, giải thích cho mình. Vì luôn luôn là có thể mọi người xem cái app này.\n\n**[47:59]** Có thể là Airbnb, hoặc là dạng app cho personal trainer và lifestyle trainer. Thì ví dụ ở đây, em muốn tìm kiếm kiểu \"What the hell\", thì trước tiên em sẽ sắp xếp một cái prompt. Một là, nếu context là bây giờ em just about to have a meeting with client that asks us to improve their user experience. Sau đó là ý context của bên ngoài, rồi context của bên mình là \"I have some idea of what they may want\". Câu hỏi là có cần input luôn cả cái brief của cái đưa mình không? Ở đây không có. Sau đó là, this chính là cái này.\n\n**[49:08]** Nó sẽ là objective, adjective, context. This is the email they sent to us. Sau đó, em muốn cái vision chính là \"What is the vision, goals, and objectives for them asking us to help improve?\". Từ cái này, em sẽ sinh ra một số context em dùng để gửi lại cho bên phía AI. Thực tế thì cái này nó chắc em làm rồi chứ? Ờ, cái đứng ra là model nào cũng được, nhưng thinking model sẽ giúp mình kéo ra những góc nhìn mà mình không phát hiện ra. Những thinking model rất là siêu về mấy cái đấy.\n\n**[50:15]** Thì cũng hơi functional, user app-centric. Từ cái context này, em sẽ biết app nó là gì, sau đó hình dung cái vision họ đang muốn cần là cái gì. À, chính là có cái gì chi tiết hơn về user, user experience. Thì như vậy, em sẽ hỏi câu hỏi là \"What images, what bọn này muốn?\". Bọn này không muốn gì đâu, bọn này đang muốn là sẽ clone cái app này, chứ không phải là improve cái app này đâu. Cái app đã có sẵn rồi, và giờ nó muốn clone lại, mirroring đúng không?\n\n**[51:07]** Cái này là một cái đã có sẵn, lại mình làm. Với cái chuyển trường hợp này thì sẽ sinh ra một số câu hỏi như \"Are there ways their app is extending to? What are your thoughts?\". Sau đó, dần dần em xây dựng một cái picture. Từ cái picture này, sẽ sinh ra một cái prompt model cuối cùng để gửi cho bên phía làm cho mình. Ví dụ là tiếp tục về một cái dự án đã proven model shortcomings. Như vậy mình sẽ có một số cái mình phải chú ý. Chú ý bên phía mình sẽ phải thử bằng tay những cái gì nhỉ? \n\n**[52:15]** Chi tiết về concept validation này. Chính là cái gì nó work rồi dùng cái đó thôi. Concept như vậy thì em sẽ làm một cái prompt là \"Give me a proposal to pass on what I learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what the story is đằng sau họ đang muốn cái gì, and what they want us to consult, develop?\". \n\n**[53:23]** Về cái chuyện proposal này nó sẽ ra dạng như thế nào,  sau đó từ cái này, vì mỗi thứ mình dùng với AI nó sẽ có reference sẵn rồi, em sẽ copy một cái reference mình có sẵn. Là cái proposal đã làm sẵn, ví dụ trước là cái này. Sau đó là mình sẽ copy cái proposal của bên phía chẳng hạn đi, nhân đi, đi này đi nhỉ. Hình như là hình như internet đâu á? À, chắc copy nhầm này. Đúng ra là mọi người có thể ra cái này, hoặc là download. Use reference to create the proposal, or just in case, don’t take elements. \n\n**[55:16]** From but do follow the proposal format để adapt to what we learned and what they wanting to meet the trust. Sau đó, luôn luôn là mình sẽ expect cái proposal này nó không ổn định. Nó sẽ ổn định lúc mình bỏ thêm những idea, những idea mình thấy là mình có thể involve bản thân mình vào. Vì do mình đang xem khía cạnh của họ là dạng như thế này, thì bên phía mình sẽ làm được cái gì? Ví dụ skillset bên phía em khuyến khích là giỏi về user experience, user flow, data flow. Trong này mình có Mirror được cái app và optimize cái data flow, user flow chẳng hạn.\n\n**[56:10]** Hoặc là bên phía anh Thành là optimize về security và performance. Làm như thế nào để apply đúng cái project proposal này? Thêm về mấy cái kiểu good-to-have: performance và security. Nếu là dạng MVP thì mấy cái này sẽ không consider mấy chuyện hack. Là những cái design liên quan với data. Ví dụ bên phía em thì hay thiết kế data dạng là temporal state, event store, hoặc là thiết kế uniform.\n\n**[56:51]** Như thế nào để apply đúng kỹ năng của mình trên computer science về cái này? Cho nó không phải đơn giản quá, nhưng sẽ simplify, maintain cho cái chuyện cái app này nó đưa ra. Nếu mà trên đây với cái tham khảo này đi, bây giờ sẽ tới kiểu anh sẽ cần mấy cái để chốt được cái deal đúng không? Mình sẽ phải cần những câu hỏi để hỏi xem với bọn đó như thế nào. Giống như con open deal, mình open book á. Mà mình nói đi thì phải cần mấy câu hỏi đấy nữa, kèm với chuyện gần như phải suggest được cái lịch làm việc, cái milestone làm việc tiếp theo giữa mình với bọn đó.\n\n**[57:28]** Cần mấy câu hỏi đấy nữa, kèm với chuyện là gần như phải. Em làm như nào? Building rapport, sau đó là xem về burning questions chúng nó. Thì nếu mình có chuyên về nghề của mình, thì mình sẽ suy ra mấy cái câu hỏi cũng không khó lắm. Nhưng nếu mình thấy là mình hơi bị stuck, mình có cái block gì đó, thì mình sẽ nhờ AI cho hỏi mấy cái question. \n\n**[58:14]** \"So we haven’t met with this partner yet, with this client yet, but we want to make a deal with them. What should I do to help build rapport and meet the three burning questions I need to get this deal off the ground and solve any technical concerns?\". Thì cái này là good start, mình sẽ dùng cái này cho bên phía AI suy ra một số câu cho mình. Sau đó mình sẽ dựa trên cái này suy ra thêm. Nếu mình có suy ra thêm thì mình sẽ bổ sung thêm ở trên proposal và add thêm cũng realistic thôi. Không phải riêng bên phía Gemini, nhưng có một số app như Claude hoặc là ChatGPT, mình sẽ phải làm như thế nào. \n\n**[59:12]** Những cái due diligence mình sẽ phải làm như thế nào? Những cái burning question, ví dụ ở trên này mình không có context của trước, thì dùng đi. Nó kiểu như thế ngoài đó. Mình muốn đặt mấy cái goal như vậy, đứng ra là ở trên cái proposal đầu tiên, mình đang hơi nghi ngờ là mirroring là tại sao họ mirror? Nó sẽ hở ra ở trong cái intent của cái proposal đầu tiên mình xây dựng cho họ. Nên là nó sẽ liên quan với cái này. Lúc mình có thêm không nhất thiết. \n\n**[59:50]** Sẽ dùng luôn cái này, nhưng từ cái này em sẽ suy ra là, à, maybe góc nhìn về handling real-time thì sao? Maybe bên phía họ thì không phải real-time, nó sẽ kiểu như booking appointment app. Và nếu mình ghi về dạng real-time, họ có muốn đi hướng vision đó không? Để đem ra consult xem là họ muốn cái app nó kiểu đẹp hơn, ổn hơn, hay là họ muốn cái mới hơn, hoặc kiểu risky hơn? Nó sẽ là mấy cái step mình hỏi, mình chém, để xem họ reply như thế nào thôi. Và nó không có hại.\n\n**[01:00:34]** Vì nó cũng là câu hỏi hợp lý mà. Rồi, ví dụ như bước tiếp theo dev này nó hit đi, thì sau đó cái đoạn mà lên to-do rồi, kể mọi thứ thì như nào? Dạ, dạ, nó cứ hình dung. Em có một số cái cứ hình dung là cái điều này đã OK rồi. Sau đó em bỏ sung cái technical direction mình đồng ý để đi tiếp theo với họ. Ví dụ là real-time đi, \"We think they want something like this, but are open to the idea of a more real-time something like Grab, Uber for the personal trainer\". Trước tiên em sẽ xây dựng cái Technical proposal.\n\n**[01:01:33]** Như chắc không cần đâu, thông thường em sẽ xây dựng cái đó để làm rõ góc nhìn. Nhưng từ khía cạnh này, thì ví dụ là \"Help me create tasks for frontend, backend\". Tại vì cái đoạn giữa mà Tôm em sẽ figure out ra tất cả mấy cái diagram, flow, rồi tất cả mọi thứ. Phải chốt cái đấy trước, mới base cái đấy bắt đầu làm cái breakdown đúng không? Nên để đơn giản hóa hôm nay mình sẽ nhờ bên phía AI suy ra luôn.\n\n**[01:02:11]** Cái này nó là một cái góc sơ sơ, nhưng mình sẽ bổ sung thêm là \"We are planning to use Timescale and RxJS to do the sync and part real-time features of the app. We are most comfortable with React for frontend, and our house mostly uses all this in mind. Create and format tasks with description, user story, and acceptance criteria\". Mình sẽ nhờ bên phía AI viết giúp mình cái này luôn. Sau đó, nếu mình dùng thì mình sẽ copy cái copy epic là cái gì, copy story là cái gì, copy cái story, sau đó bỏ xuống cái criteria.\n\n**[01:03:44]** Cái này thì bên phía em thì làm thêm cho về cũng là cho bản thân. Vì ở đây đang là story, giải thích cái story, xử lý cái story. Lúc mình đến technical, technical nó chỉ cần confirm là nó có đạt đúng tiêu chí của story không. Vì nếu story đó nó tồn tại chung với cái vision của họ, coi như mình làm thành công bên phía họ rồi phải? Nhưng mà dự như cái sườn này là bắt đầu scale lên được một cái chất.\n\n**[01:04:23]** Chờ cho tất cả những cái liên quan cho backend. Thông thường trong technical proposal hoặc là cái context, em sẽ bỏ xuống thêm boilerplate, những cái code mình đã dùng rồi, những cái concept mình muốn apply ở trên cái app này. Với goal chính là goal của mình dựa trên goal của họ. Copy bên phía họ thì nếu có cái lúc có cái đấy xong, sau đó xây dựng mấy cái test này, thì sẽ có đầy đủ để mình breakdown đúng cái task mình cần thiết nhất. Ờ, đúng là nó sẽ độ chính xác tầm 90 phần trăm, nhưng 10 phần trăm còn lại nó sẽ bị thừa.\n\n**[01:05:02]** Nhưng mà đỡ hơn là mình bắt đầu ở chỗ kiểu zero đúng không? Rồi, chắc tới đây thôi. Giờ Tom anh bắt đầu có con, với lại khách hàng thật rồi. Tí nữa giao hết cho Tom nhé. Nay chốt tới đây thôi bạn ơi. Đây là nghĩa là bước đầu tiên để show được quá trình làm phần mềm á. Nếu mà mình có một kỹ năng mềm tốt, với lại capture được cái domain và tất cả quá trình làm việc á, có thể leverage AI rất là nhiều để mà quá trình làm ra một. \n\n**[01:05:39]** Người ban đầu lúc trước, một cái quá trình như vậy sẽ tốn khoảng 2 ngày, 3 ngày, 4 ngày gì đấy. Giờ quá trình làm xong, soạn rồi, vẽ diagram rồi, present cái idea, những hệ thống kiểu cũ á, nó nhanh rất là nhiều ha. Nên khi xong là đây là một cái skill trong team mình, Tom đang ở mức độ này. Ờ, mà Tôm đang tự tin là nó đang khoảng bao nhiêu phần trăm hả anh? Anh không rõ lắm. Mà anh nghĩ chắc đâu đó, chắc sẽ trên 50 phần trăm ha, trên 50 bé hơn 90. Hy vọng là những cái bước về sau thì sẽ có những buổi sau.\n\n**[01:06:21]** Mình lại làm thêm vài buổi với Tom. Còn giờ chắc là tạm thời dừng ở đây. Các câu hỏi có liên quan thì anh em sẽ hỏi sau. Giờ anh đây. Bye bye, hẹn gặp lại mấy anh em nhé.\n\n---\n\n### English transcript\n\n**[00:00]** Let’s get started. Hey everyone, thanks for waiting. Where are Thành and Cường? Has Cường joined the room yet? I saw he registered for Friday, but he’s up here now, right? Where’s Thành this week? Oh, he’s here, standing right there. Tuấn, Tom, hop on stage now.\n\n**[04:51]** We’re going through some articles, and suddenly this link, Tom, isn’t it great? Let me fix it. Today’s stats: 186 transactions, 1 user, 30 ICY members as usual, 5 inactive, 1482 fakes. Which are the top two most active chat channels? The top three? Who’s chatting the most? Oh, we’re in trouble! Anyone else around? Are we missing someone today? There are two old topics: one is “run and report”, I posted the link this morning, I think, let me double-check. The second is Cường’s design piece; I don’t know the details yet.\n\n**[06:03]** What’s this one about? I’m sitting here listening and totally lost. The third piece follows up on the series from before you guys wrote it, worked on it, and now it’s taken solid shape. After three months, the team’s got some small updates, and this direction’s getting a bit clearer. The system feels outdated, though; I’ll forward a link later for everyone to review via email.\n\n**[07:10]** Sign up and try it out, we’ll dive in later. That’s the plan. We’ll probably ship Hải’s piece first, then Cường’s, then Tom’s the parts Tom worked on. That’s today’s content, I think. Guys, check if anyone’s missing or if it feels too short. Anything else related we should add? Who’s not here? Has Thành joined yet? Oh, bro Thành’s on fire out of work to do. I think so too.\n\n**[08:55]** Hold on a sec, let’s wait till everyone’s here, then we’ll speed-run these topics. They’re pretty straightforward. Try to sum up your piece of concept, idea, n 10 minutes max. Don’t go overboard so we can save time for the other session. If you need more than 10, stretch it a bit, alright? Next week’s the office schedule; this week’s just regular check-in.\n\n**[09:57]** Next week, based on the sign-up list, I’ll suggest to Huy Nguyễn we do a roll-call game to get everyone in. It’s basically policy now, full attendance next week. The next part: those old projects are nearly wrapped up. Now it’s all about deploying blockchain and AI, they’re the kings of the game. Anyone wanting to work on them directly needs to plan it out. Any duplicates? Any more ideas?\n\n**[10:58]** Guess we’ll start with Hải’s piece first. Hải, go ahead and present! Uh, everyone can see my visuals, right? I’ll summarize the frontend report for January. Last December, React 19 dropped, and alongside it, Next.js 15.1 rolled out a new version too.\n\n**[12:07]**\n\nTo support both Next.js and React 19. On the React side, I see they’re working on a pretty cool API called View Transition. Browsers already have this View Transition API, but React didn’t support it before. Some libraries have built on that external API, but when integrated into React, they hit a few performance snags. Yeah, they’re waiting for React’s version of this API to improve support and tackle those performance issues more cleanly.\n\n**[12:48]** This API’s for animating transitions between two stages of a webpage. Like, if you scroll down here, it’s like this example below. The first stage has the box up top, the second stage has it below. Instead of the stage just jumping straight down, View Transition helps us create an animation effect, sliding back and forth smoothly. Same deal with images, it adds animation effects too.\n\n**[13:28]** When switching images, instead of instantly jumping to the next one. Yeah, this API’s still in the experimental phase. You’ve got to use the experimental version to try it out. But it promises a performance boost when implemented. Before, Motion supported this, but only in a vanilla environment. When scaled up, it ran into some performance hiccups because it had to handle pre- and post-set states.\n\n**[14:07]** On this front, over at SCS, there’s stuff like Deno Deploy. It used to only support static site deployment, but now it fully supports deploying Next.js too, including server-side rendering. Now we can use Deno as a replacement, blending it in to deploy an NS app. Yeah, nothing much to say on that yet. Then there’s this Transformer Z library, pretty neat. At its core, it’s about converting models.\n\n**[15:03]** At its core, it’s about converting models written in Python into JavaScript, so we can run these models directly in the browser without needing APIs or Python itself. Like in this article, it can handle sentiment testing say, positive or negative or object detection, like spotting a cat. Essentially, I think other models or pipelines can work too, as long as they’re supported by this library.\n\n**[18:33]** We had to support a setup where, network or not, all data still gets saved. So we chose to store it in IndexedDB, then push it to the server once the connection’s back. That’s the gist of it. Down here, it’s got step-by-step instructions for handling it. Doing it this way runs into a few issues, like data lists failing during sync, for example. It points out some ways to tackle those problems.\n\n**[19:22]** Something like that. Did An just post a link or something? What’s Zero? What did An just say related or not? The other day, I saw Lập mention this “local first” thing probably the same deal, right? Everyone’s tackling the same problem, racing to solve it. Next up, there’s a mention from the Win side. This one’s got an update, it supports that thing now. Before, with Node.js, you had to use a command line to compile TypeScript into JS to run it. Now it runs directly.\n\n**[20:01]** Like, it runs straight from the command line, loading the file as-is. From what I see, there’s another piece about this dev guy, talking about dependencies at MBM. They keep dropping new versions, and each one comes with breaking changes. He says it’s a pain, wants to update versions but worries the app can’t keep up. There’s not always time to fix everything. So he’s not big on React, went a different route. He says this one’s more stable, less prone to constant shifts. He prefers it over the others. Meanwhile, HTMX is popping off, sitting at number one.\n\n**[21:07]** Yeah, one last quick bit about Neon. This one’s a database service provider. They just switched from Webpack to something else. During the process, they hit some snags and realized Webpack’s got limitations. Like, it doesn’t support things well, there’s a long list of issues right here. But the end result after switching? They feel the new setup beats Webpack. First, it’s got fewer bugs, more reliable than Webpack. Second, its config is simpler. They say with just a dozen or two Webpack plugins, it makes their setup way lighter. I’m not sure why they went with that.\n\n**[22:03]** But the final outcome after the switch is they think its hot reload is better than Webpack’s. It triggers fewer checks during full reloads compared to Webpack. Second, its config is simpler. Like they said, with about a dozen or twenty Webpack plugins or so, it keeps their setup much lighter.\n\n**[23:01]** This piece mostly covers the challenges and the final results of switching from Webpack to that other thing. So, does that mean what we’re working on is shifting too? Are we moving from Webpack to this new one as well? What about that React stuff up there?\n\n**[23:50]** Switching to HTMX, huh? That’s two now., what else is there? Using Deno? And TP, is that the main framework now? Yeah, alright, it’s in. For the other articles, you guys can check them out in here. Uh, what was it—Hải, can you repost that link? Thanks, Hải, and thanks, everyone, for the replies. What’s HTMX, and why’d it get picked? HTML with logic baked in? Like, it injects some stuff straight into the HTML and uses it to handle things directly, no fuss. This trick goes back to Backbone.js and Knockout.js days. \n\n**[25:06]** A decade ago, and now they’re doing it the same way again. Any questions, guys? One minute for extra comments. Anything need updating? If there’s something, Hải, toss the link in random channel or group chat, whatever. Next up. Let’s move quick to Cường’s topic on database design. Alright, starting now. History lesson, huh? This stuff, these practices, they’ve been around since 2017.\n\n**[26:17]** Just a recap, right? Summing it up? Summing up data design skills, entity tips? Nah, not exactly data management. More like practices for handling the knowledge as we build and scale up our database. Alright, I’ll dive in. The database and the system we’re developing always go hand in hand. When our software scales up to meet business demands, we’ve got no choice but to scale the database too, to manage a huge amount of data over the years.\n\n**[26:51]** Take Amazon: in 2015, they had about 50 million data points, then by 2020, it grew to needing to handle 200 million. So why do we need these practices? When your database hits hundreds or thousands of schemas, management systems like SQL Server or other data management tools, you look at the schema diagrams, tables, or data, and you can’t possibly grasp it all.\n\n**[27:27]**\n\nThe context why were these changes applied to the system? To boil it down, there are a few practices. It’s gotta be a combo of people and systems to manage this knowledge. All of this is just practices not about picking a database management system or designing the schema itself. It’s about how we share database knowledge, store that knowledge. And when database changes get rolled out, there’s a separate system to manage those changes like continuous integration and stuff like that.\n\n**[28:02]** Those changes have to follow a few refactoring rules. Regarding no-sharing, we usually have someone called a DBA in our organization. This person manages and shares all the knowledge and changes applied to the database within the system. For example, if we’ve got multiple dev teams, say Dev 1 working on Software A and Dev 2 on Software B, both need to check with the DBA when pushing changes to the system’s database. The DBA verifies each change to see what it does and decides if it makes sense for the main database.\n\n**[28:34]** When a dev pushes their database changes, they verify with the main system to check if the APIs calling the database are affected. Then they assess whether the change is necessary. If it impacts the system too heavily, the DBA might reject it and ask the dev to update, refactor, or adjust it to fit better. Once the change is approved, the DBA documents what it means, why it’s needed, and posts a migration for the master database to start updating.\n\n**[29:14]** That data also needs to be stored somewhere everyone can easily access and search, so they understand why these changes matter. All these changes go into a repository, much like a coding project. This repository holds all the database artifacts, including scripts to run the database, login credentials, configurations, and the maximum capacity these instances can handle, plus system documentation. It’s similar to a coding project and gets managed with version control.\n\n**[29:51]** And searchable, so we know why these changes are necessary. All those changes get stored in a repository, just like a coding project. Any questions, guys?\n\n**[30:39]** So everyone can check and review the changes, their context, and history in the system, each time a change happens, the person pushing the migration creates a pull request with a description. This description explains why the change is needed, how essential it is, and which systems it’ll affect. The reviewers, mostly devs from the APIs directly impacted by this change, step in to take a look.\n\n**[31:14]** After those changes get merged into the master branch, there’s versioning so we can rollback or deploy these versions to individual systems for development, testing, and finally production. When we’ve got multiple dev instances across versions, and we’re working on separate systems, we have to check out from an instance of the master database for development use. That way, when we tweak something or add a new migration, it doesn’t directly mess with the main database.\n\n**[31:52]** At that point, we need a CI system. Whenever we change something in the instance we’re developing on, we can easily verify if the change breaks the master database. Plus, when someone pushes a new change to the master database, we get notified about schema updates or resource conflicts before it slows down our dev progress. When rolling out a database change, it involves a few steps, like modifying a database schema.\n\n**[32:25]** When pushing a change, we have to create a migration script for that database. Once the script’s merged, we update the database access code so the API can use the new change. For database changes like adding a new column, it might not always require tweaking the API’s access layer when the change goes live, since some APIs don’t need to touch that new column. For instance, if we’ve got a user table with name and address, and a new service needs to add a birthday field to the user table, older services, like one grouping users by address, don’t need API changes to integrate this new update.\n\n**[33:07]** For changes with big impacts, like introducing a non-null value or splitting a table, all dependent services need to update their data access layer to avoid errors. Take that user table from earlier, for example. If we split the user table, every service using it has to overhaul its access layer to prevent bugs. Alternatively, we could use something called a transition interface to gradually apply the new changes and roll them out without crashing the old APIs.\n\n**[33:45]** After refactoring and applying the change to the master database, we still need to notify all services using this database to prevent breaking those APIs. At the same time, folks can coordinate to resolve config issues when the master database changes. For a recap, during software development, as the software grows, the database has to grow too. To keep everyone in the loop about the info and context of each database change, we need to leverage all our knowledge to share and organize it effectively.\n\n**[34:32]** Also, all these changes have to be released thoroughly to avoid timing conflicts or resource clashes between database updates. This piece has value in its perspective. It’s probably like a dev’s viewpoint, but it focuses on shifting the main object of work.\n\n**[35:21]** The info and context of each change within this database require us to use all our knowledge for knowledge sharing and to organize it well. Plus, all these changes must be released thoroughly to avoid timing conflicts or resource clashes among database updates. That’s it. Any questions, guys?\n\n**[35:58]** It’s not about the codebase but the database, right? Leaning heavily that way. Hearing this part feels a bit meta, like it’s more relevant to big systems. For systems like ours now, it’s kinda tough to apply, huh? A system with about 20 tables already feels a bit sluggish, and looking at it gets overwhelming. Exactly. So this ties into documentation, managing versioning, and monitoring. Not version monitoring, but notifications for other teams, right? Yeah, it’s still limited, but spot on. Bringing this in makes sense because of the perspective.\n\n**[36:50]** Data management comes before the other stuff. The logic’s here, and tomorrow the data will run. Any questions for Cường, guys? If not, we’ll wrap up here. This piece has value in its perspective. Thinking about it, for backend devs wanting to strike it rich, you’ve got to stick with projects long-term. The longer the project, the more money it’s got. That’s how it seems. Sticking with a long-running project is solid at its core, but devs usually get lazy. When something drags on too long, they get bored, and their behavior turns weird.\n\n**[37:40]** Before moving to the next piece, to contribute to today’s session, here’s a keyword of the week. While reflecting on stuff, I picked up a new one, a fresh term for those who don’t know yet, like me. Today I came across this school of thought called Luddism. Luddism’s a word from the 19th century, tied to the Industrial Revolution. Industries like textiles got automated, and that ticked off some folks, called Luddites or something, who went and smashed those machines.\n\n**[38:34]** Those machines stole their jobs, their livelihoods, so they wrecked them. That turned into a movement called Luddism, where the working class pushed back against modernization trends. The next keyword digging deeper is Neo-Luddism, alongside this Luddism stuff right here. Check it out if you want, guys. It feels pretty relevant to what’s coming up for us, based on predictions from the other day.\n\n**[39:18]** When we were chatting, we figured there’d be a lot of pushback soon. On Reddit, there’s a post from two years back about a new Neo-Luddism wave popping up. Now it’s everywhere up there, huh? So, from my angle, I’d say we shouldn’t jump on this bandwagon. Progress keeps moving forward, and we shouldn’t fight the wheel of history. There’s even a subreddit called Luddism, diving straight into it. Not just about automation, but all sorts of tech pushback, feelings of being lost or out of place. Pretty interesting keyword, right, guys?\n\n**[40:08]** Not getting stuck in that, huh? Then there’s a second thing tied to this. I just sat down and dug into it recently, and it’s U.S. geopolitics. There’s a perspective on how the U.S. is evolving. I was researching capital markets, tracking where investment money flows, and stumbled into this topic. It’s the second theme, pretty interesting. Maybe you guys will care about it. This ties into macroeconomics. Turns out it stems from that angle and shifts into macroeconomics.\n\n**[40:56]** The U.S. is trending toward just two camps. One is isolationism, meaning pulling back. There’s another word we could use for it, figuring that out here. So, in this movement, what’s it saying? The U.S. will likely shrink inward, stop spreading resources everywhere for trade, and gather them up to hunker down defensively. That’s the first cluster. Right now, from all the news I’m seeing, it’s leaning that way, protectionism or isolationism. This cluster aside, the second direction I see is globalization.\n\n**[41:41]** With globalization, innovations and jobs focus more on trading with each other, boosting cross-border commerce. Nations toss stuff all over the place. The U.S. would trend outward, along with allies in that camp. Countries following that path would also open up more, moving freely everywhere. That’s the state of things, based on reports from 1945 up to recently, breaking into smaller phases.\n\n**[42:20]** In the post-war phase with Japan, after hitting them hard twice, they helped Japan and Germany rebuild after the war. That kicked off globalization in that direction. It’s the initial phase. It started there, and when Japan got too strong, right? It got dialed back by certain events. There’s this event here, plus one called VIA, with more details out there. But that’s the basics. So what’s the main idea? The core idea is there’s a trend we’re learning from history.\n\n**[43:09]** From the Great Depression in 1930 up to now, 2020, there’s a sense the U.S. is swinging back to protectionism. That’ll pull other countries along too. Everyone’s putting their own nation first. So, hopping around everywhere will slow down compared to this phase. The red line’s China, this colored line’s Russia. Russia got a boost after ’91, went for Crimea, then got dialed back. Now it’s China’s turn.\n\n**[43:54]** So how’ll this affect things? Globally, it’ll mean tougher markets. It’ll favor certain countries in that direction. Not sure about Vietnam. Vietnam’s in the top four for import-export delta with the U.S., still getting a boost. Not sure if we’ll cash in big, but generally, folks will move slower with their money. Two main paths here. One is tech keeps pumping out stuff, replaying constantly, so this term might pop up again.\n\n**[44:35]** Plus, the global economic trend’s getting thick. I reckon it ties into what we’ve talked about. Markets are getting pickier, proven by this, huh? Easy to see how we’ll need to adapt. Let’s get Tom up to showcase some skills. I think the team could use them. Our crew needs the skills Tom’s picked up from working with whoever.\n\n**[45:31]** In our team right now, there’s some stuff I won’t dive into about software dev trends. We’ve hashed that out plenty. But working with Tom, I noticed he’s got a slick skill. Almost the whole software dev life cycle, Tom handles solo, using coding chops and automating with tools, even writing his own agents. From initial dev to capturing project insights, then planning it out.\n\n**[46:07]** Everything, Tom nails it. So I want him to show a bit [music] about his approach during work. From getting the brief to reaching your planning stage, how’s it go, what’ve you done? Oh, cool, I’ll share my screen then. Hope there’s nothing sensitive. I figure we’ll grab the brief we’ll dive into soon, right there. Yeah, let’s roll with that one. We don’t even know what it is yet, haven’t dug in. So the starting point’s basically zero.\n\n**[46:55]** It’s just got a sample brief, up to when I get to that part. Alright, I’ll share my screen, find that spot, yeah? Cool, so usually my logic’s like this. We’ve got data, and I want to unpack its key points. If we’ve got these images, say, I’ll strip it all down, then extract stuff. What’s this app got that we need to watch? Okay, then it’s the directions I want it to take, explaining it for me. Cause it’s always possible folks see this app—\n\n**[47:59]** As maybe Airbnb, or some personal trainer and lifestyle trainer app. So here, I’m trying to figure out “What the hell,” right? First, I’d set up a prompt. Say the context is I’m about to meet a client asking us to improve their user experience. Then there’s their external context, and ours is “I’ve got some guesses on what they might want.” Question is, do we need to input our whole brief too? Not here. Then it’s this, the main bit.\n\n**[49:08]** It’s objective, adjective, context. This is the email they sent us. Then I want the core vision, “What’s the vision, goals, and objectives for them asking us to help improve?” From that, I’ll generate some context to send back to the AI side. In reality, I’ve probably done this already, huh? Yeah, any model works, but a thinking model helps pull out perspectives we miss. Those thinking models are ace at that stuff.\n\n**[50:15]** So it’s kinda functional, user app-centric. From this context, I’ll figure out what the app is, then picture the vision they’re after. Oh, it’s really about something more detailed on users, user experience. So I’d ask, “What images, what do these guys want?” They don’t want much, they’re looking to clone this app, not improve it. The app’s already there, and now they want to mirror it, right?\n\n**[51:07]** It’s an existing thing we’re redoing. With this shift, it sparks questions like, “Are there ways their app’s extending? What’re your thoughts?” Then I gradually build a picture. From that picture, I’ll craft a final prompt model to send to our side’s team. Like, moving forward on a proven model’s shortcomings. That way, we’ve got stuff to watch out for. We’ll need to manually test what, exactly?\n\n**[52:15]** Details on this concept validation. It’s just using what already works. With that concept, I’d make a prompt like, “Give me a proposal to pass on what I’ve learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what’s the story behind what they want, and what they want us to consult, develop?”\n\n**[53:23]** On how this proposal will shape up, after that, since each AI tool we use has ready references, I’ll copy an existing one we’ve got. A pre-made proposal, say from before, like this. Then we’d copy a proposal from their side, maybe, duplicate it, tweak it here and there. Wait, internet’s out? Oh, probably copied the wrong thing. Should be, you guys can pull this up or download it. Use the reference to create the proposal, or just in case, don’t lift elements.\n\n**[55:16]** From it, but follow the proposal format to adapt to what we’ve learned and what they’re aiming for to build trust. After that, we always expect this proposal won’t be stable. It’ll firm up once we toss in ideas, ideas I think we can bring ourselves into. Since we’re seeing their angle like this, what can our side deliver? For example, my skillset leans toward excelling at user experience, user flow, data flow. Here, we can mirror the app and optimize its data flow, user flow, stuff like that.\n\n**[56:10]** Or, from anh Thành’s side, it’s optimizing for security and performance. How do we apply this correctly to the project proposal? Adding in some good-to-have stuff like performance and security. If it’s an MVP, we wouldn’t consider hacking concerns much. It’s more about designs tied to data. For example, my side often designs data with temporal state, event store, or uniform patterns.\n\n**[56:51]** How do we apply our computer science skills to this properly? Not overly simple, but simplifying and maintaining what this app delivers. If we go with this and the reference here, now it’s like anh needs some stuff to lock in the deal, right? We’ll need questions to figure out how it works with them. Like an open deal, all cards on the table. To get there, we need those questions, plus we’ve got to suggest a work schedule and next milestones between us and them.\n\n**[57:28]** Need those questions, along with the fact we’ve pretty much got to do it. How do I handle it? Building rapport, then digging into their burning questions. If we’re sharp in our craft, coming up with questions isn’t too hard. But if I feel stuck, hitting some block, I’d ask AI for question ideas.\n\n**[58:14]** “So we haven’t met with this partner yet, this client yet, but we want to make a deal with them. What should I do to help build rapport and find the three burning questions I need to kick this deal off and address any technical concerns?” That’s a solid start. I’d use it to have the AI spit out some questions for us. Then I’d build on that. If I come up with more, I’d add them to the proposal, keeping it realistic. Not just Gemini, but apps like Claude or ChatGPT, how do we approach it?\n\n**[59:12]** How do we handle the due diligence? For burning questions, say we’ve got no prior context up here, just use it. It’s like that out there. I want to set goals like that, starting with the first proposal. I’m a bit skeptical about mirroring, why do they want to mirror? It’ll show in the intent of the initial proposal we built for them. So it ties into this. When we’ve got more, it’s not always set.\n\n**[59:50]** I’d use this as is, but from here I’d figure, maybe a real-time handling angle? Perhaps their side isn’t real-time, more like a booking appointment app. If we pitch real-time, do they want that vision? To consult on whether they want the app prettier, stabler, or newer, riskier even? It’s steps where we ask, throw stuff out, see how they reply. No harm in it.\n\n**[01:00:34]** Cause it’s a fair question anyway. Say this dev step lands, then what’s next with the to-do list and laying it all out? Yeah, yeah, it’s like picturing it. I’ve got some stuff I picture as already sorted. Then I flesh out the technical direction we agree on to move forward with them. Like real-time, “We think they want something like this, but are open to a more real-time thing, like Grab or Uber for personal trainers.” First, I’d draft the technical proposal.\n\n**[01:01:33]** Probably not needed though, usually I’d build that to clarify the angle. But from this view, it’s like, “Help me create tasks for frontend, backend.” Cause in that middle stretch, Tôm here figures out all the diagrams, flows, everything. Gotta lock that down first, then base the breakdown on it, right? So to simplify today, we’ll have AI churn it out.\n\n**[01:02:11]** It’s a rough angle, but we’ll add, “We’re plannin","title":"OGIF Office Hours #39 - Frontend updates, Database scaling, AI workflow, and Macro insights","short_title":"##39 Frontend report, DB Scaling, AI Workflow","description":"In OGIF 39, the team explored React 19 and Deno Deploy updates, database scaling with CI and migrations, Tom’s AI-driven dev workflow, and macro insights on protectionism and globalization trends.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Wed Feb 12 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/39-20250207.md","slugArray":["updates","ogif","39-20250207"]},{"content":"\n### Topics and Highlights\n\n- **Session setup \u0026 check-in**: Kicked off with a casual vibe, confirming no all-hands this week and setting up three talks. Phát skipped his slot, and the team troubleshooted screen sharing for demos.\n- **AI fine-tuning overview**: Explored fine-tuning vs. retraining, using a doctor’s note example to show how fine-tuning embeds knowledge while retraining leans on token-heavy prompts.\n- **Fine-tuning demo**: Showcased a Duty 40 Mini fine-tuning job on Open AI (~4800 tokens), comparing pre- and post-tuning results, with a nod to local vs. hosted model trade-offs.\n- **Data archiving essentials**: Biên broke down archiving vs. backup for apps with 50K-1M daily transactions, focusing on metadata, cloud storage, and recovery to optimize query performance.\n- **Archiving tools \u0026 Q\u0026A**: Highlighted tools like AWS, Google Cloud, and Timescale, plus hot/warm/cold storage options (Azure, Backblaze), with audience questions on scheduling and platform quirks.\n- **Datalake foundations**: An traced datalakes from 1980s databases to today’s cloud systems, contrasting warehouse ETL (structured) with datalake ELT (raw data) workflows.\n- **Notion’s datalake scaling**: Detailed Notion’s growth to 96 instances and 400+ shards by 2023, shifting from warehouse to datalake with Debezium CDC, Kafka, Hudi, and S3 for analytics.\n- **Interactive wrap-up**: Fielded questions on datalake vs. replication, async processing, and external data handling (e.g., social media), ending with reflections on big data skillset.\n\n### Vietnamese transcript\n\n**[00:00]** Hình như tuần sau mới có all-hand. Không thấy ai tạo event gì hết, chắc tuần này cứ bình thường thôi nhá. Anh em kiểm tra xem màn hình sharing có vấn đề gì không. Hay lên luôn nhỉ? Hôm nay chắc có ba bài thôi đâu đó. Phát vừa bảo tuần này cậu không có gì mới, chắc skip hôm nay rồi. Anh em thử share màn hình cá nhân xem sao nào. Xem trước được không?\n\n**[10:42]** Theo lịch chắc anh nhỉ, để em lên trước nhá. Fine-tuning này, chủ đề này không mới lắm đâu. Bài này chỉ là 100.5 thôi, không phải 101, nên chỉ giới thiệu sơ sơ, chưa đi sâu được đâu. Tại em cũng mù mờ lắm, nên chắc giới thiệu sơ vậy thôi. Hôm nay em giới thiệu bài fine-tuning. Đây là agenda của bài này nè.\n\n**[12:24]** Introduction là nếu mọi người dùng AI, chắc có nghe tới khái niệm fine-tuning rồi. AI có mấy mô hình đa số được fit vào dữ liệu từ một ngày nào đó, với mấy cái data privacy hoặc data của domain riêng. Dữ liệu này không xuất hiện trong knowledge của mô hình nền tảng (foundation model). Để mô hình có được kiến thức đó, người ta thường dùng retraining, đúng không? Nhưng còn một cách khác gọi là fine-tuning. Cuối bài, em sẽ so sánh hai cách này, xem lúc nào nên dùng cái nào, lúc nào không.\n\n**[13:08]** Trước mắt, cứ hiểu fine-tuning như cách để mở rộng kiến thức cho mô hình nền tảng vậy. Fine-tuning là gì? Hiểu đơn giản là mọi người retrain lại mô hình, lấy một mô hình nền, đưa vào một dataset gì đó để fine-tune, nghĩa là retrain lại nó. Sau khi fine-tune xong, ta được một mô hình đã điều chỉnh, gọi là fine-tuned model.\n\n**[13:44]** Tại sao fine-tuning mang được kiến thức mới? Hiểu đơn giản là trong một AI model, kiến thức được lưu qua mấy cái mạng nơ-ron. Fine-tuning sẽ cập nhật các weight, các thông số của mạng nơ-ron đó, để nó phù hợp với kiến thức mới. Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi.\n\n**[14:19]** Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi. Khi fine-tune mô hình trong thực tế, không phải chỉ ném dataset vào rồi retrain là xong. Đúng là ra một mô hình fine-tuned, nhưng không biết cái mô hình sau retrain này có tốt hay không. Em sẽ giới thiệu một workflow mà bên ngoài thường dùng để fine-tune.\n\n**[14:59]** Cái flow này, nó gồm nhiều bước như thế này. Mọi người có thể chia thành hai cụm, hai cụm nhá. Cái flow này, em sẽ chia thành hai cụm. Em đi qua cụm một trước. Cụm đầu tiên là cụm ở bên trái, hiểu đơn giản là một base model ban đầu. Sau đó, mọi người có một dataset mới, một cái gì đó mới, mọi người quăng vô, fine-tune nó. Rồi nó ra một model, mọi người sẽ supervise nó, có nghĩa là mọi người retrain nó dưới kiểu là retrain nó. \n\n**[15:38]** Sẽ cho nó thêm kiến thức, với input này thì output sẽ ra như này. Nó sẽ ra một cái gọi là supervised fine-tuning. Sau đó, để em đi qua phần tiếp. OK, cái fine-tuning này là retrain on data, nghĩa là sẽ cho một cặp input-output trong dataset để nó học. Nó sẽ học được những kiến thức mới đó. Để bước này hoàn hảo, dataset phải được clean. Nó phải clean, không được lẫn với những cái khác. Nghĩa là nó phải specific cho cái domain mà mình muốn train nó.\n\n**[16:31]** Quay lại hình này, sau khi mọi người có một cái model đã được retrain xong, mọi người mang lên production, mọi người dùng, đúng không? Lúc này, bên ngoài sẽ sử dụng một cái system gọi là human feedback. Kiểu như là response của model này có làm bạn hài lòng không, chấm từ 1 tới N sao, kiểu vậy á. Mọi người sẽ collect cái data đó. Nó nằm ở bước này, mọi người sẽ thu thập human feedback từ cái retrained model của mọi người.\n\n**[17:06]** Dựa vào cái feedback đó, mọi người gọi bước này hơi tốn tài nguyên chút. Mọi người sẽ phải retrain một cái model riêng. Cái model này dùng để đánh giá xem response này được chấm bao nhiêu điểm. Kế đó, mọi người tới bước thứ ba, bước cuối. Ở bước này, mọi người sẽ dùng thuật toán như reinforcement learning để kết hợp với cái retrained model và cái reward model của mọi người.\n\n**[17:46]** Mọi người retrain, mọi người lại fine-tune cái model một lần nữa. Nó sẽ ra cho mọi người một cái gọi là model tối ưu. Cái vòng lặp này cứ tiếp tục, tiếp tục mãi. Mọi người có cái model đã retrain xong, thu thập human feedback, rồi kết hợp ba cái đó để retrain cái model thêm lần nữa. Càng ngày, cái model sẽ càng ok hơn với những gì mà mình muốn. Đó là cái flow mà em thấy bên ngoài, trong production, người ta hay dùng. \n\n**[18:24]** Trong quá trình fine-tuning, ,ọi người sẽ thường nghe tới khái niệm gọi là catastrophic forgetting. Nghĩa là sao? Nghĩa là khi mọi người retrain kiến thức mới vào, nó sẽ làm giảm performance với những kiến thức cũ. Tại sao chuyện này xảy ra? Như em đã nói, kiến thức của một model dựa vào mấy cái weight, dựa vào kiến trúc của cái model đó và những tham số của nó. Tham số dynamic trong một model là mấy cái weight. Khi mọi người retrain, mấy cái weight này thay đổi, đúng không?\n\n**[19:00]** Khi nó thay đổi, có phải là kiến thức cũ sẽ bị giảm bớt độ chính xác đi không? Nếu trong dataset của mọi người có nhiều dữ liệu bị overfitting, nghĩa là dataset của mọi người quá đúng, quá đúng trong cái dataset đó. Khi một người quăng cái gì mới vào, nó sẽ sai với những cái cũ đi. Người ta gọi đó là overfitting, nghĩa là nó bị fold in quá mức vào những cái training data. Khi gặp data mới, nó sẽ giảm performance. \n\n**[19:42]** Nên lúc này, bên ngoài người ta sử dụng một kỹ thuật gọi là parameter-efficient fine-tuning, gọi là PEFT. Nó có nhiều cách, nhiều kỹ thuật trong method này, như LoRA này kia. Nhưng trung quy, đa số bọn họ không phải update hết tất cả các weight trong cái model đó. Bọn họ sẽ chỉ đóng băng những layer nào không cần thiết. Họ sẽ đóng băng mấy cái layer không cần thiết, rồi chỉ update một số lượng nhất định các weight thôi. Để tránh trường hợp kiến thức cũ bị mất đi quá nhiều. Đó là cái cơ bản. Còn sâu hơn về mấy cái algorithm đằng sau, mọi người có thể tự tìm hiểu. \n\n**[20:27]** Quay lại câu hỏi lúc ban đầu, ha, nó với retraining khác nhau thế nào, nên dùng cái nào? Có cái bảng đây, mọi người có thể dễ dàng nhận ra. Retraining là dữ liệu phụ thuộc vào database của mọi người. Cứ quăng vào, quăng vào, lúc nào data cũng được update liên tục. Còn fine-tuning là mọi người retrain lại model, nên lúc nào data cũng chỉ ở cái chỗ mà mọi người đã retrain thôi. \n\n**[21:16]** Kế tiếp là customize and learning style. Nghĩa là cái retraining, mục đích của nó là cho mình một cái knowledge base để mình lấy mấy cái knowledge base đó ra tham chiếu, sử dụng. Còn fine-tuning thì sao? Nó upgrade cái não của model lên, để nó có sẵn cái knowledge đó luôn. Còn mấy cái ở dưới thì chắc mọi người tự tìm hiểu tiếp ha.\n\n **[21:57]** Em có một cái ví dụ như vậy. Ví dụ như là mọi người muốn làm một cái system để giải thích những cái note của bác sĩ, đúng không? Những cái note của bác sĩ, mọi người có thể biết là những cái note của bác sĩ nó có rất là nhiều từ chuyên ngành. Và những từ chuyên ngành đó nó còn viết tắt, viết kiểu luộm thuộm nữa. \n\n**[22:44]** Nếu mọi người sử dụng fine-tuning á, mọi người sẽ cho nó học hết tất cả những cái kiến thức luộm thuộm, những cái shorthand, những cái handwriting đó của bác sĩ. Nên khi mọi người input một cái note của bác sĩ vô, nó sẽ trả lời được rất đúng. Còn nếu mọi người dùng retraining á, khi mọi người input một cái note của bác sĩ vô, nó sẽ kiếm được những cái relevant data, mang ra đọc. Nhưng bản chất là cái model nó không hiểu được những từ đó, nên nó cũng sẽ không đưa cho mọi người một câu trả lời chính xác.\n\n**[23:16]** Mọi người có thể hiểu như này: fine-tuning là mình nhờ một bác sĩ đọc một cái note của bác sĩ. Còn retraining là mọi người đưa cho một người có kiến thức rất rộng đọc một cái note của bác sĩ. Người đó có thể kiến thức rất rộng, nhưng về mấy cái chuyên ngành, mấy cái chuyên ngành thật sự, thì nó không đủ sâu như của một bác sĩ thực thụ. Nên độ chính xác sẽ không cao.\n\n**[23:57]** Thứ hai, mọi người có thể nói, bây giờ với retraining, mình dùng một cái system prompt để list hết mấy cái shorthand của bác sĩ ra trong system prompt, nó sẽ tự hiểu thôi. Nhưng làm vậy, mọi người sẽ bị tốn token, đúng không? Tại vì khi mọi người dùng retraining, mọi người lấy hết cái retraining data ra, quăng một cái knowledge retraining vô, lại cộng thêm đống cái zero-shot, mấy cái description, mấy cái đi kèm theo nó trong một cái prompt á, thì nó rất tốn token.\n\n**[24:34]** Và khi ở trong một cái long conversation với một cái model, nó đâu phải chỉ dựa vào câu hỏi của mình đâu. Nó sẽ dựa vào tất cả các cuộc trò chuyện từ trước tới giờ của mình mà nó trả lời cho mình. Lúc này, nó sẽ dẫn tới trường hợp là nó bị limit bởi token. Đó là cái drawback khi sử dụng retraining, là nó sẽ tốn token. Tại vì mọi người cần token để chạy cái system prompt của mọi người nữa. Còn fine-tuning, bản chất là model nó đã có kiến thức rồi, nên không cần phải có system prompt.\n\n**[25:14]** Đó là sơ qua về fine-tuning. Chắc có cái demo cho mọi người xem sẽ rõ hơn ha. Bây giờ em sẽ fine-tune một cái model là Duty 40 Mini ha. Em có một cái dataset như này. Ừ, như này thì mỗi thứ nó sẽ có một cái system như retraining, rồi user hỏi cái này thì muốn nó trả lời vậy, đúng không? Em cộng 10 cái, 10 record trong cái dataset này, em sẽ fine-tune nó.\n\n**[26:13]** Trước khi fine-tune, em sẽ cho nó chạy qua một đoạn code để em estimate được. Tại vì em dùng Open AI, nên sẽ tốn tiền. Nên mình sẽ tính được estimate là nó sẽ charge mình bao nhiêu. Em dùng xong, khúc cuối nó sẽ kiểu, tầm khoảng 4800, sắp xỉ 4800 token. Cái này chỉ là tham khảo thôi, nhưng em thấy nó cũng đúng. Sau đó, em sẽ upload cái file data này lên Open AI. Nó sẽ cho em cái file ở trên cái Open AI của em.\n\n**[27:03]** Rồi em sẽ training nó. Em sẽ tạo một cái fine-tuning job. Lúc này, ở trên Open AI, nó sẽ chạy một cái job này. Mọi người có thể lên đây, mọi người đọc, mọi người quan sát. Nó sẽ không trả kết quả liền, nó sẽ tạo một cái job để pending ra đó, để trên Open AI nó fine-tune cho mình. Trong lúc chờ, mình có thể theo dõi quá trình của nó như thế nào. Sau khi xong đâu rồi, nó sẽ thông báo cho mình. Mình cứ stamp cái câu này, cứ check cái câu này để coi nó đã hoàn thành hay chưa. Mình đọc ở cái chỗ đó.\n\n**[27:58]** Sau khi xong, nó sẽ cho mình mấy cái result status. Sau khi fine-tune xong, với cùng một câu hỏi, ví dụ đây là cái câu hỏi em sử dụng, em dùng câu hỏi này. Cái câu hỏi này gần giống với một cái record trong đống dataset của em. Sau khi em chạy, nó sẽ trả lời như vậy. Nhưng trước khi fine-tune, em dùng một cái model bình thường nha, model bình thường thì nó sẽ trả lời kiểu vậy.\n\n**[28:49]** Có nghĩa là em fine-tune thì nó đã thành công. Đó là cái cách em sử dụng Open AI để fine-tune một cái model. Demo của em tới đây thôi. Anh em có câu hỏi gì không? Đúng rồi, cái này demo em xài tuning chứ để tự fine-tune bằng local mà xịn xịn thì chắc không đủ đồ. Dạ, đồ ngon nhõ thôi. Thực ra có mấy cái model trước, tô nó trên LoRA các thứ, cũng có thể demo được. Nhưng tô không, bài này easy, bài này kiểu một...\n\n**[29:47]** Lẻ tẻ trầm mấy á. Đúng, chắc cũng ổn mà. Nói chung, những đội enterprise hay không muốn tốn thời gian xây dựng GPU thì sẽ dùng cách này. Diagram GPT hồi trước, GPT-4o Mini ra thì fine-tuning đã miễn phí, dùng cái này cũng tiện lợi cho họ. Cái cửa hàng demo cho anh em là sử dụng một cái như kiểu service ấy. Open AI cung cấp service fine-tuning, đưa lên mấy cái model của nó luôn. Mình pick mấy cái model, chắc là pick model mini á. Chắc chi phí nó không cao lắm.\n\n**[30:37]** Đấy cũng là một cách. Nhưng vấn đề thực ra là mình vẫn không phải người own cái model đấy. Bản chất là vẫn host ở trên server của họ. Còn có một cách khác là tự build server và tự running. Trường hợp hôm nay đã khác. Anh em xem, hôm qua em có thử một cái model có 3 billion parameter thôi. Nhưng nó chạy hai ba tiếng, nó chưa xong đâu anh. Thực ra bài này, cái version nó đơn giản hơn một cái bây giờ, nhỏ hơn của bài trước. đầu.\n\n**[31:26]** Nó là một cái full flow liên quan đến gì ta, reinforcement feedback. Ý là cái em giới thiệu ở bên ngoài production á, là khi người ta tuning á. Người ta không phải chỉ fine-tune xong là dùng liền, người ta phải đánh giá lại coi nó có đúng không. Người ta phải cho nó vô cái cycle để càng cải tiến cái fine-tuned model nữa, kiểu vậy. Đây là một cái flow như vậy. Bản chất nó cũng model thôi, đâu có gì đâu. Quan trọng là mọi người biết được những cái cost để đánh giá cái approach thôi.\n\n**[32:05]** Tại ra nó vẫn là bài toán accuracy, đúng không? Mình chọn cách nào để làm cái output nó chính xác hơn. Những cái method như retraining hay fine-tuning, nó sẽ có những nhược điểm khác nhau. Và thực ra kể cả fine-tuning, nó cũng có nhiều method fine-tuning khác nhau. Chắc là cần đi sâu hơn để xác định mấy cái đó. Cái này vẫn hơi general. Chắc vậy, Hoàng. Nếu có điều kiện thì chắc đi sâu hơn tí nữa.\n\n**[32:48]** Sâu hơn theo kiểu là có mấy cái method liên quan đến phần retraining các thứ. Sử dụng fine-tuning method á, có một số cái method nó tương đối tiết kiệm về mặt tài nguyên. Tất nhiên, nó sẽ đánh đổi với một số thứ khác, kiểu vậy. Giới thiệu cái đó để anh em xem thử đâu đó. Mọi người hỏi, Đạt hỏi là khi nào cần fine-tuning. Nói là fine-tuning cần khi mà mọi người muốn nó có một cái kiến thức, một cái specific topic nào đó. Mọi người có thể cân nhắc sử dụng fine-tuning.\n\n**[33:39]** Nhưng trong tất cả trường hợp, em thấy bên ngoài, đa số mọi người sẽ prefer dùng retraining. Tại vì nó dễ và tốn ít tài nguyên hơn. Nhưng một số trường hợp như lúc này, cái ví dụ em nói về cái note của bác sĩ á, suppose là nên dùng tuning. Rồi tùy cái kiến trúc, tùy cái mình chia system của mình ra nhiều system nhỏ, system nhỏ nó như thế nào nữa, tùy. Có thể có một vài cái use case như kiểu chúng nó muốn host mấy cái model bé bé, model bé chẳng hạn.\n\n**[34:18]** Chỉ kiểu dành để làm một cái task cụ thể thôi. Ví dụ như phân tích thời tiết, độ ẩm các thứ để perform cái action nào đấy. Ví dụ như thay đổi cái theme của điện thoại hay để chỉ action nào đấy chẳng hạn. Có thể retrain cái model bé bé để chỉ cần làm chuyện đó thôi, không cần phải cần network các thứ gì cả. Chắc vậy. Từ giờ chắc là Biên ha? \n\n**[36:13]** Dụng cái và build cái recovery process cho nó. Chi tiết như nào thì nó sẽ có một vài phần chính. Trước tiên là cái lý do mà mình cần cái kỹ thuật này và so sánh nó với một cái quen thuộc hơn là backup. Sau đó là đi vào việc để mình build và những cái mình cần để ý, những cái gì. Đầu tiên là trên thực tế, thường có những tổ chức, những công ty mà chạy những cái app với lưu lượng dữ liệu cao á. Ví dụ như giao dịch chứng khoán này nọ. Như em ví dụ này là kiểu 50.000 transaction.\n\n**[37:12]** Như em ví dụ này là kiểu 50.000 mỗi ngày là ít á, kiểu vọt lên 500.000, triệu transaction mỗi ngày. Sau khoảng thời gian, cái lượng data nó sẽ phồng lên rất rất lớn, ảnh hưởng đến cái việc mà mình query data và ảnh hưởng đến cái trải nghiệm người dùng. Trong những cái data đó, sẽ có những cái data mà dùng rồi thì nó rất ít được access lại nữa. Ví dụ như lịch sử trên 7 năm trước chẳng hạn. Nó sẽ dẫn đến một cái vấn đề, làm sao để mình giải quyết cái đống data đó. Nên mình mới dùng cái kỹ thuật là data archiving.\n\n**[38:14]** Nó sẽ có những cái lợi ích để counter lại những chuyện bên trên. Đầu tiên là cái data mà mình sử dụng, mà nó set liên tục á, query ghi đọc liên tục á, thì nó thường tốn chi phí cao. Mình sẽ dùng cái kỹ thuật này, mình sẽ đem data của mình bỏ qua một cái chỗ khác, chi phí rẻ hơn, access ít hơn. Từ đó, nó sẽ làm tăng được cái performance của app của mình trong việc query hay aggregate data các thứ.\n\n**[39:07]** Về mặt pháp lý hay reusable, những cái data đó nó sẽ được bảo vệ an toàn, không bị ảnh hưởng bởi những yếu tố bên ngoài. Để sau này khi mình dùng lại, mình có thể lấy ra dùng được. Như mọi người hay nói, mọi người sẽ liên tưởng đến cái data backup, thường dùng trong việc restore data, restore cái system hay app nếu có lỗi xảy ra. Mà hai thằng này, nó sẽ khác nhau ở chỗ là data backup á, nó sẽ dùng cho cái việc hotfix cái system nhiều hơn. Còn cái thằng archiving...\n\n**[39:59]** Data archiving thì nó hướng về cái việc lưu trữ data một cách lâu dài. Nó sẽ có cái chi tiết so sánh như này. Để mình đi build một cái architecture, một cái system để archive data, xong rồi dùng nó để recovery lúc mình cần thì sẽ làm như sau. Mọi người thấy, nó sẽ có ba cái note chính. Thứ nhất là mình lưu data lại, mình dùng metadata để interact với những cái data đó, rồi mình bỏ lên một chỗ, ví dụ như những cái cloud-based service, cloud storage service, để mình lưu trữ cái data đó.\n\n**[40:51]** Về chi tiết, để lưu trữ cái dữ liệu á, đầu tiên mình phải xác định những cái dữ liệu cần được lưu trữ. Phải phân tích xem dữ liệu nào hay được sử dụng, dữ liệu nào không được sử dụng, ít được truy cập. Sẽ có nhiều công cụ để mình làm những cái đó. Ví dụ như phân tích từ business requirement, hoặc từ các công cụ phân tích, mấy cái công cụ phân tích á. Từ đó, mình mới biết cái data nào là cần, cái nào có thể đem đi archive lại.\n\n**[42:05]** Sau đó, mình sẽ gói nó lại, dùng một vài biện pháp như vector hóa nó, encode nó, rồi dùng checksum các thứ để đảm bảo cái data nó sẽ đúng. Sau này, khi mình sử dụng lại, mình truy cập lại một cách nhanh chóng. Tại vì những cái database này, nó gói lại ở một cái storage khác với cái mình hay set, nên mình cần phải lưu lại cái metadata của nó. Ví dụ như lưu theo tháng ha, hoặc lưu theo account, để sau mình query lại thì dễ hơn.\n\n**[43:07]** Sau khi archive xong, mình muốn sử dụng lại á, thì vừa đây là cái ví dụ em để recovery. Mình sẽ tận dụng những cái metadata lúc nãy, mình search lại những cái block data mà mình cần, rồi đưa về cái môi trường tính toán lại nó khi cần thiết. Cái này nó có lợi ích là khi mình làm những chuyện này, nó sẽ không tác động đến cái data production của cái ứng dụng đang chạy. Mình có thể làm song song được. Mình muốn làm gì với nó thì làm, không chọc ngoáy vào trong cái production, sẽ đảm bảo an toàn được.\n\n**[43:51]** Cho cái trải nghiệm người dùng, như sản phẩm của mình đó. Nói đến đây, có một vài cái practice cho việc sử dụng, xây dựng cái hệ thống này. Nó cũng đơn giản lắm nhỉ. Mình sẽ phải review lại những cái policy mà mình đặt ra để cái hệ thống này chạy, xem data nó có trọn vẹn hay không. Mình sẽ automation những cái step của cái process này. Hiện tại cũng có nhiều tool hỗ trợ mình rồi, ví dụ như AWS, hay Google, đều có những cái như...\n\n**[45:14]** Google Cloud chẳng hạn. Mình chỉ cần viết những cái đơn giản để đẩy lên trên đó thôi. Và mình không thể thiếu cái monitoring để xem data này có hoạt động tốt hay không. Xong rồi, có những kỹ thuật khác như checksum này nọ, để đảm bảo data của mình luôn trọn vẹn. Khi mình cần, cũng sẽ có những chiến lược như schedule trước cái data. Tại vì những cái data này nó tồn tại lâu, nó cũng sẽ lớn, cũng sẽ phồng lên trên cái storage, cái cloud storage mà mình dùng để lưu trữ nó.\n\n**[46:05]** Nên sẽ có những chiến lược như khi nào cần thì phải schedule trước, bao nhiêu thời gian đó để nó replicate data cho mình chẳng hạn. Kế của em chỉ như vậy thôi. Lý thuyết kiểu để giải quyết cái mục đích cuối cùng là nói mọi người về việc giải quyết những cái data tồn động lâu dài, nhưng không sử dụng đến nhiều trong cái hệ thống mà mình build thôi. Ví dụ như bên ngân hàng chẳng hạn, sẽ có kiểu user trade, trade của user nó lên đến cả trăm triệu record chẳng hạn.\n\n**[46:45]** Sau này, nó sẽ lên nữa. Tức là query những cái data gần thôi, nhưng nó cũng rất tốn thời gian, kiểu vậy. Đó là những cái mà em nói hôm nay, hết. Mọi người có hỏi gì không? Khi mà store data, zip data, là mình sẽ zip một cái đoạn fragment trong quá khứ mà nó không sử dụng data đấy cho mục đích hiện tại, đúng không? Dạ, đúng rồi, đúng rồi. Đồng ý, việc em sẽ phải xóa. Khi xong, em phải xóa cái đó, đúng rồi. Nên mình sẽ có những cái load lại để tính toán khi cần.\n\n**[47:41]** Nên mình mới có mấy cái kiểu để mình làm nó an toàn. Mình có hỏi kìa. Em chưa biết cái cơm của Thỏ có biết cái này không, so sánh được không? Đứng ra là Timescale, nó có cơ chế move chunk. Ví dụ là mình compression như bình thường thôi. Thêm về cái vụ là mình có hot, warm, và cold storage. Ví dụ mình backup hàng tuần thì để trên hot storage của Azure. Nếu là cũ quá, ví dụ 2, 3, 4, 5 năm, thì để trên cold storage của Azure, hoặc là Backblaze. Nó sẽ có riêng cái dịch vụ cho mình move cái chất data đó.\n\n**[48:53]** Đúng cái vị trí object hoặc block storage, mình tương tác với Timescale để đảm bảo lúc mình cần tiết kiệm tiền với data cũ. Có thể tiết kiệm được, vốn có thể query, với trade-off là mình sẽ query hơi chậm với data hơi cũ thôi. Dạ, cái em hiểu là để tùy vào cái platform mình dùng để build ha anh. Ví dụ như bên Microsoft thì cũng sẽ có những cái tùy vào thời gian của database, hoặc tùy vào tuổi thọ của data, hay dung lượng này nọ, thì sẽ có những cái level khác nhau.\n\n**[49:40]** Ví dụ nó sẽ có delay, hay bình thường vẫn access, hay delay cho những cái mà không dùng một thời gian lâu nữa. Cái đó là để mình cụ thể trên từng tool thôi. Còn chung chung, nó là anh đang cái này làm gì thì đứng ra là Timescale thì phù hợp cho cái kiểu pattern này, cho về time series. Bên phía Azure thì họ làm cho nó phù hợp với status, hơi giống như Timescale, nhưng nó kiểu giúp mình partition và shard đúng theo kiểu mình mong muốn.  \n\n**[50:39]** Mỗi một cái nó sẽ có ưu điểm, nhược điểm riêng. Với AWS thì đứng ra là với cái dịch vụ này thì phải coi chừng cái hardware cho lưu cái data này, nó có ổn định không. Ví dụ bên phía Azure cold storage thì nó dùng đĩa, đĩa gì ta, đĩa hơi khá đặc trưng. Phải dùng cái máy laser để in vào trong đó. Nên query rất nhanh, nhưng insert thì cũng hơi chậm, kiểu insert một đống cũng mất vài phút. Vì phải có một cái laser cứng để in ở trên đó, không có virtualization layer. \n\n**[51:23]** Mỗi một service và mỗi một cái kiểu tool mình dùng cho compress và lưu trữ sẽ có ưu điểm, nhược điểm riêng, theo cái platform mình subscribe. Dạ, đúng rồi. Cái này không chỉ là mấy cái tool kiểu như AWS hay Google service. Nó là kiểu mình cũng có thể cân nhắc cho cái business của mình nữa. Nên cái này kiểu chung chung thôi. Còn từng platform, nó sẽ dùng những kỹ thuật khác nhau. Mục đích chung cuối cùng là để giải quyết cái vấn đề data nó lớn lên, nhưng ảnh hưởng đến cái việc mình query, mình nó chạy thôi\n\n**[52:16]** Nhiều cách giải quyết cho câu chuyện optimize query, đúng không? Khi mà vấn đề là do data quá lớn, thì có một vài cách. Cách của biên là một cách, tức là sẽ có một phần data mình đang không xài đến, thì ta cắt đi ra, lưu đâu đấy. Về sau mà có cần đến past data thì insert lại xài sau. Còn mình để đâu đó tầm bao nhiêu phần trăm data hiện tại, đủ để xài mục đích hiện tại, query đi nó nhanh hơn. \n\n**[52:59]** Còn một số cách khác thì xài thằng tooling, có một số kiểu database hay kiểu như Timescale, thì nó sẽ optimize luôn cho chuyện query với lượng data lớn lớn. Em nghĩ là bên dưới thì nó cũng sẽ tự động kiểu nó buff lên đâu đó, nó giữ giúp mình thôi, đúng không anh? Nên mình tỉ mỉ bên dưới, mình dùng là interface thôi. Cái bên dưới thì gần gần như nhau, như các em ta. Cảm ơn biên, vậy thôi. Chắc bài cuối của An, không biết có liên quan không. Không biết còn liên quan một tí gì đến cái cộng đồng viên không.\n\n**[54:00]** Chắc có thì chắc cũng nói sơ sơ thôi, cũng không nhiều cái. Cũng gần giống như bài của Biên, nhưng use case cũng gần giống á. Nó mở rộng ra tí thôi. Tí rồi thì bài này là nói sơ về cái datalake với lại cái use case của thằng Notion. Mình nói cái datalake trước. Datalake thì chắc mọi người nghe miết rồi, xưa giờ cũng hơi lâu rồi đó. Mình nhìn lại cái quá trình phát triển của tụi datalake này, coi là mình đang đi tới đâu.\n\n**[54:54]** Thật ra từ lúc bắt đầu, hồi tầm 1980 gì đó, là thời đại của thằng database, mấy thằng database warehouse, mấy cái mà mình đang xài hiện tại á. Về table các kiểu, tạo table rồi xử lý data. Sau này, tới cái đợt tầm năm 2000 các kiểu, tụi mấy thằng big tech bắt đầu thu thập data nhiều á. Rồi nó tận dụng mấy data đó, thì mới sinh ra mấy thằng để giải quyết vấn đề lưu trữ data và xử lý data trên dữ liệu lớn. Như là mấy dữ liệu lưu theo dạng file đồ á. Mấy cái này, mấy thuật ngữ như là cái MapReduce này nè.\n\n**[55:44]** Hình như trong cái memo của mình có một bài về MapReduce. Nếu mọi người không biết thì có thể search lại, tìm đọc thử xem cái MapReduce hồi xưa nó làm cái gì. Nó là cái tiền thân của tuổi. Sau này nó tích hợp vô thôi, giờ không xài nữa, nhưng chắc là nó tích hợp sẵn hết rồi. Sau cái thời gian phát triển của thằng này, mới bắt đầu 2010, thì mới đẻ ra, trước 2010 tí, đẻ ra khái niệm về datalake, big data, cloud, là cái internal data warehouse á, trên cloud á. Nó cloud thôi.\n\n**[56:28]** Sau này, cái đợt bây giờ á, thì nó bắt đầu phát triển hơn nữa, là về cái lake và datamart. Lake chắc bản chất là kết hợp giữa mấy cái của tụi datalake và cái warehouse thôi, để rồi đặt thành cái house. Như là mấy thằng như thằng Datadog, nó đang làm sao không biết, nhưng mình chắc là đang nói về cái này hơi đi sau thời đại tí. Để tập trung vào, chắc mình coi sơ một cái data architecture chung chung trước. Cái này, bữa cái bài của Tom có đăng, cũng có một cái diagram. Nó cũng tinh gọn hơn cái này, tinh gọn hơn tí, là cũng về cái data đi qua mấy cái layer, là processing rồi mới tới thằng gì đó. \n\n**[57:20]** Cái này nó sẽ thể hiện rõ hơn tí, là trong một cái datalake, mình sẽ lưu những loại data gì. So với thằng data warehouse, mình chỉ lưu mấy thằng structured data thôi, hoặc là mấy cái như lưu table data clean hết rồi. Còn thằng datalake này thì nó raw data, nó sẽ cả structured, unstructured, semi-structured data luôn. Nó sẽ lưu dạng raw, sau đó nó mới xử lý data, transform data, rồi nó quăng qua cho cái đám bên BI analytics, hoặc là quăng vô cái warehouse khác để chứa cái data đã được process rồi á.\n\n**[58:18]** Còn cái layer mà analytics sandbox này, thì nó là một cái layer để cho tụi data scientist, hoặc mấy thằng mà cần dùng cái raw data, process data, mà nó không ảnh hưởng tới cái process chính. Bên đây á, thì nó sẽ làm việc trên cái sandbox này để xử lý data cho tụi mấy thằng đó, mấy thằng cần raw data, nhưng không ảnh hưởng trực tiếp tới cái ruồng chính. Cái giống như hồi nãy Biên có nói á, có làm á đó, là nó sẽ lấy data, rồi nó lưu ở đâu đó để sử dụng sau này, hoặc để process gì đó không biết, nhưng mà nó không muốn ảnh hưởng tới process chính của cái app, thì nó sẽ là cái đống này.\n\n**[59:09]** Ở chỗ này, mọi người thấy là mình có khái niệm là cái ETL á, là extract, transform, và load. Bên cái warehouse xưa giờ mình làm á, nó sẽ là extract, transform, và load, nó đi theo thứ tự đó luôn. Nhưng trong cái này, mình sẽ thấy rõ là cái thằng datalake á, nó sẽ là extract và load trước. Rồi sau khi nào cần á, nó bắt đầu process data, là transform. Transform sẽ đứng sau, load sẽ đứng trước. Đó là cái khác biệt giữa hai thằng.\n\n**[59:52]** Đây là chỗ so sánh khác biệt giữa thằng data warehouse và datalake thôi. Đó là dữ liệu bên warehouse, nó được clean, structured, organized thành cái table. Còn thằng này thì nó lưu dạng file, raw data các thứ, semi-structured rồi đó, CSV hoặc mấy cái JSON. Cái process nó cũng sẽ khác nhau giữa thằng lake và lake này. Truy vấn thì thằng warehouse sẽ truy vấn bằng SQL, còn kia thì xử lý trực tiếp trên cái dữ liệu luôn. Mấy thằng hỗ trợ xử lý trực tiếp dữ liệu, như thằng Spark đó, thì nó sẽ hỗ trợ mấy cái đó. Nói qua về cái thằng Notion.\n\n**[01:00:46]** Datalake thì cái use case của thằng Notion, mọi người biết là Notion mình xài cũng hơi nhiều rồi đó. Hồi xưa, nó cũng đi từ từ thôi. Mấy cái tổ chức, mấy cái block hồi xưa, nó tổ chức thì cũng kiểu data bình thường, giống như mình, là mấy cái app nhỏ nhỏ. Mấy cái block của nó bắt đầu tăng dần. Block của nó được hiểu là mấy cái gì, rồi nó sẽ bao gồm cái title trong trong đó. Nó sẽ gọi là block. Số lượng block của nó tăng lên liên tục theo ngày giờ.\n\n**[01:01:35]** Gì đó thì bắt đầu sau này, nó phình ra, nó sẽ bắt đầu sử dụng mấy cái kỹ thuật như là sharding, sharding xưa. Như nhớ có bài của Hải Vũ có xe gì đó, nó scale horizontally. Nó bắt đầu tách ra sharding này nọ, rồi mấy cái instance. Trong giai đoạn từ 2021 đến 2023, nó sẽ có 32 instance. Mỗi instance sẽ có 15 cái shard. Rồi từ 2023 trở đi á, nó bắt đầu chia lại, nó lại tăng lên. Số lượng tăng lên nữa, thì đó là 96 cái instance. Và mỗi cái instance, nó sẽ là 5 cái shard. Nhân lên tầm 400 mấy á, bốn trăm mấy.\n\n**[01:02:27]** Để mà xử lý thì lúc này, nó hơi to, đúng không? Khi mà data nó bắt đầu to lên á, nó sẽ có những nhu cầu. Sau này sẽ có những nhu cầu về cái analytics, hoặc là mấy cái về làm bên machine learning á, tập dữ liệu này nọ, mẹo mẹo rồi. Nó sẽ bắt đầu setup một cái data warehouse architecture của nó. Cái này là cái tiền thân trước khi setup cái datalake. Nó sẽ làm data warehouse để xử lý data. Cái luồng cơ bản của nó setup để thu thập data, mấy cái về thay đổi data của mấy cái block trong từng shard.\n\n**[01:03:21]** Nó sẽ sử dụng cái file transfer để nó ingest mấy cái data từ mấy shard này nè. Nó đổ về cái gì, rồi nó gộp mấy thằng đó lại thành một cái single database to. Cái này sẽ gặp khó khăn trong việc là nãy mình nói, nó đang có khoảng bốn trăm mấy cái shard, đúng không? Nó sẽ gặp khó khăn trong việc là quản lý bốn trăm mấy connection thằng này. Xong rồi mấy cái khó khăn trong việc scaling. Số lượng data thay đổi trong mỗi cái block của thằng Notion, nó xảy ra thường xuyên và nó rất nặng, sẽ...\n\n**[01:04:13]** Gây khó khăn trong việc đọc ghi trong cái table to này. Sau đó, nó mới bắt đầu setup một cái internal datalake của nó. Cái internal datalake này, có note là nó sẽ không thay thế thằng này hoàn toàn, mà nó chỉ sử dụng cái mới thôi. Còn cái này, nó vẫn tận dụng trong một vài tác vụ, kiểu nhẹ hơn, cho mấy cái table thay đổi data không có nặng lắm. Với lại nó cần cái gì. Còn thằng này, nó expect cái luồng này á, là nó sẽ đánh những cái data nó cần để cho những mục đích mà analytics hoặc là machine learning.\n\n**[01:05:08]** Data nó có thể chấp nhận cái độ trễ là vài tiếng, vài phút, tiếng gì đó. Nó sẽ sử dụng cái data trong đây. Cái lượng setup thì cũng đơn giản thôi. Nó sẽ sử dụng cái thằng Debezium CDC này nè. Nó là cái capture data change á, để nó watch cái thằng database này, bắn về Kafka. Sau khi nó bắn cái đống event data change về Kafka, thì có một thằng bên đây là Hudi hay gì đó, nó lấy event đó, nó quăng về thằng S3. Rồi bắt đầu từ thằng này, thằng nào muốn sử dụng thì vô đây, nó lấy về, nó setup tiếp, xài data warehouse hoặc xài mấy cái chủ đích về shard gì đó, thì vô đây nó lấy, nó xài.\n\n**[01:05:51]** Cái đó là cái thật ra, cái case Notion. Chắc là có thể xài thằng này thử. Vì nó cũng là cái thằng đứng ở ngoài, nó watch vô cái đống đó. Nếu mà xài AWS hay retraining á, sẽ xài cái một là cái thằng Redshift hay gì quên rồi. Nó sẽ watch thằng đó, những thay đổi trên cái database, xong rồi nó sẽ lưu hết về trong một cái bucket hay cái gì đó. Xong rồi từ đó, mình bắt đầu xử lý sau. Cái luồng bên này là có thể sử dụng cái này. Hồi nãy setup một cái demo, nhưng mà có vẻ hơi fail rồi.\n\n**[01:06:51]** Tại vì nó chưa có được cái thằng server, nên là nó fail. Để sau đi, rồi chắc chỉ có như đó. Với lại có cái kiểu góc nhìn đó, là cái process này nè. Là cái process mà chắc tụi enterprise, nó sẽ có thể áp dụng. Nó là process kiểu chung chung mà đa số tụi enterprise sau này, em nghĩ là nó có thể. Nhu cầu của nó khi mà cái data lớn lên á, thì cái nhu cầu của nó cũng sẽ đi theo hướng này thôi. Đó là nó cần data, thu thập data để làm cái gì đó, và không ảnh hưởng tới cái luồng chính.\n\n**[01:07:52]** Mình thì xưa giờ toàn focus vào cái việc làm việc với mấy cái model AI. Nhưng mà mình nghĩ là sau này, mình cũng cần cái skill set gì đó để mình biết cách xử lý những data như thế này, tụi mà nó data lớn hơn kiểu vậy. Cho xin lỗi cái, anh nào đây? Anh đang nhìn nhận cái process này, thì nó khác gì với chuyện là mình replicate cái database của mình ra một instance khác để phục vụ chuyện retraining ấy anh? Là tại vì ở đây, đứng ra là ý ở đây, thật ra là kiểu em đang sinh giống như kiểu sinh data sang một...\n\n**[01:08:54]** Cái shard khác, đúng không? Data warehouse, đúng không? Và sử dụng upload kit process cho những cái tác vụ mà nó không, kiểu mình làm mình làm async được ấy, chứ không cần phải trực tiếp trên nguồn data chính. Câu hỏi là đối với cả mấy cái model dạng như sharding hay sử dụng master-slave ấy, thì sao không theo kiểu cứ duplicate cái database của mình ra thôi? Duplicate data thì nó vẫn chỉ là một cái data warehouse ở dưới dạng table ha. Còn thật ra cái này, nó chỉ là cái process, nghĩa là một process cho database thôi\n\n**[01:09:40]**\n\n. Nó có thể có những cái event khác. Như là ví dụ, mình sẽ có nhiều cái external data, không hẳn là mình chỉ có một cái battery, database không. Ví dụ mình có mấy cái capture như là từ social media, hoặc là mấy cái tụm lum la nào đó, chả biết. Nhưng mà nó có thể là nhiều loại data khác nhau, gom về, quăng qua thằng này. Thằng Hudi bạn này, nó sẽ là thằng chịu trách nhiệm xử lý cái raw data đó, để nó quăng vào cái thằng S3 này. Nó lưu...\n\n**[01:10:23]**\n\nMọi thứ dưới cái đống này. Nó vô luôn, mọi thứ về dạng file gì đó, gom hết vô đây, để bắt đầu sau này, mấy thằng ngoài sao n mới có cái slot để xử lý. Thật ra tụi nó cũng có một câu hỏi là tại sao không dùng mấy thằng database như MySQL hay PostgreSQL á? Nó sẽ có mấy cái... Tại sao phải sử dụng cái thằng capture data change mà không sử dụng mấy thằng đó? Mấy thằng đó, nó có cơ chế để streaming mấy cái event change của nó luôn. Mấy thằng đó, event stream, nó thường sẽ stream trực tiếp từ database này qua database khác.\n\n**[01:11:09]**\n\nCòn thằng này, nó sẽ là capture cái event đó và đưa đâu cũng được. Vì là nếu mình không có thằng Kafka này ở đây á, thì mình cần một service nào đó, mình cần cái real-time data xử lý liền luôn á, mình không cần phải vô Kafka. Cái thằng CDC này vẫn có thể bypass qua đó được, kiểu kiểu vậy, chứ không hẳn là từ database sang database kiểu như vậy. Ta cũng có nhìn ý là kiểu, thấy là nếu mà theo góc nhìn về operation chẳng hạn ấy. Tất nhiên nếu mà có multiple datasource và sử dụng những cái partition tool các thứ, nó khác nhau ấy.\n\n**[01:11:50]**\n\nDatabase khác nhau, thì cái này cũng sẽ cả, thật ra là gom nó lại vào một cái datalake, sao cho một số cái tác vụ, nó cụ thể thôi ấy. Thực ra là một số team, như kiểu team AI hay team về mặt làm report, hay data, thì người ta cũng sẽ chỉ cần work trên cái data warehouse này thôi, kiểu vậy. Hoặc là có extend cho bên nào khác nữa, thì cũng sẽ make sense, phân vùng data riêng cho từng cái team riêng, đúng không? Có họ thêm cái vụ mà cái button, cái button ETL bên database bình thường với cả bên datalake, thì nó sẽ là ELT, đúng không?\n\n**[01:12:39]**\n\nĐúng rồi, ELT, anh sẽ hiểu là mình extract, nghĩa là mình tìm đúng file, đúng không? Mình load cái file đấy lên đây, và transform nó thành dạng kiểu structured data ha. Ý là nó sẽ transform, nó chỉ là cái action, nó xảy ra ở sau khi mình có raw data rồi. Còn ETL, nghĩa là extract là sẽ lấy data từ cái đám data source á. Xong rồi nó sẽ có cái quá trình log thẳng vào cái raw data, thẳng vào mấy cái gì đó của mình. Nó gọi là cái raw landing, cái layer raw landing. Xong rồi mình mới có cái gọi là transform. \n\n**[01:13:34]** Sau đó, sau cái landing sẽ có transform để xử lý data, thì nó sẽ ra sau. Còn cái thằng kia là nó extract xong, rồi transform, nó mới quăng thẳng vào cái warehouse, đó là cái database của mình. Hay anh em có hỏi gì không? Rồi, cảm ơn An, đúng rồi. Anh em nhé, rồi bye anh em, mỗi tuần vui vẻ.\n\n---\n\n### English transcript\n\n**[00:00]** It seems like the all-hands meeting is next week. I don’t see anyone creating any events, so this week will probably just be normal, right? Guys, check if there’s any issue with screen sharing. Should we just start? Today, I think we’ll have about three presentations. Phát just said he doesn’t have anything new this week, so he’ll probably skip today. Guys, try sharing your personal screens and see how it goes. Can we preview it first?\n\n**[10:42]** According to the schedule, it’s probably you, right, bro? Let me go first then. This fine-tuning topic, it’s not really that new. This presentation is just 100.5, not 101, so it’s only a brief intro, not going deep into it yet. Honestly, I’m pretty clueless about it too, so I’ll just give a quick overview. Today, I’ll present about fine-tuning. Here’s the agenda for this session.\n\n**[12:24]** The introduction is, if you guys use AI, you’ve probably heard of the concept of fine-tuning. AI has these models that are mostly fitted to data from some specific day, with stuff like data privacy or data from a particular domain. That data doesn’t show up in the knowledge of the foundation model. To get that knowledge into the model, people usually use retraining, right? But there’s another way called fine-tuning. At the end of this, I’ll compare these two methods, looking at when to use which one and when not to.\n\n**[13:08]** For now, just think of fine-tuning as a way to expand the knowledge of a foundation model. What is fine-tuning? Simply put, you retrain the model. You take a foundation model, feed it a dataset to fine-tune it, meaning you retrain it. After fine-tuning, you get an adjusted model, called a fine-tuned model.\n\n**[13:44]** Why does fine-tuning bring in new knowledge? Simply put, in an AI model, knowledge is stored through neural networks. Fine-tuning updates the weights, the parameters of that neural network, to fit the new knowledge. When you throw new knowledge in, those weights change, and at that point, the model has updated its knowledge.\n\n**[14:19]** When you throw new knowledge in, the weights change, and at that point, the model has updated its knowledge. When fine-tuning a model in practice, it’s not just about throwing a dataset in and retraining it. Sure, you get a fine-tuned model, but you don’t know if that retrained model is any good. I’ll introduce a workflow that people outside commonly use for fine-tuning.\n\n**[14:59]** This flow, it’s got several steps like this. You can split it into two clusters, two clusters, alright? This flow, I’ll divide it into two clusters. I’ll go through the first cluster first. The first cluster is the one on the left, simply understood as a base model to start with. Then, you have a new dataset, something new, you throw it in, fine-tune it. Then it produces a model, and you supervise it, meaning you retrain it in a way that’s like retraining it.\n\n**[15:38]** It’ll add more knowledge, with this input, the output will come out like this. It results in something called supervised fine-tuning. After that, let me move to the next part. Alright, this fine-tuning is retraining on data, meaning you give it a pair of input-output in the dataset for it to learn. It’ll pick up that new knowledge. For this step to be perfect, the dataset has to be clean. It has to be clean, not mixed with other stuff. Meaning it has to be specific to the domain we want to train it on.\n\n**[16:31]**\n\nBack to this diagram, after you have a model that’s been retrained, you bring it to production, you use it, right? At this point, people outside use a system called human feedback. It’s like, does the response from this model satisfy you? Rate it from 1 to N stars, something like that. You’ll collect that data. It’s part of this step—you’ll gather human feedback from that retrained model of yours.\n\n**[17:06]** Based on that feedback, people call this step a bit resource-intensive. You’ll have to retrain a separate model. That model is used to evaluate how many points this response gets. Next, you move to the third step, the final one. In this step, you’ll use algorithms like reinforcement learning to combine it with the retrained model and your reward model.\n\n**[17:46]** You retrain, you fine-tune the model one more time. It’ll give you something called an optimized model. This loop keeps going, going forever. You have a retrained model, collect human feedback, then combine those three things to retrain the model again. The more you do it, the better the model gets at what we want. That’s the flow I’ve seen people use out there in production.\n\n**[18:24]** During the fine-tuning process, you’ll often hear about a concept called catastrophic forgetting. What does that mean? It means when you retrain with new knowledge, it reduces performance on the old knowledge. Why does this happen? As I said, a model’s knowledge depends on its weights, its architecture, and its parameters. The dynamic parameters in a model are those weights. When you retrain, those weights change, right?\n\n**[19:00]** When they change, doesn’t that mean the old knowledge gets less accurate? If your dataset has a lot of overfitting data, meaning your dataset is too perfect, too perfect within that dataset, then when someone throws something new in, it’ll mess up the old stuff. They call that overfitting, meaning it’s folded in too much to the training data. When it encounters new data, performance drops.\n\n**[19:42]** So at this point, people out there use a technique called parameter-efficient fine-tuning, or PEFT. It has lots of methods, techniques within this approach, like LoRA and stuff. But generally, most of them don’t update all the weights in the model. They’ll just freeze the layers that aren’t necessary. They freeze those unneeded layers and only update a certain number of weights. That’s to avoid losing too much of the old knowledge. That’s the basic idea. For deeper stuff about the algorithms behind it, you can look it up yourselves.\n\n**[20:27]** Back to the question from the start, how’s it different from retraining, and which should we use? There’s a table here, you can easily see it. Retraining depends on your database. You keep throwing stuff in, throwing stuff in, and the data gets updated constantly. But with fine-tuning, you retrain the model, so the data stays only where you retrained it.\n\n**[21:16]** Next is customize and learning style. Meaning, retraining’s purpose is to give us a knowledge base that we can pull from to reference and use. But fine-tuning? It upgrades the model’s brain, so it has that knowledge built-in already. The stuff below that, you guys can probably look into it more yourselves, yeah?\n\n**[21:57]** I’ve got an example like this. For instance, say you want to build a system to explain doctors’ notes, right? Doctors’ notes, as you might know, have tons of technical terms. And those technical terms are often abbreviated, written all sloppy too.\n\n**[22:44]** If you guys use fine-tuning, you’ll make it learn all that messy knowledge, the shorthand stuff, the handwriting stuff from doctors. So when you input a doctor’s note, it’ll give you a really accurate answer. But if you use retraining, when you input a doctor’s note, it’ll find the relevant data and pull it up to read. But the thing is, the model doesn’t actually understand those terms, so it won’t give you an accurate answer either.\n\n**[23:16]** You can think of it like this: fine-tuning is like asking a doctor to read a doctor’s note. Retraining is like giving it to someone with really broad knowledge to read a doctor’s note. That person might know a ton, but when it comes to the specialized stuff, the real technical terms, they don’t have the depth of an actual doctor. So the accuracy won’t be high.\n\n**[23:57]** Second, you might say, alright, with retraining, we can use a system prompt to list out all the doctor’s shorthand in the system prompt, and it’ll figure it out on its own. But doing that, you’ll end up using a lot of tokens, right? Because when you use retraining, you pull out all the retraining data, throw in a retraining knowledge base, plus a bunch of zero-shot stuff, descriptions, and whatever else goes with it in a prompt, that takes up a ton of tokens.\n\n**[24:34]** And when you’re in a long conversation with a model, it’s not like it only relies on your question. It bases its answers on all the conversations you’ve had with it from the start. At that point, it leads to a situation where it’s limited by tokens. That’s the drawback of using retraining—it eats up tokens. Because you need tokens to run your system prompt too. But with fine-tuning, the thing is, the model already has the knowledge, so you don’t need a system prompt.\n\n**[25:14]** That’s a quick rundown on fine-tuning. Probably having a demo for you guys to see would make it clearer, yeah? Now I’ll fine-tune a model called Duty 40 Mini. I’ve got a dataset like this. Yup, like this, each thing has a system like retraining, then the user asks this and wants it to answer like that, right? I’ve got 10 things, 10 records in this dataset, and I’ll fine-tune it.\n\n**[26:13]** Before fine-tuning, I’ll run it through a piece of code so I can estimate it. Since I’m using Open AI, it’ll cost money. So we’ll calculate an estimate of how much it’ll charge me. After I run it, at the end it’s like, around 4800, close to 4800 tokens. This is just a reference, but I think it’s pretty accurate. Then I’ll upload this data file to Open AI. It’ll give me the file up on my Open AI account.\n\n**[27:03]** Then I’ll train it. I’ll create a fine-tuning job. At this point, on Open AI, it’ll run this job. You guys can go up here, read it, check it out. It won’t give results right away, it’ll create a job and leave it pending there, so Open AI can fine-tune it for us. While waiting, we can track how the process is going. Once it’s done, it’ll notify us. We just keep stamping this sentence, checking this sentence to see if it’s finished or not. We read it from that spot.\n\n**[27:58]** Once it’s done, it’ll give us some result statuses. After fine-tuning, with the same question. For example, this is the question I used, I used this question, it’s pretty close to one of the records in my dataset pile. After I run it, it’ll answer like this. But before fine-tuning, I used a normal model, just a regular model, and it answered like that.\n\n**[28:49]** Meaning, when I fine-tuned it, it worked. That’s how I used Open AI to fine-tune a model. That’s it for my demo. Any questions, guys? Yeah, for this demo, I used tuning, but to do fine-tuning locally with something fancy, I probably don’t have the gear. Yup, just small-time gear. Actually, with some earlier models, I ran them on LoRA and stuff, and they could’ve been demoed too. But I didn’t, this one’s easy, it’s like a basic one.\n\n**[29:47]** A bit scattered and slow, huh? Yeah, it’s probably fine though. Generally, enterprise teams or those who don’t want to spend time building GPUs will use this method. Back with Diagram GPT, when GPT-4o Mini came out, fine-tuning was free, so using this was pretty convenient for them. The demo shop for you guys is using something like a service. Open AI provides a fine-tuning service, putting it right up on their models. We pick some models probably the mini ones, I guess. The cost probably isn’t too high.\n\n**[30:37]** That’s one way to do it. But the issue is, we’re still not the ones owning that model. The thing is, it’s still hosted on their server. There’s another way, like building your own server and running it yourself. Today’s case was different. Check it out, guys, yesterday I tried a model with just 3 billion parameters. But it ran for two or three hours and still wasn’t done, bro. Actually, this one, its version is simpler than what we have now, smaller than the previous one from earlier.\n\n**[31:26]** It’s a full flow related to what was it reinforcement feedback. The point is, what I introduced about production out there is when people tune stuff. They don’t just fine-tune it and use it right away, they have to evaluate it again to see if it’s right. They put it into a cycle to keep improving the fine-tuned model, something like that. This is one of those flows. At its core, it’s just a model, nothing special. The key is you guys knowing the costs to judge the approach.\n\n**[32:05]** Because it’s still about accuracy, right? We pick a method to make the output more accurate. Methods like retraining or fine-tuning they’ve got different downsides. And honestly, even with fine-tuning, there are lots of different fine-tuning methods. We’d probably need to dig deeper to figure those out. This is still kinda general. Probably so, Hoàng. If we’ve got the chance, we should dive a bit deeper.\n\n**[32:48]** Deeper in the sense of looking at methods related to retraining and stuff. With fine-tuning methods, some of them are pretty resource-efficient. Of course, there’s a trade-off with some other stuff, like that. I’m introducing this so you guys can check it out somewhere. People asked—Đạt asked when we need fine-tuning. I’d say fine-tuning is needed when you want it to have knowledge on a specific topic. You can consider using fine-tuning then.\n\n**[33:39]** But in all cases, from what I’ve seen out there, most people prefer retraining. Because it’s easier and uses fewer resources. But in some cases, like right now, the example I gave about doctors’ notes, I’d suppose tuning is better. Then it depends on the architecture, how we split our system into smaller systems, what those smaller systems are like it varies. There might be some use cases where they want to host small models, tiny ones maybe.\n\n**[34:18]** Just for doing a specific task. Like analyzing weather, humidity, stuff like that, to perform some action. For example, changing your phone’s theme or triggering some action or whatever. You could retrain a small model just for that, no need for a network or anything fancy. Probably like that. From now on, it’s Biên’s turn, yeah?\n\n**[36:13]** Using it and building a recovery process for it. How it works in detail, it’s got a few main parts. First is the reason we need this technique and comparing it to something more familiar like backup. Then it’s about how we build it and the things we need to watch out for, what stuff. To start, in reality, there are often organizations, companies running apps with high data traffic. Like stock trading stuff, for example. My example here is something like 50,000 transactions.\n\n**[37:12]** My example is like 50,000 a day is low it could shoot up to 500,000, a million transactions a day. After a while, that data volume swells up huge, affecting how we query data and impacting the user experience. In that data, there’s stuff that, once used, barely gets accessed again. Like history from over 7 years ago, for instance. That leads to a problem how do we deal with that pile of data? So we use a technique called data archiving.\n\n**[38:14]** It’s got benefits to counter those issues up there. First off, the data we use, the stuff that’s constantly being set, queried, read, and written all the time, it usually costs a lot. With this technique, we take our data and move it somewhere else, somewhere cheaper with less access. That way, it boosts our app’s performance when querying or aggregating data and such.\n\n**[39:07]** In terms of legal stuff or reusability, that data will be kept safe, not affected by external factors. So later, when we need to use it again, we can pull it out and use it. As people often say, you’ll think of data backup, which is usually used to restore data, restore the system, or the app if something goes wrong. But these two things are different in that data backup is more for hotfixing the system. As for archiving data archiving focuses on storing data long-term.\n\n**[39:59]** It has a detailed comparison like this. To build an architecture, a system to archive data, and then use it for recovery when we need it, here’s how it works. You guys see, it has three main notes. First, we store the data, we use metadata to interact with that data, then we put it somewhere, like cloud-based services or cloud storage services, to keep that data stored.\n\n**[40:51]** In detail, to store the data, first we have to figure out which data needs to be stored. We need to analyze which data gets used a lot, which doesn’t get used, or gets accessed rarely. There are lots of tools to help us do that. For example, analyzing from business requirements or using analytics tools, those analytics tools. From there, we figure out which data is necessary, which can be archived.\n\n**[42:05]** After that, we’ll package it up, using a few methods like vectorizing it, encoding it, then using checksums and stuff to make sure the data stays correct. Later, when we use it again, we can access it quickly. Because these databases are packaged in a storage different from what we usually set, we need to save its metadata. For instance, store it by month, yeah, or by account, so it’s easier to query later.\n\n**[43:07]** After archiving, when we want to use it again, just now I gave an example for recovery. We’ll use that metadata from earlier, search for the data blocks we need, then bring it back to the computing environment when necessary. The benefit here is that when we do this stuff, it doesn’t mess with the production data of the running app. We can do it in parallel. Whatever we want to do with it, we do, without poking around in production, so it keeps things safe.\n\n**[43:51]** For the user experience, like our product. Speaking of this, there are a few practices for using and building this system. It’s pretty simple, right? We’ll have to review the policies we set up for this system to run, check if the data stays intact. We’ll automate the steps of this process. Nowadays, there are plenty of tools supporting us already, like AWS or Google, they’ve got stuff like...\n\n**[45:14]** Google Cloud, for example. We just need to write some simple stuff to push it up there. And we can’t skip monitoring to see if this data is working well or not. Then, there are other techniques like checksums and such, to ensure our data always stays intact. When we need it, there’ll also be strategies like scheduling the data beforehand. Because this data sticks around for a long time, it’ll grow big, it’ll swell up in the storage, the cloud storage we use to keep it.\n\n**[46:05]** So there’ll be strategies like, when we need it, we have to schedule in advance, how much time it’ll take to replicate the data for us, for instance. That’s my plan, that’s it. The theory is kind of to address the ultimate goal of explaining to you guys about handling data that sticks around long-term but isn’t used much in the system we build. Like in banking, for example, there’s stuff like user trades, user trades hitting hundreds of millions of records or something.\n\n**[46:45]** Later on, it’ll grow even more. Meaning querying just the recent data, but it still takes a ton of time, something like that. That’s what I talked about today, done. Any questions, guys? When we store data, zip data, it’s like we zip up a fragment from the past that doesn’t use that data for current purposes, right? Yup, exactly, exactly. Agreed, I’ll have to delete it. Once it’s done, I’ve got to delete that, yeah. So we’ll have ways to reload it for calculations when needed.\n\n**[47:41]** That’s why we’ve got these methods to keep it safe. I’ve got a question over here. I don’t know if Thỏ’s crew knows about this, can we compare it? Standing out is Timescale, it’s got a chunk-moving mechanism. For example, we compress it like normal. Plus, there’s this thing about having hot, warm, and cold storage. Like, if we back up weekly, it goes on Azure’s hot storage. If it’s too old, say 2, 3, 4, 5 years, then it’s on Azure’s cold storage or Backblaze. It’s got a separate service for us to move that data stuff.\n\n**[48:53]** Right to the object or block storage spot, we interact with Timescale to make sure we save money with old data when we need to. It can save costs, it can still query, with the trade-off being that querying is a bit slow with older data. Yup, what I get is it depends on the platform we use to build, right, bro? For example, with Microsoft, it’ll depend on the database’s timing or the data’s lifespan or capacity and stuff, so there’ll be different levels.\n\n**[49:40]** For example, there’ll be delays, or normal access still, or delays for stuff that hasn’t been used in a long time. That’s for us to specify on each tool. But generally, it’s like, bro, what’s this doing? Standing out is Timescale, it fits this kind of pattern, for time series stuff. On Azure’s side, they make it fit the status, kinda like Timescale, but it helps us partition and shard the way we want.\n\n**[50:39]** Each one has its own pros and cons. With AWS, standing out is that with this service, you’ve got to watch the hardware storing this data, whether it’s stable or not. For example, Azure’s cold storage uses disks, what kind of disks, some pretty unique ones. They’ve got to use a laser machine to burn it in there. So querying is super fast, but inserting is kinda slow, like inserting a bunch takes a few minutes. Because it needs a hard laser to burn it on there, no virtualization layer.\n\n**[51:23]** Each service and each type of tool we use for compressing and storing has its own pros and cons, depending on the platform we subscribe to. Yup, exactly. This isn’t just about tools like AWS or Google services. It’s like we can also weigh it for our business too. So this is kinda general. Each platform uses different techniques. The ultimate common goal is to tackle the issue of data growing big but affecting how we query, how it runs.\n\n**[52:16]** Lots of ways to solve the query optimization problem, right? When the issue is that the data’s too big, there are a few approaches. Biên’s way is one approach, meaning there’s a chunk of data we’re not using, so we cut it out, store it somewhere. Later, if we need past data, we insert it back to use it. For now, we keep some percentage of current data, enough for current purposes, so querying is faster.\n\n**[52:59]** Other ways use tooling, some database types, like Timescale, optimize querying for huge data right off the bat. I think underneath, it kinda auto-buffs it somewhere, holds it for us, right, bro? So we fuss over the details underneath, we just use the interface. Underneath, it’s pretty much the same, like us kids. Thanks, Biên, that’s it. Probably An’s last piece, not sure if it’s related. Not sure if it ties a bit to the community stuff.\n\n**[54:00]** If there is, it’s probably just a quick rundown, not a lot. Pretty similar to Biên’s piece, but the use case is kinda close too. It expands a bit more. Alright, so this piece is a quick talk about datalakes and Notion’s use case. Let’s talk datalakes first. Datalakes, you guys have probably heard about them tons, been around for a while now. Let’s look back at how these datalakes evolved, see where we’re at.\n\n**[54:54]** Actually, from the start, around 1980 or something, it was the era of databases, database warehouses, the stuff we’re using now. Table stuff, creating tables and processing data. Later, around the 2000s or so, the big tech folks started collecting tons of data. They used that data, so new stuff popped up to handle storing and processing data on big datasets. Like data stored as files and such. These things, terms like MapReduce, for instance.\n\n**[55:44]** I think in my memo, there’s an article on MapReduce. If you guys don’t know, you can search it up, check it out to see what MapReduce did back then. It was the ancestor of this era. Later, it just got integrated in, not used standalone anymore, but it’s probably all built-in now. After that development phase, around 2010, it started giving birth, a bit before 2010, to concepts like datalakes, big data, cloud, internal data warehouses on the cloud. It’s just cloud stuff.\n\n**[56:28]** Now, these days, it’s evolving further, into lakes and datamarts. Lakes are probably just a mix of datalake stuff and warehouses, then turned into a house. Like Datadog or whatever they’re doing, I don’t know, but we’re probably talking about this a bit behind the times. To focus in, let’s take a quick look at a general data architecture first. This one, Tom’s piece the other day posted it, had a diagram too. It’s a bit more streamlined than this, a bit more concise, about data going through layers, processing then to some other thing.\n\n**[57:20]** This one shows it a bit clearer, about what kinds of data we store in a datalake. Compared to a data warehouse, we only store structured data, or stuff like table data that’s all cleaned up. But this datalake, it’s raw data, it’ll handle structured, unstructured, semi-structured data all together. It stores it raw, then it processes the data, transforms it, and tosses it over to the BI analytics crew or into another warehouse to hold the processed data.\n\n**[58:18]** Then there’s this analytics sandbox layer, which is a layer for data scientists or folks who need to use raw data, process data, without messing with the main process. Over here, they’ll work on this sandbox to handle data for those guys, the ones who need raw data but don’t directly affect the main flow. It’s like what Biên said earlier, doing that stuff, taking data and storing it somewhere for later use or to process something, I don’t know, but it doesn’t want to mess with the app’s main process, so it’s this pile.\n\n**[59:09]** Here, you guys see we’ve got this concept called ETL, extract, transform, and load. With warehouses, what we’ve done so far is extract, transform, and load, it follows that order straight up. But in this one, you’ll clearly see the datalake does extract and load first. Then when it’s needed, it starts processing the data, that’s transform. Transform comes after, load comes before. That’s the difference between the two.\n\n**[59:52]** This is just the spot comparing the differences between data warehouses and datalakes. With warehouses, the data is cleaned, structured, organized into tables. But this one stores it as files, raw data and stuff, semi-structured already, CSV or JSON files. The processing is different between this lake and that lake too. Querying, the warehouse uses SQL, while over there, it processes directly on the data itself. Tools that support direct data processing, like Spark, handle that stuff. Moving on to Notion.\n\n**[01:00:46]** For datalakes, the use case of Notion, you guys know we’ve been using Notion quite a bit already. Back in the day, it started slow. The organizations, the blocks from before, they were organized like normal data, just like us, small apps. Its blocks started growing gradually. Blocks are understood as what, and they’ll include the title in there. They call it a block. The number of blocks keeps increasing constantly by the day and hour.\n\n**[01:01:35]** Something like that, then later it started swelling up, and it began using techniques like sharding, old-school sharding. Like I remember Hải Vũ’s article mentioning something about it, scaling horizontally. It started splitting into sharding and stuff, then instances. From 2021 to 2023, it had 32 instances. Each instance had 15 shards. Then from 2023 onward, it started splitting again, increasing even more. The number went up again, so that’s 96 instances. And each instance had 5 shards. Multiply that, it’s around 400-something, four hundred and some.\n\n**[01:02:27]** To handle that, at this point it’s pretty big, right? When the data starts getting big, it’ll have some needs. Later on, it’ll have needs for analytics or stuff related to machine learning, datasets and tricks and all that. It started setting up its own data warehouse architecture. This was the precursor before setting up the datalake. It did a data warehouse to process data. The basic flow it set up was to collect data, stuff about data changes in the blocks across each shard.\n\n**[01:03:21]** It used file transfers to ingest the data from these shards here. It dumped it into something, then merged those things into one big single database. This ran into issues because, like I said earlier, it’s got about four hundred-something shards, right? It struggled with managing four hundred-something connections for this thing. Plus the scaling challenges. The amount of data changing in each block of Notion happens often and is super heavy, so it made reading and writing in this big table tough.\n\n**[01:04:13]** After that, it started setting up its own internal datalake. This internal datalake has a note that it won’t completely replace this one, it just uses the new stuff. The old one, it still uses for some tasks, lighter ones, for tables where data changes aren’t too heavy. And it needs something. But this one, it expects this flow to tag the data it needs for purposes like analytics or machine learning.\n\n**[01:05:08]** The data can handle a delay of a few hours, a few minutes, something like that. It’ll use the data in here. The setup amount is pretty simple. It uses this thing, Debezium CDC, you know. It’s the capture data change thing, to watch this database and shoot it over to Kafka. After it shoots that pile of event data changes to Kafka, there’s a thing over here, Hudi or something, that grabs those events and tosses them to S3. Then from this point, whoever wants to use it goes in here, grabs it, sets it up further, uses it for data warehouses or some shard-related purposes, they take it from here and use it.\n\n**[01:05:51]** That’s actually the Notion case. We could probably try using this thing. Because it’s also one of those that stands outside, watching that pile. If we use AWS or retraining, it’d use something like Redshift or whatever, I forget. It’d watch that, the changes on the database, then save it all into a bucket or something. From there, we start processing afterward. This flow here could use that. Earlier, I set up a demo, but it kinda flopped.\n\n**[01:06:51]** Because it didn’t have the server yet, so it failed. Let’s leave it for later, probably just that much for now. Plus there’s this perspective, this process here. It’s a process that enterprise folks could probably apply. It’s a kinda general process that most enterprises later on, I think, might use. Their needs, when the data grows big, will probably head in this direction anyway. It’s that they need data, collect data to do something, without messing with the main flow.\n\n**[01:07:52]** For us, so far we’ve always focused on working with AI models. But I think later on, we’ll also need some skill set to know how to handle data like this, stuff where the data’s bigger, you know. Sorry, which bro is this? You’re looking at this process, how’s it different from us replicating our database to another instance for retraining purposes, bro? Because here, standing out, the point is, it’s like I’m kinda generating data to another different shard, right?\n\n**[01:08:54]** And using an upload kit process for tasks that don’t, like, we can do async, you know, instead of needing to work directly on the main data source. The question is, for all those models like sharding or using master-slave setups, why not just duplicate our database? Duplicating data, it’s still just a data warehouse in table form, yeah. But actually this, it’s just a process, meaning a process for the database.\n\n**[01:09:40]** It could have other events. Like, for example, we’ll have lots of external data, not just one battery, a database, you know. Say we’ve got captures from social media or some random messy stuff, who knows. But it could be lots of different data types, gathered up, tossed to this thing. This Hudi buddy here, it’s the one responsible for processing that raw data, to throw it into this S3 thing. It stores everything under this pile.\n\n**[01:10:23]** It goes right in, everything in some file format, all dumped in here, so later on, the outside folks have a slot to process it. Actually, they had a question too, why not use databases like MySQL or PostgreSQL? They’ve got their own... Why use this capture data change thing instead of those? Those ones, they’ve got mechanisms to stream their event changes already. With them, event streams usually stream straight from one database to another.\n\n**[01:11:09]** But this one, it captures that event and sends it wherever. Because if we don’t have this Kafka here, we’d need some service, we’d need real-time data processed right away, without going through Kafka. This CDC thing can still bypass to there, kinda like that, not exactly from database to database like that. We also noticed something, like, it feels like, from an operations angle, for instance. Of course, if there are multiple datasources and we use partition tools and stuff, they’re different.\n\n**[01:11:50]** Different databases, so this will also, actually, bundle it into a datalake, so some tasks are specific, you know. Actually, some teams, like the AI team or the reporting team or data folks, they’d probably just need to work on this data warehouse, like that. Or if it extends to other sides too, it’d make sense, splitting data zones for each team separately, right? They added this thing about the button, the ETL button in regular databases versus datalakes, it’d be ELT, right?\n\n**[01:12:39]** Yup, ELT, you’d get it as extract, meaning we find the right file, right? We load that file up here and transform it into something like structured data, yeah. The idea is it transforms, it’s just an action, it happens after we’ve got the raw data. But ETL means extract is pulling data from the data source pile. Then it’s got a process to log straight into the raw data, straight into some stuff of ours. They call it raw landing, the raw landing layer. Then we’ve got what’s called transform.\n\n**[01:13:34]** After that, after the landing, there’s transform to process the data, so it comes out later. But the other one extracts, then transforms, then tosses it straight into the warehouse, that’s our database. Any questions, guys? Alright, thanks An, yup. See you, guys, bye, have a fun week","title":"OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion","short_title":"##37 AI Fine-tuning, Data archiving, Datalakes","description":"In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q\u0026A to boost our technical skills.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sun Dec 29 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/37-20241227.md","slugArray":["updates","ogif","37-20241227"]},{"content":"\n**Topic Highlights**\n\n- **Go Weekly #16**: Phat discussed concurrent data structures in Go, focusing on `sync.Map`. He explored its structure, use cases, and performance trade-offs in high-read, low-write scenarios. He also touched on garbage collection issues reported by the Go team.\n- **Generative AI UX Design Patterns**: Nam presented on UX design patterns for AI integration, covering System Scope Relationship, Spatial Relationship, and Functional Relationship. He explained how AI can be incorporated at various levels in digital products and discussed different ways to present AI features in user interfaces.\n- **Yelp Usecase AI**: Dat presented real-world AI use cases from Yelp, explaining how AI is used for recommendation systems, text editing, and image summarization. He explored AI applications in generating datasets, spam detection, and auto-generating short video reviews for restaurants.\n- **LLM Pattern**: Hoang introduced design patterns for integrating LLMs (Large Language Models) into applications. Key patterns included in-context learning, data preprocessing, and multi-agent collaboration, highlighting their practical use in AI-powered systems.\n- **Dify Git Analyze**: Cat demonstrated a Git repository analysis tool built using Dify. The tool scrapes content from repositories and supports diagram generation for code structure analysis, with a focus on optimizing the knowledge retrieval process in large datasets\n\n---\n\n**Vietnamese Transcript**\n\n**0:28** Chủ đề hôm nay vẫn có Go Weekly, và Nam đang thử nghiệm phần commentary về thiết kế hàng tuần. Chúng ta sẽ theo dõi trong vài tuần tới xem nội dung như thế nào.\n\n**11:19** Nam sẽ trình bày tiếp cho anh em, và sau đó sẽ có một vài bài của Hoàng, Cát, Đạt. Chúng ta đang nghiên cứu về các trường hợp sử dụng mà các công ty khác đang áp dụng, hoặc các công cụ mà dev đang sử dụng, và có thể sẽ mở một bài chia sẻ trong tuần này hoặc tuần sau. Bài hôm nay sẽ xoay quanh việc tạo một nút thiết kế UX. Trước đây, có rất nhiều câu hỏi về phạm vi mà AI đang áp dụng và vai trò của nó sẽ như thế nào – liệu nó chỉ đóng góp như một thành phần nhỏ riêng lẻ hay là cả một ứng dụng trong các sản phẩm số. Hôm nay, em sẽ giải đáp thắc mắc đó, tức là AI đang đóng vai trò như thế nào và cách thức hoạt động của nó ra sao.\n\n**12:11** Đầu tiên, em sẽ nói về \"System Scope Relationship.\" Hình ảnh này sẽ mô tả AI được tích hợp vào các hệ thống ở nhiều cấp độ khác nhau, từ một thành phần nhỏ lẻ đến một hệ sinh thái toàn diện hơn. AI có thể chỉ là một phần nhỏ trong một thành phần hoặc có thể phát triển thành các tính năng lớn hơn, giúp tự động hóa nhiều chức năng. Điều này sẽ giúp người dùng trải nghiệm ứng dụng dễ dàng hơn. AI có thể đóng vai trò trong bất kỳ phần nào của sản phẩm số – từ thành phần, luồng xử lý, tính năng cho đến toàn bộ ứng dụng, hoặc thậm chí là một nền tảng hoặc hệ sinh thái.\n\n**12:53** Ví dụ, trong một ứng dụng, AI có thể đóng vai trò một tính năng nhỏ, giúp người dùng thao tác nhanh hơn thay vì phải làm thủ công. Hoặc AI có thể là toàn bộ một ứng dụng như ChatGPT, nơi toàn bộ ứng dụng được xây dựng trên nền tảng AI, phục vụ cho một mục đích nhất định. Hoặc AI có thể là một nền tảng như Rewind AI, với nhiều tính năng hỗ trợ AI cho nhiều công việc khác nhau trong cùng một ứng dụng. Đây là phạm vi của AI trong các sản phẩm hiện nay.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=nx8a1rNN4wSuuPBo\u0026amp;start=688\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**13:39** Tiếp theo, về \"Spatial Relationship,\" phần này giúp chúng ta hiểu về cách tính năng AI được bố trí và sắp xếp trong giao diện người dùng (UI). Có nhiều cách để tích hợp AI vào thiết kế, và quan trọng là làm sao để bố trí chúng sao cho hợp lý, tối ưu trải nghiệm người dùng mà không gây rối mắt hay phức tạp giao diện. Spatial Relationship ảnh hưởng trực tiếp đến trải nghiệm người dùng. Ví dụ, AI có thể hoạt động độc lập hoặc song song với các tính năng khác, nhưng vẫn giữ không gian riêng của mình. Khi hiểu được các mối quan hệ này, chúng ta có thể chọn cách sử dụng và sắp xếp tính năng AI một cách tối ưu, không gây phân tâm cho người dùng.\n\n**15:11** Có sáu cách để trình bày tính năng AI, bao gồm:\n\n1. **Separate**: AI hoạt động độc lập.\n2. **Alongside**: AI được đặt bên cạnh các tính năng khác.\n3. **Layer**: AI hoạt động dưới dạng lớp phủ.\n4. **Integrated Parent**: AI đóng vai trò chính trong điều hướng hoặc quản lý nội dung chính.\n5. **Integrated Child**: AI đóng vai trò nhỏ hơn, bổ trợ cho tính năng chính.\n6. **Point**: AI chỉ xuất hiện như một biểu tượng nhỏ, giúp người dùng hiểu thêm về cách nó hoạt động.\n\n**16:41** Tiếp theo là \"Functional Relationship,\" phần này mô tả các mối quan hệ chức năng giữa AI và các tính năng khác trong hệ thống. AI có thể tồn tại độc lập nhưng vẫn adapt (thích nghi) với các nội dung và tính năng của hệ thống ở mức cao hơn. AI có thể tích hợp với các tính năng hiện có để cải thiện hiệu suất, thay vì người dùng phải thao tác thủ công. Khi hiểu rõ cách hoạt động chức năng của AI, chúng ta sẽ xác định rõ vai trò của nó trong ứng dụng và thiết kế để các hành động chức năng của nó không bị xung đột, cũng như không làm gián đoạn luồng sử dụng của người dùng.\n\n**17:28** Có sáu cách để mô tả mối quan hệ chức năng của AI:\n\n1. **Separate**: AI hoạt động riêng biệt.\n2. **Aware Of**: AI tách biệt nhưng có khả năng nhận biết các thay đổi trong tính năng chính.\n3. **Acting Up**: AI tương tác qua lại giữa các tính năng.\n4. **Feature Incorporate**: AI được tích hợp như một phần của một tính năng hiện có.\n5. **Usage**: AI được sử dụng theo cách mà nó tương tác với các phần khác trong ứng dụng.\n6. **Usage Conventionally**: AI tương tác hai chiều với các tính năng khác một cách trực tiếp.\n\n**18:14** Nó sẽ không ảnh hưởng trực tiếp đến tính năng chính, nhưng nó sẽ có tác động qua lại với AI và từ đó giúp cải thiện tính năng chính. Đây là một ví dụ cụ thể hơn về cách sử dụng của nó, chẳng hạn như trong code này có thể generate một panel bên phải.\n\nTiếp theo là **Acting Up**, nghĩa là hai bên sẽ có tác động qua lại, có thể trao đổi dữ liệu qua lại với nhau. Ví dụ, tính năng A có thể hiểu được dữ liệu từ tính năng B và ngược lại. Các dữ liệu này sẽ được trao đổi qua lại liên tục để cải thiện sự tương tác.\n\nTiếp theo là **Feature Incorporate**, nghĩa là AI được tích hợp trực tiếp vào các tính năng hiện có của ứng dụng. Cuối cùng là **Usage Conventionally**, nghĩa là AI sẽ tương tác theo cách thông thường với các tính năng khác, giống như cách các ứng dụng truyền thống hoạt động.\n\nVí dụ như khi bạn dùng một ứng dụng và có nhiều tính năng khác nhau, AI sẽ đóng vai trò trong các phần như feature, nhưng không phải lúc nào cũng là phần chính, mà sẽ đóng vai trò bổ trợ.\n\n**19:06** Ví dụ khác là ứng dụng Quora hay các ứng dụng khác, AI sẽ có nhiều tính năng nhỏ được tích hợp vào, như kiểu gợi ý trả lời câu hỏi, giúp người dùng thực hiện các tác vụ dễ dàng hơn. Vậy là nãy giờ em đã đi qua ba phần chính:\n\n1. **System Scope**: Giới thiệu cách AI tích hợp vào sản phẩm.\n2. **Spatial Relationship**: Giới thiệu cách sắp xếp AI trong giao diện người dùng.\n3. **Functional Relationship**: Giới thiệu các mối quan hệ chức năng giữa AI và các tính năng khác.\n\nNhững phần này giúp tối ưu hóa sản phẩm, cải thiện trải nghiệm người dùng và nâng cao hiệu quả cho ứng dụng AI.\n\n**19:57** Điều này rất quan trọng bởi vì nếu mình hiểu rõ cách áp dụng AI, tính năng mình làm sẽ mang lại nhiều giá trị hơn cho người dùng. Ví dụ mà em quên chưa nhắc đến là phần \"separate.\" Em đã đưa ra một số ví dụ, nhưng để quay lại một chút về \"separate\" – tính năng AI hoạt động độc lập. Mình có thể xem xét trường hợp Microsoft có một cái slider để generate hình ảnh song song với tính năng khác. Hoặc với một ứng dụng như Shopee, AI sẽ đóng vai trò hỗ trợ bên cạnh tính năng chính của ứng dụng.\n\n**20:53** Đó là những ví dụ minh họa cho việc sắp xếp và bố trí AI trong giao diện và sản phẩm. Anh Thành có thấy phần này như thế nào? Em thấy nó giống với các patterns thông thường trong thiết kế.\n\n**22:01** Anh Thành: Đúng rồi, những cái này là các mẫu patterns mình hay dùng trong việc thiết kế ứng dụng AI, hoặc khi tích hợp AI vào một ứng dụng hoặc sản phẩm riêng biệt. Về cơ bản, nó là những cấu trúc quen thuộc để mình hiểu rõ hơn về cách áp dụng AI. Em có thể phân loại, chia nhỏ chúng ra thành những tính năng nhỏ hơn. Phần này rất rõ ràng.\n\n**23:31** Cảm ơn Nam. Ok, tiếp theo là bài của Hoàng và Đạt nhé.\n\nHôm nay, em sẽ giới thiệu một bài gọi là \"AI Button trong các ứng dụng LLM.\" Trước khi vào bài, em sẽ nói qua về nội dung và agenda. Đầu tiên là chúng ta sẽ tìm hiểu về các design patterns liên quan đến AI Button. Những cái pattern này được áp dụng trong nhiều ứng dụng khác nhau. Em sẽ lấy ra những cái phổ biến và dễ hiểu nhất để giới thiệu cho mọi người.\n\n**24:35** Bài này sẽ xoay quanh việc sử dụng ứng dụng AI trong các sản phẩm số. Ứng dụng này tận dụng sức mạnh của các mô hình AI để giải quyết các bài toán cụ thể hoặc hỗ trợ người dùng trong các tác vụ. Khi sử dụng LLM, nhiều người có thể gặp vấn đề là mô hình không đưa ra đúng kết quả như mong đợi. Điều này là do bản chất của các mô hình này chỉ dựa trên khả năng phản hồi dựa trên chuỗi dữ liệu. Có nhiều cách để giải quyết vấn đề này. Một trong những cách tốn kém nhất là phải điều chỉnh lại toàn bộ mô hình từ đầu. Điều này có thể mất nhiều thời gian và nguồn lực.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=Jrnm_7QXsbImTctp\u0026amp;start=1435\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**25:15** Mình có một cách gọi là **in-context learning**, có nghĩa là AI có thể học trực tiếp ngay trong ngữ cảnh hiện tại khi bạn đang sử dụng nó. Đây là một kỹ thuật như là few-shot learning hoặc zero-shot learning, giúp AI tự học mà không cần phải được huấn luyện lại từ đầu. Ví dụ, bạn chỉ cần cho AI một vài ví dụ nhỏ trong ngữ cảnh và nó sẽ tự điều chỉnh cách hoạt động của mình dựa trên những gì được cung cấp. Thay vì phải retrain toàn bộ mô hình, cách này giúp tiết kiệm thời gian và tài nguyên rất nhiều, và nó vẫn đảm bảo AI có thể học từ ngữ cảnh cụ thể mà bạn cung cấp.\n\n**25:52** Với trường hợp này, **in-context learning** được sử dụng rất nhiều trong **prompt engineering**. Mọi người sẽ cung cấp các ví dụ có sẵn trực tiếp vào prompt và mô hình sẽ học từ những ví dụ đó để tạo ra các kết quả tiếp theo. Đó là ý tưởng chính của in-context learning. Về cơ bản, thiết kế sẽ hoạt động như thế này: bạn có một truy vấn, sau đó bạn xây dựng prompt với các ví dụ cần thiết và dữ liệu few-shot learning, rồi bạn đưa nó qua mô hình, mô hình sẽ trả về kết quả dựa trên các ví dụ đó. Tuy nhiên, nó không chỉ dừng lại ở các ví dụ, mà còn bao gồm rất nhiều yếu tố khác.\n\n**26:37** Nhìn rộng hơn, in-context learning liên quan đến việc cung cấp ngữ cảnh vào prompt bằng cách truyền vào các thông tin mà mô hình không có sẵn. Vì đây là một mô hình được huấn luyện trước, kiến thức của nó bị giới hạn, vì vậy bạn truyền thêm thông tin vào ngữ cảnh và prompt để mô hình học trong quá trình tạo ra kết quả. Ví dụ, trong chẩn đoán hình ảnh y khoa, mô hình có thể không có đủ kiến thức chuyên môn. Vì vậy, bạn cung cấp kiến thức đó vào ngữ cảnh và prompt để mô hình học trong quá trình tạo ra kết quả. Đó là cốt lõi của in-context learning.\n\nTiếp theo là nút thiết kế thứ hai quan trọng, được gọi là **data preprocessing/ editing**.\n\n**27:54** Phần này miêu tả quy trình chuẩn bị dữ liệu cho mô hình ngôn ngữ (LM). Như mọi người biết, LM hoạt động dựa trên các cơ sở dữ liệu vector, sử dụng so sánh vector để tìm các điểm dữ liệu tương tự. Quy trình này thường liên quan đến việc xử lý dữ liệu đa phương tiện và các loại thông tin khác nhau. Để đảm bảo đầu ra là tối ưu, việc áp dụng các bước xử lý trước dữ liệu là rất quan trọng. Ví dụ, bạn có thể xử lý trước văn bản bằng cách lọc ra các chi tiết không cần thiết để làm ngắn lại, hoặc với hình ảnh và âm thanh, bạn có thể loại bỏ nhiễu hoặc nén dữ liệu để giảm kích thước trước khi đưa qua mô hình ngôn ngữ.\n\n**29:19** Việc xử lý trước hoặc chỉnh sửa dữ liệu giúp mô hình hoạt động hiệu quả hơn. Có nhiều cách để xử lý trước, tuỳ thuộc vào loại dữ liệu hoặc ngữ cảnh. Bạn sẽ thực hiện điều này dựa trên các yêu cầu cụ thể. Nút thiết kế tiếp theo mà tôi muốn đề cập đến là một thiết kế thường được sử dụng, mặc dù có nhiều tên gọi khác nhau. Tôi gọi nó là **example agent**. Đây là một thiết kế thường thấy khi bạn muốn truy vấn của mình đi qua nhiều ngữ cảnh khác nhau. Ví dụ, nếu bạn có một ứng dụng đánh giá bài viết, bạn có thể cho bài viết đó đi qua một đường ống nơi mỗi agent đánh giá bài viết từ một góc độ khác nhau.\n\n**30:11** Một agent có thể đánh giá bài viết từ góc nhìn của một nhà văn, một agent khác có thể từ một góc nhìn khác. Sau khi đi qua tất cả các agent này, sẽ có một lớp tổng hợp cuối cùng để kết hợp hoặc xử lý các kết quả đó, và cuối cùng cung cấp cho người dùng một kết quả tổng hợp. Thiết kế này thường thấy trong các hệ thống đánh giá, nơi bạn đánh giá kết quả từ các mô hình khác nhau và chọn ra kết quả tốt nhất dựa trên các điều kiện đã được thiết lập trước.\n\n**30:55** Nút thiết kế tiếp theo, gọi là **agentic button**. Vậy agentic có nghĩa là gì? Trong ngữ cảnh của các mô hình ngôn ngữ (LMs), **agentic LMs** ám chỉ việc nâng cấp khả năng của mô hình. Vì mô hình chỉ biết những gì nằm trong dữ liệu huấn luyện của nó, chúng ta sẽ nâng cấp nó để tăng cường sức mạnh của nó và giảm thiểu sự can thiệp của con người. Thiết kế này giúp hệ thống tự động hoá nhiều hơn, cho phép nó hoạt động với ít sự can thiệp của con người hơn.\n\n**32:24** Thiết kế này có một số thành phần chính giúp bạn đạt được mức độ tự động hóa này. Có bốn thành phần chính: **reflection**, **planning**, **execution**, và **multi-collaboration**. Mỗi thành phần này đều giúp hệ thống của bạn trở nên tự động hóa hơn. Đầu tiên, chúng ta hãy nói về **reflection**. Reflection liên quan đến việc đánh giá kết quả ban đầu của mô hình dựa trên một tiêu chí hoặc một chỉ số cụ thể để xác định xem kết quả đó đã được tối ưu hóa chưa. Nếu chưa, hệ thống sẽ điều chỉnh và lặp lại quá trình này, tiếp tục tạo ra kết quả cho đến khi đạt được kết quả tối ưu.\n\n**33:06** Reflection giúp giảm thiểu sự can thiệp của con người vì thay vì tạo ra một kết quả ban đầu không đáp ứng mong đợi của bạn, hệ thống sẽ tinh chỉnh dựa trên các tiêu chí đã được thiết lập trước, cuối cùng đưa ra một kết quả chính xác hơn mà không cần điều chỉnh thủ công.\n\nReflection button này có nghĩa là nó sẽ đánh giá cái output ban đầu của một con AI, rồi nó sẽ đánh giá dựa theo một tiêu chuẩn nào đó hoặc là một cái chỉ số nào đó để xem là cái kết quả này đã tối ưu chưa. Nếu chưa tối ưu nó sẽ thêm thắt một chút và nó sẽ chạy vòng lại con AI đó để nó tạo ra kết quả khác cho tới khi nào đạt được kết quả tối ưu nó sẽ trả cho mình cái kết quả cuối cùng. cái này nó sẽ giúp giảm thiểu việc con người phải can thiệp vào quá trình làm việc, bởi vì nếu mà output đầu tiên không đúng ý mình, mình không cần phải tự chỉnh lại nữa mà nó sẽ tự tối ưu.\n\n**33:42** Button thứ hai là tool. Tool có thể là external, nó có thể là external API hoặc là những cái function mà mọi người code. Những cái tool này được sử dụng để cho model có thể lấy được những knowledge từ thế giới bên ngoài, những real-time knowledge, những external resource mà nó không được train sẵn. Như OpenAI hay là Claude đều có hỗ trợ. Khi đó, con model có thể tự biết khi nào cần gọi tool dựa vào cái description mà mọi người viết trên cái tool đó. Model sẽ tự biết cách lấy và extract thông tin từ tool, rồi trả về cho con LM để nó generate ra output.\n\n**34:30** Kế tiếp là planning. Planning button có nghĩa là mọi người cho con LM có khả năng lập kế hoạch, để tránh việc phải prompt đi prompt lại nhiều lần. Ví dụ, nếu có một task phức tạp, mình sẽ có một cái prompt lớn cho nó plan ra tất cả các step mà nó cần làm theo kiểu step by step. Cách này sẽ cho nó làm những việc nhỏ trước, rồi cuối cùng kết hợp lại thành một cái task lớn. Cái kiểu planning design này có nhiều biến thể, và đây là biến thể đơn giản nhất: lập kế hoạch xong rồi làm từng bước một.\n\n**35:10 C**uối cùng là multi-collaboration. Cái này em đã present cách đây một tháng rồi. Nói chung, nó giống như kiểu là AI giỏi việc nào làm việc đó. Mình có một cái context đúng không? mình chia nó ra, rồi đưa qua từng người. Người nào giỏi việc đó nó sẽ giải quyết việc đó, xong rồi pass qua con agent tiếp theo. Cứ thế, cuối cùng nó sẽ complete được cái requirement. Cái design này sử dụng tính chất divide and conquer khá nhiều. Chia việc lớn thành việc nhỏ, rồi đưa việc nhỏ cho người giỏi chuyên môn. Đây là một cái design button mà em thấy khá nhiều nơi bên ngoài sử dụng.\n\n**36:24** Đó là những design button mà em thấy nhiều nơi sử dụng và hiểu nhất. Em đã trình bày xong. Mọi người có câu hỏi gì không?\n\n**37:10** Hoàng, em nói lại cái phần planning, để confirm lại cái comment của anh Bảo. Nó giống như là kiểu đọc cái prompt đúng không? Nó sẽ hiểu cái prompt của anh trước, xong rồi nó sẽ chia cái prompt ra thành những cái nhỏ hơn, xong rồi nó sẽ có những con worker, có thể là những IDE worker hoặc là những cái prompt nhỏ để nó hoàn thành task đó. Đúng không?\n\n**37:40** Đúng rồi, anh có thể hiểu như vậy. Mình có thể chia prompt ra, ví dụ như là một task phức tạp, nó sẽ chia ra nhiều cái plan nhỏ. Những cái plan nhỏ này sẽ làm step by step. Ví dụ nó làm plan 1 trước, rồi làm plan 2, rồi làm plan 3. Sau khi hoàn thành tất cả các plan, nó sẽ tổng hợp lại ở một cái chỗ nào đó, hoặc là một cái component cuối cùng để nó ra được câu trả lời cuối cùng.\n\n**38:06** Ý là nó giống như cái con Zero mà hôm trước anh Tom present ấy. Con worker sẽ có thể làm một số task như đọc file, xóa file, sửa file, hay là talk với Internet, gửi email các thể loại. basically, agent các thứ như vậy.\n\n**38:52** Đúng rồi, bản chất của nó là thay vì làm một cục rất lớn để giải quyết hết cái task đó, mình phải đi prompt đi prompt lại nhiều lần để nó cho ra kết quả. Mình có một cái prompt trước, để chia nhỏ thành các task nhỏ, rồi sau đó có một cái pipeline để nó đi qua từng con worker, làm những việc nhỏ nhỏ cho mình.\n\n**39:23** Ok, kéo lên slide 14 đi Hoàng, slide 14. Anh cũng thấy là kiểu con này giống giống con Mule Automation mà Tom setup đúng không? Con Mule button mà Tom setup ấy. Em đã code xong rồi nhưng nhìn cái design này với cả cái button giống hệ nhau này.\n\n**39:46** Ừ, cái này là thằng Tpm nó chạy loop rồi, nhìn ra giống giống một tí. Nó giống planning mà anh Tom vừa nói, là nó break task ra từng phần, rồi xử lý từng phần một. Nó có iteration trong đó, giống như là nó có một list các step mà em đã mô tả ở trên. Back lại cái của em, chính là chỗ mà agent đang thấy. Cái của anh thấy nó giống planning hơn, là nó chia plan ra trước, rồi làm step by step từng plan một, đi qua mỗi vòng làm từng cái một. Còn cái này nó giống như là làm song song với nhau, nó parallel với nhau, để ra output xong rồi đánh giá lại output đó, rồi đưa ra kết quả cuối cùng. Chắc anh nhầm cái work rồi, đã correct lại.\n\n**42:28** Đúng rồi, thử đi. Nó là kiểu như vậy đó, nó chia ra thành nhiều việc khác nhau. Nó giống như là classify, nó chạy qua từng cái. Cái này giống multi-collaboration hơn, vì nó giống như question classifier, chỉ chạy một trong mấy cái này thôi. Mỗi agent làm việc đúng chuyên môn của nó, rồi combine lại.\n\n**43:33** Nhưng mà anh thấy mấy phần như reason với input analysis có đúng không? Của Tom, phần expert ấy. Riêng vụ pick domain ấy, nó có classifier ở đó, nhưng mà mấy phần reason với input analyzer là những agent khác nhau. Bên group đó là expert thôi, mình consider nó như là một group expert đúng không? Và nó combine với năm cái agent mình phía dưới.\n\n**45:33** Nếu mà làm tất cả mọi thứ trong cùng một cái prompt, em chắc chắn nó sẽ không ra được kết quả mình mong muốn đâu. Vì context quá nhiều và không có example cụ thể. Đầu tiên là accuracy chắc chắn sẽ giảm vì quá nhiều dữ liệu cùng lúc. Cái chính là phải chia ra nhiều layer, từng bước một. Thực tế mình cần output từ con LM, chứ không thể hardcode từ trước được. Mình chỉ muốn một cái prompt đơn giản nhất, để nó làm ra các câu trả lời nhỏ, rồi từ đó có một câu trả lời lớn.\n\n**46:59** Đúng rồi, khi làm nhỏ ra, mình sẽ biết vấn đề nằm ở đâu để debug. Như anh đã nói, specify kỹ, chia ra từng layer, nếu thấy sai ở đâu mình sửa ở đó. Còn nếu quăng một cục, mình sẽ không biết nó sai chỗ nào, rồi phải sửa rất nhiều lần.\n\n**48:43** Đúng rồi anh. Ví dụ như tạo một cái event trong calendar vào ngày mai, nếu không có sự kiện trong giờ đó tạo event, còn nếu có rồi thông báo. Nếu mình quăng một cục request đảm bảo nó sẽ rối ngay, vì nó phải thực hiện theo step by step. Nếu chia thành từng layer, test từng bước sẽ ổn hơn.\n\n**49:23** Nó sẽ em chắc là 99% là nó sẽ mù luôn á. Nếu mà còn nếu mình chia cái thành layer cơ, thành nhiều lớp layer á, làm test bài test nó sẽ ok hơn. Rồi, hô nào nên nữ, nên văn phòng là có Tôm ở đấy người chửi nhau. Anh không có hỏi nào chắc là cảm ơn Hoàng trước. À, đến Đạt nhé. Đạt nhờ. À, em không thị xem màn hình. Ok rồi, mọi người thấy màn hình của em chưa? Ừ, thấy rồi.\n\n**50:38** Hôm nay em nói về Yelp use cases. Từ từ Đạt, để anh giới thiệu context một chút. đợt này team mấy bạn sẽ focus vô đâu đó và đi search thử mấy cái phần use case ấy. Use case ở đây có nhiều dạng. Cái dạng mà Đạt đang sharing nó sẽ là mình xem thử các bên startup hay enterprise nó đang apply vào để giải quyết vấn đề gì. Là có thể là những cái green field, tức là những cái hoàn toàn mới. Hoặc là những cái mà nó optimize cho cái phần current workflow của chúng đó, kiểu vậy. Nó sẽ viết những use case và report lại hàng tháng, những cái phần update. Ngoài ra có một cái phần dạng use case khác nữa đó là những cái phần tuning mà để boost phần development của bên phía bên phía là tech các thứ. nó sẽ có những cái technique hay là có những cái phần editor mới, hay là mấy cái tool mới các thứ. đ cũng sẽ report cái phần đấy đâu đó trong tech. Đang testing thử trong khoảng hai tuần một đấy. đây là một cái bài đầu tiên chắc con Yelp này, nó đang dạng là con start-up phải không, chắc là. tiếp tục giới thiệu cho anh em một tí về cách mà bọn này đang apply AI là như thế nào?\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=l1ZUsZApjB78hPcD\u0026amp;start=3018\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**52:01** Yelp là cái đơn vị nó đưa ra cái software, nó cung cấp cái software cho các store, các bên mà doanh nghiệp muốn làm các đơn vị nhỏ lẻ như kiểu là giao hàng nhanh, hay là nhà hàng, rồi các bánh dụng cụ cơ bản, kiểu như vậy. Yelp này nó bán cái software cho mọi người làm việc đó. em sẽ chia sẻ chút về thằng này, nó sử dụng AI vào trong cái tooling của nó như thế nào.\n\n**53:00** trước đó chúng nó có một cái machine learning system rồi, bây giờ nó app thêm AI vào để giúp cho cái việc recommendation nó đúng hơn. bọn Yelp này nó có trên hệ thống của chúng nó, nó có nhiều cái thể loại đánh giá như kiểu đánh giá nhà hàng nó không bị tốt chẳng hạn. dựa trên những cái review đó, chúng nó có làm cái trò là text editing để so sánh được những cái kết quả mà spam hay không á. nó sẽ sử dụng AI vào trong cái việc gì. Thứ nhất là chúng nó sẽ tạo, chúng nó sử dụng AI để làm cái việc làm dataset, để train được cái model đánh giá là nó đang spam hay nó đang review tốt hay xấu như thế nào á. nó sẽ sử dụng AI để tạo ra cái dataset dựa trên LinkedIn. ở trong đây, em đọc có thấy bảo là chúng có sử dụng số tính như Zero-shot và Few-shot để làm dataset. chúng nó chỉ sử dụng một số cái model ở trên Hugging Face, rồi xong chúng nó làm classify để đánh giá được là review tốt hay xấu. đây là một cái use case cho cái việc AI dùng để làm text editing.\n\n**54:18** À, sang cái use case thứ hai của chúng nó, là chúng nó có sử dụng Clip Model. Clip Model bản chất của nó là xử lý hình ảnh. Xử lý hình ảnh có nghĩa là sao? Có nghĩa là dựa trên review, dựa trên review... đợi em chút để em kiếm nè. À, Clip á, nó sẽ xử lý hai thứ. Một là cái caption của cái ảnh, và cái ảnh nó như thế nào. qua Clip này á, nó sẽ hiểu được cái context của cái ảnh là cái gì. chúng nó sử dụng Clip vào trong những cái công việc như là những cái người ta đi vào trong một quán ăn hay một cái quán nhậu á, chúng nó sẽ review, chụp ảnh để capture lại những cái thứ này. Và ví dụ như hình ảnh của một cái món sản phẩm đi, trước khi apply Clip nó không đánh giá được, nó không đánh giá được là nó có bánh quế không, nó chỉ đánh giá được mỗi gà rán thôi chẳng hạn. Sau khi apply Clip vào á, nó sẽ biết được là có gà rán và có bánh quế. bản chất, nó sử dụng cái Clip này là một phần của AI, là nó xử lý ảnh, xử lý ảnh và caption của ảnh, và hình ảnh thành vector để nó so sánh với nhau. đây là hai use case của nó. những cái use case này được áp dụng cho cái gì?\n\n**55:38** Hai cái use case trên nó sẽ áp dụng trong cái tình huống là khi mà mình có nhiều review á, mình có thể summarize nó lại thành một cái highlight review ở trên đây. dựa trên những cái thứ mà nó chuyển thành vector được á, nó có thể annotation được cái việc là những cái hình ảnh đang nói cái gì, nó support cho mình được cái gì ở trong đây. Đợi một chút, nó sẽ highlight cho mình luôn. nó sẽ biết được cho mình cái context của cái ảnh là gì, nó có thể annotation được cái việc này. đó là cái use case của cái việc mà AI dùng để làm image summarization.\n\n**56:15** Đầu năm nay nó có release thêm cái là Yelp Assistant. Dựa trên những cái nền tảng cũ của chúng nó, chúng nó có thể tạo ra chatbot rồi, xong nó có thể review lại cái highlight như thế này, mình cứ hỏi nó xong nó recommendation cho mình cái gì thôi. Đơn giản là như vậy. Ngoài ra em có thấy một cái use case cũng khá đặc biệt, có nghĩa là trong cái giai đoạn từ 2020 á, nó nổ ra cái câu chuyện là làm clip ngắn review các thứ á. chúng nó có một cái nguồn dataset nhất định cho cái việc đó. em thấy chúng nó bảo chúng nó sắp release một cái như anh Tom có đề cập, cái bọn đó có thể chuyển văn bản thành giọng nói á. dựa trên cái nguồn dataset review này á, có lẽ chúng nó support review thêm cái việc mà làm video clip ngắn để mô tả cái nhà hàng.\n\n**57:45** Dựa trên những cái review, những cái video mà người ta tới người ta review á, mình có thể tạo ra được một cái đoạn script, xong cho nó chạy qua AI, nó tự động làm ra một cái video về một cái nhà hàng như mình. đây là use case của bọn này, đơn giản nó có thế thôi. Ok, quay lại cái câu hỏi đầu tiên, cái này nó sẽ dạng là dùng AI để label data, đúng không?\n\n**58:35** Ok, vậy là check xem là cái comment là negative hay positive, đúng không? Kể kiểu đấy là một ví dụ. Cái thứ hai nữa là nó sử dụng cái clip model, đúng không? Chắc là sẽ dạng giống như Vision, nhưng mà live hơn, cũng để dán nhãn, đúng không? Để dán nhãn giống như cái của bên phía Plot, dán nhãn cho ảnh. hai cái use case đó, nó sẽ được ứng dụng trong cái việc gì?\n\n**59:18** Em nghĩ có một cái ý khá hay mà nó chưa nói tới, là câu chuyện là nó có nguồn dataset sẵn. Như là ai tới review, ai tới đánh giá các thứ, dựa trên những cái clip ngắn như thế này, nó có thể tạo ra được một cái video intro về cái nhà hàng đó. Nó sử dụng AI. Em nghĩ là nó sử dụng AI để viết kịch bản, rồi sau đó đưa kịch bản đó cho một con AI voice để nói. Nhưng mà hình ảnh nó lấy ở đâu? Như kiểu là video nó sẽ lấy từ đâu ra?\n\n**59:57** Từ trong cái review, ai tới review họ sẽ có một cái video để review. Ok, tự động tạo advertisement, đúng không? Dạ vâng, cho TikTok hay những nền tảng như TikTok các thứ, kiểu summarize từ review của user. Nghe cũng có vẻ sáng tạo đấy. Ừ, chắc anh em confirm mấy cái của anh bảo làm rồi đúng không?\n\n**01:00:07** Đang vậy, cái này ok là cái caption. Ok, đúng hầu như là đúng anh. Bạn nói đúng, là chúng nó sẽ, em nghĩ em nghĩ cái use case này bọn này ban đầu á, cái mục đích ban đầu của bọn này là làm recommendation. trước đó, trước khi có AI chúng nó đã có một cái hybrid recommendation model trước. Căn bản là nó sẽ... Em nghĩ là khi mà có cái này á, nó dẹp gần hết cái model cũ này luôn. Em nghĩ có một cái khá hay là cái business messaging mà chúng nó không có đề cập nhiều. Có nghĩa là em nghĩ là nó sẽ dựa trên là có review top 50 review chẳng hạn. Xong top 50 cái interaction, kiểu như rating như thế nào. Thứ nhất là review tốt, n rating tốt, cái business messaging của nó sẽ tốt. Mà Yelp không đề cập vấn đề này, mình không trách nó được.\n\n**01:00:51** Ok, anh em có câu hỏi cho Đạt không? Bài đầu tiên đấy. Đạt bảo đang thêm mấy cái, mình phải enterprise nữa, nhưng mà thầy thấy đang Viettel với cả FPT, với cả VNG các thứ, đang chưa biết thấy chúng nó thế nào. Đạt kêu mấy cái tool, cái tool gì coding của bên phía FPT hả, đang kêu cùi.\n\n**01:01:41** Hì, một bản for, một bản for của của continue à? Nó thế, nó thế không tốt. Nó hơi cùi, thô. Hai, chị hết rồi à? Chắc vậy. Đạt nhé. Hôm nay mấy bài về Yelp và Tech Linh chắc tuần sau, tuần sau, tuần sau nữa, nếu kịp.\n\n**01:02:01** Tí demo luôn đi, Đạt luôn. Để Đạt demo một tí cái gì nhỉ? Cá đang là một con bot, để có thể question với cả question một cái short code dưới dạng kiểu developer mà hiểu rõ hơn về code, hay là test kiểu như là một vai trò auditor đi kiểm tra chất lượng của code. Đạt đang demo dev cái workflow hay con bot dựa trên diff đó cho anh em xem thử nào. Mình bật hình rồi Đạt ơi.\n\n**01:03:01** đây là một cái project để em xin vào club ai nha. Trình em gọi là hơi 'newb' nên project này mà có lem quá mọi người thông cảm. Workflow cơ bản là em sẽ lấy query, rồi trích xuất ra được cái URL của repo. Ở đây em có dùng lại cái scrapper của anh Tom, nhưng mà nó chưa đúng ý em, nên em có tạo một con scrapper ở local nó sẽ lấy được tất cả content của repo luôn. Nhưng mà cái đó nó quá lâu với quá lớn. Ờ, default hiện tại em chưa thấy làm cách nào mà bỏ vào con context được, trừ khi dùng cái knowledge retrieval, mà dùng knowledge retrieval em không có gọi là trực tiếp được mà phải bỏ vào trước. Mình không có chọn, không có chọn repo được.\n\n**01:05:45** Cái scraper này của anh Tom nó không có lấy content của file, cho nên em chưa vẽ diagram được. Vẽ diagram có thể em dùng, tí nữa em test thử. Cái này là em lấy được content của những file nè, ở root, ở những file doc. Những file đó không chắc câu hỏi của Huy vừa đưa ra chắc là cũng không trả lời được. Để em thử, em có sẵn cái full của em vô đây rồi, offline nhỉ. Bên phía in sẵn content rồi, chứ không online. Cái này em generate bằng luôn, cũng không có.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=DGxSbJrTJjDCyUp5\u0026amp;start=3798\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:07:07** Cái này nó sẽ scrap full content, nó sẽ đầy đủ hơn. Để em thử đặt câu hỏi của bên Huy hay của Hoàng các em thử nào. Mình thử BC chat lên rồi đặt câu hỏi xem. Anh có không? À không. Maybe là cái context này quá lớn, cái phần knowledge retrieval này em chưa tìm được cách mà cho nó vào context tốt được.\n\n**01:09:19** Retrieve tối ưu lắm. Cái file text này cũng mấy chục ngàn dòng, mấy chục ngàn dòng á. Mở lên xem thử nà, đ đang dùng mini hả, đổi sang máy đ xịn hơn xem có ok hơn không. Đồ mini hơi cùi. 2 triệu từ như thế, từ làm sao mà nó còn xong được ta? Em nghĩ là phải có một cái server, cái dedicated server luôn nó mới ok. Anh đang tò mò tại sao nó chạy được ấy, bởi vì 29U word à, như vừa thấy à. Nhân với cả 4 này là số to. Kích cỡ đấy, Follow up xem thử. Ok, tức là em vẫn là từ cái context thôi đúng không, là mình cũng chỉ dạng là query kiểu query vb đúng không, chứ không phải mình nhập hết tất cả cái đấy vào context.\n\n**01:10:57** Ok, đúng rồi anh. em chưa nắm được là cái retrieval của thằng dify nó sẽ chạy như thế nào. Không biết nó chạy có đúng không, nó retrieve có đúng không. Em chưa tracing được nó mà em có cái tool tracing ở phía trước nữa, có thể test lại thử xem như thế nào. Nhưng mà ý là nếu mà kiểu retrieval như này chắc là kết quả nó sẽ không đúng được đâu anh. Anh cũng đang chưa biết là nó sẽ run bao nhiêu data ấy. Kiểu nó chỉ prefer 2-300 thôi, kiểu data không thể nào đủ mà để làm mấy cái task kiểu này. Cái này ít nhất cũng phải vài trăm tương đối data ấy. Dạ cái này còn work in progress.\n\n**01:11:35** Đùa đấy, cứ lên công ty là có AI Club rồi. À, là của full version hay là fix được cái vụ này demo với bọn anh ở trên office nhé, mà try em để lại cho anh. OK, để em xem nó vẫn không build ra chắc mọi người coi đỡ. Cái chắc build bị gì đó, mọi người thấy màn hình không ạ?\n\n**01:13:08** Dạ tuần này như em nói tuần trước em sẽ up cái bài sync.Map này. Em thấy nó hay với chi tiết để mọi người mà xài Go có cái nhìn tổng quan hơn về map nói chung. Và cái thằng sync map này đi qua trước là phần context. khi mọi người viết map đúng không, mà mình nếu mà mình viết concurrent map hay operation đó, mình làm concurrent á, về bản 1.16 trước nó sẽ không báo đâu, nhưng mà nó vẫn không safe nha. Còn bản từ 1.16 trở đi á nó sẽ error như thế này đó. Cho nên là để mà solve được problem này bình thường mọi người có thể viết map kèm với tại package sync, viết manual đó được.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/s7doIOUDGgA?si=0nH3Rjv-OZ5FAoAP\u0026amp;start=4394\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:13:56** Bên cạnh đó nó có một cái option khác đó là thằng sync.Map này. chút nữa đến cuối mình sẽ sẽ nói tại sao nó lại được đề ra xài và cái usecase của nó như thế nào. thằng này nó được đề ra để mà mình không cần quan tâm lắm về cái việc mà mình phải xài mutex để lock lại cho việc synchronize. Tức là mình chỉ có việc xài thôi. Xài nó trông đơn giản như thế này nha, nó friendly như là mình viết map kiểm tra value vậy. Ví dụ như mình load một cái key lên có value ok nó sẽ giống như là việc map value bình thường thôi. Trong như này, nếu có là ok true, còn nếu không có false của y chang.\n\n**01:14:38** Còn có một số cái function mà mình có thể xài rất handy. Đây là bảng 12.23 sẽ có clear, clear hết. Ví dụ như load là để lấy value, store là để update hoặc store cái key. Update vậy. Delete các thứ. ngoài cái việc mà mình viết concurrent đó đi đó, bên cạnh đó khi mà mình range, tức là mình loop một cái map nó cũng bị race condition nữa. thằng sync map này nó có cái hàm range này, mình xài mình sẽ không quan tâm nó là ấy, nó sẽ không bị nhưng mà như hàm range bình thường thôi. nó sẽ không cho mình cái cái snapshot mà gọi là consistent nhất, là khi mà mình vừa mới vô cái snapshot nó không được update là.\n\n**01:15:28** Mà mình range, tức là mình loop một cái map, nó cũng bị race condition nữa. thằng `sync.Map` này nó có cái hàm `Range`, mình xài, mình sẽ không quan tâm nó là cái gì, nó sẽ không bị như bình thường đâu. Nhưng mà như hàm `Range` bình thường thôi, nó sẽ không cho mình cái snapshot mà gọi là consistent nhất, là khi mà mình vừa mới vào cái snapshot nó không được update. trong lúc đó mình sẽ phải thay đổi cách viết, nhưng ít nhất là nó sẽ không bị phải error như thế này.\n\n**01:16:06** Đến cái phần bên dưới nó work như thế nào á. mọi người, nếu mà mọi người viết khi mà xem `CH` và `definition` cái `map` nó được cấu trúc như thế này: nó sẽ bao gồm hai cái `map`. Đó, nghe đến đây là mọi người sẽ thấy hơi kinh, nghe hơi thốn `RAM` với `memory`. Nó có một cái `Read Only map` và một cái `Dirty map`. nghe như thế mọi người có thể đoán được là nó sẽ làm việc theo kiểu là những cái value mà nếu mà được `write` nó sẽ được viết vào cái thằng `Dirty map` này hết. Cứ viết `update` vào đây, `update` vào đây, con này nó sẽ giống như là.\n\n**01:16:46** Cái `Read Only map` này nó sẽ là những cái khi mà mình đọc vào á, mình sẽ luôn đọc ở đây. Còn `write` sẽ luôn `write` mới vào thằng `Dirty map`. Còn cái flow bên dưới nó làm việc như thế nào chút xíu nữa mình sẽ nhìn cái chart flow mình sẽ thấy. À, cả hai cái `map` này có một điểm chung: nó đều có một cái con trỏ `entry` nha mọi người, để ý để dễ hiểu cái flow. Ví dụ, ở đây mình thêm một cái `entry` mới, đúng không? nó sẽ thêm vào `Dirty map` và nó đều trỏ đến `entry` này. Cái này nó sẽ giống như là một cái `flag` để đánh dấu rằng là cái `map` này đã được thay đổi rồi. Tức là cái thằng `Read Only map` này nó không phải là mới nhất nữa. Khi này bên dưới nó sẽ nhìn và hiểu rằng là thằng `Dirty map` mới là cái nên đọc vào.\n\n**01:17:27** Hình này thể hiện rằng là ví dụ như mình `update` một cái value nào đó, do bên dưới nó là con trỏ đúng không, mình chỉ việc `update` cái con trỏ đó thôi, không cần phải `update` từng cái value như là mình làm với `map` truyền thống. để làm được điều này á, bên dưới nó để ra một cơ chế là ba cái trạng thái (`state`) cho cái con trỏ `entry` này. `State` thứ nhất là `normal state`, đúng không? `Normal state` tức là những cái value cũ của `map`, nó đang có đủ và có thể xài được, không có bị gì hết. Còn trạng thái `amended` là khi mà `entry` đã bị sửa lại. Còn `delete state` là khi một `entry` nào đó đã được `delete` khỏi `map`, nhưng nó chưa được remove hoàn toàn nha. Tức là nó sẽ được...\n\n**01:18:59** ...assign cái con trỏ `entry` vào `new entry`, chứ chưa remove ra. Còn cái `expired state` là xóa hoàn toàn, giống như là `hard delete` là mất khỏi `map` luôn. Để hình dung rõ hơn, mọi người có thể nhìn cái flow như thế này nha: ví dụ ban đầu cái `map` của mình đang có một cái `key1` và `value1` đúng không? bên `Dirty map` chưa có gì cả, tức là chưa được thêm bớt gì. Sau đó, mình thêm một cái `key2` nào đó, đúng không? nó sẽ được thêm vào `Dirty map`, và khúc này là thằng `map` đã `amended` rồi, nó đã có một cái `flag` `amended` ở đây.\n\n**01:19:40** Sau đó, khi mình xóa (`delete`) một cái `key`, `map` này sẽ bị gán `new entry`, đúng không? Bên này cũng sẽ được tương tự gán `new entry`, giống như cái hình trước. Tức là mình chỉ cần cập nhật con trỏ thôi, không cần phải cập nhật value. Rồi, sau khi `delete` xong, đúng không, để `promote` được cái `Dirty map` này, mình phải cập nhật lại qua bên `Read Only map`, để `Dirty map` trở về `new state`, giống như đưa về trạng thái ban đầu.\n\n**01:20:18** Tương tự, thêm một `key3` nữa, khi thêm cái `key3` này á, cái `state` này nè, sau khi nó đã trở về `new state` rồi đúng không, mình thêm `key3` vào á, nó xác định rằng thằng này đã được `delete` rồi, nó sẽ là `delete` hoàn toàn. Điều này có nghĩa là lần sau, khi nó so sánh với `Dirty map`, nó biết rằng bên này cái `value1` đã bị xóa rồi, không còn nữa. cái `Read Only map` lúc này chỉ còn lại `key2` và `key3`.\n\n**01:20:51** Cho nên chính vì lý do này, `sync.Map` không có hàm `len` cho mọi người xài. Tại vì nếu như mọi người dùng hàm `len` ở đây, sẽ không biết được `value` của nó, tại vì lúc đó nó sẽ đếm cả những cái `value` đã `expired` hay `deleted`. Mọi người có thể thấy, chính vì cái cấu trúc của `sync.Map` được build như thế này, use case của nó được recommended là nên dùng cho những use case mà đọc (`read`) nhiều hơn ghi (`write`). Tức là nếu mà `write` hoặc `delete` nhiều, mọi người tưởng tượng chỗ này nó xài con trỏ liên tục, và có một cái issue bên Go team đã report là thằng này không bao giờ được garbage collected.\n\n**01:21:36** Sau đó Go team họ confirm rằng cái `sync.Map` này được sinh ra chủ yếu để support mấy cái bên trong Go Library thôi. Nếu mọi người thấy nó `handy` vì có những function dễ xài có thể xài, nhưng nếu use case của mọi người mà cần lưu trữ (`store`), hoặc là `update`, `delete` nhiều không nên xài, vì nó sẽ làm chậm hệ thống.\n\n**01:22:24** Dạ chắc chỉ vậy thôi ạ. Em có code lại cái bài bên này là cái bác này hay share mấy bài cũng khá chi tiết, mọi người có thể follow theo dõi. Ủa, cái này là topic gì Phát? Cho anh coi lại cái bài kịch bản đúp đầu r. À, cái sync map à? Ừ, sync map á. Ủa, nó có khác gì với lại cái anh vừa pass vô không vậy? Khác ở cái gì? Hình như là khác á anh. Ý là cái này em nhớ không nhầm là kiểu như cộng đồng tùy. Anh ví dụ use case họ muốn viết một cái gì đó mà họ thấy. Đó, anh nhìn thấy, họ ghi cái trong cái bên đấy, link mà nhìn thấy cách nó chạy mà lý do tụi nó làm thêm cái gì ấy nhỉ?\n\n**01:23:30** Anh nhìn thấy nè, họ thêm một cái lớp nữa để họ xài. Ví dụ như là họ sẽ có những cái use case đúng không? Ví dụ như họ muốn implement generic trên `sync.Map` đó. Cái này cũng có ảnh hưởng do cái vụ link nãy em nói, do thằng này nó không được garbage collected nè. Đó, kiểu vậy. ví dụ như bên này Go team ở dưới, họ đã confirm chốt xong cái này là cái `sync.Map` này họ kêu là cái này là `intended`, intentional choice rồi, cho nên họ sẽ không sửa. Họ sẽ không đổi đúng không? Bây giờ cộng đồng làm gì mình chỉ biết là họ tự xài thôi. Ý là họ thích cái việc `sync.Map` này được để ra dễ xài, có mấy cái function ngon lành, họ ráng thêm một tầng nữa, rồi chế những cái mà họ cảm thấy là ok, mình có thể xài được. Kiểu vậy.\n\n**01:24:07** Ủa mà sao cái clip này cũng lâu mà bữa nay lại chọn à? Ờ, thế kiểu insight thôi, insight cho mọi người xài. Ý là cái use case này cũng có thể được apply cho bên mình. Ví dụ như bên enterprise đúng không? ví dụ mình xài `map`, mà mình xài concurrency đúng không? mọi người sẽ tự viết một cái `struct`, xong rồi mọi người sẽ nhét một cái `mutex` vào, rồi tùy người sẽ ngồi bắt đầu viết lại. Đủ các kiểu. Trong khi đó thằng `sync.Map` rất handy, như nãy em show anh, là mấy cái function này là nó luôn follow cái chuẩn, là anh muốn `load` anh phải gọi hàm này. Kiểu vậy, nó chuẩn hơn.\n\n**01:24:50 M**ọi người sẽ tự viết một cái `struct` như thế, xong rồi mọi người sẽ nhét một cái mutex vào, tùy người sẽ ngồi bắt đầu viết rồi đú các kiểu. Trong khi đó `sync.Map` rất là handy. `Sync.Map` này như em show anh nãy đó, những cái function của nó, nó luôn follow cái chuẩn này hết. Anh muốn load anh phải gọi hàm này, kiểu vậy nó sẽ chuẩn hơn. Nhưng mà như cái bài này là mình phải để ý những cái trade-off của nó, xài cho đúng quy. Ok, hiểu rồi, tức là quy chuẩn cái cách mà sử dụng `map` hả? Với lại workflow hả?\n\n**01:25:26 C**ảm ơn Phát. Rồi để tranh thủ, mấy cái Thành ơi, nhất là xin anh em thêm 10 phút nữa nhé. Nó sẽ hơi tốn thời gian thêm xíu. Nhất là anh nhận được tổng cộng 11 cái submission cho cái bài test của mình. Có ít bài hình ở trên, mấy anh em nhìn ở trên tí. Deadline của mình là đến ngày 20, tức là tuần sau nhé. Bữa trước anh thông báo như là 27 ha, phải không? 26, 27 gì đó là deadline, mấy anh em coi tranh thủ còn một tuần nhìn bài đó rồi làm ha. Cái bài đó nó sẽ quan trọng, có một số cái mà chi tiết của từng bài đó anh chưa có nhìn kỹ. Chỉ có bài của Tôm bữa trước, Tôm nó quăng nhanh lên trên lobby quá, thành ra là có nhìn sơ qua xíu. Nhưng mà còn của mấy anh em chưa nhìn rồi. Nhưng mà cái ý chính là mọi người xem thử nha, cái chất lượng bài của mình á, tập trung ở chuyện là đợt này khi mà market nó thay đổi nhiều vậy, cái demand của thị trường cho cái nghề làm software nó có sự thay đổi lớn á.\n\n**01:26:15** Tất nhiên những cái nhu cầu nó vẫn sẽ còn ở đó thôi, nhưng mà cái số lượng đó nó giảm xuống. Thành ra đó anh gọi là cái sự thay đổi về cái nhu cầu thị trường gần như với góc nhìn của anh trải qua nó là giống như 2014, nhưng mà on-over-again, vậy là sự thay đổi công nghệ mới ra, mọi thứ mới ra, thị trường mới rồi những cái tiềm năng mới nó sẽ xuất hiện trên đó. cái bài test nó sẽ quan trọng với việc là giúp cho mình, nhất là test về văn hóa, nhìn lại trong cái lúc mà tụi anh muốn check lại cái team á, muốn là hai cái đội: đội làm research study với cả đội làm consulting nó có một cái sự phân hóa rõ ràng.\n\n**01:27:36** Nó có một cái sự phân hóa rõ ràng. Như trong cái bài viết anh post lên notion cách đây khoảng hai tuần hả, sẽ có sự phân hóa rõ ràng. Tương lai nó sẽ có thêm một số những cái policy mới cho chính sách về lợi ích khác nhau giữa hai đội nữa. Nhưng mà hiện nay là, như mình thấy đó, mọi người thấy OGIF dần dần nó được chuyển qua gần như thành cái buổi là report lại tất cả những cái study. Cái phần mà anh em đang coi mới và report lại trên này. Có thể những bài đó do được add, có thể những bài đó là do mọi người bắt đầu anh nhìn thấy, có một vài thành viên trong team mình thật sự là thấy cái kiến thức mới đó, xong rồi pick up những kiến thức mới đó để mà coi.\n\nTừ từ thấy rõ ràng là tụi anh muốn cái sự phân hóa đó nó diễn ra càng ngày càng rõ hơn. Và cũng có chính sách rõ ràng cho cái chuyện đó. Tức là ai mà thích coi mấy cái phần topic nhiều hơn, xong rồi ra ứng dụng ở tới mức là MVP, hay là ứng dụng vô những cái dự án nếu có, hoặc là đi deep dive thêm về kiến thức á, sẽ có một cái benefit khác. Những anh em nào mà không nhất thiết để phải ngồi coi những cái phần liên quan tới phần study như vậy, cứ ngồi làm dự án bình thường thôi. Nhưng mà nó sẽ có một số vấn đề khác đi kèm mà anh cũng có list ra trong cái link notion cách đây hai tuần. mọi người xem nhìn lại cái link đó một tí, để biết là vì cái định hướng như vậy nên là cái bài test này nó mang ý nghĩa là xem thử coi là cái mức độ của mọi người trong chuyện bắt kịp kiến thức mới, hoặc là cái độ tương thích với lại văn hóa trong cái giai đoạn mà tất cả mọi thứ nó thay đổi như vậy tới mức nào ha.\n\n**01:29:20** Để hiểu vì cái mục tiêu là như vậy, nên là cái lúc mà chấm cái bài á, anh sẽ là người duy nhất chấm cái bài đó. Team mấy anh chị khác không có chấm đâu. Tất cả mọi người sẽ phải làm mà, nên là anh nghĩ rằng anh set cái standard cho chuyện đó. Nên là mấy bạn chịu khó làm bài đó tự làm là một chuyện. Thứ hai nữa là bài nào mà chất lượng thấp thật ra cũng không có vấn đề gì hết, chấm điểm thấp một xíu thôi, nhưng mà vừa làm hết vẫn sẽ được đủ điểm để mà coi như là pass cái đó. Chỉ là sau đó cái kết quả trước mắt thể hiện được á, là anh sẽ phân cụm thành hai cụm khác nhau.\n\nĐội Foundation hay là đội Lab á, vẫn là đội core của mình từ năm nay, ha. Đó là cái thông báo chính. Nên là trên 11 cái bài này, nếu bạn nào làm xong rồi mà cảm thấy là mình có thể làm tốt hơn được cho cái chuyện mà anh vừa mới nói đó, đội mình thật ra là cái team Foundation và cái team Lab á vẫn sẽ được ưu tiên nhiều hơn trong những vấn đề khác nhau. Được ha. Nên là nếu mà anh em cái bài đó mà đang kiểu làm qua loa á, tập trung ngồi làm kỹ lại tí. Check hai thứ ha: văn hóa trên đó là một, thứ hai nữa là kiến thức.\n\n**01:29:56** Sau đó cái kết quả trước mắt thể thấy được á, là anh sẽ ân cụm thành hai cụm khác nhau, cái đội Foundation hay là đội Lab á vẫn là sẽ đội core của mình từ từ từ 8-9 năm nay ha. đó là vậy, đó là cái thông báo chính. Nên là trên 11 cái bài này, nếu bạn nào làm xong rồi mà cảm thấy là mình có thể làm tốt hơn được cho cái chuyện là anh vừa mới stay ra, là đội mình thiệt ra là cái team Foundation, cái team Lab á vẫn sẽ được ưu tiên nhiều hơn trong những vấn đề khác nhau. Được ha. Nên là nếu mà anh em cái bài đó mà đang kiểu làm qua loa á, tập trung ngồi làm kỹ lại tí, check hai thứ ha: văn hóa trên đó là một, thứ hai nữa là kiến thức cho cái cụm thông tin cái cụm gần nhất mà nó đang có vẻ hot nhất là LLM thôi.\n\nNhưng thực ra team mình vẫn cover rất là nhiều mảng khác nhau, vẫn đang có xem về design, mấy bạn cũng đang xem đúng không. Vẫn có đội đang xem đúng không. Go vẫn đang xem. Blockchain có vẻ nó qua trend tí rồi, thị trường nó đang sideways thôi, nhưng mà về demand của consulting nó vẫn yêu cầu những cái đó rất là nhiều.\n\n**01:31:46** Mấy cái mini app cho telegram, họ mua về rồi clone nhanh lên, thấy góc nhìn của mấy bạn làm business logic (BL) và tech (TCH) bây giờ nó khác một xíu rồi, không còn như ngày đầu nữa. Nhưng mà với consulting mình vẫn có thể sử dụng thôi, bình thường. Hoặc là mình có thể nhìn theo một góc nhìn khác, theo dạng là nó như một cái asset class mới xuất hiện. Với vai trò là developer, mình phải nhìn nó theo góc nhìn làm sao để nó ảnh hưởng đến cái workflow của mình như thế nào, quản lý tài sản ra sao.\n\n**01:32:29** Đó là vấn đề về bài test nhé. Mấy anh em chú ý cái đó. Thứ hai, nãy có nhắc tới cái định hướng về team và số lượng nhân sự. Trong đó có nhắc lại cái link notion hôm trước anh có gửi nhé. Đội Foundation, đội chính khi start lại lần nữa như vậy. Lúc trước team tụi anh bắt đầu chỉ có ba người thôi, sau đó dần dần tăng lên bốn người, rồi lên năm người. Có thêm Quan, có thêm Hiếu, có thêm mấy bạn khác. Nhưng mà ban đầu start với ba người, giờ đội hình xịn hơn rồi. Bây giờ 40 người toàn là thứ dữ, chắc chắn sẽ đi nhanh hơn. Câu chuyện chung là vậy, đánh giá chung cũng là như thế, nên mấy anh em nắm tình hình nha.\n\n**01:33:12** Cái thứ ba nữa có liên quan là Huy Nguyễn, nếu mà xong rồi, chắc tuần sau xem lại thống kê con số về ICY giùm anh nha. Hôm trước em cũng báo là số lượng bắt đầu chạy hơi nhiều, nên mình phải xem lại, cân lại con số cho nó hợp lý. Riêng phần này nhờ Huy và Thành chủ động làm giùm, xử lý giùm anh, xem lại cân số cho nó hợp lý. Thành có một công việc phụ là phần benefit cho thành viên team Lab, xem thử đề xuất như thế nào. Nó có thể được coi là một cái payon, nhưng mình sẽ không trả qua kênh bình thường, mà sẽ có cái cơ chế khác.\n\n**01:33:52** Nhưng mà mấy thành viên team Lab sẽ có cái đó, mọi người quen với cái đó rồi. Cuối cùng là, riêng phần về LLM hiện tại, trong cái list câu hỏi có một câu hỏi quan trọng là làm sao để sử dụng, tìm hiểu bên ngoài sử dụng LLM như thế nào và adapt ra sao. Nhấn mạnh lại câu đó, vì nó là một câu mang ý nghĩa trong việc làm knowledge discovery. Câu hỏi này liên quan đến việc test là không chỉ đơn thuần là dùng, mà là tất cả các công cụ mà mấy anh em thấy được trong team hiện tại. Khi có người sử dụng hiệu quả, có người sử dụng kém hiệu quả hơn, RT (retrieval technology) nó thành một spectrum rất rõ ràng, những người thấp là thấp, những người cao rất cao.\n\n**01:34:38** Tụi anh muốn nâng cái standard đó lên. Spectrum đó tụi anh muốn rút ngắn lại, càng cô động lại càng tốt. Bây giờ nó đang rất dài. Câu này ngoài việc dùng tool để làm discovery, nó còn mang ý nghĩa xem ngành nghề của mình sẽ như thế nào trong việc ứng dụng đó để nâng cao competency của mình, làm việc có năng suất hơn. Đó là toàn bộ vấn đề, và mọi người xác nhận lại xem cái mình làm có đúng chưa, nó có tầng ý nghĩa sâu xa hơn vậy.\n\n**01:35:20** Cuối cùng để kết thúc buổi này, Thành ơi, mấy buổi OGIF sau, những phần mà Tom đã làm liên quan đến việc xây dựng structure của một cái LLM app, có thể lấy cái đó ra phân tích thử nhé. Phân tích lấy cái đó để làm sâu hơn luôn nhé.\n\n**01:35:56**Toàn bộ mọi người hy vọng là tất cả anh em đều pass hết để đi chơi cho nó vui vẻ. Tuần sau sẽ có một cái bài khác. Tuần sau request là bên chỗ của Minh L. Minh ơi, chắc là lên làm một cái demo nha, tiếp tục về cái finite state machine, FSM á. Vì trong định hướng những công nghệ nền tảng như blockchain, AI, nhưng phần chính vẫn sẽ là các anh em làm engineer sẽ có một ngách khác để đi, đó là hiểu rõ các hệ thống lớn vận hành thế nào. Tương lai, nếu mình không phải là người sinh ra để làm data manipulation AI sẽ làm giùm mình, mình không cần tự thiết kế hay làm mấy việc của junior nữa.\n\n**01:37:35** Cách duy nhất để lên senior là hiểu rõ vấn đề và làm kiến trúc thôi. Phần finite state machine đóng vai trò tương đối quan trọng, liên quan đến chuyện scale mà trước giờ tụi mình đã nói nhiều. Trước đó Minh có đọc và hiểu đúng góc nhìn mà anh đang muốn hướng tới. Nên là xem thử làm bài phân biệt các loại general server của nó nhé. Server state machine và event-based server. Rồi làm một cái sample để biểu diễn và implement nó luôn bằng Erlang nha. Erlang có sẵn hết các framework rồi.\n\n**01:39:01** Bài này chắc là khi nào Minh Lưu. ready, nếu tuần sau không kịp có thể là hai tuần. Đề nghị mấy bạn backend và mấy bạn sen team mình gom lại, có gì confirm trước nhé. Vì bài này rất quan trọng trong chuyện phân tích thiết kế phần mềm. Bài này rất quan trọng. Trước giờ mọi người chỉ nói tới modeling và làm C4 thôi, nhưng Erlang là ngôn ngữ đi sát cái này nhất rồi, thường mọi người sẽ không biết hết. Chúng ta không nhất thiết phải học Erlang nhưng có thể nhìn cách thiết kế và build của họ để làm phần đó rất chuẩn, giống như là họ có framework sẵn, mình chỉ cần gắn vào để sử dụng thôi.\n\n**01:39:37 T**ranh thủ, ngày 20 tháng 10 là chủ nhật, Mỹ với Ngọc và Giang có post rồi. Hôm đó là các chị em đi chơi, còn không ở Sài Gòn đại diện team sẽ chúc mọi người phát tài. Chúc mọi người phát tài chắc hợp lý nhất trong trường hợp này. Một chút chúc khác có vẻ không liên quan lắm. Rồi vậy nha, anh em tham gia được đăng ký với Mỹ để book bàn và đi cho hợp lý.\n\n**01:41:19 N**hờ Thành những buổi sau cấu trúc lại thành mấy cái talk nhé. Rồi làm goal đó, team mình có thêm Builder-club nữa, đội đó chắc để xem mấy anh em lúc trước làm Super Bit ổn định lại hoặc làm console ổn định lại anh sẽ cấu trúc lại sau nhé. Đợt này chắc là nghỉ ngơi đầy đủ rồi. Rồi ok, anh em có câu hỏi gì cho bài test không kết thúc ở đây nhé. Rồi tạm biệt mấy anh em, hẹn gặp lại tuần sau. Cảm ơn Thành, cảm ơn tất cả mọi người.\n\n---\n\n**English Transcript**\n\n**0:28** The topic still includes Go Weekly, and Nam is currently testing the weekly design commentary. Let's see how it goes over the next few weeks.\n\n**11:19** Nam will continue to present to the team, and there are a few topics from Hoang, Cat, and Dat. We’re currently researching various use cases that other companies are applying and some of the tools being used by developers. There will likely be a presentation this week or next about these findings. The focus will be on generating a UX design button. In the past, there have been questions about where AI is applied and how it plays a role, whether it serves as a small, standalone component or as part of a broader application for digital products. Today, I will address how AI contributes and how it functions.\n\n**12:11** First, I will talk about system scope relationships. This diagram illustrates how AI is integrated into systems at different levels, from a small component to a comprehensive ecosystem. AI can be a small part of a component or evolve into a larger function, automating features to improve user experience (UX). Here, AI plays a crucial role in digital products, and when integrated, it can fit into various parts, from components to flows, to features, or even as an entire application. It can be part of a platform or ecosystem.\n\n**12:53** For example, as a feature within an app, AI can help users interact with the app more easily, saving time by automating tasks that would otherwise be done manually. As a standalone application, there are many examples like ChatGPT, which serves a specific purpose, or as a platform like Rewind AI, which offers multiple features supporting AI in different tasks within the same app. These are examples of the scope of AI's current operations.\n\n**13:39** Next, regarding the spatial relationship, this helps us understand how AI features are placed and organized within the user interface (UI). There are several ways to integrate AI into design, and it's important to know how to position them in the app so that they optimize user experience without causing confusion or making the interface too complex. Spatial relationships directly affect user experience. For example, AI can operate independently or alongside other features while still maintaining its own space. When you understand these relationships, you can choose how to place and use AI features in a way that enhances usability without overwhelming the user.\n\n**15:11** There are six different methods for presenting AI: it can be entirely separate, alongside other features, layered, integrated with the parent feature, or in small points such as icons. These methods include:\n\n- Separate: AI operates as a separate feature.\n- Alongside: AI is placed next to other features.\n- Layer: AI overlays with another feature.\n- Integrated Parent: AI serves a major role in navigating and managing core content.\n- Integrated Child: AI operates as a secondary, smaller feature.\n- Point: AI is a small icon or widget that helps the user understand its function.\n\n**16:41** Moving on to the functional relationship, this describes the functional interactions between AI and other features in the system. AI can exist separately but still adapt to the overall content and functionality of the app at a higher level. AI can integrate with existing features to improve performance, replacing manual tasks. Understanding how AI works functionally allows us to define its role clearly in the app and design in a way that ensures the functional actions don’t conflict with one another and don't disrupt the user flow.\n\n**17:28** There are six methods to describe this functional relationship, which are similar to the spatial relationships I mentioned earlier:\n\n1. Separate: AI operates independently.\n2. Aware Of: AI exists separately but is aware of how it affects the main feature.\n3. Acting Up: AI interacts back and forth with other features, adapting data between them.\n4. Feature Incorporate: AI is incorporated as a part of an existing feature.\n5. Usage: AI adapts based on how it's used within the app.\n6. Usage Conventionally: AI communicates directly with other features in a two-way interaction.\n\nI will provide an example of this functional relationship in the code I am about to show, where AI generates a panel on the right side of the screen.\n\n**19:06** For example, the acting-up relationship means AI can be aware of and react to changes made by other features, like data syncing between two systems. In contrast, feature incorporation would mean AI is integrated as part of the overall functionality of a specific feature.\n\n**19:57** That covers the main aspects I’ve discussed so far, with three key elements for integrating AI into product design: optimizing product features, improving user functionality, and enhancing the overall effectiveness of the AI-powered system. It’s important to understand how to apply AI properly to provide clear value to the user. If we understand how to apply AI effectively, it becomes easier to design a system that brings value to the user by integrating AI in a meaningful way.\n\n**20:53** I realized I missed an example earlier, so let me go back and explain. I’ll share a few examples that I think will clarify the functional relationships we discussed. For instance, in Microsoft, there’s a tool that generates images, this operates alongside other features in a parallel fashion. There’s also a feature that sits beside the main functions of the app but doesn’t serve as a core part of the experience.\n\n**22:01** Yes, that's a good example. The functional actions and spatial relationships you presented seem to be similar to common patterns. These are just standard patterns for AI design, how to integrate an AI feature into an app or design an AI-driven app, depending on how it’s categorized.\n\n**22:31** Yes, these are patterns we often use when designing AI applications or integrating AI into a separate application or product. Essentially, they are familiar structures to help us better understand how to apply AI. You can categorize and break them down into smaller features. This part is very clear.\n\n**23:31** Thank you, Nam. Ok, next will be Hoàng and Đạt’s presentation.\n\nToday, I will introduce a topic called \"AI Button in LLM Applications.\" Before diving in, let me briefly cover the content and agenda. First, we will explore design patterns related to the AI Button. These patterns are applied in various applications. I’ll pick out the most common and understandable ones to introduce to everyone.\n\n**24:35** This presentation will revolve around using AI in digital products. These applications leverage the power of AI models to solve specific problems or assist users in tasks. When using LLMs, many may encounter the issue where the model does not provide the expected result. This happens because the model operates based on its ability to respond using the data it has been trained on. There are multiple ways to address this issue. One of the most expensive ways is to retrain the entire model from scratch, which can take a lot of time and resources.\n\n**25:15** We have a technique called **in-context learning**, which means AI can learn directly within the current context while you are using it. This technique includes few-shot learning or zero-shot learning, allowing the AI to learn without needing to be retrained from scratch. For example, you only need to provide the AI with a few small examples in the context, and it will adjust its behavior based on what is provided. Instead of retraining the entire model, this method saves a lot of time and resources while still ensuring the AI can learn from the specific context you give it.\n\n**25:52** In this case, **in-context learning** is widely used in **prompt engineering**. People provide available examples directly into the prompt, and the model learns from those examples to generate subsequent results. That's the main idea of in-context learning. Essentially, the design works like this: you have a query, then you build a prompt with the necessary examples and few-shot learning data, and you pass it through the model, which returns a result based on those examples. However, it doesn’t stop at just examples; many other factors are involved as well.\n\n**26:37** Broadly speaking, in-context learning involves feeding the context into the prompt by providing information that the model doesn’t inherently have. Since this is a pre-trained model, its knowledge is limited, so you provide additional information in the context and prompt for the model to learn during the result generation process. For instance, in medical image diagnosis, the model may not have enough specialized knowledge. Therefore, you provide that expertise into the context and prompt so the model can learn during the result generation process. That’s the core of in-context learning.\n\nNext, we have another important design button, which is **data preprocessing/editing**.\n\n**27:54** This section describes the process of preparing data for the language model (LM). As you know, LMs operate based on vector databases, using vector comparisons to find similar data points. This process often involves handling multimedia data and various types of information. To ensure optimal output, applying data preprocessing steps is crucial. For example, you can preprocess text by filtering out unnecessary details to shorten it, or with images and audio, you can remove noise or compress the data to reduce size before passing it through the language model.\n\n**29:19** Data preprocessing or editing helps the model operate more efficiently. There are many ways to preprocess, depending on the type of data or context. You perform this based on specific requirements. The next design button I want to mention is a commonly used one, though it goes by different names. I call it the **example agent**. This design is commonly seen when you want your query to pass through multiple contexts. For example, if you have a content review application, you can let that content pass through a pipeline where each agent evaluates the content from a different perspective.\n\n**30:11** One agent might evaluate the content from a writer's perspective, and another agent might do so from a different angle. After going through all these agents, there will be a final synthesis layer to combine or process those results, ultimately providing the user with a comprehensive output. This design is often seen in evaluation systems where results from different models are evaluated, and the best outcome is chosen based on predefined conditions.\n\n**30:55** The next design button is called **agentic button**. So, what does agentic mean? In the context of language models (LMs), **agentic LMs** refer to enhancing the model's capabilities. Since the model only knows what’s in its training data, we upgrade it to increase its power and minimize human intervention. This design helps the system become more automated, allowing it to operate with less human interference.\n\n**32:24** This design has several key components that help you achieve this level of automation. There are four main components: **reflection**, **planning**, **execution**, and **multi-collaboration**. Each of these components helps make your system more automated. First, let’s talk about **reflection**. Reflection involves evaluating the initial results of the model based on a specific criterion or metric to determine if the result has been optimized. If it hasn’t, the system adjusts and repeats the process, continuing to generate results until it reaches an optimal outcome.\n\n**33:06** Reflection helps reduce human intervention because, instead of producing an initial result that doesn’t meet your expectations, the system refines itself based on pre-established criteria, eventually delivering a more accurate result without manual adjustment.\n\nThe Reflection button means that it will evaluate the initial output of an AI, then assess it according to a certain standard or metric to see if the result has been optimized. If not, it will adjust slightly and run the AI again to generate another result until the optimal result is achieved. This helps reduce the need for human intervention, as if the first output is not what you expected, you don’t need to manually adjust it, the system will optimize itself.\n\n**33:42** The second button is the tool. Tools can be external, such as external APIs or functions that people code. These tools are used to allow the model to access knowledge from the outside world, real-time knowledge, or external resources that it hasn’t been pre-trained on. For example, OpenAI or Claude both support this. The model can know when to call the tool based on the description you write for the tool. The model will know how to retrieve and extract information from the tool and then return it to the LM to generate an output.\n\n**34:30** Next is planning. The planning button means that you give the LM the ability to plan, preventing the need to prompt multiple times. For example, if you have a complex task, you provide a large prompt for the LM to plan out all the steps it needs to take in a step-by-step manner. This allows it to perform smaller tasks first, which are eventually combined into a larger task. This planning design has many variations, and this is the simplest version: planning and then executing step by step.\n\n**35:10** Finally, multi-collaboration. I presented this about a month ago. Essentially, it's like having the AI excel at a particular task. You have a context, right? You divide it and pass it through to different agents. Each agent is good at its specific task, and after they complete their tasks, it passes on to the next agent. In this way, it can complete the requirement. This design heavily utilizes the divide-and-conquer principle, breaking a large task into smaller tasks and assigning each to a specialized agent. This is a design button I’ve seen being used in many places.\n\n**36:24** Those are the design buttons that I’ve seen used in many places and understand the most. I’ve finished my presentation. Does anyone have any questions?\n\n**37:10** Hoàng, can you repeat the part about planning to confirm Bảo’s comment? It’s like it reads the prompt, right? It understands your prompt first, then breaks it down into smaller tasks, and then there are workers, perhaps IDE workers or smaller prompts, to complete the task. Is that correct?\n\n**37:40** Yes, you can think of it that way. You can split the prompt, for example, in a complex task, into several smaller plans. These smaller plans will be done step by step. For instance, it executes plan 1 first, then plan 2, then plan 3. Once all the plans are completed, they are compiled somewhere or in a final component to produce the final answer.\n\n**38:06** It’s like the Zero you presented last time, right? The worker can do tasks like reading files, deleting files, modifying files, or interacting with the Internet, sending emails, and so on. So basically, agents work in this way.\n\n**38:52** Exactly. Instead of handling a massive task all at once, which requires repeated prompting, you start with a prompt that breaks the task into smaller tasks, and then a pipeline runs through each worker, handling small tasks for you.\n\n**39:23** Ok, pull up slide 14, Hoàng. Slide 14. I also see this is kind of like the Mule Automation setup that Tom created, right? The Mule button that Tom set up. I’ve finished the code, but this design and the button look exactly the same.\n\n**39:46** Yes, this is a looping process with Tom, which looks somewhat similar. It’s like planning, as Tom mentioned, where it breaks down the task into parts and handles each part. It has iterations within it, like a list of steps you described earlier. Referring back to yours, the agents can see that. What I’m seeing looks more like planning: it splits the plan upfront and then works step by step on each plan, moving through each round one by one. This one, though, works more in parallel, where they run simultaneously, produce the output, evaluate it, and then return the final result. I think I got the workflow mixed up; it’s now corrected.\n\n**42:28** Exactly, give it a try. It works like that, breaking down into different tasks. It’s more like a classification, running through each one. This is closer to multi-collaboration because it’s like a question classifier, where only one agent runs for each task. Each agent works on its specific expertise, then combines everything.\n\n**43:33** But do you think the parts like reasoning and input analysis are correct? Tom’s expert part. Specifically, for picking domains, there’s a classifier, but reasoning and input analyzers are separate agents. In that group, they’re experts, right? We consider them a group of experts, and they combine with the five agents underneath.\n\n**45:33** If we try to do everything within a single prompt, I’m certain it won’t give us the desired result. The context is too large and lacks specific examples. The main issue is that accuracy will definitely decrease because there’s too much data at once. The key is to split it into multiple layers, step by step. In reality, we need the output from the LM; we can’t hardcode it all in advance. We just want the simplest prompt so it can generate small answers that ultimately lead to a large answer.\n\n**46:59** Exactly, by breaking it down, we can identify where the problem lies and debug it. Like you mentioned, specify clearly and break it down into layers. If something goes wrong, we can fix that part. If you throw everything in at once, you won’t know where the error is, and you’ll have to fix it repeatedly.\n\n**48:43** Exactly. For example, creating an event in the calendar for tomorrow, if there’s no event at that time, it creates the event, but if there is already one, it sends a notification. If we throw in a large request at once, it will get confusing because it has to execute step by step. Breaking it into layers and testing each step will make it work better.\n\n**49:23** I'm almost certain that 99% of the time, it will get lost if it’s done in one go. However, if we split it into layers, into multiple layers, and do the tests, it will work much better. Ok, let's go. If anyone's at the office, Tom’s probably there to argue with. If no one has any more questions, thanks to Hoàng first. Now, Đạt, you’re up. Đạt, are you sharing your screen? Ok, can everyone see my screen? Yes, we can.\n\n**50:38** Today, I’m going to talk about Yelp use cases. Wait a second, Đạt, let me introduce some context first. So this time, the team will focus somewhere and search for some use cases. There are different types of use cases. The type Đạt is sharing is where we look at how startups or enterprises are applying AI to solve specific problems. It could be something completely new, like a greenfield, or it could be optimizing the current workflow of their system. They will write use cases and report updates monthly. In addition, there’s another type of use case, which involves tuning to boost the development on the tech side. They will also report that part somewhere in tech. We’re testing this for about two weeks. This is the first report, and it’s about Yelp. Yelp is a startup, right? Now, Đạt, introduce how they are applying AI.\n\n**52:01** Yelp is a company that provides software to stores and businesses that want to offer services like fast delivery, restaurants, or basic utilities. Yelp sells the software for those tasks. I’ll share a bit about how they use AI in their tools.\n\n**53:00** Before this, they had a machine learning system, but now they’ve added AI to improve the accuracy of their recommendations. Yelp has many types of reviews on its system, like restaurant reviews, which may not always be good. Based on those reviews, they do some text editing to compare whether the results are spam or legitimate. AI is used here in several ways. First, they use AI to create datasets to train a model to assess whether a review is spam or a good/bad review. They use AI to generate datasets based on LinkedIn. From what I’ve read, they use techniques like Zero-shot and Few-shot learning to create these datasets. They use some models from Hugging Face and then classify the reviews as good or bad. This is one use case where AI is applied in text editing.\n\n**54:18** Now onto the second use case, they use the Clip Model. The Clip Model primarily processes images. What does that mean? It means that based on reviews... wait a minute, let me find the reference... Ah, Clip processes two things: one is the caption of the image, and the other is the image itself. Through Clip, it can understand the context of the image. Yelp uses Clip for tasks such as when someone goes into a restaurant or pub and posts reviews or captures images of the place. For example, before applying Clip, it couldn’t identify if there were waffles in a dish; it could only identify fried chicken. After applying Clip, it can now recognize both fried chicken and waffles. Essentially, it uses Clip as part of AI to process images, captions, and convert images into vectors to compare them. So, these are the two use cases for Yelp.\n\n**55:38** These two use cases are applied in situations where you have many reviews, and you can summarize them into a highlight review. Based on the information converted into vectors, it can annotate what the images are conveying, and what they are supporting. Just give it a moment, it will highlight it for you. It understands the context of the image and can annotate it accordingly. This is the use case for how AI is used in image summarization.\n\n**56:15** Earlier this year, Yelp released the Yelp Assistant. Based on their existing platform, they were able to create a chatbot that reviews highlights like this. You simply ask, and it recommends something for you. It's as simple as that. Additionally, I noticed a use case from 2020 when the trend of short review clips started becoming popular. Yelp had a dataset specifically for that purpose. They mentioned that they are about to release something, as Tom referred to, that can convert text to speech. Based on the review dataset, they might support creating short video clips to describe a restaurant.\n\n**57:45** Based on reviews or videos posted by people, Yelp could generate a script and run it through AI to automatically create a video about a restaurant. That’s the use case. It’s simple as that. Ok, going back to the first question, this use case is essentially using AI to label data, right?\n\n**58:35** Ok, so it checks whether the comment is negative or positive, right? That’s one example. The second one is using the Clip Model, correct? It’s similar to Vision but more live, also for labeling, right? Like with Plot, labeling for images. So, these two use cases are applied for what?\n\n**59:18** I think there's an interesting point that hasn't been mentioned yet, which is the story about having a ready-made dataset. For instance, when someone leaves a review or gives a rating, based on these short clips, Yelp could generate an intro video for the restaurant. It uses AI for that. I think they use AI to write the script and then pass that script to an AI voice to narrate. But where do they get the images from? How do they get the video content?\n\n**59:57** From the review, when someone comes to review, they will have a video to review. Ok, so it's automatically generating an advertisement, right? Yes, for TikTok or similar platforms, summarizing user reviews. Sounds pretty creative. Yeah, I guess you guys have confirmed what Bảo mentioned, right?\n\n**01:00:07** Yeah, this one is about the caption, and it's mostly correct. You're right, I think the initial purpose of this use case was for recommendation. Before they had AI, they already had a hybrid recommendation model in place. Basically... I think with this new AI, they will likely replace the old model. One interesting point that wasn't mentioned much is business messaging. I think it’s based on, say, the top 50 reviews or top 50 interactions, how are the ratings, and if the reviews are good and the ratings are good, then the business messaging will also be good. But Yelp didn’t bring up this topic, and we can’t blame them for that.\n\n**01:00:51** Does anyone have any questions for Đạt? This is his first presentation. Đạt mentioned he’s working on adding more, probably for enterprise too. But I’ve seen Viettel, FPT, and VNG, and I’m still not sure how they are doing things. Đạt said some of FPT's coding tools are kind of lame.\n\n**01:01:41** Haha, is it just a continuation of a previous version? Yeah, it’s not great. It’s a bit rough and underdeveloped. Are we done with that? I guess so. Ok, Đạt. Today we’ve covered Yelp and Tech Linh, so maybe next week or the week after that, if time permits.\n\n**01:02:01** Let's do a demo real quick, Đạt. Could you demo something for us? What about a bot that can handle questions or understand short code from a developer’s perspective? Or something like an auditor checking the code quality? Could you demo the workflow or the bot you’re working on with that diff you mentioned? Please turn on the screen, Đạt.\n\n**01:03:01** So this is a project I’m working on for joining the AI Club. I’m pretty new at this, so if the project looks rough, please bear with me. The basic workflow is that I take a query and extract the URL of a repository. Here, I reused Tom’s scraper, but it didn’t fully meet my needs, so I created my own local scraper to fetch all the content from the repo. However, that takes too long and generates too much data. As of now, I haven't found a way to add it to the context unless I use knowledge retrieval. But to use knowledge retrieval, I have to prepare it in advance; I can’t select the repo directly.\n\n**01:05:45** Tom’s scraper doesn’t capture the content of the files, so I haven’t been able to draw a diagram yet. I might use it for the diagram later, I’ll test it out. This scraper only fetches the content from the root directory and some doc files. Those files might not answer Huy’s question accurately. Let me try it; I have my full setup ready offline. The content is already prepared, not online. This was generated directly, so it doesn’t have it either.\n\n**01:07:07** This scraper fetches the full content, so it’s more complete. Let me try asking questions like Huy’s or Hoàng’s. Let’s try BC chat and ask a question there. Do you have it? Ah no. Maybe the context is too large, and I haven’t figured out how to integrate it properly into the knowledge retrieval part.\n\n**01:09:19** The retrieval process is very optimized. This text file has tens of thousands of lines, tens of thousands! Let’s open it and see. Are you using a mini machine? Try switching to a more powerful machine to see if it runs better. The mini machine is a bit weak. Two million words... how is it even handling that? I think you’d need a dedicated server to run it efficiently. I’m curious how it's even running; we’re talking about 29U words, as we saw. Multiply that by 4, and the number is huge. The size... Let's follow up and see. Ok, so you’re working directly from the context, right? You’re querying like a typical query vb, rather than feeding all the data into the context.\n\n**01:10:57** Right, exactly. I’m not sure how the retrieval in this diffy system works. I don’t know if it’s retrieving the correct data or if it’s retrieving at all. I haven’t been able to trace it, but I have a tracing tool that I can test later to see how it works. But the idea is that if the retrieval works like this, it probably won’t give accurate results. You’re unsure about how much data it's running, right? It seems to only prefer 2-300 items, and that’s not enough data for these kinds of tasks. This requires at least several hundred data points. So yeah, this is still a work in progress.\n\n**01:11:35** Just joking, there’s always the AI Club at the company! Oh, so is this the full version, or is it the fixed one? If it’s fixed, demo it for us in the office, and try to leave it for me. OK, let me see. It still hasn’t built, so people are just watching for now. The build seems to have some issues, can everyone see the screen?\n\n**01:13:08** So, this week, as I mentioned last week, I will upload the sync.Map article. I think it's really useful, with details that give people using Go a general overview of maps. Regarding sync.Map, let's first go over the context. When writing maps, especially concurrent maps or concurrent operations, before version 1.16, it wouldn’t show any errors, but it wasn’t safe either. From version 1.16 onward, it throws an error like this. So to solve this issue, people usually write maps with a sync package, like using manual sync.RWMutex.\n\n**01:13:56** Besides that, there’s another option called sync.Map. Later, I’ll explain why this option exists and what its use case is. This sync.Map was created so that you don’t have to worry much about using mutexes to lock data for synchronization. You just use it. It’s as simple as this, and it’s friendly, just like using a map to check values. For example, when you load a key with a value, if it's available, it returns true; otherwise, it returns false, just like a regular map.\n\n**01:14:38** Additionally, it has several handy functions. For example, version 12.23 has `clear` to clear everything, `load` to get a value, `store` to update or store a key, and so on. Besides writing concurrently, when you range (loop) over a map, race conditions can also occur. However, with sync.Map’s range function, it handles that, so you don’t have to worry about it. It doesn’t behave like a typical range function. However, it doesn’t give you a fully consistent snapshot. When you first enter, the snapshot may not be updated. So, during this, you have to change your writing method, but at least it won’t error like this.\n\n**01:16:06** Now, let’s go over how it works. When you write and define the map, it’s structured with two maps. At this point, you might be thinking, “Wow, this sounds like it’s heavy on RAM and memory!” There’s a Read-Only map and a Dirty map. From this, you can infer that values, when written, will be updated in the Dirty map. It just keeps updating there, while the Read-Only map.\n\n**01:16:46** The Read-Only map will always be used when you're reading. Meanwhile, the writes will always be made to the Dirty map. As for the underlying flow, we’ll look at the chart in a moment to better understand it. Both of these maps have a common point: they both use a pointer called an entry. Pay attention to this part to make the flow easier to follow. For example, when you add a new entry, it will be added to the Dirty map, and both will point to this entry. This works as a flag that indicates the map has been changed. So, at this point, the Read-Only map is no longer the most up-to-date version. The system will know that the Dirty map is the one to read from.\n\n**01:17:27** This diagram shows that, for example, when you update a value, since it’s using a pointer underneath, you only need to update the pointer itself, not each value as you would in a traditional map. To achieve this, the system implements a mechanism that defines three states for the entry pointer. The first state is the **normal state**, meaning that the old values in the map are still intact and can be used, without any issues. The second state is **amended**, meaning that the entry has been modified. And the third state is the **delete state**, where an entry has been deleted from the map, but it hasn’t been completely removed. It’s still held in a transitional state, and the entry pointer is moved to a new position, but it hasn’t been fully removed yet.\n\n**01:18:59** The pointer `entry` is assigned to the `new entry`, but it hasn’t been removed yet. The `expired state` refers to complete deletion, like a hard delete, meaning the entry is completely removed from the map. To help visualize this, you can refer to this flow: for example, at the beginning, the map has a `key1` and `value1`, and at this point, the `Dirty map` has nothing, meaning nothing has been added or changed yet. Then, if you add a `key2`, it will be added to the `Dirty map`, and at this point, the map is marked as `amended` because a `flag` indicating `amended` is set here.\n\n**01:19:40** Afterward, when you delete a `key`, the map will be assigned a `new entry`, right? The same thing happens on the other side, as it is also assigned a `new entry`, similar to the previous diagram. In essence, you’re only updating the pointer without having to update the values themselves. Then, after completing the deletion, to promote the `Dirty map`, you must update it through the `Read Only map` so that the `Dirty map` returns to a `new state`, like resetting it to the original state.\n\n**01:20:18** Similarly, when adding a new `key3`, after the state has returned to the `new state`, and you add `key3`, the system identifies that the previous entry has been deleted entirely. This means that next time when it compares with the `Dirty map`, it knows that the `value1` has been deleted and no longer exists. At this point, the `Read Only map` will only contain `key2` and `key3`.\n\n**01:20:51** Due to this, `sync.Map` does not have a `len` function for users to utilize. This is because, if you used the `len` function here, it would not account for the actual values, as it would count even those that have been expired or deleted. As you can see, because `sync.Map` is structured this way, its use case is recommended for scenarios that require more reading (`read`) than writing (`write`). If you perform a lot of `write` or `delete` operations, just imagine the pointer being used continuously, and there’s even an issue reported by the Go team that this map is never garbage collected.\n\n**01:21:36** Later, the Go team confirmed that this `sync.Map` was designed primarily to support some of the internal Go Library processes. If you find it handy because of its user-friendly functions, you can use it, but for use cases that involve storing (`store`), updating, or deleting often, it’s not recommended, as it may slow down the system.\n\n**01:22:24** That’s about it. I’ve written some code based on this topic, and there’s a blogger who shares detailed posts on this subject, so you might want to follow them. Oh, what’s the topic, Phát? Show me the post again. Ah, `sync.Map`, right? `Sync.Map`. Does it differ from what you just passed in? What’s different? I think it does. The point is that I remember, depending on the community, people might have different use cases. For instance, if someone wants to implement something specific, they can adapt it as needed.\n\n**01:23:30** You can see that some people add an extra layer on top for their own use cases. For example, some want to implement generics on `sync.Map`. This is partly because of the linking issue I mentioned earlier, where the map isn’t garbage collected. That’s the problem. The Go team has already confirmed that this behavior is intentional, and they won’t fix it. They’re not going to change it, right? So now, what the community does is figure out how to work around it. They like how easy `sync.Map` is to use, with those nice functions, so they just add another layer to customize it further, making it usable for their specific needs.\n\n**01:24:07** Why are we discussing this old clip now? Oh, it’s just an insight for people to use. This use case can be applied to enterprise environments too. For example, in enterprise projects, if we use `map` and need concurrency, typically, people would write a custom `struct`, add a `mutex`, and then write everything themselves. But `sync.Map` is handy, as I showed earlier, with functions that follow certain standards. If you want to `load`, you have to call a specific function, and everything is structured that way, making it more reliable.\n\n**01:24:50** People usually write a custom `struct`, then add a `mutex`, and start writing all the necessary logic themselves. Meanwhile, `sync.Map` is really handy. As I showed you earlier, it has several functions that adhere to a specific standard. If you want to `load`, you must call a specific function. This structure ensures consistency. However, you still need to be aware of the trade-offs when using `sync.Map`, making sure to apply it correctly in the right context. Right, so you have to understand the proper workflow and `map` usage.\n\n**01:25:26** Phát: Yes, about `Map`. Ok, thanks, Phát. Now, let’s quickly get through a few things. Thành, I need about 10 more minutes from the team. It’ll take a bit more time because I’ve received a total of 11 submissions for the test. Not many have attached images, so some of you can review them. Our deadline is set for the 20th, which is next week. I think earlier, I mentioned the 27th, or was it 26th or 27th? Anyway, that's the deadline. So, please review everything this week and get the submissions ready. This test is important because the market is shifting significantly, and there’s a big change in the demand for software roles.\n\n**01:26:15** Of course, the demand is still there, but the volume has decreased. That's why I refer to it as a shift in the market demand, similar to what happened around 2014, where it’s like things are changing all over again. New technology is coming out, new opportunities, new markets, and emerging potentials. So, this test will be essential in helping us assess, especially when it comes to team culture. We’re taking this opportunity to evaluate the team, particularly to see how the research study team and the consulting team are becoming more distinct.\n\n**01:27:36** There’s a clear distinction between the two teams now. Like I mentioned in the post on Notion about two weeks ago, this distinction is becoming more pronounced. In the future, there will be more specific policies related to different benefits between these two teams. But for now, as you can see, OGIF is gradually becoming a session where we report on all the studies that the team has been reviewing and reporting back on. Some of those reports might be added later, and you can see that some team members have been picking up new knowledge and sharing it.\n\n**01:27:36** Gradually, it's becoming clearer that we want this differentiation to become more distinct over time. And there will be clear policies around this. So, those who enjoy diving deep into topics and taking them to the level of MVP, or applying them in actual projects, or going deeper into knowledge, they will get different benefits. Those who don't necessarily want to focus on study-related topics can continue working on projects as usual, but there will be other issues involved, which I've listed in the Notion link from two weeks ago. Everyone should review that link to understand the direction we're going in. This test is designed to assess how well you can keep up with new knowledge and how aligned you are with the culture during this time of significant changes.\n\n**01:29:20** Because of these goals, I’ll be the only one grading this test. None of the other team members will be grading. Everyone has to do it, and I’ve set the standard for this. So, the important thing is that everyone does the test themselves. Even if the quality isn’t the best, it’s fine. I’ll just give it a lower score, but as long as it’s completed, you’ll pass. The immediate outcome I see from this is that I’ll group the results into two clusters.\n\nThe Foundation team and the Lab team, they’re still the core teams we’ve had for the past eight or nine years. This is the main announcement. If you’ve finished your test and feel like you can improve based on what I’ve just said, the Foundation team and the Lab team will still be prioritized in various aspects. So, if you feel like you did the test carelessly, please take some time to do it thoroughly. Focus on two things: the culture aspect and the knowledge.\n\n**01:29:56** The immediate result you’ll see is that I’ll group the results into two clusters. The Foundation team and the Lab team will remain the core of our team from now until the next eight or nine years. That’s the main announcement. Out of these 11 submissions, if anyone feels they can improve after hearing what I’ve said, please focus on making it better, especially since the Foundation and Lab teams will be prioritized more in different areas. So, if you feel like you’ve done it hastily, take the time to refine it. Check two things: the culture aspect and the latest, hottest topic cluster, which right now is LLM.\n\nBut in reality, our team still covers many different areas. We still have people focusing on design, and others still working on Go, right? Blockchain might have moved a bit out of the spotlight, and the market is going sideways, but consulting still demands a lot of expertise in those areas.\n\n**01:31:46** Regarding mini apps for Telegram, they quickly clone them, and now the business logic (BL) and tech (TCH) approaches have shifted a bit from the early days. But for consulting, we can still use them as usual, or we can view them from a different angle, where they become a new asset class. As developers, we should look at how these affect our workflow and how we manage assets.\n\n**01:32:29** That’s the matter concerning the test. Pay attention to that. Second, as mentioned earlier, regarding team direction and numbers, I mentioned the Notion link I sent earlier. The Foundation team, which started over again, initially had just three people, and then gradually it grew to four, then five. We added Quan, Hiếu, and others. Initially, it was just three of us, but now the team is much stronger. With 40 people, all highly skilled, we’ll certainly move faster. That’s the general overview, so everyone should be aware of the current situation.\n\n**01:33:12** Third, Huy Nguyễn, once you’re done, next week please take a look at the ICY numbers. Earlier, you mentioned the numbers were starting to grow, so we’ll need to review and balance those out. For this task, Huy and Thành, please take charge and ensure it’s handled properly. Thành also has an additional task, which is to review benefits for the Lab team members and propose something. It could be considered as a payon, but it won’t go through the normal channels, as there will be a different mechanism for this.\n\n**01:33:52** But the Lab team members will have that, and everyone’s familiar with it. Lastly, regarding the LLM, in the current question list, there’s an important question about how to use LLM externally and how to adapt it. Emphasize that question, as it’s about knowledge discovery. The test question is not only about using it but about all the tools our team currently uses. When some people use them effectively and others less so, it creates a very clear spectrum, those who are weaker remain weaker, and those who are stronger stand out much more.\n\n**01:34:38** We want to raise the standard. We want to shorten that spectrum, to make it as compact as possible. Right now, the gap is too wide. Beyond using tools for discovery, this question also asks us to look at how our field of work can apply these tools to elevate our competencies and make us more productive. That’s the whole issue, so everyone should confirm whether what they’ve done is correct or not. It has a deeper meaning than it seems.\n\n**01:35:20** Lastly, to wrap up today’s session, Thành, for the next OGIF meetings, apart from diving deeper into use cases, there are things Tom has done related to building the structure of an LLM app. We could take that and analyze it. Let’s break it down and dive deeper into it.\n\n**01:35:56** Hopefully, everyone passes the test so we can all have a good time. Next week, there will be another test. Next week, Minh L., can you do a demo? Continue with the finite state machine, FSM. As part of our focus on foundational technologies like blockchain and AI, the key point is that engineers will have a different path forward. The goal is to understand how large systems operate. In the future, if you’re not the one handling data manipulation, AI will do that for us, we won’t need to design things ourselves or do junior-level tasks anymore.\n\n**01:37:35** The only way to become senior is to understand the issues and work on architecture. The finite state machine plays an important role, especially in scaling, something we’ve talked about a lot. Minh has read through it and understood the direction we’re aiming for. So, we need to do a comparison between the types of general servers it covers. State machine-based servers versus event-based servers. Then create a sample to show how it’s modeled, implemented using Erlang. Erlang already has the frameworks for it.\n\n**01:39:01** This topic will proceed when Minh Lưu is ready. If it’s not next week, it could be in two weeks. I suggest that the backend team and the senior team gather together, and if there’s anything, confirm it beforehand. This topic is critical for software analysis and design. It’s a very important session. Up until now, we’ve only talked about modeling and doing C4 diagrams, but Erlang is the language that goes deepest into this area. Most people don’t know it entirely. We don’t necessarily need to learn Erlang, but we can look at how they design and build systems to handle this area properly, as they already have frameworks available. We just need to plug them in and use them.\n\n**01:39:37** Speaking of which, October 20th is a Sunday, and Mỹ, Ngọc, and Giang have already posted about it. The ladies are going out on that day, and for those not in Saigon, the team representatives will wish everyone prosperity. It seems like wishing prosperity is the most appropriate thing to say in this situation. Any other wishes might not fit as well. Alright, so if anyone wants to join, register with Mỹ to book a table and plan accordingly.\n\n**01:41:19** Thành, in the upcoming meetings, structure things into talks. Then set the goal for that. Our team now has a Builder Club as well. I’ll look into how the team members who used to work on Super Bit and console are stabilizing things, and I’ll restructure afterward. This time, it seems like we’ve had a good rest. Alright, does anyone have any questions about the test? If not, we’ll wrap up here. Alright, goodbye everyone, see you next week. Thanks, Thành, and thanks to everyone.\n\n---\n","title":"OGIF Office Hours #28 - Golang sync.Map, Generative AI UX design patterns, Yelp's AI use cases, Design patterns in LLM application, and Dify github analyzer","short_title":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","description":"OGIF Office Hours #28 covered Go Weekly #16 by Phat on sync.Map concurrency, Nam's Product Commentary #4 on Generative AI UX design patterns, Dat Nguyen's presentation on Yelp's AI use cases including recommendation systems, Hoang's discussion on LLM application design patterns, and Cat's demonstration of a Dify-based Git repository analysis tool.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Mon Oct 21 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/28-20241018.md","slugArray":["updates","ogif","28-20241018"]},{"content":"\n### Topic highlights\n\n1. Nam's presentation on \"UX Guide to Prompt with AI\"\n   - Overview of current AI-human interaction trends\n   - Introduction to the \"Race\" (Role, Action, Context, Expectation) concept in AI prompting\n   - Discussion of new methods to improve AI UX:\n     - Context Through Rephrasing\n     - Implicit Referencing\n     - Continue Conversation\n     - Racing and AI Scoring\n     - System Prompting\n   - Focus on designing AI tools for better user experience beyond speed and accuracy\n\n2. Minh's presentation on computing the union of two finite automata\n   - Applications of finite state machines in programming\n   - Use of automata in input validation (e.g., regex for email, phone number checks)\n   - Application in event-driven systems and event buttons\n   - Demonstration using Go source code\n\n3. Phat's presentation on Go Weekly commentary\n   - Overview of recent developments in the Go programming language\n   - Discussion of notable changes and updates\n   - Insights into the Go community and ecosystem\n\n4. Lap's presentation on Frontend Report for September\n   - Overview of recent trends and developments in frontend technologies\n   - Discussion of notable frameworks, libraries, and tools\n   - Insights into the frontend community and ecosystem\n\n---\n\n**Vietnamese Transcript**\n\n**08:02** Có thông tin gì cần phổ biến không? Không thì chắc để phát lên trước, nay thấy nhiều bài quá, để ưu tiên cho mấy bạn mới. Ok, nay có tới năm bài.\n\n**09:13**  Xin mời anh em.  Anh em nào có topic thì lên sớm nhờ. Dạ, pha start trước rồi, chưa đến phần Thành, mời Nam.  Dạ, bài của em là \"User Experience AI.\" Cái bài này chị team làm đi, rồi Thành bạn đã chuẩn bị chưa? Hôm nay, Nam sẽ chia sẻ về đề tài có tên là \"UX Guide to  Prompt with AI.\"\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hq_u9GQdNMg?si=FdFGv418n1MC5Apq\u0026amp;start=608\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**10:20**\nThì em nói overview trước về tình hình hiện tại. Giao tiếp giữa con người và AI là chủ đề rất phổ biến hiện nay, và sự xuất hiện của LLM (Large Language Models) là công cụ hữu ích mà team mình đang muốn tìm hiểu. Bài hôm nay em sẽ dành cho ai quan tâm đến User Experience (UX) của AI, cụ thể hơn là cách các tool hiện đang thiết kế cho sự tương tác giữa người dùng và AI tốt hơn. Hiện nay có khá nhiều tool và platform ra đời, nhưng họ thường tập trung vào việc cải thiện tốc độ prompt và độ chính xác, thay vì chú trọng đến trải nghiệm người dùng.\n\n**11:08** Cái khái niệm \"Race\" (Role, Action, Context, Expectation) rất phổ biến trong việc prompt AI. Người dùng cần prompt theo cấu trúc này để AI có thể tạo ra output chính xác nhất. Tuy nhiên, không phải trường hợp nào cũng áp dụng được \"Race.\" Có nhiều công ty đã phát triển những phương pháp mới để cải thiện UX của AI, giúp tương tác giữa người dùng và AI mượt mà hơn.\n\n**12:04** Phương pháp đầu tiên là \"Context Through Rephrasing.\" Phương pháp này giúp AI truy vấn lại ngữ cảnh của câu hỏi trước, để trả lời câu hỏi tiếp theo một cách liên mạch, không cần từ đầu phải có cấu trúc prom chuẩn chỉnh. Ví dụ: Câu hỏi đầu tiên là “Who is the wife of Superman?” Tiếp theo, câu hỏi “When did they get married?” AI sẽ hiểu ngữ cảnh và liên kết đúng. Nhưng nếu không có ngữ cảnh phù hợp, như câu “What day did Titanic sink?”, AI sẽ không thể đưa ra kết quả đúng.\n\n**12:50** Tiếp theo là \"Implicit Referencing,\" ví dụ khi hỏi về số tầng của một building, AI sẽ tự động assume đó là một tòa nhà nổi tiếng như \"Willis Tower in Chicago.\" Nếu hỏi “What day?” mà không có ngữ cảnh liên quan, AI không thể trả lời chính xác. Các câu hỏi cần có sự liên kết chặt chẽ với nhau để AI có thể trả lời tốt hơn, và điều này cũng áp dụng cho \"Context Through Rephrasing.\"\n\n**14:19** Một khái niệm tương tự là \"Continue Conversation,\" như trong Google Assistant. Các câu hỏi được nối tiếp một cách tự nhiên, và mỗi câu hỏi mới sẽ liên quan đến những câu hỏi trước đó để tạo ra một chuỗi hội thoại liên tục.\n\n**15:03** Phương pháp tiếp theo là \"Racing and AI Scoring.\" Google Assistant cũng áp dụng phương pháp này. Nó cung cấp nhiều tùy chọn dựa trên các ngữ cảnh khác nhau, giúp người dùng có kết quả tốt hơn. AI cũng có thể tự động học từ những lựa chọn của người dùng để cải thiện khả năng tương tác. Ví dụ, khi AI không rõ ngữ cảnh, nó sẽ đưa ra các tùy chọn cho người dùng chọn.\n\n**16:03** Cuối cùng là \"System Prompting.\" Lý thuyết này định hướng cho AI hoạt động theo ngữ cảnh và mục tiêu người dùng đặt ra. Nó giúp AI tạo ra output chính xác mà không cần tuân theo một chuẩn prompt cố định. Ví dụ, cùng một câu hỏi “Plan for releasing a software product,” Chat GPT có thể đưa ra các khái niệm chung chung, trong khi GPT mini sẽ đưa ra các câu hỏi chi tiết hơn để giúp người dùng prompt tiếp và đạt kết quả chính xác hơn.\n\n**17:45** Bài hôm nay sẽ tập trung vào việc thiết kế tool AI sao cho user experience tốt hơn, không chỉ dựa vào tốc độ hay độ chính xác, mà còn chú trọng đến sự tương tác và trải nghiệm tổng thể của người dùng.\n\n**18:50** Tóm tắt lại bài này cho các bạn, đặc biệt là các bạn designer, bài của Nam có hai khía cạnh chính. Thứ nhất, nó giải thích cấu trúc của \"Race\" và cách áp dụng nó. Thứ hai, nó đưa ra một framework thiết kế tool AI tập trung vào việc prompt sao cho tương tác giữa AI và người dùng tốt hơn. Cấu trúc \"Race\" này gồm Role, Action, Context, và Expectation, và nó giúp cải thiện UX của AI.\n\n**19:20** Giải thích cái cấu trúc sơ qua của chuyện là cái Race nó như thế nào, có những thể loại Race như thế nào là những phần nãy giờ Nam nói. Cái thứ hai đó là cái layer về chuyện xây dựng (build) một ứng dụng tập trung vào chuyện prompting, thì cấu trúc đó gắn vào ra làm sao. Khi viết một cái Race, bạn sẽ phải nói rõ Role, Action, Context, và Expectation.\n\n**20:03** Race được mô tả rất rõ ràng: Role là gì, Action là gì, mọi thứ được mô tả theo cái Expect là gì, Task là gì. Tóm lại, chữ R thì cơ bản nhất là các designer sẽ nhìn và hiểu cấu trúc của một câu Race. Nó sẽ có cấu trúc nhất định, dựa trên đó mà đưa ra kết quả tiêu chuẩn. Input là như vậy, và output sẽ nhận được kết quả tương ứng.\n\n**20:47** Phần thứ hai, phần cuối của bài này, sẽ nói về chuyện khi mình đã hiểu cấu trúc của một cái prompt rồi, và hiểu luôn cách để prompt sao cho chuẩn xác. Khi thiết kế, cần phải chú ý những gì? Phần này sẽ là phần mở vì bài này giống như là bài 101 cho các bạn designer để nhìn qua và hiểu cơ bản.\n\n**21:30** Nam đã nói khá nhiều về cái chữ R, nên đôi lúc có thể mọi người sẽ hiểu lầm là bài này đang giải thích chi tiết lại cái đó. Nhưng thực ra bài này là giới thiệu về prompting cho UX designer. Ok, câu hỏi sẽ là, có ai có thắc mắc gì không? Bài này khá cơ bản, team mình xài nhiều rồi, demo cũng nhiều rồi. Có một phần cần lưu ý, bài này đặc biệt hơn ở chỗ giới thiệu về system prompting, mà các bài khác không có.\n\n**22:31** Bài này giới thiệu cái system prompting mà các bài hướng dẫn khác thường không nhắc tới. Các bài viết cho người dùng cuối (end user) thường không đề cập đến điều này nhiều. Bài này nhắc tới system prompting vì nó viết dưới góc nhìn của designer – một người trong đội build. System prompting sẽ khác so với prompting thông thường, vì nó điều khiển cách AI hoạt động theo mục tiêu cụ thể của hệ thống.\n\n**23:06** Cấu trúc của system prompting khác so với các loại R thông thường mà mọi người thường thấy khi đọc research. Thường thì các bạn chỉ thấy nói về 200 kiểu R khác nhau, nhưng không có góc độ là viết cho người build app. Bài này dành cho designer, không phải là end user, mà là người đứng giữa, để kết nối các phần lại với nhau.\n\n**23:48** Bài này khác với những bài viết dành cho engineer vì nó không chỉ giới thiệu về tooling để xây dựng prompts, mà còn nói về việc kết hợp các kiểu R lại với nhau. Đây là bài ở mức độ trung gian, phù hợp cho các bạn làm designer, có vai trò đứng giữa, không trực tiếp build nhưng cũng không phải người dùng cuối. Nó giúp kết nối hai phần này với nhau.\n\n**24:33** Ok, cảm ơn Nam. Tuần sau chắc sẽ scope lại bài theo content cho mọi người dễ hiểu hơn. Còn đi sâu vào chi tiết thì sẽ hơi khó để mọi người nắm bắt hết nội dung. Cảm ơn em nhé. Mời bạn tiếp theo. Để xem thử, không xem màn hình được, à vào lại rồi.\n\n**25:41** Bài của em hôm nay là về một vấn đề nhỏ trong kỹ thuật lập trình, đó là cách compute một tổ hợp (Union) của hai cái finite automata hay còn gọi là finite state machine. Em sẽ demo nó trên một cái source code Go. Chủ đề này hôm nay sẽ có một số mục chính. Trước tiên sẽ giải thích các ứng dụng của automata để mọi người dễ hình dung trước, rồi sẽ đi vào chi tiết.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hq_u9GQdNMg?si=nEFRO7pOhdBxjouy\u0026amp;start=1617\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**26:41** Ứng dụng mà của finite state machine mà mọi người thường thấy nhất là nó sẽ dùng trong việc có một cái button và một cái input, thì mình sẽ muốn kiểm tra xem input này đưa vào cái button đó nó sẽ match hay là nó fail. Nó đơn giản chỉ là như vậy thôi. Cái dễ thấy nhất thường sẽ là dùng regex để kiểm tra xem một đoạn text có phải là một email hoặc là số điện thoại, hoặc là số nhà hay không. Mình sẽ có một cái button giống như vậy, và mình sẽ đưa một đoạn text vào cho nó kiểm tra xem nó có match với điều kiện đó không.\n\nNgoài regex, dễ thấy nhất, thì ở trong những cái hệ thống event-driven nó sẽ có cái gọi là event button. Mọi người sẽ define một cái button dưới dạng một cái state machine, sau đó, mỗi event sẽ là một state, nó sẽ đi qua event button này và sẽ được filter qua xem nó có match với cái event đó hay không. Nếu match thì nó sẽ đi qua và tới cái state tiếp theo để nó làm tiếp, còn nếu không thì nó sẽ fail và không đi qua được.\n\n**27:27** Đây là một ví dụ của nó: Ví dụ, mình có một cái event bus, tất cả các event sẽ đi qua cái event bus này và sẽ được filter qua các rule. Nếu một event thỏa mãn điều kiện của rule thì nó sẽ đi qua để tiếp tục xử lý. Điều này thường thấy nhất trong các hệ thống cloud hiện tại, họ dùng rất nhiều cái hệ thống này để quản lý các event và filter chúng qua những cái rule như vậy. Mọi người có thể thấy trong những cái hệ thống lớn như của Amazon chẳng hạn, event của họ sẽ đi qua một chuỗi các rule như thế.\n\n**28:05** Ví dụ là mình có một cái button, và tất cả những cái item nào có cái field image, trong cái field image đó có một cái object là width với giá trị là 800, thì nó sẽ được pass qua hết. Và ở dưới thì nó cũng sẽ thêm một vài cái rule nữa, chẳng hạn như những cái field khác nhau mà mình thêm vào cho cái button đó. Đó là một ví dụ về việc finite state machine và event button hoạt động như thế nào. Khi một event được đưa vào hệ thống, nó sẽ đi qua các rule, và nếu thỏa mãn các điều kiện thì nó sẽ được pass qua để tiếp tục các bước xử lý tiếp theo.\n\n**28:56** Đây là một ví dụ cụ thể về hệ thống event của Amazon, nơi mà các event của họ sẽ đi qua một chuỗi các rule để được filter và xử lý. Hầu hết các hệ thống cloud hiện nay đều sử dụng những mẫu button tương tự để quản lý và xử lý các event một cách có tổ chức và hiệu quả.\n\n**29:37** Trong thực tế, bây giờ mình sẽ đi ngược lại một chút về finite automata (f automata) dưới góc độ toán học, nó là cái gì. Thực chất, nó đơn giản là một cái machine trong đó có một tập hợp những states. Để đi từ một state này tới một state tiếp theo, nó cần phải đi qua một transition. Ví dụ, để từ start state tới một end state, nó sẽ luôn cần phải có một điểm bắt đầu gọi là start state và một điểm kết thúc gọi là end state. Vì vậy, nó được gọi là finite state machine bởi vì nó luôn có điểm bắt đầu và điểm kết thúc.\n\n**30:21** Ở giữa, sẽ có một tập hợp các transitions và states để di chuyển từ điểm bắt đầu tới điểm kết thúc. Một input symbol là cái để đưa vào một state để nó di chuyển tới một state khác, và trong thực tế, input symbol thường sẽ là một ký tự. Tí nữa mình sẽ bàn chi tiết về vấn đề này. Accepting state là trạng thái mà khi input của nó được chấp nhận, thì nó sẽ được di chuyển tới một state khác thông qua một transition. Nếu như không chấp nhận thì nó sẽ không di chuyển tới đâu cả, coi như transition đó không dẫn tới một state nào.\n\n**31:04** Có hai loại finite automata, đó là deterministic finite automata (DFA) và nondeterministic finite automata (NFA). Điểm khác biệt duy nhất ở đây là với DFA, mỗi state sẽ có một input symbol duy nhất dẫn tới một transition tới một state khác. Còn với NFA, nó có thể có nhiều transitions cho cùng một state, thậm chí có thể không có transition nào hết. Điều này chỉ khác nhau về cách thể hiện con đường từ điểm bắt đầu tới điểm kết thúc, chứ thật ra một cái finite state machine đều có thể được biểu diễn dưới dạng DFA hoặc NFA, chỉ khác nhau cách biểu diễn thôi.\n\n**31:51** Về phần Union của hai cái finite automata, thì nó sẽ là tổ hợp của tất cả các states và transitions của hai cái finite automata cộng lại. Đặc điểm của nó là nếu ví dụ cái event A pass qua được cái finite automaton FA1 và event B pass qua cái finite automaton FA2, thì tổ hợp của hai cái này, Union của hai cái này, nó phải đảm bảo là cả event A và event B đều pass qua được.\n\n**32:45** Tại sao chúng ta cần phải tính toán Union của hai cái finite automata? Trong thực tế, ví dụ như khi dùng một cái event button, ta sẽ không dùng một cái button mà sẽ dùng nhiều button kết hợp lại với nhau. Ví dụ ở trong hình, chúng ta có thể define rất nhiều button, và trong event của mình có thể chứa nhiều đoạn thông tin match với những button này. Khi muốn kết hợp những button đó lại với nhau, chúng ta sẽ cần tính toán tổ hợp của tất cả chúng lại.\n\n**33:24** Nhưng mình sẽ không tính tất cả một lần, mà sẽ tính từng cái một, từng cặp một, ví dụ như cặp A và cặp B trước, sau đó lấy tổ hợp của A và B, rồi lại tính với cặp C. Chỉ cần tính toán hai button một lúc thôi, đây là mức cơ bản nhất để tính ra được tổ hợp của tất cả các button này.\n\nĐể tính toán Union của hai cái finite automata, theo lý thuyết thì chúng ta sẽ phải tính hết tất cả các states và transitions của hai automata đó. Ví dụ mình có hai cái button A và B, trong mỗi button sẽ có rất nhiều states, ví dụ như từ A1 đến Ax và từ B1 đến Bx. Khi tính toán theo kiểu lý thuyết, mình sẽ phải tính toán tất cả các trường hợp kết hợp giữa hai button này, ví dụ A1-B1, A2-B2, A1-B2, A2-B1, có rất nhiều trường hợp.\n\n**34:07** Trong thực tế mình chỉ quan tâm đến việc khi đưa event vào button thì mình muốn biết nó có match hay không thôi. Mình chỉ cần quan tâm xem event đó sau khi đưa vào button, nó có thể đi đến state cuối hay không. Việc tính toán tất cả các states trong Union sẽ rất lãng phí và không cần thiết. Do đó, cách tiếp cận thực tế là mình sẽ có hai finite automata với các states từ A1 tới Ax và từ B1 tới Bx. Khi tính tổ hợp của chúng, mình sẽ kiểm tra xem state đó có dẫn tới một transition nào trong A hoặc B hay không. Nếu không, mình sẽ bỏ qua.\n\n**35:40** Ví dụ như nếu state chỉ dẫn tới A1 và không có transition nào dẫn tới B, thì mình sẽ chỉ tính toán cho nhánh của A, bỏ qua nhánh của B. Ngược lại, cũng tương tự với B. Trường hợp cuối cùng là nếu A1 di chuyển tới Ax và B1 cũng di chuyển tới Bx, thì chúng ta sẽ tạo ra một state mới là Ax-Bx. Lúc này, mình sẽ bắt đầu đệ quy lại và tiếp tục tính toán từ bước đầu tiên này. Ví dụ, mình có A1-B1, và sau đó có A2-B2, thì mình sẽ lặp lại bốn bước này để tính toán tiếp.\n\n**36:15** Khi đó, số lượng states mà mình có thể bỏ qua và không cần tính hoặc không cần lưu trữ sẽ giảm đi rất nhiều so với phương pháp lý thuyết ở trên.\n\nVề input symbol, như trong ví dụ về event button hoặc regex, trong thực tế, các button và input mà chúng ta truyền vào chương trình đều sẽ ở dạng ký tự. Ký tự ở đây thường sẽ là những cái.\n\n**37:04** Ký tự dưới dạng UTF-8, tức là những ký tự đơn giản ví dụ như từ a tới z hoặc từ 0 đến 9. Nó sẽ nằm gọn trong UTF-8, bao gồm 244 ký tự. Mặc dù UTF-8 xài 8 bits (2^8 - 1 = 256), nhưng những bits từ 245 đến 255 thì dư ra, nên mình không xài, chỉ có 244 bits đầu tiên thôi. Rồi, đây là phần detail implementation trong code. Đây là cái workflow cơ bản mà mình sẽ thực hiện. Không biết có zoom được không? Zoom hình hồi nãy thử xem, không thấy chữ gì hết. Từ từ, để xem lại.\n\n**38:21** Rồi, cái FA là gì? Finite automata hả? Đúng rồi, automata. Cái hàm này sẽ merge hai cái automata lại với nhau. Đầu tiên nó sẽ tạo ra một cái key để mình đánh dấu lại, để không phải đi lại những cái state mà mình đã xử lý rồi. Sau đó, nó sẽ check thử cái key này, nếu không trùng, thì nó bắt đầu làm việc. Nó sẽ tạo một cái combined state rỗng, bao gồm tất cả các transitions của hai cái finite automata đó. Rồi nó sẽ tiếp tục merge từng cái finite automata lại với nhau, từng cái một. Kéo lên, mình đọc chưa kịp cái đó.\n\n**39:33** Trong quá trình implementation, sẽ có một số điểm cần chỉnh sửa trong cấu trúc dữ liệu để lưu lại những states đó. Đề tài này của em là em đang chạy cho cái example nào vậy? Hình này đang chạy sample nào? Đây chắc để em show code thì dễ nhìn hơn, phải nhìn vào đề bài mới biết đang code cho bài nào. Rồi, sau cái ví dụ này, sẽ có nhiều câu hỏi cho mình. Nhưng bài này chỉ cần Phúc và Tuấn hiểu là được, mọi người hiểu code rõ không?\n\nVí dụ hồi nãy về bài toán event button, ở đây mình sẽ define một cái button chẳng hạn. Cái button em define ra dưới dạng T, và đây là cái event. Khi mình chạy đoạn code này, nó chỉ là một phần logic thôi, ngoài ra còn nhiều đoạn code khác nữa. Nhưng khi chạy cái đó, kết quả mình mong đợi là nó sẽ kiểm tra xem event này có match với button này không. Nó sẽ trả về kết quả là đúng hay không đúng, event có match với button không.\n\n**41:13** Đúng rồi, đó là bài toán mà vấn đề này đang giải quyết. Đây là đoạn code đơn giản, em sẽ chạy ra kết quả, chỉ kiểm tra xem nó có match với button này hay không thôi. Ví dụ ở đây, button này có transition với từ \"user_register,\" chẳng hạn. Nếu em sửa lại điều gì đó khác, thì nó sẽ không match. Còn nếu event thỏa mãn điều kiện thì nó sẽ match, kiểu như vậy.\n\n**42:09** Em sẽ đi thẳng tới phần logic chính. Nó sẽ là cái hàm để merge hai cái finite state machine lại với nhau. Mỗi cái finite automata này đại diện cho một cái button. Cứ tưởng tượng mình có nhiều cái button. Ở đây mới chỉ có một cái button thôi, ví dụ em tách cái này thành hai button. Hàm của mình có nhiệm vụ merge hai cái button đó lại để tạo ra một cái button tổng.\n\n**43:31** Như đã nói, để tránh tính toán quá nhiều, trước tiên mình sẽ giải thích các tham số truyền vào hàm. Hai cái FAState là hai cái structure đại diện cho hai cái button hồi nãy của mình. Mỗi cái structure này bao gồm một small table để đại diện cho một dãy những input symbols và những transitions tương ứng. Epsilon là state mà tự đưa lại chính vị trí của nó, mình sẽ bỏ qua nó tạm thời. Ở đây mình chỉ quan tâm tới hai cái states này thôi. Đơn vị nhỏ nhất ở đây mà mình muốn làm input là file, bởi vì một ký tự sẽ được thể hiện dưới dạng UTF-8 và bao gồm 244 bits. Bây giờ mình sẽ loop qua từng bit trong từng ký tự này để so sánh.\n\n**44:20** Ví dụ như trong ví dụ này, mình đưa một cái event vào, sau khi mình compute xong hai cái button này, nó sẽ bắt đầu từ từng ký tự. Ví dụ nó sẽ đi từ ký tự dấu ngoặc, rồi sẽ kiểm tra xem có transition nào tới cái \"user\" không. Nếu không có, nó sẽ skip phần ID, vì nó không match. Bạn này đang đi vào chi tiết cách so sánh từng ký tự.\n\n**45:02** Skip đoạn này đi. Chỉ cần xem lợi ích và ý tưởng của việc cài đặt thôi, còn so sánh chi tiết thì không cần thiết ở đây. Lợi ích của phương pháp này đơn giản là nó giúp thuật toán không phải tính toán lại nhiều lần. Thuật toán khá đơn giản, chỉ là compare, như mình đã phân tích. Nếu một ký tự không có dẫn tới một transition nào, mình sẽ bỏ qua. Hoặc nếu nó chỉ dẫn tới một nhánh, mà nhánh đó không có transition nào, thì mình cũng bỏ qua luôn.\n\n**46:34** Ví dụ nếu state A1 của mình đi đến state B, mà state B không tồn tại, thì mình sẽ bỏ qua nhánh đó. Ngược lại, nếu state này đã tồn tại trong map rồi, mình cũng sẽ bỏ qua. Chỉ khi nào thỏa hết các điều kiện thì mình mới bắt đầu tính toán và loop qua từng state trong finite automata. Sau khi loop qua từng bước với mỗi state tương ứng trong B, mình sẽ ra được cái state tổng và gán nó vào, sau đó return ra kết quả.\n\n**47:17** Mọi người có hiểu không? Hỏi Minh xem. Minh đi coi cái này, anh kêu Minh quăng cái link lên. Bữa anh em nghe có hiểu không? Quan trọng là coi lại cái diagram đầu tiên, vì mình sợ mọi người không hiểu. Diagram này lâu lắm rồi, hơn cả tháng rồi đúng không? Đúng rồi, diagram này, mọi người có hiểu không?\n\n**49:11** Hệ thống như event bus của Amazon cần phải merge có khi lên đến hàng triệu cái button, nên sẽ có một số chi tiết trong phần implementation để làm những việc này nhanh hơn. Rồi qua một hình khác nữa đi, hình kế tiếp, hình mà nó rẽ hai nhánh, dùng hình đó để nói dễ hơn. Hình rẽ hai nhánh, đúng rồi. Cái hình rẽ hai nhánh, cái nào mà nên? Ừ chắc dùng giữ hình này đi, mấy anh kia có hiểu không?\n\n**50:11** Nói luôn cho rõ, lỡ nói bài này, mọi người nắm thì nó sẽ ok hơn. Hoàng có hiểu không? Em có hiểu cái này làm gì không? À, Phúc hỏi là có dùng bit operator không? Này thì chưa, chưa đến mức đó, chỉ là chi tiết so sánh rồi, không liên quan tới phần đó đâu. Tuấn, Minh ơi, Vincent đâu rồi, mọi người có hiểu bài này không? Không hiểu hả? Bài này nó ẩn tới ba lớp trong đó của phần Minh nói nha. Để mình clear vài thứ cho anh em đỡ lẫn lộn.\n\n**51:03** Đầu tiên là finite automata, tức là cái machine trạng thái, giống như cái state transition diagram mà mấy anh em hay vẽ. Nhớ không? Nó có những trạng thái (states) và các nút (nodes) đại diện cho trạng thái đó. Cái thứ hai là các điều kiện (input symbols) để di chuyển từ trạng thái này qua trạng thái khác, gọi là transition. Đọc cho dễ hiểu, dễ nhớ nhé. Nó giống như cái state transition diagram, đó là cái đầu tiên.\n\n**51:47** Cái thứ hai, cái mà Minh vừa show ra, là hai loại finite automata này. Tại sao lại show ra hai loại này? Vì có những trạng thái rất đơn giản, nó chỉ đi một chiều và không quay ngược lại được. Ví dụ, giờ ăn trưa chẳng hạn. Khi mình đi ra Hà Đô để ăn trưa, mình có thể đi ăn phở, đi bộ ra quán phở, ăn xong rồi đi về. Đó là hoàn thành một cái finite state trạng thái hũ hạn. Nó không có quay đầu lại được, đó là finite automata.\n\n**53:10** Một ví dụ khác, có một trạng thái finite automata mới là đi ăn trưa, nhưng lần này đi xuống siêu thị mua cơm, trả tiền rồi đi về. Vẫn là một cái finite automata, nhưng nó có các states khác với cái trước.\n\n**53:57** Câu chuyện là làm sao tính toán tổ hợp (union) của hai cái finite automata này. Giả sử có hai trạng thái tổ hợp, một là đi ăn phở, hai là đi mua cơm siêu thị. Ta cần build một cái union cho hai trạng thái đó, giống như Minh đã nói về việc merge hai cái state machines lại. Ở đây, chúng ta sẽ có hai lựa chọn: một là đi ăn phở, hai là đi siêu thị mua cơm. Ý tưởng là làm sao tổ hợp tất cả các lựa chọn có thể có giữa hai hệ thống states khác nhau.\n\n**54:30** Mình tính tổ hợp của nó thì mình gộp lại thôi. Trong cái trường hợp hồi nãy, ví dụ đó, mình sẽ có bao nhiêu options? Một người đi ăn phở, người kia đi siêu thị, đúng không? Ví dụ cũng là tính tổ hợp, nhưng đây là tổ hợp khác nha. Bài toán là có hai cái hệ thống finite automata (FA), và mình yêu cầu là tính union của chúng lại. Sau đó, mình kiểm tra như thế nào? Sẽ có hai phần: phần thứ nhất là đi theo luồng ban đầu – đi ăn phở, và phần thứ hai là đi theo luồng siêu thị. Một trường hợp khác là khi chập hai hệ điều kiện lại với nhau, nó sẽ sinh ra một hệ phụ nữa. Khi đó, mình lại phải tiếp tục tính toán và compute thêm.\n\n**55:17** Rồi, idea cơ bản của việc làm union của nhiều finite automata là vậy. Nó giống như phần mà Minh đã show lúc nãy, Minh có show cái source code. Rồi, coi lại cái hàm lúc nãy đi. Được rồi, ở đây, hàm của Minh có cái hàm để merge tất cả các states lại. Tức là, như đây, mình giả lập là có hai cái hệ states thôi đúng không, của hai cái finite automata khác nhau. Sau đó mình gộp lại thành một hệ chung, rồi ra được cái bảng lớn. Trên cái bảng lớn đó, mình mới tiếp tục tính toán với từng điều kiện.\n\n**56:04** Giờ mình đã build xong cái bảng lớn, một cái array lớn là danh sách tổng các điều kiện nằm ngay đó. Bây giờ mình sẽ bắt đầu tính toán. Khi có một điều kiện mới đưa vào, nó sẽ bắt đầu bằng cách kiểm tra tất cả các điều kiện của hệ đầu tiên. Nếu không được, thì nó kiểm tra các điều kiện của hệ thứ hai. Nếu vẫn không match, thì nó sẽ kiểm tra tiếp điều kiện của hệ tổ hợp giữa hai hệ trước đó. Logic tính toán cơ bản là như vậy.\n\n**57:02** Cái khó của bài này, nếu anh em cảm thấy hơi lẫn lộn, là do quá trình mô hình hóa từ toán học sang lập trình. Cho xem lại cái hàm và cái hình lúc nãy. Hình này là để giải thích tại sao trong bài regular expressions, người ta lại mention điều đó. Khi mình check trong cái dấu ngoặc vuông trong regular expressions, mình phải đi qua bao nhiêu điều kiện trong đó. Vì có nhiều options như vậy, mỗi option lại được mô hình hóa thành toán để dễ xử lý. Mỗi điều kiện là một cái finite automata, và chúng ta tính union của các điều kiện này.\n\n**58:38** Mô hình hóa toán logic thành lập trình là quá trình tính toán với nhiều hệ khác nhau. Mình phải tính toán xem điều kiện union của các hệ 1, 2, 3, 4... Nếu không có điều kiện nào, thì mình phải tính tiếp hệ giao của từng cặp điều kiện. Bài toán này là như vậy. Nếu anh em muốn tìm hiểu thêm, đây là một bài quan trọng vì nó giúp cho hiểu cách mà mình làm trong các hệ thống logic lớn.\n\n**59:21** Chắc bữa trước đưa cho Minh coi rồi. Tại vì điều này quan trọng, bởi nó liên quan tới việc làm hàm cộng trong logic. So sánh phổ biến nhất là login authentication, như bài log in, nó sẽ dính tới bài này. Trong quá trình log in, sẽ có nhiều điều kiện, ví dụ như nó pass bằng 2FA, hoặc email, hoặc SMS, hoặc password. Mỗi thứ này có thể mô hình hóa thành một cái finite automata riêng, và khi mình compute, mình gom tất cả lại.\n\n**01:00:00** Ví dụ như có yêu cầu là người dùng phải có vừa face scan vừa có QR code trên app để đăng nhập. Đó là một điều kiện tổ hợp (giao), chứ không đơn giản là một điều kiện if như bình thường. Trong mô hình toán học, ta sẽ dễ dàng xử lý hơn vì nó sẽ có tính chất đệ quy để tính toán các tổ hợp logic phức tạp. Cái quan trọng nhất là làm sao để tính được tổ hợp này mà không phải tính lại nhiều lần. Output sẽ là success hoặc failure. Nhưng đối với các bạn junior, khi thêm một case mới mà không có tư duy mô hình hóa, họ sẽ làm rất nhiều if statements lộn xộn, gây khó khăn khi bảo trì code.\n\n**01:01:29** Khi thêm một feature mới mà không có tư duy về mô hình hóa, code sẽ trở nên rối rắm và không hiệu quả. Họ sẽ phải sửa lại nhiều lần, gây ra nhiều lỗi và mất nhiều thời gian hơn để kiểm thử và sửa lỗi. Đó là lý do tại sao bài toán này quan trọng, vì nó ảnh hưởng tới việc thiết kế hệ thống, đặc biệt là các hệ thống như login authentication, nơi mà việc tổ hợp nhiều điều kiện là rất phổ biến.\n\n**01:02:23** Còn câu hỏi nào không? Không có hả? Minh, em có hiểu hết không? Ok, chắc hết giờ rồi. Thành ơi, còn bài nào nữa không? Chắc còn hai bài nữa, tranh thủ làm nốt thôi. Để em xem nào. Mọi người thấy màn hình không? Ok, tuần này em chỉ biết được hai bài, còn một bài nữa nhưng nó dài quá, chắc hẹn tuần sau. Bài đó cần chi tiết hơn về cách sử dụng.\n\n**01:03:36** Bài tiếp theo là về Go và cách embed file. Go embed là gì? Nó cho phép mình embed một cái file trực tiếp vào trong binary. Điều này giúp mình giảm thiểu việc handle các external files. Cách sử dụng là khi mình embed một file vào binary, quá trình handle sẽ trở nên đơn giản hơn. Nhưng có một hạn chế, đó là nếu file quá lớn, thì binary của mình sẽ phình to ra. Nên cần phải cẩn thận khi sử dụng.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hq_u9GQdNMg?si=Jiktpv6YYMbiOzp8\u0026amp;start=3793\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:04:15** Cách sử dụng như thế này: mình chỉ việc import rồi sử dụng như bình thường. Ví dụ, mình có một cái file message, mình embed file đó vào, rồi có thể truy cập file đó trực tiếp trong binary. Đối với nhiều file, mình có thể thêm ký tự sau như thế này. Sau đó, mình dùng biến đã embed để read file hoặc access như một file bình thường. Hoặc mình có thể embed nguyên một thư mục Directory luôn.\n\n**01:04:57** Thường thì chúng ta muốn embed cái static file như file ảnh, HTML, hoặc cái gì đó kiểu vậy. Về cái limitation mình nói trước đó, cái thứ hai là về reflect một cái package. Bài này thì nội dung cũng không mới. Thật ra cũng không mới nếu như mà em xài Go và có đọc trước bài của bác R rồi. Em sẽ nói qua luôn. Context của ông tác giả này cũng giống như mình thôi. Ví dụ như ổng đang xài một cái tool, một cái codebase nào đó, xong ổng gặp vấn đề về reflect và viết lại bài này. Thì nhìn chung, reflect có ba phần cần chú ý.\n\n**01:05:49** Ba phần quan trọng là: từ interface value cho đến reflection object, và ngược lại. Phần cuối cùng là khi muốn modify thì những cái value đó phải settable – nghĩa là phải được export, tức là phải viết trên capitalized value. Interface value là gì? Reflection object là gì? Interface value là mỗi hàm mà mình xài trong package reflect, nó luôn được hiểu là một interface{} rộng, cho nên nó là giá trị interface. Còn cái giá trị này là reflection object, và ngược lại.\n\n**01:06:36** Về chiều đi: ValueOf sẽ trả về một reflection object. Còn chiều ngược lại: từ đây, mình có thể dùng method là .Interface() để trả ngược lại giá trị ban đầu. Bên Go thì hiện tại nó đã mặc định sẵn rồi. Nếu muốn cập nhật (update) cái gì đó, mình cần tìm cái settable. Reflect có method này để mình có thể dùng và cập nhật được.\n\n**01:07:12** Còn một ý nữa là bên bài viết đó cũng đã nói rồi – nên tránh dùng reflect trừ trường hợp bất khả kháng. Vì nếu mình dùng các hàm như FieldByName, nếu input không được kiểm soát tốt, nó có thể dẫn đến panic hoặc crash ngay lập tức. Nên chỉ dùng khi thực sự cần thiết thôi, trong những trường hợp bí bách.\n\n**01:07:53** Còn một bài khác nữa mà em nói là dài, nói về map. Bài này so sánh giữa việc xài map bình thường với khi cần hỗ trợ concurrency, thì mình cần dùng locking strategy. Có thể xài mutex hay cái gì đó, hoặc lock khác. Bên package sync có hỗ trợ một cái sync.Map. Bài đó sẽ so sánh giữa hai cách tiếp cận này. Thực sự thì không có cái nào hơn cái nào, mà tuỳ vào use case mà xài.\n\n**01:08:35** Rồi chắc tới phần sau. Phát dạ, đơn giản thôi. Minh Trần đang hỏi về logic của ba automata, rồi yêu cầu thêm hai automata nữa. Thì Minh Trần đang hiểu sai thứ tự rồi. Thứ tự sẽ là theo logic khác. Tại sao anh Huy lại nói về các automata của hệ thống Việt Nam? Là bởi vì trước đây họ tiếp cận từ góc độ máy công nghiệp, những hệ máy tự động hoá. Không thể build tụi nó để chúng tương tác với nhau, chạy với nhau tự động, vì chi phí thử và sai rất cao.\n\n**01:09:31** Vì thế, để đảm bảo hiệu quả, họ phải tính toán về mặt logic trước, xem có bao phủ hết các trường hợp không. Ví dụ, khi có ba hệ thống, thêm hai hệ thống nữa là thành năm hệ thống. Trước tiên, phải xem thiết kế logic có chồng chéo gì không. Sau đó, mình mới list out các cây logic ra và tính toán. Đó là bước đầu tiên, sau đó mới tiến hành cài đặt.\n\n**01:10:09** Còn phần lập trình thực hiện sau đó tuỳ vào cách bạn muốn làm: có thể là union các hệ thống hoặc làm hàm cộng. Quan trọng là trước khi làm gì, mình phải đảm bảo phần logic đã cover được hết các trường hợp. Logic ở đây không chỉ là chuyển từ A sang B, mà là hệ logic tổng quát, bao gồm việc tính toán trên cây logic.\n\n**01:10:50** Union trong bài này nói về logic toán học một chút thôi. Còn khi implementation, nếu anh em đã làm thì chắc cũng dễ dàng làm được hàm cộng. Chỉ cần hiểu là trong union logic, chúng ta gom các điều kiện lại và tính toán. Sau khi gom các điều kiện đó, câu hỏi sẽ là trong hàm cộng, nó thực hiện như thế nào. Thường thì mọi người sẽ cố gắng explicitly từng bước một.\n\n**01:11:53** Ok, chắc ổn rồi. Thành còn gì nữa không? Ok, được rồi, để bên DevOps xử lý tiếp. Vào thử lại xem có vấn đề gì không. Mọi người tranh thủ làm bài test sớm nhé. Đợt này có deal về tài chính và AI, nên hãy chú ý.\n\n**01:13:04** Rồi, em sẽ nói qua luôn. Trong tháng 9 này thì không có nhiều tin nổi bật. Em sẽ nói nhanh thôi. Đầu tiên là về React, với những keywords như server actions, server functions, và React compiler. Có một bài viết trên freeCodeCamp về kiến trúc của React 19, nói rõ về cách tối ưu hoá hiệu suất. Nếu mọi người có thời gian thì nên đọc bài này.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hq_u9GQdNMg?si=2kaVp-rYPWxf3Rup\u0026amp;start=4446\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**01:15:12** Về Next.js, không có gì mới. Chỉ có một cái lùm xùm hồi đầu tháng khi OpenAI chuyển từ Next.js sang Remix. Tóm lại, OpenAI muốn trang của họ nhẹ hơn vì Next.js hơi nặng đối với kiểu trang SPA của chatbox. Vì vậy, họ quyết định quay về dùng Remix. Điều này liên quan đến một xu hướng mà em cảm thấy đang xuất hiện trong cộng đồng engineer: các framework như Next.js dần không được ưa chuộng như trước nữa.\n\n**01:15:51**\nCòn về Next.js, không có gì nhiều, chỉ vậy thôi. Ngoài ra, có một cái OpenNext, nó đang hướng tới hỗ trợ việc host Next.js trên tất cả các runtime. Hiện tại, Next.js chỉ chính thức hỗ trợ hosting trên các môi trường cụ thể, còn nếu muốn host ở những môi trường khác thì rất khó khăn.\n\n**01:16:28** Thằng OpenNext này thì mục tiêu của tụi nó là muốn hướng tới một cách mà mọi người có thể host Next.js trên tất cả các môi trường, kiểu không còn bị Versel độc quyền nữa. Vậy nên, có thằng này làm để giải quyết vấn đề đó. Qua cái mục này thì em thấy có hai cái thú vị đang được feature ở đây. Có một thanh niên đang implement một hệ thống real-time đơn giản dùng TypeScript và React. Nói chung, nó khá đơn giản, nhưng mà khi đọc thì thấy nó vui vui, kiểu như là JavaScript làm được mọi thứ vậy.\n\n**01:17:14** Next là một bài về chủ đề này. Nó hơi giống như commentary, nhưng mà em để bên này vì thấy nó cũng khá liên quan. Bài này bàn về trạng thái của ES5 trên web. ES5 giống như đợt trước em có nói, kiểu như CSS3 vậy, mấy cái công nghệ này tồn tại quá lâu rồi. Bài này khảo sát xem các thư viện, trang web trên internet liệu còn bao nhiêu trang web đang còn dùng ES5 hay đã move qua ES6 hết rồi. Tóm lại, kết luận là 89% của top 10.000 trang web hiện tại đã shift sang ES6 rồi.\n\n**01:17:55** Cho nên, kết luận ở đây là ES5 vẫn xài, nhưng mà khi làm một cái gì đó mới, nhất là khi làm thư viện mới, thì nói chung là không nên hướng tới ES5 nữa, vì nó cũng sắp lỗi thời rồi, nó đã quá lâu rồi. State of ES5 vẫn khá là liên quan.\n\n**01:18:44** Về phía trending, không có gì nhiều, nhưng có một bài này em thấy khá vui. Người ta đang có một cái open letter để kêu gọi thằng Oracle bỏ cái trademark của JavaScript. Em mới biết là JavaScript thuộc về trademark của Oracle. Khi nhắc tới Oracle, mọi người chỉ biết về Java thôi, gần như không có sản phẩm nào liên quan tới JavaScript, nhưng mà trademark của JavaScript lại thuộc về Oracle. Nên cái open letter này là kiểu mấy ông lớn trong giới developer yêu cầu Oracle thả cái trademark của JavaScript ra đi, đừng giữ nữa, vì Oracle không có đóng góp gì cho cộng đồng JavaScript.\n\n**01:19:22** Có rất nhiều người nổi tiếng đã ký vào bức thư này, những người creator của Node.js và JavaScript, kiểu rất nhiều người họ đã ký. Em cũng mới biết tới, nhưng mà thấy cũng hay hay.\n\n**01:20:16** Rồi tiếp theo, quay lại mấy cái framework và library mới, nhưng chắc phần này em sẽ skip vì không có gì đặc biệt. Có một cái bài commentary mà anh Thành gửi cho em, nó liên quan về cái sentiment của cộng đồng về React và JavaScript, đặc biệt là Next.js. Bài này khá dài, nhưng tóm lại ý chính của họ là các framework như Next.js càng ngày càng nặng.\n\n**01:20:55** Họ chỉ trích xu hướng hiện tại của cộng đồng engineer React là các front-end developers đang chạy theo các framework và library lạm dụng quá nhiều JavaScript. Điều này có lợi cho trải nghiệm của developer (DX), nhưng lại làm giảm trải nghiệm của người dùng (UX) vì phải ship quá nhiều JavaScript về phía client. Tóm lại là code thì sướng, nhưng sản phẩm cuối cùng thì người dùng lại không thích. Đặc biệt là họ chỉ đích danh thằng Next.js, nói rằng cần đẩy ngược các phần nặng về lại server để cải thiện trải nghiệm người dùng.\n\n**01:21:28** Cái sentiment này khá rõ ràng, họ đang kêu gọi mọi người chuyển ngược về server-side nhiều hơn. Em thấy bên front-end cứ đi vòng quanh vậy thôi, từ server chuyển xuống client, rồi từ client nặng quá lại kêu chuyển về server.\n\n**01:22:19** Nói chung là đó là mấy bài em thấy thú vị trong đợt tháng 9 vừa rồi. Drama thì cũng có chút thú vị, lôi kéo sự chú ý. Bên Go thì ổn định quá nên không có drama, thành ra không ai nói gì nhiều. Còn bên JavaScript thì cứ có drama suốt, từ server-side script, JavaScript modules, kiểu như drama quanh đi quẩn lại. Cộng đồng này lúc nào cũng vậy, không có biên chuẩn rõ ràng thì lúc nào cũng cãi nhau về chuyện đó.\n\nCảm ơn mọi người. Về phần reminder, các anh em tranh thủ làm bài test sớm nhé, để có thời gian xử lý kịp. Còn một số phần team lab cần xem lại, đặc biệt là báo cáo kết quả sau khi chuyển hết phần này, xem còn gì cần report không. Thành, chắc tụi mình sẽ wrap up ở đây và tập trung vào các case đã nói trước rồi, nhé.\n\n**01:23:30** Rồi, cảm ơn mọi người! Về phần reminder, các anh em tranh thủ làm bài test sớm nhé, để có thời gian xử lý kịp. Còn một số phần team lab cần xem lại, đặc biệt là báo cáo kết quả sau khi chuyển hết phần này, xem còn gì cần report không. Thành, chắc tụi mình sẽ wrap up ở đây và tập trung vào các case đã nói trước rồi, nhé.\n\n**01:24:04** Lập, xem lại giúp nhé. Cát có share một bài bên ngoài, bài đó mới post lên, dễ hiểu, mọi người thử xem nhé. Xu hướng hiện tại của mình sẽ kết thúc chu kỳ của mấy cái software không cần suy nghĩ nhiều, mà tập trung vào phần tooling nhiều hơn. Giờ mấy cái kỹ thuật cơ bản về toán và logic đang quay lại. Sắp tới, team lab sẽ report các chủ đề theo hướng logic nhiều hơn, để anh em nghe quen dần.\n\n**01:24:50** Tom hồi giữa tuần có dựng lại một cái library mới tên là WebUI sau khi cái library cũ bị shut down. Mọi người thử xem. Thằng này khá là legit, khi mình hỏi về chủ đề compile union của hai cái finite automata, nó trả lời toàn bộ theo công thức toán hết, nhìn rất dễ hiểu. Nó giải thích rõ ràng từng bước làm thế nào để tính union của hai hệ thống finite automata.\n\n**01:26:15** Đây, hai hệ rời, hệ chập lại điều kiện để tính toán điều kiện của hai cái dưới đây. Nó nói về chuyện state transitions, anh đang thử nghiệm một vài khái niệm về toán trên đây, và thấy khá là legit. Nếu được, khuyến khích anh em xài con này nhiều hơn. Con này, Tôm đã ngồi mod lại cái system của nó rồi. Ban đầu có thể sẽ nhìn hơi khó chịu một chút, vì mình đã quen nói chuyện bằng ngôn ngữ bình thường khi giải đề toán rất bình thường. Giờ mình phải step back lại một chút, để thấy rằng mấy kiến thức về khối A (toán học) giờ nó có giá trị. Full logic luôn. Rồi, advance như thế nào, practical implications là như thế nào, kỹ thuật làm như thế nào, đều có hết.\n\n**01:26:57** Thành thử ra, với những gì đang diễn ra, với các inquiry và commentary, anh ngồi dò, chắc chắn là thị trường sẽ chuyển qua hướng tài chính. Từ năm ngoái đến giờ, sau đợt blockchain và crypto, team mình hiện tại, đội của Huy Nguyễn vẫn đang tiếp tục, và đội đó vẫn rất ổn. Một số kỹ thuật mới hơn bên blockchain, như Monas hay gì đó, thì mình chưa xem, nhưng chắc cũng tương tự thôi, không có kỹ thuật nào khác biệt quá.\n\n**01:28:29** Về mảng tài chính, mình đang tích cực gần với các cycles về tài chính. Domain đó đang build up theo hướng này, và có vẻ như đây là một cửa rất sáng. Hướng này team mình cũng đang dần làm gần hết rồi, giờ chỉ còn lại là số lượng case study nhiều hay ít nữa thôi. Anh nghĩ vậy là hợp lý. Chất lượng đội ngũ cũng đang được cải thiện dần. Như bài lúc nãy đưa cho Minh, không nhớ là đã đưa cho Minh Lư chưa, nhưng Minh có xem được thì chắc cũng đã hiểu được tầm 6/10. Nhưng anh tin rằng nó vẫn hơn rất nhiều so với nhiều người khác. Mặc dù có sợ, nhưng không report lại được. Minh đã mở hai phần của finite state ra và giải thích rất legit.\n\n**01:29:12** Định hướng của team là theo hướng đó. Nếu khéo, thì chắc là hết tháng này, mình có thể đẩy thêm được nội dung về toán logic nhiều hơn một chút, cố gắng so sánh giữa những khái niệm cũ và những khái niệm mới mà mình đang biết, tương đương với nhau thôi. Biên lại, chỉnh lại cho phù hợp. Ok, không yêu cầu tất cả mọi người phải đi theo hướng đó, nhưng theo quan sát cơ bản, anh thấy thị trường đang dịch chuyển theo hướng đó. Để giữ giá trị khác biệt, mình chơi game theo cách khác chút. Đó là message để anh em aware.\n\n**01:29:58** Nếu không có gì khác thì còn một bài của anh Thành, chắc để sau. Còn lại, mọi thứ, thằng vừa rồi nếu mọi người chưa có access, thì xin chỗ Tom nhé. Hẹn gặp lại anh em tuần sau. Thứ Tư tuần sau sẽ không có buổi họp giữa tuần, mọi người ngồi và làm tiếp theo hướng mình vừa nói nhé. Content làm sao để anh em hiểu.\n\n**01:30:57** Ok, chắc là vậy nhé. Bye bye anh em, hẹn gặp lại anh em vào Thứ Sáu tuần sau. Thứ Tư tuần sau sẽ không có buổi nào ở giữa nữa. Mọi người dành thời gian xem thêm nội dung liên quan tới những gì cần để hoàn thành bài test nhé. Cảm ơn tất cả anh em. Bye bye, hẹn gặp lại.\n\n---\n\n**English transcript**\n\n**08:02** Is there any information that needs to be shared? If not, let's have Phát go first, I see a lot of topics today, so let's prioritize the new members. Ok, there are five topics today.\n\n**09:13** Please come forward. Whoever has a topic, please go early. Yes, Phát will start first, and then Thành’s part will follow. Nam, are you ready? Today, Nam will share a topic called \"UX Guide to Prompt with AI.\"\n\n**10:20** I’ll give an overview of the current situation first. Interaction between humans and AI is a popular topic today, and the emergence of Large Language Models (LLM) is a useful tool that our team is looking into. Today’s topic is for anyone interested in the User Experience (UX) of AI, specifically how tools are currently designed to improve the interaction between users and AI. Many tools and platforms are being developed today, but they mostly focus on improving prompt speed and accuracy instead of focusing on the user experience.\n\n**11:08** The concept of \"RACE\" (Role, Action, Context, Expectation) is quite common in prompting AI. Users need to prompt in this structure for AI to generate the most accurate output. However, not every case applies to \"RACE.\" Many companies have developed new methods to improve AI UX, helping make the interaction between users and AI smoother.\n\n**12:04** The first method is \"Context Through Rephrasing.\" This method helps AI query the context of the previous question to answer the next question coherently, without needing a perfectly structured prompt from the start. For example, if the first question is \"Who is the wife of Superman?\" and then you ask, \"When did they get married?\" AI will understand the context and connect the dots. But if there is no relevant context, such as asking, \"What day did the Titanic sink?\" AI won’t be able to provide the right answer.\n\n**12:50** Next is \"Implicit Referencing,\" for example, when asking about the number of floors in a building, AI might assume it's a famous building like the \"Willis Tower in Chicago.\" If you ask, \"What day?\" without proper context, AI cannot give an accurate answer. Questions must be tightly connected for AI to answer better, and this also applies to \"Context Through Rephrasing.\"\n\n**14:19** A similar concept is \"Continue Conversation,\" like in Google Assistant. Questions are naturally linked, and each new question relates to the previous ones to create a continuous conversation.\n\n**15:03** The next method is \"Racing and AI Scoring.\" Google Assistant also applies this method. It provides multiple options based on different contexts, helping users get better results. AI can also learn from user choices to improve interaction. For example, when AI is unclear about the context, it will give users options to choose from.\n\n**16:03** Lastly, there is \"System Prompting.\" This theory directs AI to operate based on the context and user-defined goals. It helps AI generate accurate output without following a fixed prompt standard. For example, when asking, \"Plan for releasing a software product\". ChatGPT may provide general concepts, while GPT mini will ask more detailed questions to help users continue prompting for more precise results.\n\n**17:45** Today’s discussion focuses on designing AI tools to improve user experience, not just in terms of speed or accuracy but also in terms of user interaction and overall experience.\n\n**18:50** To summarize this for everyone, especially designers, Nam's topic has two main aspects. First, it explains the structure of \"RACE\" and how to apply it. Second, it presents a framework for designing AI tools, focusing on how to prompt effectively to improve the interaction between AI and users. The RACE structure includes Role, Action, Context, and Expectation, and it helps enhance AI UX.\n\n**19:20** To explain briefly, R stands for Role, and there are different types of R’s that Nam mentioned earlier. For designers to understand the R structure, it's important to look at how to build an app focused on prompting and how this structure ties in. It involves introducing the R technique, a common technique Nam mentioned earlier, called RACE. When writing a RACE, you need to clearly state the Role, Action, Context, and Expectation.\n\n**20:03** RACE is described very clearly: what is Role, what is Action, everything is explained, including what Expectation is and what Task is. In summary, the most basic thing for designers is to understand the structure of a RACE prompt. It follows a specific structure, which produces standard results. The input follows that structure, and the output will provide corresponding results.\n\n**20:47** The second and final part of this presentation will discuss what to pay attention to when designing once you understand the structure of a prompt and how to prompt accurately. This part is open-ended because this is like a 101 guide for designers to look at and understand the basics.\n\n**21:30** Nam has talked quite a bit about the letter R, so some people might misunderstand that this topic is just explaining that concept in detail. But in fact, this presentation is introducing prompting to UX designers. Ok, any questions? This topic is quite basic; our team has used it a lot, and we’ve demoed it many times. There’s one point to note: this topic is special because it introduces system prompting, which other guides don’t cover.\n\n**22:31** This presentation introduces system prompting, which is usually not mentioned in other guides. Guides for end-users (end users) rarely mention this. This topic covers system prompting because it’s written from a designer’s perspective – someone who is part of the team building it. System prompting differs from regular prompting because it controls how AI operates based on the system's specific goals.\n\n**23:06** The structure of system prompting differs from the usual R's that people often see when reading research. Typically, you see discussions about 200 different types of R’s, but there is no perspective from someone building the app. This topic is for designers, not for end-users, but for those in-between to connect the different parts.\n\n**23:48** This is different from articles for engineers because it not only introduces the tools to build prompts but also discusses how to combine different R types. This is an intermediate-level topic, suitable for designers who play the intermediary role, not directly building but also not the end-users. It helps bridge these two parts together.\n\n**24:33** Ok, thanks, Nam. Next week, we will scope this topic again to make the content easier to understand for everyone. Going into detail might be a bit difficult for everyone to grasp. Thank you, Nam. Now, on to the next speaker. Let’s see if we can view the screen. Ah, it’s back.\n\n**25:41** My topic today is a small problem in programming techniques, which is how to compute the union of two finite automata, also known as finite state machines. I will demo it using Go source code. Today’s topic will have a few key points. First, I’ll explain the applications of automata for everyone to get an idea, then we’ll go into the details.\n\n**26:41** The most common application of finite state machines is that when you have a button and an input, you want to check whether the input matches or fails. It’s as simple as that. The most common example is using regex to check whether a piece of text is an email, a phone number, or a street number. We have a button like that, and we feed in a piece of text to check if it matches the condition.\n\nAside from regex, another common case is in event-driven systems, where we have event buttons. You define a button in the form of a state machine, and each event is a state. The event will go through the event button and get filtered to see if it matches that event. If it matches, it moves on to the next state for further processing; otherwise, it fails and doesn’t go through.\n\n**27:27** Here’s an example: Suppose we have an event bus, and all the events pass through this event bus and get filtered through the rules. If an event satisfies the rule, it proceeds for further processing. This is most commonly seen in modern cloud systems, where they use a lot of these systems to manage events and filter them through rules like this. You can see this in large systems like Amazon, where their events pass through a series of rules.\n\n**28:05** For example, suppose we have a button, and all items with a field \"image\" that contains an object with a width of 800 will pass through. And we can also add a few more rules for different fields that we add to that button. This is an example of how finite state machines and event buttons work. When an event enters the system, it passes through rules, and if the conditions are met, it will pass through to the next processing steps.\n\n**28:56** This is a specific example of Amazon’s event system, where their events pass through a series of rules to be filtered and processed. Most cloud systems today use similar button patterns to manage and process events in an organized and efficient way.\n\n**29:37** In practice, let's go back a bit to finite automata (f automata) in mathematical terms, what it is. In essence, it's simply a machine with a set of states. To move from one state to the next, it needs to go through a transition. For example, to go from the start state to the end state, there will always need to be a start point called the start state and an endpoint called the end state. That’s why it’s called a finite state machine because it always has a start and an end.\n\n**30:21** In between, there will be a set of transitions and states to move from the start point to the end point. An input symbol is what you feed into a state to move it to another state, and in practice, the input symbol is often a character. We’ll talk more about this in detail later. The accepting state is the state where if the input is accepted, it moves to another state through a transition. If it’s not accepted, it doesn’t go anywhere, as if the transition doesn’t lead to any state.\n\n**31:04** There are two types of finite automata, deterministic finite automata (DFA) and nondeterministic finite automata (NFA). The only difference is that with DFA, each state has a single input symbol leading to a transition to another state. With NFA, there can be multiple transitions for the same state, and there may even be no transitions at all. This difference is just about how the path from the start to the end is represented, but both finite state machines can be expressed as either DFA or NFA. It’s just a matter of representation.\n\n**31:51** Regarding the union of two finite automata, it’s the combination of all the states and transitions of the two finite automata. Its feature is that if event A passes through finite automaton FA1 and event B passes through finite automaton FA2, the union of these two must ensure that both event A and event B pass through.\n\n**32:45** Why do we need to compute the union of two finite automata? In practice, for example, when using an event button, we don’t use just one button, we use multiple buttons combined. For example, in the diagram, we can define many buttons, and in our event, we may have multiple pieces of information that match these buttons. When we want to combine these buttons, we need to compute the union of all of them.\n\n**33:24** But we don’t compute them all at once; we compute them one at a time, for example, first with pairs A and B, then take the union of A and B and compute it with pair C. We just need to compute two buttons at a time, this is the most basic level of calculating the union of all these buttons.\n\n**34:07** To compute the union of two finite automata, theoretically, we would have to calculate all the states and transitions of both automata. For example, if we have two buttons, A and B, and each button has many states, such as from A1 to Ax and from B1 to Bx. Theoretically, we would have to calculate all combinations of these two buttons, like A1-B1, A2-B2, A1-B2, A2-B1, there are many combinations.\n\n34:07 In practice, we only care about whether, when feeding an event into the button, we want to know if it matches or not. We only care if, after feeding the event into the button, it can reach the final state or not. Calculating all the states in the union would be wasteful and unnecessary. Therefore, the practical approach is to have two finite automata with states from A1 to Ax and from B1 to Bx. When calculating their union, we check if the state leads to any transition in A or B. If not, we skip it.\n\n**35:40** For example, if the state only leads to A1 and there’s no transition leading to B, then we only calculate the branch for A and skip B. Similarly, we do the same for B. The last case is if A1 moves to Ax and B1 also moves to Bx, then we create a new state, Ax-Bx. At this point, we recursively start again and continue calculating from this first step. For example, we have A1-B1, and then we have A2-B2, so we repeat these four steps to continue calculating.\n\n**36:15** This way, the number of states we can skip and not calculate or store is significantly reduced compared to the theoretical method mentioned earlier.\n\nRegarding input symbols, like in the event button or regex examples, in practice, the buttons and inputs we feed into the program are always in the form of characters. Characters here are often those.\n\n**37:04** Characters in UTF-8, which are simple characters like from a to z or from 0 to 9. They fit into UTF-8, which includes 244 characters. Although UTF-8 uses 8 bits (2^8 - 1 = 256), the bits from 245 to 255 are left over, so we don’t use them, just the first 244 bits. Now, here’s the implementation detail in the code. This is the basic workflow we’ll follow. Not sure if it can be zoomed in? Try zooming in on the diagram from earlier, the text isn't visible. Wait a moment, let’s check again.\n\n**38:21** Alright, what is FA? Finite automata? Yes, automata. This function will merge two automata together. First, it creates a key to mark the states that have already been processed so that we don’t have to revisit them. Then, it checks this key, and if it’s not a duplicate, it starts working. It creates an empty combined state that includes all the transitions of the two finite automata. Then it continues merging each finite automaton, one by one. Scroll up, I couldn’t read that part in time.\n\n**39:33** During implementation, there will be some points where we need to modify the data structure to store those states. Which example is your topic based on? Which sample is this diagram running? Let me show the code; it’ll be easier to see. You have to look at the problem to know which code we're dealing with. After this example, we’ll have more questions. But this topic only needs Phúc and Tuấn to understand, does everyone understand the code clearly?\n\n**40:13** The earlier example about the event button, here we define a button, for example. The button I defined is in the form of T, and here’s the event. When we run this code, it’s just part of the logic; there are more parts of the code. But when running that, the expected result is that it will check if this event matches this button. It will return whether the event matches the button or not.\n\n**41:13** That’s right, that’s the problem this code is solving. This is simple code; it will run and return results, just checking if it matches this button or not. For example, here, this button has a transition with \"user_register,\" for instance. If I change something else, it won’t match. But if the event meets the condition, it will match, something like that.\n\n**42:09** I’ll go straight to the main logic. It’s the function to merge two finite state machines together. Each of these finite automata represents a button. Imagine we have multiple buttons. Here we only have one button, but imagine I split this into two buttons. My function is responsible for merging those two buttons into one total button.\n\n**43:31** As mentioned earlier, to avoid too much calculation, I’ll first explain the parameters passed into the function. The two FAState are two structures representing the two buttons we had earlier. Each structure includes a small table to represent a series of input symbols and their corresponding transitions. Epsilon is the state that returns to its own position, we’ll skip that for now. Here, we only care about these two states. The smallest unit we want to use as input is the file because a character is represented in UTF-8 and includes 244 bits. Now, we’ll loop through each bit in these characters to compare.\n\n**44:20** For example, in this case, when I input an event, after computing these two buttons, it starts from each character. For example, it starts with a bracket and checks if there’s any transition to \"user.\" If there isn’t, it skips the ID part because it doesn’t match. This part goes into the details of comparing each character.\n\n**45:02** Skip this part. Just look at the benefits and the idea behind the implementation; there’s no need to compare in detail here. The benefit of this method is simple: it helps the algorithm avoid recalculating too many times. The algorithm is quite simple, it’s just comparing, as we analyzed earlier. If a character doesn’t lead to a transition, we skip it. Or if it only leads to one branch, and that branch doesn’t have any transitions, we skip it as well.\n\n**46:34** For example, if state A1 moves to state B, but state B doesn’t exist, we skip that branch. Conversely, if that state already exists in the map, we skip it. Only when all conditions are met do we start calculating and loop through each state in the finite automata. After looping through each step with the corresponding state in B, we get the final state and assign it, then return the result.\n\n**47:17** Does everyone understand? Ask Minh. Minh, check this out. I told Minh to send the link. Did you all understand this? The important thing is to review the first diagram because I’m afraid people don’t understand it. This diagram was a long time ago, more than a month ago, right? Yes, this diagram, does everyone understand?\n\n**49:11** Systems like Amazon’s event bus need to merge sometimes up to millions of buttons, so there will be some details in the implementation to speed up these tasks. Switch to another diagram, the next one, the one that branches into two; use that to explain better. The branching diagram, which one should we use? Let’s stick with this diagram, do the others understand?\n\n**50:11** Let’s explain it clearly so that everyone gets it. Hoàng, do you understand? Do you get what this is doing? Ah, Phúc asked if it uses a bit operator. Not yet, it hasn’t reached that level, it’s just detailed comparisons, unrelated to that part. Tuấn, Minh, where’s Vincent? Does everyone understand this topic? No? This topic is nested in three layers from what Minh discussed earlier. Let’s clear a few things to make it less confusing for everyone.\n\n**51:03** First, finite automata, or state machines, are like state transition diagrams that you often draw. Remember? They have states and nodes representing those states. Second, there are conditions (input symbols) that allow you to move from one state to another, called transitions. Read it carefully to understand and remember it better. It’s like a state transition diagram; that’s the first point.\n\n**51:47** The second point is the two types of finite automata that Minh just showed. Why show these two types? Because some states are simple, they only move in one direction and can’t reverse. For example, lunchtime. When you walk out to Hà Đô to eat pho, you can walk to the pho restaurant, eat, and return. That completes a finite state; it doesn’t reverse, that’s a finite automaton.\n\n**53:10** Another example is a new finite automaton where you go to the supermarket to buy lunch, pay, and return. It’s still a finite automaton, but it has different states than the previous one.\n\n**53:57** The issue is how to calculate the union of these two finite automata. Let’s say there are two states: one where you eat pho, and the other where you go to the supermarket to buy lunch. We need to build a union of these two states, like how Minh talked about merging two state machines. Here, we have two choices: one is to eat pho, and the other is to go to the supermarket. The idea is to compute all possible combinations of these two state systems.\n\n**54:30** We calculate their union by simply combining them. In the earlier example, how many options do we have? One person goes to eat pho, and the other goes to the supermarket, right? This example is also calculating the union, but it’s a different kind of union. The problem is that there are two finite automata (FA), and we need to compute their union. Then we check how to combine them. There are two parts: one part follows the original flow, eating pho, and the other part follows the supermarket flow. Another case is when you combine two sets of conditions, which will generate a new set. At that point, we have to continue calculating and computing more.\n\n**55:17** The basic idea of creating a union of multiple finite automata is like that. It’s similar to what Minh showed earlier, Minh showed the source code. Now, let’s go back to that function. Alright, here, Minh’s function has a method to merge all the states. This means we simulate having two state systems only, right? From two different finite automata. Then we merge them into a unified system, which gives us a big table. From that big table, we continue to calculate based on each condition.\n\n**56:04** Now, we’ve built the big table, a large array that contains the complete list of conditions. Now, we’ll start calculating. When a new condition is input, it begins by checking all the conditions of the first system. If it doesn’t match, it checks all the conditions of the second system. If it still doesn’t match, it checks the conditions of the union of the two systems. The basic calculation logic is just like that.\n\n**57:02** The difficulty of this problem, if you feel a bit confused, lies in the process of modeling from mathematics into programming. Do you get it, Tư? Lucky, did you grasp it? Ok, follow the problem. Show the function and the diagram from earlier. This diagram explains why regular expressions mention this. When you check in square brackets in regular expressions, you need to go through all the conditions inside. Because there are so many options, each option is modeled mathematically to make it easier to handle. Each condition is a finite automaton, and we compute the union of these conditions.\n\n**58:38** Modeling mathematical logic into programming is the process of calculating with different systems. We need to calculate the union condition of systems 1, 2, 3, 4... If none of the conditions are met, we have to calculate the intersection of each pair of conditions. This problem is like that. If you want to learn more, this is an important topic because it helps you understand how we operate in large logic systems.\n\n**59:21** I think I gave this to Minh earlier. This is important because it relates to creating logic addition functions. A common comparison is login authentication, like in the login process, which relates to this problem. During the login process, there are many conditions, for example, passing 2FA, email, SMS, or password. Each of these can be modeled into its own finite automaton, and when we compute them, we combine them all.\n\n**0 1:00:00** For example, there might be a requirement for the user to have both face scan and QR code to log in. That’s a combined condition (intersection), not just a simple if statement. In mathematical modeling, we can handle this easily because it has recursive properties to compute complex logic combinations. The most important thing is how to calculate this combination without recalculating many times. The output will either be success or failure. But for junior developers, when adding a new case without the modeling mindset, they’ll end up creating many messy if statements, making it hard to maintain the code.\n\n**01:01:29** When adding a new feature without a modeling mindset, the code becomes messy and inefficient. They’ll have to rewrite it many times, causing many bugs and wasting time fixing and testing. That’s why this problem is important, as it impacts system design, especially in systems like login authentication, where combining multiple conditions is common.\n\n**01:02:23** Any more questions? No? Minh, do you understand it all? Ok, looks like we’re out of time. Thành, are there any more topics? There are two more, but let’s try to wrap them up quickly. Let me check. Can everyone see the screen? Ok, this week, I only have two topics, there’s another one, but it’s too long, we’ll do it next week. That one requires more detail in terms of usage.\n\n**01:03:36** The next topic is about Go and how to embed files. What is Go embed? It allows us to embed a file directly into the binary. This helps reduce the need to handle external files. The way it works is that when we embed a file into the binary, the handling process becomes simpler. But there’s a limitation: if the file is too large, the binary will expand. So, be careful when using this.\n\n**01:04:15** The usage is like this: we just import it and use it as usual. For example, we have a message file, we embed that file into the binary, and then we can access that file directly. For multiple files, we can add additional characters like this. Then, we use the embedded variable to read or access it like a normal file. Or we can even embed an entire directory.\n\n**01:04:57** Usually, we want to embed static files like images, HTML, or something like that. As for the limitation I mentioned earlier, the second topic is about reflecting a package. This topic is not new. It’s not really new if you’ve used Go and read the article by Dr. R. I’ll go over it quickly. The context is that the author encountered a problem with reflection while using a tool or codebase and wrote this article. In general, reflection has three key points to note.\n\n**01:05:49** The three important points are: from interface value to reflection object, and vice versa. The final point is when you want to modify the values, they need to be settable, meaning they need to be exported, or written with a capitalized value. What is an interface value? What is a reflection object? The interface value is, in every function we use in the reflect package, it’s always understood as a wide interface{}, so it’s an interface value. And this value is a reflection object, and vice versa.\n\n**01:06:36** As for the direction: ValueOf returns a reflection object. In the opposite direction, from here, we can use the method .Interface() to return the original value. In Go, this is now default. If you want to update something, you need to find something settable. Reflect has this method for you to use and update.\n\n**01:07:12** Another point mentioned in that article is to avoid using reflect unless absolutely necessary. If you use functions like FieldByName, if the input is not well-controlled, it can lead to panic or crashes immediately. So, only use it when absolutely necessary, in cases where there’s no other option.\n\n**01:07:53** There’s another article that I mentioned that’s quite long, about map. It compares using regular maps with the need to support concurrency, where you’ll need a locking strategy. You can use mutex or something else, or some other lock. The sync package provides a sync.Map. The article compares these two approaches. There’s no better option; it depends on the use case.\n\n**01:08:35** Now, let’s move on to the next part. Phát, simple stuff. Minh Trần is asking about the logic of three automata and then adding two more automata. But Minh Trần is misunderstanding the order. The order will follow different logic. Why did Huy mention automata systems in Vietnam? Because previously, they approached it from the perspective of industrial machines, automated systems. They couldn’t build them to interact with each other or run automatically because the cost of trial and error was too high.\n\n**01:09:31** So, to ensure effectiveness, they had to calculate the logic first to see if it covered all cases. For example, when there are three systems, and you add two more, making five systems. First, you have to see if the logic design overlaps. Then, we list out the logic trees and calculate. That’s the first step, then we proceed with implementation.\n\n**01:10:09** As for implementation, it depends on how you want to do it: you can union the systems or create an addition function. The important thing is that before you do anything, you must ensure the logic covers all the cases. Logic here is not just about going from A to B; it’s a general logic system, including calculations on logic trees.\n\n**01:10:50** Union in this article talks about mathematical logic a bit. When it comes to implementation, if you’ve done it before, you’ll find it easy to create an addition function. Just understand that in union logic, we combine conditions and calculate. After combining those conditions, the question is how the addition function performs. Usually, people try to explicitly go step by step.\n\n**01:11:53** Ok, seems good now. Thành, is there anything else? Ok, got it, DevOps team will handle the rest. Try it again and see if there’s any issue. Everyone, please do your tests early. We’ve got a financial and AI deal this time, so pay attention.\n\n**01:13:04** Now, I’ll move on. In September, there weren’t many notable updates. I’ll go over it quickly. First, about React, with keywords like server actions, server functions, and React compiler. There’s an article on freeCodeCamp about React 19’s architecture, detailing how to optimize performance. If you have time, you should read that article.\n\n**01:15:12** Regarding Next.js, nothing much new. There was some noise at the beginning of the month when OpenAI switched from Next.js to Remix. In short, OpenAI wanted their page to be lighter because Next.js was a bit heavy for a chatbox SPA (Single Page Application). So, they decided to switch back to Remix. This relates to a trend I’m noticing in the engineer community: frameworks like Next.js are becoming less favored.\n\n**01:16:28** The goal of OpenNext is to make it so that people can host Next.js on any environment, no longer being restricted by Vercel’s exclusivity. So, this project is here to solve that problem. Moving on, I see two interesting things being featured. There’s someone implementing a simple real-time system using TypeScript and React. It’s pretty simple, but reading it makes you smile, like JavaScript can do everything.\n\n**01:17:14** Next is a piece about this topic, which is kind of like commentary, but I’m putting it here because it’s still relevant. This article discusses the state of ES5 on the web. ES5 is like what I mentioned before, like CSS3, technologies that have been around for too long. This article surveys how many websites on the internet are still using ES5 or if they’ve moved on to ES6. In short, the conclusion is that 89% of the top 10,000 websites have already shifted to ES6.\n\n**01:17:55** So, the takeaway here is that ES5 is still in use, but when building something new, especially when building a new library, generally, you shouldn’t aim for ES5 anymore because it’s almost outdated; it’s been around for too long. The state of ES5 is still somewhat relevant.\n\n**01:18:44** Regarding trending topics, there’s not much, but there is an amusing one. There’s an open letter asking Oracle to give up the JavaScript trademark. I just found out that JavaScript belongs to Oracle's trademark. When people think of Oracle, they only know Java, and almost no product relates to JavaScript. But the trademark for JavaScript is owned by Oracle. So this open letter is basically developers asking Oracle to release the JavaScript trademark, not hold onto it anymore, because Oracle hasn’t contributed anything to the JavaScript community.\n\n**01:19:22** Many big names have signed the letter, including the creators of Node.js and JavaScript, many well-known people have signed it. I just found out, but it seems interesting.\n\n**01:20:16** Moving on to new frameworks and libraries, but I’ll probably skip this part because there’s nothing special. There’s a commentary from Thành, relating to the sentiment in the React and JavaScript community, especially around Next.js. The article is quite long, but to sum it up, the main point is that frameworks like Next.js are getting heavier.\n\n**01:20:55** They criticize the current trend in the React engineer community: front-end developers are chasing after frameworks and libraries that overuse JavaScript. This benefits the developer experience (DX), but it reduces the user experience (UX) because too much JavaScript is being shipped to the client. In short, writing code is fun, but the final product doesn’t make the user happy. They specifically mention Next.js, saying that the heavy parts need to be pushed back to the server to improve the user experience.\n\n**01:21:28** This sentiment is quite clear; they’re calling for a shift back to more server-side solutions. I feel like the front-end community keeps going in circles: moving from the server to the client, and now the client is too heavy, so they’re calling to move back to the server.\n\n**01:22:19** In general, those are the interesting topics I found in September. Drama is always interesting; it grabs attention. In Go, everything is stable, so there’s no drama, which is why no one talks much. But in JavaScript, there’s always drama, from server-side scripts to JavaScript modules, the drama just keeps coming back around. The community is always like this; without clear standards, people are always arguing.\n\n**01:23:30** Thanks, everyone. Regarding the reminder, please do your tests early so there’s time to handle everything. The team lab still has some work to review, especially the reports after everything is transferred over. Thành, I think we’ll wrap up here and focus on the cases we mentioned earlier.\n\n**01:24:04** Lập, please review. Cát shared an article outside, it’s new, easy to understand, everyone take a look. The current trend seems to be ending the cycle of software that doesn’t require much thought, focusing more on tooling. Now, basic techniques in math and logic are making a return. In the future, the lab team will report more on topics in logic, so everyone gets used to it.\n\n**01:24:50** Tom rebuilt a new library called WebUI after the old one was shut down. Everyone take a look. This one is quite legit. When you ask it about the union of two finite automata, it responds with everything in mathematical formulas, very easy to understand. It explains clearly how to compute the union of two finite automata systems.\n\n**01:26:15** Here, the two systems combine conditions to calculate. It talks about state transitions, I’m testing a few mathematical concepts on it, and it seems quite legit. If possible, I encourage everyone to use this more. Tom has already modified its system. Initially, it might seem a bit uncomfortable because we’ve been used to using normal language to talk about these problems for so long. Now, we have to step back a little, and see that knowledge in math is valuable now. Full logic. Then, how to advance, practical implications, how to implement it, everything is there.\n\n**01:26:57** Given what’s happening with the inquiries and commentary, I’ve been checking, and I’m certain the market is shifting toward finance. Since last year, after the blockchain and crypto wave, our team, Huy Nguyễn’s team, is still moving forward, and that team is still doing well. Some newer techniques in blockchain, like Monas or something, we haven’t looked at yet, but it’s probably similar, no major technical differences.\n\n**01:28:29** In finance, we’re actively working with the financial cycles. That domain is building up this way, and it seems like a bright opportunity. Our team is also almost done with this, now it’s just a matter of how many case studies there are. I think this direction is reasonable. The team’s quality is gradually improving. Like the material I gave to Minh earlier, I can’t remember if I gave it to Minh Lư yet, but if Minh has seen it, he should understand about 6/10. But I believe it’s still much better than many others. Even though there might be fear, he didn’t report back. Minh opened two parts of the finite state machine and explained them very well.\n\n**01:29:12** The team’s direction is to follow this path. If done skillfully, by the end of this month, we could push more content about math and logic. We’ll try to compare old concepts with new ones we already know. Let’s refine and adjust. Ok, it’s not required that everyone follow this path, but based on basic observation, I see the market is shifting in that direction. To keep our value distinct, we have to play a different game. Consider this as a message for everyone to be aware.\n\n**01:29:58** If there’s nothing else, there’s one more article from Thành, but we’ll leave it for later. As for the rest, if anyone doesn’t have access to the thing from earlier, ask Tom. See you next week. Next Wednesday, there won’t be a mid-week meeting, everyone sit and continue along the direction we just discussed. Let’s work on the content so that everyone can understand.\n\n**01:30:57** Ok, I think that’s it. Bye-bye, everyone, see you next Friday. Next Wednesday, there won’t be a mid-week meeting. Everyone take time to review the content related to what needs to be done for the test. Thanks, everyone. Bye-bye, see you.\n","title":"OGIF Office Hours #27 - Go Weekly, Frontend Report Sep, UX Guide to Prompt with AI, Computing the Union of Two Finite Automata","short_title":"#27 Go weekly, Frontend, AI UX, Finite Automata","description":"In OGIF office hour 27, covering presentations on UX Guide to Prompt with AI, computing the union of finite automata, and other topics including Go Weekly and Frontend Report for September.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Mon Oct 14 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/27-20241011.md","slugArray":["updates","ogif","27-20241011"]}],"newMemos":[{"content":"\nAt [Company/Community Name], we’re obsessed with working smarter, not harder. This month, we’re tapping into the power of AI to speed up our processes—and we need YOUR ideas to make it happen! Whether it’s automating writing, deploying AI agents, or streamlining workflows, share your proposal and earn *15 icy* for every approved idea.\n\nHere’s How It Works:\n\n- **What We’re Looking For**: Creative ways to use AI to accelerate tasks—like writing, research, or operations. We want proposals, guidelines, or practices that save time and boost efficiency. (See an example below!)\n- **How to Submit**: Send your idea to [submission link/form/channel] with a brief explanation. No idea’s too small!\n- **Reward**: Earn *15 icy* per approved submission added to our AI-powered playbook. Standout ideas could score up to 25 icy!\n- **Review Process**: Our team reviews weekly and will let you know if your idea’s a go.\n- **What’s Next**: Winning ideas get implemented, and you’ll be credited community-wide!\n\nWhy Join In?\n\n- Earn *icy* tokens to [use in community perks, trade, etc.—add token value context].\n- Help us harness AI to work faster and smarter.\n- Open to everyone—bring your A(I)-game!\n\nExample Submission:\n“Use an AI writing assistant (like Grok) to draft initial posts, memos, or emails in seconds. Team members can edit the output, cutting writing time by 50%.”\n\nReady to speed things up? Submit your AI idea at [link] and start earning icy today!\n\n---\n\nproductivity is one of our study focus.\n\nintro: ref to handbook \u003e how-we-work \u003e ## Pitching ideas\n\naccepted content\n\n- write a rfc\n- build a supporting tool\n-\n\nhow to submit\n\n- show a workflow image\n\nreward\n\nList of proposal\n\nList of articles\n","title":"Productivity","short_title":"","description":"","tags":["earn","productivity"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/000-productivity.md","slugArray":["earn","000-productivity"]},{"content":"","title":"Software Quality","short_title":"","description":"","tags":["earn","quality"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/001-quality.md","slugArray":["earn","001-quality"]},{"content":"","title":"Open Source","short_title":"","description":"","tags":["earn","open-source"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/002-open-source.md","slugArray":["earn","002-open-source"]}],"teamMemos":[{"content":"\n## Record \u0026 reward sharing at Dwarves\nIf you're part of the Dwarves community, you'll notice our culture of continuous learning and knowledge sharing. This culture isn’t just about individual growth; it’s also about building a strong, connected community.\n\nWe believe learning is key to success and our goal is to create an environment where it’s prioritized, valued, and rewarded, allowing everyone to grow. \n\nTo support this, we’ve organized a variety of engaging events and activities that encourage and recognize knowledge sharing. Additionally, we allocate a portion of our profits to acknowledge these contributions with awards.\n\n## Our approach to learning and growth\nLearning goes hand in hand with a growth mindset. In our fast-moving industry, staying ahead means picking up new knowledge all the time. The more knowledge you share, the more rewards you earn. It’s that simple.\n\n## Monthly pool of up to 2500 ICY for recognized contributions\nWe’ve set aside a monthly pool of **2500 ICY (around $4000)** to reward those who contribute valuable insights. **70%** of this pool is dedicated to those who actively share knowledge across the community. \n\nAdditionally, we place extra value on contributions in key areas like **AI/LLM**, **Golang**, **Software Architecture**, and **Blockchain**. If you’ve got expertise in these fields, your insights will be especially appreciated and may earn you more ICY.\n\nCheck out the [**🧊・earn-icy**](https://discord.com/channels/462663954813157376/1006198672486309908/1239502938918096960) channel to see how we distribute our ICY and get involved.\n\n## How you can participate and make a contribution\nIf you have something worth sharing? Drop useful links in our research channels such as [**💻・tech**](https://discord.com/channels/462663954813157376/810481888619135046/1281086341995565057), [**💡・til**](https://discord.com/channels/462663954813157376/1001883339046797342/1281097209072320615) to get recognized. Got a topic in mind? Present it in our OGIFs or submit a note to our [**memo**](https://memo.d.foundation/). Community members are welcome to participate too.\n\nWe also love open-source work, especially building tools that boost our productivity. If you’re into that, join the force at [**🦄・build**](https://discord.com/channels/462663954813157376/1280726623414390805/1280791483280261161) and start cooking up something great.\n\nWe hope this push will keep our learning culture strong and moving forward.\n\nHappy coding and sharing!","title":"Record and reward sharing at Dwarves","short_title":"","description":"Discover how the Dwarves community fosters a culture of continuous learning and knowledge sharing. With a monthly reward pool of up to 2500 ICY, contributors are recognized for sharing valuable insights, especially in areas like AI/LLM, Golang, and more. Get involved and grow with us.","tags":["reward","team","community"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_","thanh","monotykamary"],"date":"Thu Sep 05 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/01_literature/record-reward-sharing-culture.md","slugArray":["playground","01_literature","record-reward-sharing-culture"]},{"content":"\n**A Junior Backend Engineer recounts her journey at Dwarves Foundation, from being attracted by their distinctive logo to finding satisfaction in remote work and learning the important lesson that seeking help from supportive team members is key to professional growth.**\n\n![Cat Nguyen - Junior Backend Engineer at Dwarves](assets/notion-image-1744012268919-ye4mt.webp)\n\nMy journey with Dwarves began with a glance at their logo during my senior year at Bach Khoa University. While browsing through tech job listings on the university's website, Dwarves' distinctive red logo caught my eye amidst a sea of white logo backgrounds. I read the JD, found it aligned with my aspirations, aced the test, sailed through interviews, and here I am.\n\nThough I've only been with Dwarves for just over a year, my job satisfaction is a solid 10/10. Thanks to remote work, I save 2-3 hours on commuting, granting me more time for fitness, knowledge updates, and quality moments with my parents.\n\nIn my year with Dwarves, I've worked on two projects: Console Labs and another client project. Starting at Console, I received big support from **Khoi** and **Tuan Dao**. Even with seemingly simple questions, they patiently explained and provided me with reading materials. I did appreciate it.\n\nAnd the most memorable experience was tackling the client project, where the difficulty level soared, delving into domains I hadn't touched before. For two weeks in a row, I worked tirelessly until 11PM to complete difficult tasks. As a newcomer, I hesitated to seek guidance initially, only turning to **Bien Vo** for the toughest queries. Then **Thanh Pham** caught up and assigned **Hieu Phan** as my mentor. Lucky me!\n\nDespite the steep learning curve, I realized a crucial lesson: when in doubt, reach out to Dwarves Team for support; their unwavering support propels both personal and professional growth.\n","title":"#22 Cat Nguyen on team support","short_title":"Cat Nguyen","description":"Cat Nguyen shares her experience as a Junior Backend Engineer at Dwarves, highlighting the supportive team culture and how asking for help accelerated her growth","tags":["life-at-dwarves, backend-engineer, teamwork"],"pinned":false,"draft":false,"hiring":false,"authors":[],"date":"2023-11-27","filePath":"careers/life/2023-11-27-22-cat-nguyen.md","slugArray":["careers","life","2023-11-27-22-cat-nguyen"]},{"content":"These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.\n\n**Negative attitude**\n\nThe employee may exhibit a negative attitude towards their work, colleagues, or the company. They may also show signs of disengagement, lack of motivation, lack of integrity or unwillingness to learn.\n\n**Incompatibility**\n\nThe employee may **not fit** in with the company culture or may have a personality clash with other team members.\n\n**Lack of professionalism**\n\nThe employee may display **unprofessional** behavior such as ⚠️ being consistently late, no show up during working hours, missing deadlines, or failing to communicate effectively.\n\n**Poor performance**\n\nThe employee may not be meeting the expected performance standards and may be **struggling to complete tasks on time**.\n\n**Low morale**\n\nThe employee's presence may be **causing low morale** in the team or even leading to a toxic work environment.\n\n**High turnover rate**\n\nIf the employee has a high turnover rate, it could be a sign of a bad hire.\n\n![](assets/red-flags_8e2d26f28c0d107f0b2dba0b99c0da5e_md5.webp)\n","title":"Red Flags","short_title":"","description":"These red flags are things we don’t want to see in our peeps. Even when you think they are improper, don’t waste your time with a wrong group of people.","tags":["people","teamwork","performance"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Feb 16 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playbook/operations/red-flags.md","slugArray":["playbook","operations","red-flags"]}],"changelogMemos":[{"content":"\nWe kicked off our gathering after Tết, bringing the community back together in true Dwarves style - dropping some SOL and ICY tokens into everyone's wallets, bringing the community back together in true Dwarves style. Not a bad way to start the year.\n\n![](assets/15-new-year-gathering-airdrop.png)\n\n![](assets/15-new-year-gathering-airdrop-icy.png)\n\nOn the first day back, the team flooded Discord with Tết stories and updates. Between the usual tech talk and project discussions, conversations shifted to lucky money tales, family gatherings, and the inevitable food coma from too many bánh chưng and bánh tét.\n\nSome bragged about their winning streak (or admitted their losses) in traditional New Year games, both the triumphs and the mishaps. Others shared their first sips of spring wine, late-night card games, and that one cousin who somehow always wins.\n\n![](assets/15-new-year-gathering-convo.png)\n\nPhotos and stories kept rolling in. From pristine beaches and mountain retreats to hometown reunions and family feasts, each snapshot captured a different way the team spent the break. Some took the chance to travel, exploring new places. Others returned to familiar spots, embracing the warmth of home, reconnecting with loved ones over shared meals and old traditions.\n\n![](assets/15-new-year-gathering-moments-1.png)\n\n![](assets/15-new-year-gathering-2.png)\n\nAnd of course, there were those who simply recharged ,  sleeping in, catching up on games, and enjoying the rare quiet before diving back into the grind. Now, we’re back at it, picking up where we left off. The Year of the Snake has just begun.\n","title":"Weekly Digest #15: New year Gathering: Sharing Tết, starting strong","short_title":"#15 New year gathering","description":"Tết break came to an end, and the Dwarves team reunited to share stories, reconnect, and kick off the Year of the Snake in style. We brought it all back to Discord, along with a little SOL \u0026 ICY drop to start the year right.","tags":["weekly-digest","team","community"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Tue Feb 04 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/15-new-year-gathering.md","slugArray":["updates","digest","15-new-year-gathering"]},{"content":"\nSince the start, Dwarves has valued flexibility, allowing our team to work where inspiration strikes. But there's something special about being together, learning, sharing knowledge, and working side by side.\n\nThat's where our hybrid working model steps in.  \n\nWe didn’t aim to fill desks but to create a space where you can learn, connect, and rediscover the energy that comes from working alongside others, even if it’s just for a day or two.\n\n### What makes the Dwarves office different\n\nAs a borderless software team, we know not everyone finds comfort or focus working remotely all the time. Even local cafes have their charm, but let’s be honest, it’s not always smooth sailing. That’s why our engineers have the full support they need to perform at their best.\n\n**1. Work in comfort**\n\nFrom **Apple Studio Displays** to **Herman Miller chairs**, we’ve set up workspaces adapt to you. Whether you need a quiet corner or an open area for brainstorming, you’ll find a spot that suits your style.\n\n**2. Productivity perks**\n\nDistractions? Not here. With high-speed internet, tranquil meeting rooms, subsidized meals, and 24/7 access, you have everything you need to stay in the zone.\n\n**3. A supportive environment that fuels growth**\n\nEvery interaction, every conversation, is a chance to pick up something new or share what you know.\n\n![](assets/14-back-to-the-office-team.png)\n\n### Why our office check-in is worth it\n\nWe’ve added a little extra motivation to make coming into the office more rewarding. Every time you check in at **🏢・lobby**, you’ll earn **5 ICY** tokens as part of our daily team perks. It’s our way of encouraging you to take advantage of everything the office offers and to make every visit count.\n\n- **Simple and quick:** Check in, earn your ICY. Kick-off your day with a little boost.\n- **Earn while you work:** Whether you're here for a focused work or to catch up with teammates. You’re rewarded just for showing up.\n- **Stay connected:** These perks are little a reminder that we’re building something together, day by day.\n\n![](assets/14-back-to-the-office-checkin.png)\n\n### Shaping a place for real learning\n\nWhy leave your home office? What makes this place worth the trip? It’s not about fancy setups. Team members who hadn't visited in a while began dropping by, finding new ways to connect.\n\nWhen @tom picks up a new skill, he shares it with everyone in person. It was him, standing by a desk, showing others in real time. These face-to-face exchanges spread knowledge throughout the team effortlessly. We didn’t force people back; they started coming in because something changed.\n\nYou see, there are things you can’t capture in a digital thread. A quick tip, a shared screen, a face that lights up when they finally get it. That’s learning.\n\nWe didn’t aim for a typical office with rigid desks. Instead, we created a space where reaching out feels natural, and where every conversation might teach you something new, where knowledge isn’t just passed around, it’s lived and experienced.\n\n![](assets/14-back-to-the-office-teamwork.png)\n\n### Real voices: What our team says\n\nDon’t just take our word for it, here’s what some of our team members have to say about coming back to the office:\n\n- “The quick chats turn into real learning moments. In an environment where mentors and seniors are always learning, newbies feel encouraged to do the same. It’s all rooted in Dwarves' mentorship culture.” - @datnguyen\n- \"Working from the office helps me stay focused, and it's easy to reach out when I need a hand. It feels good to have that balance.\" – @vincent\n- \"I appreciate the mix of working remotely and coming in. The face-to-face chats and shared meals make it feel more connected. It truly embodies the spirit of engineers.\" – @nhuthuynh\n- \"It’s not just about working harder; it’s about working smarter. I’ve shared a lot of knowledge by bouncing ideas off my teammates\" – @tom\n- \"Being at the office a couple of days a week has made it easier to separate work and home life. Plus, it's good to see everyone now and then.\" – @nam\n\n### Where work meets growth\n\nThough this is still in the early stages, some of us have already reached the hub and made it their go-to spot for getting into the zone. We’d be glad to have you as one of them.\n\nAlong with everything, we do everything we can to level up our team. In the end, it’s not just about work; it’s about how we grow, together.\n","title":"Weekly Digest #14: Embracing hybrid work - the best of both worlds","short_title":"#14 Hybrid work harmony","description":"Discover how Dwarves embraces hybrid working, blending flexibility with in-person connections. Learn how our office space fosters productivity, learning, and real collaboration, even if it’s just for a day or two a week.","tags":["weekly-digest","team","hybrid-working"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Wed Sep 25 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/14-back-to-the-office.md","slugArray":["updates","digest","14-back-to-the-office"]},{"content":"\nWelcome to this week's edition of our roundup, where we highlight the moments that make us more than just lines of code. This week, we're jet-setting around the world with our team's summer adventures, unveiling new memo features, and cheering for a beloved teammate's new chapter in the US. From beachside workspaces to anime figurine collections, we've got it all.\n\n### Memo submission and boosting team contributions\n\nIn the upcoming phase, the team will focus on streamlining processes and developing tools to make memo submission easier for everyone. Here are two ways we're helping you stay up to speed:\n\n- Use commands like `?memo pr` and `?memo list` to check out the latest memos. (Shoutout to @ohagi for this!)\n- Receive webhook notifications in the Discord server for new PRs, speeding up memo operations.\n\nWe're also introducing tooling for submitting fleet notes directly on Discord and automating the distribution of ICY to everyone. However, the amount of ICY distributed is still below the team's limit, so there's plenty of room for more contributions. Everyone can join hands.\n\n![](assets/13-more-than-lines-of-code-icy-updates.webp)\n\n![](assets/13-more-than-lines-of-code-memo.webp)\n\n### Summer snapshots: where our remote team works\n\nThis summer, our team's \"Summer Times, Where Are You Working From?\" updates took us on a virtual journey around the world. We saw it all: Swiss mountains, Vietnamese beaches, killer nail art, desks so neat they spark joy, anime havens, and even our COO chilling beachside (while \"working,\" of course).\n\nThese snapshots of our team's lives, filled with personal flair, with a mix of personalities and passions that make our team so rad, no matter where we're working from.\n\n[Check out the full recap of our team's summer escapades.](https://memo.d.foundation/updates/digest/12-where-are-you-working-from-this-summer/)\n\n![](assets/13-more-than-lines-of-code-summer.webp)\n\n![](assets/13-more-than-lines-of-code-summer-moments.webp)\n\n### A fond farewell and best wishes to our teammate Hieu Phan\n\nAs you all saw in the lobby last Friday, we had our final team dinner with @hieuthu1 before his move to the US. The memories we made together in Vietnam will always stay with us, even as he embarks on his new journey under American skies.\n\nHe was always a caring presence at Hado, a fantastic cook, and a friend who joined us on afternoon walks after work, always connecting everyone on the team. We wish you all the best on your new adventure and don't forget to call us.\n\n![](assets/13-more-than-lines-of-code-farewell.webp)\n\n![](assets/13-more-than-lines-of-code-farewell.png)\n\n### Celebrating the newborn\n\nCongratulations are in order for @thanh, who recently welcomed a new addition to his family. The whole team is overjoyed for him and his partner as they embark on this exciting new chapter. There's been plenty of well-wishing and baby-related chatter around the team.  \n\nOf course, with such happy news, the playful banter has already begun: who's going to be the next to share a baby announcement? We'll have to wait and see.\n\nIn the meantime, we're sending Thanh and his family all the best as they navigate the joys (and sleepless nights!) of parenthood.\n\n![](assets/13-more-than-lines-of-code_13-more-than-line-code-new-born.webp)\n\nFrom tech tips to tiny toes, this week's digest reminds us that we're more than just a team. So, whether you're sharing a memo, a vacation snapshot, or a life-changing announcement, remember: that we're here to celebrate it with you.  \n","title":"Weekly Digest #13: More than lines of code","short_title":"#13 More than lines of code","description":"Welcome to this week's edition of our roundup, where we highlight the moments that make us more than just lines of code. This week, we're jet-setting around the world with our team's summer adventures, introducing new memo features, cheering for a beloved teammate's new chapter in the US, and celebrating new born. From beachside workspaces to anime figurine collections, we've got it all.","tags":["memo","team","remote","weekly-digest"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sat Jul 20 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/digest/13-more-than-lines-of-code.md","slugArray":["updates","digest","13-more-than-lines-of-code"]}],"hiringMemos":[]},"__N_SSG":true},"page":"/","query":{},"buildId":"bGFyqUhs6KXRgmgPoIo-F","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>