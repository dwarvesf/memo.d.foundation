<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/0629e1eaeea1b575.css" as="style"/><link rel="preload" href="/_next/static/css/db96805030821792.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0629e1eaeea1b575.css" data-n-g=""/><link rel="stylesheet" href="/_next/static/css/db96805030821792.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-e3db99e50f47eac1.js" defer=""></script><script src="/_next/static/chunks/framework-6e85e635ddcbf499.js" defer=""></script><script src="/_next/static/chunks/main-d8ff59cf99127f56.js" defer=""></script><script src="/_next/static/chunks/pages/_app-4a4b4c29ee0a1be8.js" defer=""></script><script src="/_next/static/chunks/09244f9f-b7c50d32ae94dc35.js" defer=""></script><script src="/_next/static/chunks/8336-e98b59feaeebd03c.js" defer=""></script><script src="/_next/static/chunks/8771-f6089e599c831775.js" defer=""></script><script src="/_next/static/chunks/pages/index-08c93f3b4a2f513e.js" defer=""></script><script src="/_next/static/6RUfDf25HpYZ1MIctalND/_buildManifest.js" defer=""></script><script src="/_next/static/6RUfDf25HpYZ1MIctalND/_ssgManifest.js" defer=""></script></head><body class="min-h-screen antialiased"><script>
              (function() {
                // Get saved theme or default to system preference
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const savedTheme = localStorage.getItem('theme');
                
                // Default to system preference if no saved preference
                const theme = (savedTheme === 'light' || savedTheme === 'dark') 
                  ? savedTheme 
                  : (prefersDark ? 'dark' : 'light');
                
                // Apply theme
                if (theme === 'dark') {
                  document.documentElement.classList.add('dark');
                  document.documentElement.setAttribute('data-theme', 'dark');
                } else {
                  document.documentElement.classList.remove('dark');
                  document.documentElement.setAttribute('data-theme', 'light');
                }
              })();
            </script><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"directoryTree":{"/pinned":{"label":"Pinned","children":{"/culture/readme":{"label":"Notes on our culture","children":{},"url":"/culture/readme"},"/culture/ogif-intro":{"label":"OGIF - Oh God It's Friday","children":{},"url":"/culture/ogif-intro"},"/playground/topics/llm/llm":{"label":"§ LLM","children":{},"url":"/playground/topics/llm/llm"}}},"/careers":{"label":"careers","children":{"/careers/apprentice":{"label":"apprentice","children":{"/careers/apprentice/2022":{"label":"2022","children":{"/careers/apprentice/2022/batch-of-2022.md":{"label":"Batch of 2022","children":{},"url":"/careers/apprentice/2022/batch-of-2022"},"/careers/apprentice/2022/2022-meet-ngoc-thanh-pham.md":{"label":"Thanh Pham","children":{},"url":"/careers/apprentice/2022/2022-meet-ngoc-thanh-pham"},"/careers/apprentice/2022/2022-meet-tuan-dao.md":{"label":"Tuan Dao","children":{},"url":"/careers/apprentice/2022/2022-meet-tuan-dao"}},"url":"/careers/apprentice/2022"},"/careers/apprentice/apprentice.md":{"label":"Apprentice program","children":{},"url":"/careers/apprentice/apprentice"}},"url":"/careers/apprentice"},"/careers/internship":{"label":"internship","children":{"/careers/internship/2019":{"label":"2019","children":{"/careers/internship/2019/2019.md":{"label":"Spring Internship 2019","children":{},"url":"/careers/internship/2019/2019"}},"url":"/careers/internship/2019"}},"url":"/careers/internship"},"/careers/life":{"label":"life","children":{"/careers/life/group":{"label":"group","children":{"/careers/life/group/2024-10-04-life-at-df-team-hangouts.md":{"label":"Life at Dwarves: Team Hangouts","children":{},"url":"/careers/life/group/2024-10-04-life-at-df-team-hangouts"},"/careers/life/group/2024-09-26-life-at-df-record-and-reward-culture-sharing.md":{"label":"Life at Dwarves: Record and reward (culture sharing)","children":{},"url":"/careers/life/group/2024-09-26-life-at-df-record-and-reward-culture-sharing"},"/careers/life/group/2024-09-23-life-at-df-hybrid-working.md":{"label":"Life at Dwarves: H4 Supporting Hybrid Working + SOP checkin (weekly digest #14)","children":{},"url":"/careers/life/group/2024-09-23-life-at-df-hybrid-working"},"/careers/life/group/2023-06-05-life-at-df-golang-102.md":{"label":"Life at Dwarves: Golang 102","children":{},"url":"/careers/life/group/2023-06-05-life-at-df-golang-102"},"/careers/life/group/2023-06-02-learning-group-report.md":{"label":"Learning Group Report","children":{},"url":"/careers/life/group/2023-06-02-learning-group-report"},"/careers/life/group/2023-06-01-software-design-group.md":{"label":"Software Design Group","children":{},"url":"/careers/life/group/2023-06-01-software-design-group"},"/careers/life/group/2023-05-23-life-at-df-community-earn.md":{"label":"Life at Dwarves: Community Earn","children":{},"url":"/careers/life/group/2023-05-23-life-at-df-community-earn"}},"url":"/careers/life/group"},"/careers/life/2024-09-26-29-dat-nguyen.md":{"label":"Dat Nguyen","children":{},"url":"/careers/life/2024-09-26-29-dat-nguyen"},"/careers/life/2024-02-19-28-duyen-tran.md":{"label":"Duyen Tran","children":{},"url":"/careers/life/2024-02-19-28-duyen-tran"},"/careers/life/2024-01-22-27-tri-tran.md":{"label":"Tri Tran","children":{},"url":"/careers/life/2024-01-22-27-tri-tran"},"/careers/life/2024-01-15-26-quoc-tuan.md":{"label":"Quoc Tuan","children":{},"url":"/careers/life/2024-01-15-26-quoc-tuan"},"/careers/life/2024-01-03-25-khoi-nguyen.md":{"label":"Khoi Nguyen","children":{},"url":"/careers/life/2024-01-03-25-khoi-nguyen"},"/careers/life/2023-12-13-24-tai-pham.md":{"label":"Tai Pham","children":{},"url":"/careers/life/2023-12-13-24-tai-pham"},"/careers/life/2023-12-12-23-hieu-nghia.md":{"label":"Hieu Nghia","children":{},"url":"/careers/life/2023-12-12-23-hieu-nghia"},"/careers/life/2023-11-27-22-cat-nguyen.md":{"label":"Cat Nguyen","children":{},"url":"/careers/life/2023-11-27-22-cat-nguyen"},"/careers/life/2023-11-20-21-minh-cloud.md":{"label":"Minh Cloud","children":{},"url":"/careers/life/2023-11-20-21-minh-cloud"},"/careers/life/2023-11-13-20-hoai-khang.md":{"label":"Hoai Khang","children":{},"url":"/careers/life/2023-11-13-20-hoai-khang"},"/careers/life/2023-11-03-19-vi-tran.md":{"label":"Vi Tran","children":{},"url":"/careers/life/2023-11-03-19-vi-tran"},"/careers/life/2023-10-30-18-tuan-tran.md":{"label":"Tuan Tran","children":{},"url":"/careers/life/2023-10-30-18-tuan-tran"},"/careers/life/2023-10-16-16-kim-ngan.md":{"label":"Kim Ngan","children":{},"url":"/careers/life/2023-10-16-16-kim-ngan"},"/careers/life/2023-10-13-17-hoang-nguyen.md":{"label":"Hoang Nguyen","children":{},"url":"/careers/life/2023-10-13-17-hoang-nguyen"},"/careers/life/2023-10-09-15-khoi-ngo.md":{"label":"Khoi Ngo","children":{},"url":"/careers/life/2023-10-09-15-khoi-ngo"},"/careers/life/2023-10-02-14-dat-pham.md":{"label":"Dat Pham","children":{},"url":"/careers/life/2023-10-02-14-dat-pham"},"/careers/life/2023-09-29-13-bien-vo.md":{"label":"Bien Vo","children":{},"url":"/careers/life/2023-09-29-13-bien-vo"},"/careers/life/2023-09-18-12-toan-ho.md":{"label":"Toan Ho","children":{},"url":"/careers/life/2023-09-18-12-toan-ho"},"/careers/life/2023-09-05-11-dinh-nam.md":{"label":"Dinh Nam","children":{},"url":"/careers/life/2023-09-05-11-dinh-nam"},"/careers/life/2023-08-17-10-cuong-mai.md":{"label":"Cuong Mai","children":{},"url":"/careers/life/2023-08-17-10-cuong-mai"},"/careers/life/2023-08-07-9-hoang-anh.md":{"label":"Hoang Anh","children":{},"url":"/careers/life/2023-08-07-9-hoang-anh"},"/careers/life/2023-07-24-8-nhut-huynh.md":{"label":"Nhut Huynh","children":{},"url":"/careers/life/2023-07-24-8-nhut-huynh"},"/careers/life/2023-06-30-7-khac-vy.md":{"label":"Khac Vy","children":{},"url":"/careers/life/2023-06-30-7-khac-vy"},"/careers/life/2022-09-21-7-my-anh.md":{"label":"My Anh","children":{},"url":"/careers/life/2022-09-21-7-my-anh"},"/careers/life/2022-08-11-6-hieu-vu.md":{"label":"Hieu Vu","children":{},"url":"/careers/life/2022-08-11-6-hieu-vu"},"/careers/life/2022-08-04-6-duy-nguyen.md":{"label":"Duy Nguyen","children":{},"url":"/careers/life/2022-08-04-6-duy-nguyen"},"/careers/life/2022-08-03-5-nam-nguyen.md":{"label":"Nam Nguyen","children":{},"url":"/careers/life/2022-08-03-5-nam-nguyen"},"/careers/life/2022-07-22-4-an-tran.md":{"label":"An Tran","children":{},"url":"/careers/life/2022-07-22-4-an-tran"},"/careers/life/2022-03-17-3-tom-nguyen.md":{"label":"Tom Nguyen","children":{},"url":"/careers/life/2022-03-17-3-tom-nguyen"},"/careers/life/2022-02-25-2-anh-tran.md":{"label":"Anh Tran","children":{},"url":"/careers/life/2022-02-25-2-anh-tran"},"/careers/life/2022-02-14-1-thanh-pham.md":{"label":"Thanh Pham","children":{},"url":"/careers/life/2022-02-14-1-thanh-pham"},"/careers/life/2021-03-31-0-tuan-dao.md":{"label":"Tuan Dao","children":{},"url":"/careers/life/2021-03-31-0-tuan-dao"},"/careers/life/2021-03-11-0-phat-nguyen.md":{"label":"Phat Nguyen","children":{},"url":"/careers/life/2021-03-11-0-phat-nguyen"},"/careers/life/2020-05-08-0-thanh-pham.md":{"label":"Thanh Pham","children":{},"url":"/careers/life/2020-05-08-0-thanh-pham"},"/careers/life/2020-04-10-0-huy-nguyen.md":{"label":"Huy Nguyen","children":{},"url":"/careers/life/2020-04-10-0-huy-nguyen"}},"url":"/careers/life"},"/careers/open-positions":{"label":"open-positions","children":{"/careers/open-positions/business-manager.md":{"label":"Business Development Manager","children":{},"url":"/careers/open-positions/business-manager"},"/careers/open-positions/growth-lead.md":{"label":"Growth Lead","children":{},"url":"/careers/open-positions/growth-lead"}},"url":"/careers/open-positions"},"/careers/manifesto.md":{"label":"Manifesto","children":{},"url":"/careers/manifesto"},"/careers/culture.md":{"label":"Culture","children":{},"url":"/careers/culture"},"/careers/life.md":{"label":"Life at Dwarves","children":{},"url":"/careers/life"},"/careers/README.md":{"label":"👋 Join the Dwarves","children":{},"url":"/careers"}},"url":"/careers"},"/consulting":{"label":"consulting","children":{"/consulting/case-study":{"label":"case-study","children":{"/consulting/case-study/screenz-ai.md":{"label":"Screenz.ai","children":{},"url":"/consulting/case-study/screenz-ai"},"/consulting/case-study/kafi.md":{"label":"Kafi","children":{},"url":"/consulting/case-study/kafi"},"/consulting/case-study/droppii.md":{"label":"Droppii","children":{},"url":"/consulting/case-study/droppii"},"/consulting/case-study/konvoy.md":{"label":"Konvoy","children":{},"url":"/consulting/case-study/konvoy"},"/consulting/case-study/cimb.md":{"label":"CIMB","children":{},"url":"/consulting/case-study/cimb"},"/consulting/case-study/swift.md":{"label":"Swift","children":{},"url":"/consulting/case-study/swift"},"/consulting/case-study/startupvn.md":{"label":"StartupVN","children":{},"url":"/consulting/case-study/startupvn"},"/consulting/case-study/open-fabric.md":{"label":"Open Fabric","children":{},"url":"/consulting/case-study/open-fabric"},"/consulting/case-study/icrosschain.md":{"label":"iCrosschain","children":{},"url":"/consulting/case-study/icrosschain"},"/consulting/case-study/hedge-foundation.md":{"label":"Hedge Foundation","children":{},"url":"/consulting/case-study/hedge-foundation"},"/consulting/case-study/searchio.md":{"label":"Search.io","children":{},"url":"/consulting/case-study/searchio"},"/consulting/case-study/tokenomy.md":{"label":"Tokenomy","children":{},"url":"/consulting/case-study/tokenomy"},"/consulting/case-study/basehq.md":{"label":"BaseHQ","children":{},"url":"/consulting/case-study/basehq"},"/consulting/case-study/momos.md":{"label":"Momos","children":{},"url":"/consulting/case-study/momos"},"/consulting/case-study/attrace.md":{"label":"Attrace","children":{},"url":"/consulting/case-study/attrace"},"/consulting/case-study/setel.md":{"label":"Setel","children":{},"url":"/consulting/case-study/setel"},"/consulting/case-study/relay.md":{"label":"Relay","children":{},"url":"/consulting/case-study/relay"},"/consulting/case-study/joinpara.md":{"label":"JoinPara","children":{},"url":"/consulting/case-study/joinpara"},"/consulting/case-study/naru.md":{"label":"Naru","children":{},"url":"/consulting/case-study/naru"},"/consulting/case-study/mudah.md":{"label":"Mudah","children":{},"url":"/consulting/case-study/mudah"},"/consulting/case-study/reapit.md":{"label":"Reapit","children":{},"url":"/consulting/case-study/reapit"},"/consulting/case-study/aharooms.md":{"label":"Aharooms","children":{},"url":"/consulting/case-study/aharooms"},"/consulting/case-study/begroup.md":{"label":"beGroup","children":{},"url":"/consulting/case-study/begroup"},"/consulting/case-study/airwatt.md":{"label":"AirWatt","children":{},"url":"/consulting/case-study/airwatt"},"/consulting/case-study/voconic.md":{"label":"Voconic","children":{},"url":"/consulting/case-study/voconic"},"/consulting/case-study/sol.md":{"label":"Sol","children":{},"url":"/consulting/case-study/sol"},"/consulting/case-study/dental-marketplace.md":{"label":"Dental Marketplace","children":{},"url":"/consulting/case-study/dental-marketplace"},"/consulting/case-study/bhd.md":{"label":"BHD Cinema","children":{},"url":"/consulting/case-study/bhd"}},"url":"/consulting/case-study"},"/consulting/bill-by-hours.md":{"label":"Pricing model: Bill by hours","children":{},"url":"/consulting/bill-by-hours"},"/consulting/partners-network.md":{"label":"Partners Network","children":{},"url":"/consulting/partners-network"},"/consulting/README.md":{"label":"💼 Consulting team","children":{},"url":"/consulting"},"/consulting/service-feedbacks.md":{"label":"Service feedbacks","children":{},"url":"/consulting/service-feedbacks"},"/consulting/fbsc.md":{"label":"FBSC","children":{},"url":"/consulting/fbsc"},"/consulting/collaboration-guideline.md":{"label":"Collaboration Guideline","children":{},"url":"/consulting/collaboration-guideline"},"/consulting/how-to-work-with-clients.md":{"label":"How to work with clients","children":{},"url":"/consulting/how-to-work-with-clients"},"/consulting/setting-the-budget.md":{"label":"Setting The Budget","children":{},"url":"/consulting/setting-the-budget"},"/consulting/fixed-budget-scope-controlled.md":{"label":"Fixed Budget Scope Controlled","children":{},"url":"/consulting/fixed-budget-scope-controlled"},"/consulting/the-adjacent-possible.md":{"label":"The Adjacent Possible","children":{},"url":"/consulting/the-adjacent-possible"}},"url":"/consulting"},"/culture":{"label":"culture","children":{"/culture/culture-test.md":{"label":"The culture test","children":{},"url":"/culture/culture-test"},"/culture/README.md":{"label":"Notes on our culture","children":{},"url":"/culture"},"/culture/ogif-intro.md":{"label":"OGIF - Oh God It's Friday","children":{},"url":"/culture/ogif-intro"},"/culture/red-flags.md":{"label":"Red flags","children":{},"url":"/culture/red-flags"},"/culture/growth-is-our-universal-language.md":{"label":"Growth Is Our Universal Language","children":{},"url":"/culture/growth-is-our-universal-language"},"/culture/focus-on-delivery.md":{"label":"Focus on delivery","children":{},"url":"/culture/focus-on-delivery"},"/culture/are-you-helping.md":{"label":"Are you helping?","children":{},"url":"/culture/are-you-helping"},"/culture/the-inner-circle.md":{"label":"The inner circle","children":{},"url":"/culture/the-inner-circle"},"/culture/making-decision.md":{"label":"Making decision as a team member","children":{},"url":"/culture/making-decision"},"/culture/beyond-the-title.md":{"label":"Beyond the title","children":{},"url":"/culture/beyond-the-title"},"/culture/asking-as-a-junior.md":{"label":"Asking As A Junior","children":{},"url":"/culture/asking-as-a-junior"},"/culture/go-the-extra-mile.md":{"label":"Go the extra mile","children":{},"url":"/culture/go-the-extra-mile"},"/culture/runs-by-ideas.md":{"label":"The Dwarves runs by ideas","children":{},"url":"/culture/runs-by-ideas"},"/culture/a-tips-of-hiring-dont.md":{"label":"A tips of hiring - Do \u0026 Don't","children":{},"url":"/culture/a-tips-of-hiring-dont"},"/culture/culture-handbook.md":{"label":"The Dwarves culture handbook","children":{},"url":"/culture/culture-handbook"},"/culture/startups-vs-junior-designers.md":{"label":"Startups Vs Junior Designers","children":{},"url":"/culture/startups-vs-junior-designers"},"/culture/people-matter.md":{"label":"How people matter should work","children":{},"url":"/culture/people-matter"},"/culture/delegation-and-believe-it-will-work.md":{"label":"Delegation and believe it will work","children":{},"url":"/culture/delegation-and-believe-it-will-work"},"/culture/constructive-feedback.md":{"label":"Constructive feedback","children":{},"url":"/culture/constructive-feedback"},"/culture/transparency.md":{"label":"Transparency","children":{},"url":"/culture/transparency"},"/culture/account-management-strategy.md":{"label":"Account management strategies","children":{},"url":"/culture/account-management-strategy"},"/culture/avoid-burn-out.md":{"label":"Avoid burn out","children":{},"url":"/culture/avoid-burn-out"},"/culture/high-performing-team.md":{"label":"Building a solid high performing team","children":{},"url":"/culture/high-performing-team"},"/culture/delegate-work-not-responsibility.md":{"label":"Delegate work, not responsibility","children":{},"url":"/culture/delegate-work-not-responsibility"},"/culture/blocking-distraction.md":{"label":"Blocking distraction","children":{},"url":"/culture/blocking-distraction"}},"url":"/culture"},"/earn":{"label":"earn","children":{"/earn/000-productivity.md":{"label":"Productivity","children":{},"url":"/earn/000-productivity"},"/earn/001-quality.md":{"label":"Software Quality","children":{},"url":"/earn/001-quality"},"/earn/README.md":{"label":"👾 Open bounties","children":{},"url":"/earn"}},"url":"/earn"},"/fund":{"label":"fund","children":{"/fund/ventures-fund-1.md":{"label":"Dwarves Ventures Fund 1","children":{},"url":"/fund/ventures-fund-1"},"/fund/ventures-fund-0.md":{"label":"Dwarves Ventures Fund 0","children":{},"url":"/fund/ventures-fund-0"}},"url":"/fund"},"/handbook":{"label":"handbook","children":{"/handbook/community":{"label":"community","children":{"/handbook/community/icy-worth.md":{"label":"How much is your ICY worth","children":{},"url":"/handbook/community/icy-worth"},"/handbook/community/icy-swap.md":{"label":"How to swap ICY to BTC","children":{},"url":"/handbook/community/icy-swap"},"/handbook/community/icy.md":{"label":"ICY","children":{},"url":"/handbook/community/icy"},"/handbook/community/discord.md":{"label":"Discord","children":{},"url":"/handbook/community/discord"},"/handbook/community/earn.md":{"label":"Earn","children":{},"url":"/handbook/community/earn"},"/handbook/community/radar.md":{"label":"Radar","children":{},"url":"/handbook/community/radar"},"/handbook/community/README.md":{"label":"Radar","children":{},"url":"/handbook/community"},"/handbook/community/showcase.md":{"label":"Showcase","children":{},"url":"/handbook/community/showcase"},"/handbook/community/sharing.md":{"label":"Sharing knowledge","children":{},"url":"/handbook/community/sharing"},"/handbook/community/memo.md":{"label":"Memo","children":{},"url":"/handbook/community/memo"}},"url":"/handbook/community"},"/handbook/guides":{"label":"guides","children":{"/handbook/guides/check-in-at-office.md":{"label":"Office check-in process for earning ICY","children":{},"url":"/handbook/guides/check-in-at-office"},"/handbook/guides/leave-request.md":{"label":"Leave request","children":{},"url":"/handbook/guides/leave-request"},"/handbook/guides/nda.md":{"label":"NDA \u0026 Agreements","children":{},"url":"/handbook/guides/nda"},"/handbook/guides/configure-company-email.md":{"label":"Configure your company email","children":{},"url":"/handbook/guides/configure-company-email"},"/handbook/guides/one-on-one-meeting.md":{"label":"1-on-1 meetings","children":{},"url":"/handbook/guides/one-on-one-meeting"},"/handbook/guides/reimbursement.md":{"label":"Reimbursement","children":{},"url":"/handbook/guides/reimbursement"},"/handbook/guides/continuing-education-allowance.md":{"label":"Continuing education allowance","children":{},"url":"/handbook/guides/continuing-education-allowance"},"/handbook/guides/password-sharing.md":{"label":"Password Sharing","children":{},"url":"/handbook/guides/password-sharing"},"/handbook/guides/email-communication-and-use.md":{"label":"Email use","children":{},"url":"/handbook/guides/email-communication-and-use"},"/handbook/guides/asset-request.md":{"label":"Request an asset","children":{},"url":"/handbook/guides/asset-request"},"/handbook/guides/effective-meeting.md":{"label":"Effective meetings","children":{},"url":"/handbook/guides/effective-meeting"},"/handbook/guides/conduct-a-meeting.md":{"label":"How to conduct a meeting","children":{},"url":"/handbook/guides/conduct-a-meeting"}},"url":"/handbook/guides"},"/handbook/misc":{"label":"misc","children":{"/handbook/misc/marketing-assets.md":{"label":"Marketing assets","children":{},"url":"/handbook/misc/marketing-assets"}},"url":"/handbook/misc"},"/handbook/navigate-changes.md":{"label":"Navigate changes","children":{},"url":"/handbook/navigate-changes"},"/handbook/making-a-career.md":{"label":"Making a career","children":{},"url":"/handbook/making-a-career"},"/handbook/as-a-community.md":{"label":"As a community","children":{},"url":"/handbook/as-a-community"},"/handbook/knowledge-base.md":{"label":"Knowledge base","children":{},"url":"/handbook/knowledge-base"},"/handbook/stock-option-plan.md":{"label":"Stock option plan","children":{},"url":"/handbook/stock-option-plan"},"/handbook/compliance.md":{"label":"Compliance","children":{},"url":"/handbook/compliance"},"/handbook/README.md":{"label":"📔 Handbook","children":{},"url":"/handbook"},"/handbook/mma.md":{"label":"MMA","children":{},"url":"/handbook/mma"},"/handbook/hybrid-working.md":{"label":"Hybrid Working","children":{},"url":"/handbook/hybrid-working"},"/handbook/routine.md":{"label":"Work routine","children":{},"url":"/handbook/routine"},"/handbook/ventures.md":{"label":"Ventures arm","children":{},"url":"/handbook/ventures"},"/handbook/purpose.md":{"label":"Purpose","children":{},"url":"/handbook/purpose"},"/handbook/how-we-hire.md":{"label":"How we hire","children":{},"url":"/handbook/how-we-hire"},"/handbook/security-rules.md":{"label":"Security rules","children":{},"url":"/handbook/security-rules"},"/handbook/what-we-stand-for.md":{"label":"What we stand for","children":{},"url":"/handbook/what-we-stand-for"},"/handbook/getting-started.md":{"label":"💎 Getting started","children":{},"url":"/handbook/getting-started"},"/handbook/places-to-work.md":{"label":"Places to work","children":{},"url":"/handbook/places-to-work"},"/handbook/dwarves-foundation-is-you.md":{"label":"You are Dwarves Foundation","children":{},"url":"/handbook/dwarves-foundation-is-you"},"/handbook/benefits-and-perks.md":{"label":"Benefits \u0026 perks","children":{},"url":"/handbook/benefits-and-perks"},"/handbook/how-we-spend-money.md":{"label":"How we spend money","children":{},"url":"/handbook/how-we-spend-money"},"/handbook/what-we-value.md":{"label":"What we value","children":{},"url":"/handbook/what-we-value"},"/handbook/who-does-what.md":{"label":"Who does what","children":{},"url":"/handbook/who-does-what"},"/handbook/moonlighting.md":{"label":"Moonlighting","children":{},"url":"/handbook/moonlighting"},"/handbook/where-we-work.md":{"label":"Where we work","children":{},"url":"/handbook/where-we-work"},"/handbook/tools-and-systems.md":{"label":"Tools and systems","children":{},"url":"/handbook/tools-and-systems"},"/handbook/faq.md":{"label":"FAQs","children":{},"url":"/handbook/faq"},"/handbook/how-we-work.md":{"label":"How we work","children":{},"url":"/handbook/how-we-work"}},"url":"/handbook"},"/opensource":{"label":"opensource","children":{"/opensource/README.md":{"label":"☀️ Open source","children":{},"url":"/opensource"}},"url":"/opensource"},"/playbook":{"label":"playbook","children":{"/playbook/design":{"label":"design","children":{"/playbook/design/prototype.md":{"label":"Low-fidelity prototype: UI Design","children":{},"url":"/playbook/design/prototype"},"/playbook/design/aarrr.md":{"label":"aarrr","children":{},"url":"/playbook/design/aarrr"},"/playbook/design/design-sprint.md":{"label":"Design Sprint","children":{},"url":"/playbook/design/design-sprint"},"/playbook/design/lean-canvas.md":{"label":"Lean Canvas","children":{},"url":"/playbook/design/lean-canvas"},"/playbook/design/ux-design.md":{"label":"UX","children":{},"url":"/playbook/design/ux-design"}},"url":"/playbook/design"},"/playbook/engineering":{"label":"engineering","children":{"/playbook/engineering/estimation-guidelines.md":{"label":"Estimation Guidelines","children":{},"url":"/playbook/engineering/estimation-guidelines"},"/playbook/engineering/repo-icon.md":{"label":"release","children":{},"url":"/playbook/engineering/repo-icon"},"/playbook/engineering/presentation.md":{"label":"monitoring","children":{},"url":"/playbook/engineering/presentation"}},"url":"/playbook/engineering"},"/playbook/operations":{"label":"operations","children":{"/playbook/operations/checklists":{"label":"checklists","children":{"/playbook/operations/checklists/leave-and-request-checklist.md":{"label":"Leave Request","children":{},"url":"/playbook/operations/checklists/leave-and-request-checklist"},"/playbook/operations/checklists/offboarding-checklist.md":{"label":"Offboarding","children":{},"url":"/playbook/operations/checklists/offboarding-checklist"},"/playbook/operations/checklists/artifact-checklist.md":{"label":"Back up Artifact","children":{},"url":"/playbook/operations/checklists/artifact-checklist"},"/playbook/operations/checklists/project-handover.md":{"label":"Project Handover","children":{},"url":"/playbook/operations/checklists/project-handover"},"/playbook/operations/checklists/project-communication.md":{"label":"Project Communication","children":{},"url":"/playbook/operations/checklists/project-communication"},"/playbook/operations/checklists/project-archive.md":{"label":"Project Archive","children":{},"url":"/playbook/operations/checklists/project-archive"},"/playbook/operations/checklists/project-initialization.md":{"label":"Project Initialization","children":{},"url":"/playbook/operations/checklists/project-initialization"},"/playbook/operations/checklists/project-case-study.md":{"label":"Project Case Study","children":{},"url":"/playbook/operations/checklists/project-case-study"},"/playbook/operations/checklists/vietnam-invoice-checklist.md":{"label":"Vietnam Invoice","children":{},"url":"/playbook/operations/checklists/vietnam-invoice-checklist"},"/playbook/operations/checklists/unemployment-social-health-insurance.md":{"label":"Unemployment, Social, Health Insurance","children":{},"url":"/playbook/operations/checklists/unemployment-social-health-insurance"},"/playbook/operations/checklists/assets-checklist.md":{"label":"Assets","children":{},"url":"/playbook/operations/checklists/assets-checklist"},"/playbook/operations/checklists/hiring-checklist.md":{"label":"Hiring","children":{},"url":"/playbook/operations/checklists/hiring-checklist"},"/playbook/operations/checklists/candidate-checklist.md":{"label":"Candidate","children":{},"url":"/playbook/operations/checklists/candidate-checklist"},"/playbook/operations/checklists/consulting-contract-checklist.md":{"label":"Consulting Contract","children":{},"url":"/playbook/operations/checklists/consulting-contract-checklist"},"/playbook/operations/checklists/billing-checklist.md":{"label":"Billing","children":{},"url":"/playbook/operations/checklists/billing-checklist"},"/playbook/operations/checklists/onboarding-checklist.md":{"label":"Onboarding","children":{},"url":"/playbook/operations/checklists/onboarding-checklist"}},"url":"/playbook/operations/checklists"},"/playbook/operations/email-template":{"label":"email-template","children":{"/playbook/operations/email-template/hung-king-commemoration-day.md":{"label":"Hung King Commemoration Day","children":{},"url":"/playbook/operations/email-template/hung-king-commemoration-day"},"/playbook/operations/email-template/follow-up-onboarding-items.md":{"label":"Follow-up Onboarding Items","children":{},"url":"/playbook/operations/email-template/follow-up-onboarding-items"},"/playbook/operations/email-template/thank-you-letter.md":{"label":"Thank you letter","children":{},"url":"/playbook/operations/email-template/thank-you-letter"},"/playbook/operations/email-template/referral-bonus-confirmation-note.md":{"label":"Referral Bonus Confirmation Note","children":{},"url":"/playbook/operations/email-template/referral-bonus-confirmation-note"},"/playbook/operations/email-template/salary-increment.md":{"label":"Salary Increment Announcement","children":{},"url":"/playbook/operations/email-template/salary-increment"},"/playbook/operations/email-template/assignment-invitation-2.md":{"label":"Assignment Inviation (Skip pre-assessment)","children":{},"url":"/playbook/operations/email-template/assignment-invitation-2"},"/playbook/operations/email-template/assignment-invitation.md":{"label":"Assignment Inviation","children":{},"url":"/playbook/operations/email-template/assignment-invitation"},"/playbook/operations/email-template/new-year-day.md":{"label":"New Year Day","children":{},"url":"/playbook/operations/email-template/new-year-day"},"/playbook/operations/email-template/welcome-onboard.md":{"label":"Welcome Onboard","children":{},"url":"/playbook/operations/email-template/welcome-onboard"},"/playbook/operations/email-template/welcome-to-dwarves-update.md":{"label":"Welcome to Dwarves Updates","children":{},"url":"/playbook/operations/email-template/welcome-to-dwarves-update"},"/playbook/operations/email-template/rejection-email.md":{"label":"Rejection","children":{},"url":"/playbook/operations/email-template/rejection-email"},"/playbook/operations/email-template/confirm-resume-date.md":{"label":"Confirm Employee's Resume Date Day","children":{},"url":"/playbook/operations/email-template/confirm-resume-date"},"/playbook/operations/email-template/milestone-sign-off.md":{"label":"Milestone sign-off","children":{},"url":"/playbook/operations/email-template/milestone-sign-off"},"/playbook/operations/email-template/farewell.md":{"label":"Farewell Letter","children":{},"url":"/playbook/operations/email-template/farewell"},"/playbook/operations/email-template/offer-letter.md":{"label":"Offer Letter","children":{},"url":"/playbook/operations/email-template/offer-letter"},"/playbook/operations/email-template/international-labour-day.md":{"label":"International Labour Day","children":{},"url":"/playbook/operations/email-template/international-labour-day"},"/playbook/operations/email-template/tet-holiday.md":{"label":"Tet Holiday","children":{},"url":"/playbook/operations/email-template/tet-holiday"},"/playbook/operations/email-template/interview-invitation.md":{"label":"Interview Invitation","children":{},"url":"/playbook/operations/email-template/interview-invitation"},"/playbook/operations/email-template/national-day.md":{"label":"National Day","children":{},"url":"/playbook/operations/email-template/national-day"},"/playbook/operations/email-template/information-about-resource-change.md":{"label":"Inform about resource change","children":{},"url":"/playbook/operations/email-template/information-about-resource-change"}},"url":"/playbook/operations/email-template"},"/playbook/operations/how-to-conduct-delivery-reports.md":{"label":"How to conduct delivery reports","children":{},"url":"/playbook/operations/how-to-conduct-delivery-reports"},"/playbook/operations/how-we-do-effective-planning-and-reporting.md":{"label":"How we do effective planning and reporting","children":{},"url":"/playbook/operations/how-we-do-effective-planning-and-reporting"},"/playbook/operations/project-schedule-delivery-guidelines.md":{"label":"Project Delivery Schedule and Guidelines","children":{},"url":"/playbook/operations/project-schedule-delivery-guidelines"},"/playbook/operations/mbti-type-intj.md":{"label":"MBTI Type INTJ","children":{},"url":"/playbook/operations/mbti-type-intj"},"/playbook/operations/mbti-type-istp.md":{"label":"MBTI Type ISTP","children":{},"url":"/playbook/operations/mbti-type-istp"},"/playbook/operations/mbti-type-estj.md":{"label":"MBTI Type ESTJ","children":{},"url":"/playbook/operations/mbti-type-estj"},"/playbook/operations/mbti-type-istj.md":{"label":"MBTI Type ISTJ","children":{},"url":"/playbook/operations/mbti-type-istj"},"/playbook/operations/applying-myersbriggs-type-indicator-in-hr.md":{"label":"Applying Myersbriggs Type Indicator In Hiring","children":{},"url":"/playbook/operations/applying-myersbriggs-type-indicator-in-hr"},"/playbook/operations/the-four-preferences.md":{"label":"The Four Preferences","children":{},"url":"/playbook/operations/the-four-preferences"},"/playbook/operations/adjust-the-way-we-work-in-basecamp-style.md":{"label":"Adjust The Way We Work In Basecamp Style","children":{},"url":"/playbook/operations/adjust-the-way-we-work-in-basecamp-style"},"/playbook/operations/bric-a-brac.md":{"label":"Bric A Brac","children":{},"url":"/playbook/operations/bric-a-brac"},"/playbook/operations/writing-management-objectives-in-smart.md":{"label":"Writing Management Objectives In Smart","children":{},"url":"/playbook/operations/writing-management-objectives-in-smart"},"/playbook/operations/hiring-for-operations-team.md":{"label":"Hiring For Operations Team","children":{},"url":"/playbook/operations/hiring-for-operations-team"},"/playbook/operations/annual-bonus-for-sales.md":{"label":"Annual bonus for sales","children":{},"url":"/playbook/operations/annual-bonus-for-sales"},"/playbook/operations/collaboration-guidelines.md":{"label":"Collaboration Guidelines","children":{},"url":"/playbook/operations/collaboration-guidelines"},"/playbook/operations/compliance-check-process.md":{"label":"Compliance Check Process","children":{},"url":"/playbook/operations/compliance-check-process"},"/playbook/operations/naming-convention.md":{"label":"Naming convention","children":{},"url":"/playbook/operations/naming-convention"},"/playbook/operations/setup-email-template.md":{"label":"Setup email template in Gmail","children":{},"url":"/playbook/operations/setup-email-template"},"/playbook/operations/types-of-employees.md":{"label":"Types Of Employees","children":{},"url":"/playbook/operations/types-of-employees"},"/playbook/operations/hiring-approach.md":{"label":"Hiring Approach","children":{},"url":"/playbook/operations/hiring-approach"},"/playbook/operations/the-okr.md":{"label":"The OKR","children":{},"url":"/playbook/operations/the-okr"},"/playbook/operations/our-metrics-for-performance-review.md":{"label":"Our Metrics For Performance Review","children":{},"url":"/playbook/operations/our-metrics-for-performance-review"},"/playbook/operations/make-remote-working-works.md":{"label":"Make Remote Working Works","children":{},"url":"/playbook/operations/make-remote-working-works"},"/playbook/operations/our-policy-for-remote-working.md":{"label":"Our Policy For Remote Working","children":{},"url":"/playbook/operations/our-policy-for-remote-working"}},"url":"/playbook/operations"},"/playbook/README.md":{"label":"Playbook","children":{},"url":"/playbook"}},"url":"/playbook"},"/playground":{"label":"playground","children":{"/playground/notes":{"label":"notes","children":{"/playground/notes/misc":{"label":"misc","children":{"/playground/notes/misc/giving-a-talk-checklist.md":{"label":"Giving a talk","children":{},"url":"/playground/notes/misc/giving-a-talk-checklist"},"/playground/notes/misc/Founder Liquidity.md":{"label":"Founder Liquidity","children":{},"url":"/playground/notes/misc/founder-liquidity"},"/playground/notes/misc/record-reward-sharing-culture.md":{"label":"Record and reward sharing at Dwarves","children":{},"url":"/playground/notes/misc/record-reward-sharing-culture"},"/playground/notes/misc/vietnam-tech-ecosystem-report.md":{"label":"Vietnam Tech Ecosystem 2024 Report","children":{},"url":"/playground/notes/misc/vietnam-tech-ecosystem-report"},"/playground/notes/misc/organize-team-know-how-with-zettelkasten-method.md":{"label":"Organize team know-how with Zettelkasten Method","children":{},"url":"/playground/notes/misc/organize-team-know-how-with-zettelkasten-method"},"/playground/notes/misc/memo-knowledge-base-meeting.md":{"label":"Memo Knowledge Base Meeting","children":{},"url":"/playground/notes/misc/memo-knowledge-base-meeting"},"/playground/notes/misc/peep-nft.md":{"label":"Claim your Peeps NFT","children":{},"url":"/playground/notes/misc/peep-nft"},"/playground/notes/misc/recording-flow.md":{"label":"How We Set Up a Recording Workflow for Dwarves Office Hours","children":{},"url":"/playground/notes/misc/recording-flow"},"/playground/notes/misc/memo-publication-workflow.md":{"label":"Memo Publication Workflow","children":{},"url":"/playground/notes/misc/memo-publication-workflow"},"/playground/notes/misc/How to make a MOC.md":{"label":"How to make a MOC","children":{},"url":"/playground/notes/misc/how-to-make-a-moc"},"/playground/notes/misc/echelon-x-singapore-2024-where-innovations-meet-inspiration.md":{"label":"Echelon X Singapore 2024: Where Innovations Meet Inspiration","children":{},"url":"/playground/notes/misc/echelon-x-singapore-2024-where-innovations-meet-inspiration"},"/playground/notes/misc/how-i-create-content-for-multiple-platforms-at-dwarves.md":{"label":"How I Create Content for Multiple Platforms at Dwarves","children":{},"url":"/playground/notes/misc/how-i-create-content-for-multiple-platforms-at-dwarves"},"/playground/notes/misc/how-to-transfer-dfg-from-eth-to-base-for-staking.md":{"label":"How to bridge $DFG from Ethereum Mainnet to Base Network for staking","children":{},"url":"/playground/notes/misc/how-to-transfer-dfg-from-eth-to-base-for-staking"},"/playground/notes/misc/how-to-earn-reward-from-staking-dfg.md":{"label":"How to earn reward from staking DFG","children":{},"url":"/playground/notes/misc/how-to-earn-reward-from-staking-dfg"},"/playground/notes/misc/level-up-your-markdown-memos.md":{"label":"Level Up Your Markdown Memos: Avoiding Common Pitfalls","children":{},"url":"/playground/notes/misc/level-up-your-markdown-memos"},"/playground/notes/misc/design-less-present-more-with-deckset.md":{"label":"Design less, present more with Deckset","children":{},"url":"/playground/notes/misc/design-less-present-more-with-deckset"},"/playground/notes/misc/how-to-recap-a-publication.md":{"label":"Recapping A publication","children":{},"url":"/playground/notes/misc/how-to-recap-a-publication"},"/playground/notes/misc/lifecycle-of-a-publication.md":{"label":"Life cycle of a publication","children":{},"url":"/playground/notes/misc/lifecycle-of-a-publication"},"/playground/notes/misc/how-to-set-up-environment-for-editing-memo.md":{"label":"How to set up environment to edit memo","children":{},"url":"/playground/notes/misc/how-to-set-up-environment-for-editing-memo"},"/playground/notes/misc/_how-to-setup-crypto-wallet-to-withdraw-icy.md":{"label":"How to set up crypto wallet to withdraw ICY","children":{},"url":"/playground/notes/misc/_how-to-setup-crypto-wallet-to-withdraw-icy"},"/playground/notes/misc/how-to-take-better-screenshots-on-mac.md":{"label":"How To Take Better Screenshots On Mac","children":{},"url":"/playground/notes/misc/how-to-take-better-screenshots-on-mac"},"/playground/notes/misc/_how-to-withdraw-icy.md":{"label":"How to withdraw ICY","children":{},"url":"/playground/notes/misc/_how-to-withdraw-icy"},"/playground/notes/misc/how-to-push-content-on-note-d.md":{"label":"How to push content on memo.d.foundation","children":{},"url":"/playground/notes/misc/how-to-push-content-on-note-d"},"/playground/notes/misc/salary-advance.md":{"label":"$icy Salary Advance","children":{},"url":"/playground/notes/misc/salary-advance"},"/playground/notes/misc/icy-in-2024.md":{"label":"$icy in 2024","children":{},"url":"/playground/notes/misc/icy-in-2024"},"/playground/notes/misc/icy-dfg.md":{"label":"💠 df protocol, $icy and $dfg","children":{},"url":"/playground/notes/misc/icy-dfg"},"/playground/notes/misc/Knowledge Journey.md":{"label":"Knowledge Journey","children":{},"url":"/playground/notes/misc/knowledge-journey"},"/playground/notes/misc/Reward Model \u0026 Nomination.md":{"label":"Reward Model \u0026 Nomination","children":{},"url":"/playground/notes/misc/reward-model-nomination"},"/playground/notes/misc/How R\u0026D contributes to Performance Review.md":{"label":"How R\u0026D contributes to Performance Review","children":{},"url":"/playground/notes/misc/how-rd-contributes-to-performance-review"},"/playground/notes/misc/configure-the-company-email.md":{"label":"Configure The Company Email","children":{},"url":"/playground/notes/misc/configure-the-company-email"},"/playground/notes/misc/tech-event---in-the-latest-transforming-healthcare-with-technology.md":{"label":"Tech Event In The Latest Transforming Healthcare With Technology","children":{},"url":"/playground/notes/misc/tech-event-in-the-latest-transforming-healthcare-with-technology"},"/playground/notes/misc/passing-the-probation-get-3-upvotes.md":{"label":"Passing The Probation Get 3 Upvotes","children":{},"url":"/playground/notes/misc/passing-the-probation-get-3-upvotes"},"/playground/notes/misc/dwarves-radio-talk-17-conduct-a-1-1-session.md":{"label":"Dwarves Radio Talk 17 Conduct A 1 1 Session","children":{},"url":"/playground/notes/misc/dwarves-radio-talk-17-conduct-a-1-1-session"},"/playground/notes/misc/dwarves-radio-talk-16-run-an-effective-performance-review.md":{"label":"Dwarves Radio Talk 16 Run An Effective Performance Review","children":{},"url":"/playground/notes/misc/dwarves-radio-talk-16-run-an-effective-performance-review"},"/playground/notes/misc/introduce-to-dwarves-memo.md":{"label":"Introduce To Dwarves Memo","children":{},"url":"/playground/notes/misc/introduce-to-dwarves-memo"},"/playground/notes/misc/objective.md":{"label":"Objective","children":{},"url":"/playground/notes/misc/objective"},"/playground/notes/misc/traits-to-assess-during-an-interview.md":{"label":"Traits To Assess During An Interview","children":{},"url":"/playground/notes/misc/traits-to-assess-during-an-interview"},"/playground/notes/misc/resource-assignment.md":{"label":"Resource Assignment","children":{},"url":"/playground/notes/misc/resource-assignment"},"/playground/notes/misc/considering-factors-for-performance-evaluating.md":{"label":"Considering Factors For Performance Evaluating","children":{},"url":"/playground/notes/misc/considering-factors-for-performance-evaluating"},"/playground/notes/misc/how-we-contribute-to-homebrew.md":{"label":"How We Contribute To Homebrew","children":{},"url":"/playground/notes/misc/how-we-contribute-to-homebrew"},"/playground/notes/misc/remote-prepare-and-get-going.md":{"label":"Remote Prepare And Get Going","children":{},"url":"/playground/notes/misc/remote-prepare-and-get-going"}},"url":"/playground/notes/misc"}},"url":"/playground/notes"},"/playground/rfc":{"label":"rfc","children":{"/playground/rfc/000-template.md":{"label":"000 RFC template","children":{},"url":"/playground/rfc/000-template"},"/playground/rfc/README.md":{"label":"RFCs","children":{},"url":"/playground/rfc"}},"url":"/playground/rfc"},"/playground/topics":{"label":"topics","children":{"/playground/topics/ai":{"label":"ai","children":{"/playground/topics/ai/securing-your-remote-mcp-servers.md":{"label":"Securing your remote MCP servers","children":{},"url":"/playground/topics/ai/securing-your-remote-mcp-servers"},"/playground/topics/ai/tool-level-security-for-remote-mcp-servers.md":{"label":"Tool-Level Security for Remote MCP Servers","children":{},"url":"/playground/topics/ai/tool-level-security-for-remote-mcp-servers"},"/playground/topics/ai/why-hollywood-and-gaming-struggle-with-ai.md":{"label":"Why Hollywood and gaming struggle with AI","children":{},"url":"/playground/topics/ai/why-hollywood-and-gaming-struggle-with-ai"},"/playground/topics/ai/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing.md":{"label":"How we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","children":{},"url":"/playground/topics/ai/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing"},"/playground/topics/ai/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy.md":{"label":"Explaining Gradient Descent in Machine Learning with a simple analogy","children":{},"url":"/playground/topics/ai/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy"},"/playground/topics/ai/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt.md":{"label":"How We Created An Ai Powered Interview System Using Openais Chatgpt","children":{},"url":"/playground/topics/ai/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt"}},"url":"/playground/topics/ai"},"/playground/topics/architecture":{"label":"architecture","children":{"/playground/topics/architecture/automata.md":{"label":"Automata","children":{},"url":"/playground/topics/architecture/automata"},"/playground/topics/architecture/observer-pattern.md":{"label":"Introduce the Observer pattern and its use cases","children":{},"url":"/playground/topics/architecture/observer-pattern"},"/playground/topics/architecture/visitor-design-pattern.md":{"label":"Visitor design pattern, the concept, problem solution and use cases","children":{},"url":"/playground/topics/architecture/visitor-design-pattern"},"/playground/topics/architecture/strategy-design-pattern.md":{"label":"Strategy design pattern, the concept, use cases and difference with the state design pattern","children":{},"url":"/playground/topics/architecture/strategy-design-pattern"},"/playground/topics/architecture/state-pattern.md":{"label":"State Pattern","children":{},"url":"/playground/topics/architecture/state-pattern"},"/playground/topics/architecture/command-pattern.md":{"label":"Command Pattern","children":{},"url":"/playground/topics/architecture/command-pattern"},"/playground/topics/architecture/prototype-design-pattern.md":{"label":"Going Through use cases of the prototype design pattern and it place among the creational patterns","children":{},"url":"/playground/topics/architecture/prototype-design-pattern"},"/playground/topics/architecture/builder-design-pattern.md":{"label":"Introduce the Builder pattern and its use cases","children":{},"url":"/playground/topics/architecture/builder-design-pattern"},"/playground/topics/architecture/singleton-design-pattern.md":{"label":"A tour of Singleton design pattern with Golang","children":{},"url":"/playground/topics/architecture/singleton-design-pattern"},"/playground/topics/architecture/sdk-event-sourcing.md":{"label":"Sdk Event Sourcing","children":{},"url":"/playground/topics/architecture/sdk-event-sourcing"},"/playground/topics/architecture/overview-on-broker-pattern-in-distributed-system.md":{"label":"Overview On Broker Pattern In Distributed System","children":{},"url":"/playground/topics/architecture/overview-on-broker-pattern-in-distributed-system"}},"url":"/playground/topics/architecture"},"/playground/topics/blockchain":{"label":"blockchain","children":{"/playground/topics/blockchain/blockchain-oracle.md":{"label":"¶ Oracle","children":{},"url":"/playground/topics/blockchain/blockchain-oracle"},"/playground/topics/blockchain/build-custom-ai-agent-with-elizaos.md":{"label":"Build custom AI Agent with ElizaOS","children":{},"url":"/playground/topics/blockchain/build-custom-ai-agent-with-elizaos"},"/playground/topics/blockchain/web3-development-with-foundry.md":{"label":"Web3 Development with Foundry","children":{},"url":"/playground/topics/blockchain/web3-development-with-foundry"},"/playground/topics/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin.md":{"label":"Implement a Token Swap from the Base chain to Bitcoin for cross-chain transactions","children":{},"url":"/playground/topics/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin"},"/playground/topics/blockchain/using-foundry-for-evm-smart-contract-developement.md":{"label":"Using Foundry for EVM smart contract development","children":{},"url":"/playground/topics/blockchain/using-foundry-for-evm-smart-contract-developement"},"/playground/topics/blockchain/ton-core-concept.md":{"label":"Ton's base concepts","children":{},"url":"/playground/topics/blockchain/ton-core-concept"},"/playground/topics/blockchain/dollar-cost-averaging.md":{"label":"Dollar Cost Averaging (DCA)","children":{},"url":"/playground/topics/blockchain/dollar-cost-averaging"},"/playground/topics/blockchain/design-system-for-layer-2-using-zk-rollup.md":{"label":"Design System For Layer 2 Using Zk Rollup","children":{},"url":"/playground/topics/blockchain/design-system-for-layer-2-using-zk-rollup"},"/playground/topics/blockchain/plonky2.md":{"label":"Plonky2","children":{},"url":"/playground/topics/blockchain/plonky2"},"/playground/topics/blockchain/polygon-zkevm-architecture.md":{"label":"Polygon zkEVM architecture","children":{},"url":"/playground/topics/blockchain/polygon-zkevm-architecture"},"/playground/topics/blockchain/starknet-architecture.md":{"label":"StarkNet architecture","children":{},"url":"/playground/topics/blockchain/starknet-architecture"},"/playground/topics/blockchain/zk-snarks.md":{"label":"zk-SNARKs","children":{},"url":"/playground/topics/blockchain/zk-snarks"},"/playground/topics/blockchain/¶ Zero-knowledge Proofs.md":{"label":"Zero-knowledge Proofs","children":{},"url":"/playground/topics/blockchain/zero-knowledge-proofs"},"/playground/topics/blockchain/multisign-wallet.md":{"label":"Multisign wallet","children":{},"url":"/playground/topics/blockchain/multisign-wallet"},"/playground/topics/blockchain/blockchain-bridge.md":{"label":"Blockchain Bridge","children":{},"url":"/playground/topics/blockchain/blockchain-bridge"},"/playground/topics/blockchain/nft-fractionalization.md":{"label":"NFT Fractionalization","children":{},"url":"/playground/topics/blockchain/nft-fractionalization"},"/playground/topics/blockchain/¶ Distributed systems.md":{"label":"Distributed systems","children":{},"url":"/playground/topics/blockchain/distributed-systems"},"/playground/topics/blockchain/¶ Blocks.md":{"label":"Blocks","children":{},"url":"/playground/topics/blockchain/blocks"},"/playground/topics/blockchain/¶ Smart Contract.md":{"label":"Smart Contract","children":{},"url":"/playground/topics/blockchain/smart-contract"},"/playground/topics/blockchain/¶ PoS.md":{"label":"PoS","children":{},"url":"/playground/topics/blockchain/pos"},"/playground/topics/blockchain/liquidity-pool.md":{"label":"Liquidity pool","children":{},"url":"/playground/topics/blockchain/liquidity-pool"},"/playground/topics/blockchain/blockchain-for-designers.md":{"label":"Blockchain For Designers","children":{},"url":"/playground/topics/blockchain/blockchain-for-designers"},"/playground/topics/blockchain/federated-byzantine.md":{"label":"Federated Byzantine","children":{},"url":"/playground/topics/blockchain/federated-byzantine"},"/playground/topics/blockchain/fabric-hyperledger-architecture-explanation.md":{"label":"Fabric Hyperledger Architecture Explanation","children":{},"url":"/playground/topics/blockchain/fabric-hyperledger-architecture-explanation"}},"url":"/playground/topics/blockchain"},"/playground/topics/data":{"label":"data","children":{"/playground/topics/data/database-design-circular.md":{"label":"Database design Circular","children":{},"url":"/playground/topics/data/database-design-circular"},"/playground/topics/data/creating-a-fully-local-search-engine-on-memo.md":{"label":"Building a Local Search Engine for Our Memo Website","children":{},"url":"/playground/topics/data/creating-a-fully-local-search-engine-on-memo"},"/playground/topics/data/local-first-software.md":{"label":"Local-first Software","children":{},"url":"/playground/topics/data/local-first-software"},"/playground/topics/data/bloom-filter.md":{"label":"Bloom Filter","children":{},"url":"/playground/topics/data/bloom-filter"},"/playground/topics/data/how-i-came-up-with-our-security-standard.md":{"label":"How I came up with our Security Standard","children":{},"url":"/playground/topics/data/how-i-came-up-with-our-security-standard"},"/playground/topics/data/data-pipeline-design-framework.md":{"label":"Data Pipeline Design Framework","children":{},"url":"/playground/topics/data/data-pipeline-design-framework"},"/playground/topics/data/quick-learning-vector-database.md":{"label":"Quick Learning Vector Database","children":{},"url":"/playground/topics/data/quick-learning-vector-database"},"/playground/topics/data/duckdb-demo-and-showcase.md":{"label":"DuckDB demo and showcase","children":{},"url":"/playground/topics/data/duckdb-demo-and-showcase"},"/playground/topics/data/introduction-to-crdt.md":{"label":"Introduction to CRDT","children":{},"url":"/playground/topics/data/introduction-to-crdt"},"/playground/topics/data/sql-sargable-queries-and-their-impact-on-database-performance.md":{"label":"SQL Saragable Queries and Their Impact on Database Performance","children":{},"url":"/playground/topics/data/sql-sargable-queries-and-their-impact-on-database-performance"},"/playground/topics/data/sql-and-how-it-relates-to-disk-reads-and-writes.md":{"label":"SQL and how it relates to Disk Reads and Writes","children":{},"url":"/playground/topics/data/sql-and-how-it-relates-to-disk-reads-and-writes"},"/playground/topics/data/managing-dataflow-and-sql-database-with-concurrency-control.md":{"label":"Managing Dataflow And Sql Database With Concurrency Control","children":{},"url":"/playground/topics/data/managing-dataflow-and-sql-database-with-concurrency-control"},"/playground/topics/data/202301191192-multi-column-index-in-db.md":{"label":"Multi-column index in DB","children":{},"url":"/playground/topics/data/202301191192-multi-column-index-in-db"},"/playground/topics/data/202211141513-materialized-view-pattern.md":{"label":"Materialized View Pattern","children":{},"url":"/playground/topics/data/202211141513-materialized-view-pattern"},"/playground/topics/data/mapreduce.md":{"label":"MapReduce","children":{},"url":"/playground/topics/data/mapreduce"},"/playground/topics/data/202210150019-migration-planning.md":{"label":"Migration Planning","children":{},"url":"/playground/topics/data/202210150019-migration-planning"},"/playground/topics/data/data-analyst-in-retail-trading.md":{"label":"Data Analyst In Retail Trading","children":{},"url":"/playground/topics/data/data-analyst-in-retail-trading"},"/playground/topics/data/sql-practices-orm-vs-plain-sql.md":{"label":"Sql Practices Orm Vs Plain Sql","children":{},"url":"/playground/topics/data/sql-practices-orm-vs-plain-sql"}},"url":"/playground/topics/data"},"/playground/topics/design":{"label":"design","children":{"/playground/topics/design/writing-content-for-multimedia-guidelines.md":{"label":"Writing Content for Multimedia Guidelines","children":{},"url":"/playground/topics/design/writing-content-for-multimedia-guidelines"},"/playground/topics/design/understanding-an-application-design.md":{"label":"Understanding An Application Design","children":{},"url":"/playground/topics/design/understanding-an-application-design"},"/playground/topics/design/what-i-learned-on-design-thinking-and-software-development.md":{"label":"What I Learned On Design Thinking And Software Development","children":{},"url":"/playground/topics/design/what-i-learned-on-design-thinking-and-software-development"},"/playground/topics/design/six-things-i-extracted-from-design-thinking.md":{"label":"Six Things I Extracted From Design Thinking","children":{},"url":"/playground/topics/design/six-things-i-extracted-from-design-thinking"},"/playground/topics/design/getting-started-with-webflow.md":{"label":"Getting Started With Webflow","children":{},"url":"/playground/topics/design/getting-started-with-webflow"},"/playground/topics/design/ui-design-best-practices-dwarves.md":{"label":"Ui Design Best Practices Dwarves","children":{},"url":"/playground/topics/design/ui-design-best-practices-dwarves"},"/playground/topics/design/the-correct-way-to-build-kpi.md":{"label":"The Correct Way To Build Kpi","children":{},"url":"/playground/topics/design/the-correct-way-to-build-kpi"},"/playground/topics/design/domain-insight-research-framework.md":{"label":"Domain Insight Research Framework","children":{},"url":"/playground/topics/design/domain-insight-research-framework"},"/playground/topics/design/grid-and-layout.md":{"label":"Grid And Layout","children":{},"url":"/playground/topics/design/grid-and-layout"},"/playground/topics/design/gestalt-principles-in-ui-design.md":{"label":"Gestalt Principles In Ui Design","children":{},"url":"/playground/topics/design/gestalt-principles-in-ui-design"},"/playground/topics/design/how-a-design-system-work.md":{"label":"How A Design System Work","children":{},"url":"/playground/topics/design/how-a-design-system-work"},"/playground/topics/design/design-better-mobile-application.md":{"label":"Design Better Mobile Application","children":{},"url":"/playground/topics/design/design-better-mobile-application"},"/playground/topics/design/domain-glossary.md":{"label":"Domain Glossary","children":{},"url":"/playground/topics/design/domain-glossary"},"/playground/topics/design/the-principle-of-spacing-in-ui-design-part-2.md":{"label":"The Principle Of Spacing In Ui Design Part 2","children":{},"url":"/playground/topics/design/the-principle-of-spacing-in-ui-design-part-2"},"/playground/topics/design/card-sorting-and-a-glimpse-at-experimental-sorting-session.md":{"label":"Card Sorting And A Glimpse At Experimental Sorting Session","children":{},"url":"/playground/topics/design/card-sorting-and-a-glimpse-at-experimental-sorting-session"},"/playground/topics/design/good-design-understanding.md":{"label":"Good Design Understanding","children":{},"url":"/playground/topics/design/good-design-understanding"},"/playground/topics/design/competency-mapping.md":{"label":"Competency Mapping","children":{},"url":"/playground/topics/design/competency-mapping"},"/playground/topics/design/design-resourcestools.md":{"label":"Design Resourcestools","children":{},"url":"/playground/topics/design/design-resourcestools"},"/playground/topics/design/design-tips-tricks.md":{"label":"Design Tips Tricks","children":{},"url":"/playground/topics/design/design-tips-tricks"},"/playground/topics/design/design-system.md":{"label":"Design System","children":{},"url":"/playground/topics/design/design-system"},"/playground/topics/design/design-workflow.md":{"label":"Design Workflow","children":{},"url":"/playground/topics/design/design-workflow"},"/playground/topics/design/three-levels-of-design.md":{"label":"Three Levels Of Design","children":{},"url":"/playground/topics/design/three-levels-of-design"},"/playground/topics/design/ui-design-fundamental.md":{"label":"Ui Design Fundamental","children":{},"url":"/playground/topics/design/ui-design-fundamental"},"/playground/topics/design/ux-model.md":{"label":"Ux Model","children":{},"url":"/playground/topics/design/ux-model"},"/playground/topics/design/the-principle-of-spacing-in-ui-design-part-1.md":{"label":"The Principle Of Spacing In Ui Design Part 1","children":{},"url":"/playground/topics/design/the-principle-of-spacing-in-ui-design-part-1"}},"url":"/playground/topics/design"},"/playground/topics/devbox":{"label":"devbox","children":{"/playground/topics/devbox/guide":{"label":"guide","children":{"/playground/topics/devbox/guide/devbox-json.md":{"label":"Devbox.json: Your Project's DNA","children":{},"url":"/playground/topics/devbox/guide/devbox-json"}},"url":"/playground/topics/devbox/guide"},"/playground/topics/devbox/introduction":{"label":"introduction","children":{"/playground/topics/devbox/introduction/¶ Nix Flakes.md":{"label":"Nix Flakes: Next-Level Package Management","children":{},"url":"/playground/topics/devbox/introduction/nix-flakes"},"/playground/topics/devbox/introduction/¶ Devbox Services.md":{"label":"Devbox Services: Tame Your Daemons with process-compose","children":{},"url":"/playground/topics/devbox/introduction/devbox-services"},"/playground/topics/devbox/introduction/¶ Devbox.md":{"label":"Devbox: Your Dev Environment on Steroids","children":{},"url":"/playground/topics/devbox/introduction/devbox"},"/playground/topics/devbox/introduction/¶ Nix Shell.md":{"label":"Nix Shell: Bulletproof Development Environments","children":{},"url":"/playground/topics/devbox/introduction/nix-shell"},"/playground/topics/devbox/introduction/¶ Devbox Plugins.md":{"label":"Devbox Plugins: Turbocharge Your Dev Setup","children":{},"url":"/playground/topics/devbox/introduction/devbox-plugins"},"/playground/topics/devbox/introduction/the-reason-for-being.md":{"label":"The reason for being","children":{},"url":"/playground/topics/devbox/introduction/the-reason-for-being"}},"url":"/playground/topics/devbox/introduction"},"/playground/topics/devbox/research":{"label":"research","children":{"/playground/topics/devbox/research/≈ Nix in building Docker images.md":{"label":"Nix: Revolutionizing Docker Image Builds","children":{},"url":"/playground/topics/devbox/research/nix-in-building-docker-images"},"/playground/topics/devbox/research/≈ Nix - Build the same thing at any time.md":{"label":"Nix - Build the same thing at any time","children":{},"url":"/playground/topics/devbox/research/nix-build-the-same-thing-at-any-time"},"/playground/topics/devbox/research/shadow-copies.md":{"label":"Shadow Copies in Docker Builds","children":{},"url":"/playground/topics/devbox/research/shadow-copies"},"/playground/topics/devbox/research/unstable-package-installation.md":{"label":"Unstable Package Installation in Docker","children":{},"url":"/playground/topics/devbox/research/unstable-package-installation"},"/playground/topics/devbox/research/pinning-nixpkgs.md":{"label":"Pinning nixpkgs in Nix","children":{},"url":"/playground/topics/devbox/research/pinning-nixpkgs"},"/playground/topics/devbox/research/≈ Nix - Minimum changes, Minimum redundancies.md":{"label":"Nix - Minimum changes, Minimum redundancies","children":{},"url":"/playground/topics/devbox/research/nix-minimum-changes-minimum-redundancies"},"/playground/topics/devbox/research/fixed-output-derivation.md":{"label":"Fixed-output Derivation in Nix","children":{},"url":"/playground/topics/devbox/research/fixed-output-derivation"},"/playground/topics/devbox/research/≈ Docker Issues.md":{"label":"Docker Build Issues","children":{},"url":"/playground/topics/devbox/research/docker-issues"},"/playground/topics/devbox/research/nix-is-faster-than-docker-build.md":{"label":"Nix is Faster Than Docker Build","children":{},"url":"/playground/topics/devbox/research/nix-is-faster-than-docker-build"}},"url":"/playground/topics/devbox/research"},"/playground/topics/devbox/story":{"label":"story","children":{"/playground/topics/devbox/story/devbox-production-success-story.md":{"label":"Devbox in Production: Our Success Story","children":{},"url":"/playground/topics/devbox/story/devbox-production-success-story"},"/playground/topics/devbox/story/devbox-local-development-env.md":{"label":"Using Devbox to setup local development environment","children":{},"url":"/playground/topics/devbox/story/devbox-local-development-env"},"/playground/topics/devbox/story/devbox-nix-and-our-devbox-adoption.md":{"label":"The overview into Nix \u0026 how we use Devbox @ Dwarves","children":{},"url":"/playground/topics/devbox/story/devbox-nix-and-our-devbox-adoption"},"/playground/topics/devbox/story/devbox-docker-adoption-and-challenges.md":{"label":"Our Docker adoption and its challenges","children":{},"url":"/playground/topics/devbox/story/devbox-docker-adoption-and-challenges"},"/playground/topics/devbox/story/devbox-a-world-before-docker.md":{"label":"The world before Docker","children":{},"url":"/playground/topics/devbox/story/devbox-a-world-before-docker"}},"url":"/playground/topics/devbox/story"},"/playground/topics/devbox/§ Devbox.md":{"label":"§ Devbox","children":{},"url":"/playground/topics/devbox/devbox"}},"url":"/playground/topics/devbox"},"/playground/topics/devops":{"label":"devops","children":{"/playground/topics/devops/how-blue-green-deployment-helped-mochi.md":{"label":"How Blue Green Deployment Helped Mochi","children":{},"url":"/playground/topics/devops/how-blue-green-deployment-helped-mochi"},"/playground/topics/devops/radio-talk-60-blue-green-deployment.md":{"label":"Radio Talk 60 Blue Green Deployment","children":{},"url":"/playground/topics/devops/radio-talk-60-blue-green-deployment"},"/playground/topics/devops/how-we-setup-cicd.md":{"label":"How We Setup Cicd","children":{},"url":"/playground/topics/devops/how-we-setup-cicd"},"/playground/topics/devops/kubernetes-helm-101.md":{"label":"Kubernetes Helm 101","children":{},"url":"/playground/topics/devops/kubernetes-helm-101"},"/playground/topics/devops/about-devops.md":{"label":"About Devops","children":{},"url":"/playground/topics/devops/about-devops"},"/playground/topics/devops/docker-microcontainers.md":{"label":"Docker Microcontainers","children":{},"url":"/playground/topics/devops/docker-microcontainers"}},"url":"/playground/topics/devops"},"/playground/topics/elixir":{"label":"elixir","children":{"/playground/topics/elixir/erlang-fsm.md":{"label":"Erlang Finite State Machine","children":{},"url":"/playground/topics/elixir/erlang-fsm"}},"url":"/playground/topics/elixir"},"/playground/topics/engineering":{"label":"engineering","children":{"/playground/topics/engineering/radix-sort.md":{"label":"Radix Sort","children":{},"url":"/playground/topics/engineering/radix-sort"},"/playground/topics/engineering/the-removal-of-apache-kafka-s-dependency-on-zookeeper.md":{"label":"The removal of Apache Kafka's dependency on Zookeeper","children":{},"url":"/playground/topics/engineering/the-removal-of-apache-kafka-s-dependency-on-zookeeper"},"/playground/topics/engineering/url-redirect-vs-rewrite.md":{"label":"URL Redirect vs. Rewrite; What’s the difference?","children":{},"url":"/playground/topics/engineering/url-redirect-vs-rewrite"},"/playground/topics/engineering/our-view-on-fullstack-engineering.md":{"label":"Our View On Fullstack Engineering","children":{},"url":"/playground/topics/engineering/our-view-on-fullstack-engineering"},"/playground/topics/engineering/working-on-a-project-interview-assessment-at-dwarves.md":{"label":"Working On A Project Interview Assessment At Dwarves","children":{},"url":"/playground/topics/engineering/working-on-a-project-interview-assessment-at-dwarves"},"/playground/topics/engineering/lessons-learned-from-concurrency-practices-in-blockchain-projects.md":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{},"url":"/playground/topics/engineering/lessons-learned-from-concurrency-practices-in-blockchain-projects"},"/playground/topics/engineering/database-designs-for-multilingual-apps.md":{"label":"Database Designs For Multilingual Apps","children":{},"url":"/playground/topics/engineering/database-designs-for-multilingual-apps"},"/playground/topics/engineering/radio-talk-61-monorepo.md":{"label":"Radio Talk 61 Monorepo","children":{},"url":"/playground/topics/engineering/radio-talk-61-monorepo"},"/playground/topics/engineering/202211081111-error-messaging.md":{"label":"Error Messaging","children":{},"url":"/playground/topics/engineering/202211081111-error-messaging"},"/playground/topics/engineering/202210131000-behavior-driven-development.md":{"label":"Behavior Driven Development","children":{},"url":"/playground/topics/engineering/202210131000-behavior-driven-development"},"/playground/topics/engineering/202210122014-forward-proxy.md":{"label":"Forward Proxy","children":{},"url":"/playground/topics/engineering/202210122014-forward-proxy"},"/playground/topics/engineering/are-we-really-engineers.md":{"label":"Are We Really Engineers","children":{},"url":"/playground/topics/engineering/are-we-really-engineers"},"/playground/topics/engineering/infinite-image-gallery-with-r3f-an-approach.md":{"label":"Infinite Image Gallery With R3f An Approach","children":{},"url":"/playground/topics/engineering/infinite-image-gallery-with-r3f-an-approach"},"/playground/topics/engineering/software-development-life-cycle-101.md":{"label":"Software Development Life Cycle 101","children":{},"url":"/playground/topics/engineering/software-development-life-cycle-101"},"/playground/topics/engineering/daemons-and-services-programming-guide.md":{"label":"Daemons And Services Programming Guide","children":{},"url":"/playground/topics/engineering/daemons-and-services-programming-guide"},"/playground/topics/engineering/remote-moderated-usability-testing.md":{"label":"Remote Moderated Usability Testing","children":{},"url":"/playground/topics/engineering/remote-moderated-usability-testing"},"/playground/topics/engineering/software-modeling.md":{"label":"Software Modeling","children":{},"url":"/playground/topics/engineering/software-modeling"},"/playground/topics/engineering/reusability-in-software-development.md":{"label":"Reusability In Software Development","children":{},"url":"/playground/topics/engineering/reusability-in-software-development"},"/playground/topics/engineering/introduction-to-software-craftsmanship.md":{"label":"Introduction To Software Craftsmanship","children":{},"url":"/playground/topics/engineering/introduction-to-software-craftsmanship"},"/playground/topics/engineering/build-an-assistant-on-the-terminal.md":{"label":"Build An Assistant On The Terminal","children":{},"url":"/playground/topics/engineering/build-an-assistant-on-the-terminal"},"/playground/topics/engineering/architecture-decision-record.md":{"label":"Architecture Decision Record","children":{},"url":"/playground/topics/engineering/architecture-decision-record"},"/playground/topics/engineering/well-crafted-software.md":{"label":"Well Crafted Software","children":{},"url":"/playground/topics/engineering/well-crafted-software"},"/playground/topics/engineering/what-is-kubernetes.md":{"label":"What Is Kubernetes","children":{},"url":"/playground/topics/engineering/what-is-kubernetes"},"/playground/topics/engineering/playaround-with-clojure.md":{"label":"Playaround With Clojure","children":{},"url":"/playground/topics/engineering/playaround-with-clojure"},"/playground/topics/engineering/istio.md":{"label":"Istio","children":{},"url":"/playground/topics/engineering/istio"},"/playground/topics/engineering/split-and-reuse-code-in-react-application.md":{"label":"Split And Reuse Code In React Application","children":{},"url":"/playground/topics/engineering/split-and-reuse-code-in-react-application"},"/playground/topics/engineering/dcos-series-part-5-gitlab.md":{"label":"Dcos Series Part 5 Gitlab","children":{},"url":"/playground/topics/engineering/dcos-series-part-5-gitlab"},"/playground/topics/engineering/dcos-series-part-4-deploy-simple-application-with-backend-database.md":{"label":"Dcos Series Part 4 Deploy Simple Application With Backend Database","children":{},"url":"/playground/topics/engineering/dcos-series-part-4-deploy-simple-application-with-backend-database"},"/playground/topics/engineering/dcos-series-part-3-service-discovery-and-load-balancing.md":{"label":"Dcos Series Part 3 Service Discovery And Load Balancing","children":{},"url":"/playground/topics/engineering/dcos-series-part-3-service-discovery-and-load-balancing"},"/playground/topics/engineering/dcos-series-part-2-deploy-simple-applications.md":{"label":"Dcos Series Part 2 Deploy Simple Applications","children":{},"url":"/playground/topics/engineering/dcos-series-part-2-deploy-simple-applications"},"/playground/topics/engineering/dcos-series-part-1-quick-look-installation.md":{"label":"Dcos Series Part 1 Quick Look Installation","children":{},"url":"/playground/topics/engineering/dcos-series-part-1-quick-look-installation"},"/playground/topics/engineering/skill-of-software-engineer.md":{"label":"Skill Of Software Engineer","children":{},"url":"/playground/topics/engineering/skill-of-software-engineer"},"/playground/topics/engineering/the-10x-engineer.md":{"label":"The 10x Engineer","children":{},"url":"/playground/topics/engineering/the-10x-engineer"},"/playground/topics/engineering/sprint-lifecycle.md":{"label":"Sprint Lifecycle","children":{},"url":"/playground/topics/engineering/sprint-lifecycle"}},"url":"/playground/topics/engineering"},"/playground/topics/frontend":{"label":"frontend","children":{"/playground/topics/frontend/react":{"label":"react","children":{"/playground/topics/frontend/react/component-composition-patterns.md":{"label":"Component composition patterns","children":{},"url":"/playground/topics/frontend/react/component-composition-patterns"},"/playground/topics/frontend/react/design-system-integration.md":{"label":"Design system integration","children":{},"url":"/playground/topics/frontend/react/design-system-integration"},"/playground/topics/frontend/react/code-splitting.md":{"label":"Code splitting","children":{},"url":"/playground/topics/frontend/react/code-splitting"},"/playground/topics/frontend/react/rendering-strategies.md":{"label":"Rendering strategies","children":{},"url":"/playground/topics/frontend/react/rendering-strategies"},"/playground/topics/frontend/react/hook-architecture.md":{"label":"Hook architecture","children":{},"url":"/playground/topics/frontend/react/hook-architecture"},"/playground/topics/frontend/react/state-management-strategy.md":{"label":"State management strategy","children":{},"url":"/playground/topics/frontend/react/state-management-strategy"},"/playground/topics/frontend/react/testing-strategies.md":{"label":"Testing strategies","children":{},"url":"/playground/topics/frontend/react/testing-strategies"}},"url":"/playground/topics/frontend/react"},"/playground/topics/frontend/Report":{"label":"Report","children":{"/playground/topics/frontend/Report/frontend-report-march-2025.md":{"label":"March 2025","children":{},"url":"/playground/topics/frontend/report/frontend-report-march-2025"},"/playground/topics/frontend/Report/frontend-report-february-2025.md":{"label":"February 2025","children":{},"url":"/playground/topics/frontend/report/frontend-report-february-2025"},"/playground/topics/frontend/Report/frontend-report-january-2025.md":{"label":"January 2025","children":{},"url":"/playground/topics/frontend/report/frontend-report-january-2025"},"/playground/topics/frontend/Report/frontend-report-second-half-of-november-2024.md":{"label":"Nov 2024 (Second Half)","children":{},"url":"/playground/topics/frontend/report/frontend-report-second-half-of-november-2024"},"/playground/topics/frontend/Report/frontend-report-first-half-of-november-2024.md":{"label":"Nov 2024 (First Half)","children":{},"url":"/playground/topics/frontend/report/frontend-report-first-half-of-november-2024"},"/playground/topics/frontend/Report/frontend-report-october-2024.md":{"label":"October 2024","children":{},"url":"/playground/topics/frontend/report/frontend-report-october-2024"},"/playground/topics/frontend/Report/frontend-report-september-2024.md":{"label":"September 2024","children":{},"url":"/playground/topics/frontend/report/frontend-report-september-2024"},"/playground/topics/frontend/Report/frontend-report-august-2024.md":{"label":"August 2024","children":{},"url":"/playground/topics/frontend/report/frontend-report-august-2024"},"/playground/topics/frontend/Report/frontend-report-july-2024.md":{"label":"July 2024","children":{},"url":"/playground/topics/frontend/report/frontend-report-july-2024"}},"url":"/playground/topics/frontend/report"},"/playground/topics/frontend/ThreeJS":{"label":"ThreeJS","children":{"/playground/topics/frontend/ThreeJS/cameras-in-threejs.md":{"label":"Cameras in ThreeJS","children":{},"url":"/playground/topics/frontend/threejs/cameras-in-threejs"}},"url":"/playground/topics/frontend/threejs"},"/playground/topics/frontend/introducing-htmx-navigating-the-advantages-and-concerns.md":{"label":"Introducing HTMX - Navigating the Advantages and Concerns","children":{},"url":"/playground/topics/frontend/introducing-htmx-navigating-the-advantages-and-concerns"},"/playground/topics/frontend/websockets.md":{"label":"WebSockets","children":{},"url":"/playground/topics/frontend/websockets"},"/playground/topics/frontend/typesafe-client-server.md":{"label":"Typesafe Client Server","children":{},"url":"/playground/topics/frontend/typesafe-client-server"},"/playground/topics/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree.md":{"label":"From Markup to Pixels - A look inside the DOM, CSSOM, and Render Tree","children":{},"url":"/playground/topics/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree"},"/playground/topics/frontend/window-and-iframe-communication.md":{"label":"Window and iframe communication","children":{},"url":"/playground/topics/frontend/window-and-iframe-communication"},"/playground/topics/frontend/adoption-of-pnpm.md":{"label":"Adoption Of Pnpm","children":{},"url":"/playground/topics/frontend/adoption-of-pnpm"},"/playground/topics/frontend/applying-mock-service-worker-msw-for-seamless-web-development.md":{"label":"Applying Mock Service Worker (MSW) for Seamless Web Development","children":{},"url":"/playground/topics/frontend/applying-mock-service-worker-msw-for-seamless-web-development"},"/playground/topics/frontend/render-optimization-in-data-fetching-libraries.md":{"label":"Render optimization in data-fetching libraries","children":{},"url":"/playground/topics/frontend/render-optimization-in-data-fetching-libraries"},"/playground/topics/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql.md":{"label":"A Fragment Colocation Pattern with React \u0026 Apollo GraphQL","children":{},"url":"/playground/topics/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql"},"/playground/topics/frontend/scroll-driven-animations.md":{"label":"Scroll-driven animations","children":{},"url":"/playground/topics/frontend/scroll-driven-animations"},"/playground/topics/frontend/react-server-component.md":{"label":"React Server Components, NextJs Route and Data Fetching","children":{},"url":"/playground/topics/frontend/react-server-component"},"/playground/topics/frontend/url-formats-for-sharing-via-social-networks.md":{"label":"URL formats for sharing via social networks","children":{},"url":"/playground/topics/frontend/url-formats-for-sharing-via-social-networks"},"/playground/topics/frontend/shadow-dom.md":{"label":"Shadow DOM","children":{},"url":"/playground/topics/frontend/shadow-dom"},"/playground/topics/frontend/retain-scroll-position-in-infinite-scroll.md":{"label":"Retain scroll position in infinite scroll","children":{},"url":"/playground/topics/frontend/retain-scroll-position-in-infinite-scroll"},"/playground/topics/frontend/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue.md":{"label":"Choosing The Right Javascript Framework A Deep Dive Into React Vs Angular Vs Vue","children":{},"url":"/playground/topics/frontend/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue"},"/playground/topics/frontend/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation.md":{"label":"Lessons Learned From Being A Part Of Corporate Micro Frontend Implementation","children":{},"url":"/playground/topics/frontend/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation"},"/playground/topics/frontend/what-is-pnpm-compare-to-npmyarn.md":{"label":"What is PNPM Compare To NPM/Yarn","children":{},"url":"/playground/topics/frontend/what-is-pnpm-compare-to-npmyarn"},"/playground/topics/frontend/continuous-translation.md":{"label":"Continuous Translation","children":{},"url":"/playground/topics/frontend/continuous-translation"},"/playground/topics/frontend/i18n-frontend-guideline.md":{"label":"I18n Frontend Guideline","children":{},"url":"/playground/topics/frontend/i18n-frontend-guideline"},"/playground/topics/frontend/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit.md":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{},"url":"/playground/topics/frontend/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit"},"/playground/topics/frontend/why-micro-frontend.md":{"label":"Why Micro Frontend","children":{},"url":"/playground/topics/frontend/why-micro-frontend"},"/playground/topics/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution.md":{"label":"Why We Chose Our Tech Stack Accelerating Development With A Robust Frontend Solution","children":{},"url":"/playground/topics/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution"},"/playground/topics/frontend/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo.md":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan Turbo Monorepo","children":{},"url":"/playground/topics/frontend/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo"},"/playground/topics/frontend/tackling-server-state-complexity-in-frontend-development.md":{"label":"Tackling Server State complexity in Frontend Development","children":{},"url":"/playground/topics/frontend/tackling-server-state-complexity-in-frontend-development"},"/playground/topics/frontend/variable-fonts.md":{"label":"Variable Fonts","children":{},"url":"/playground/topics/frontend/variable-fonts"},"/playground/topics/frontend/when-should-we-use-usereducer-instead-of-usestate.md":{"label":"When should we use useReducer instead of useState?","children":{},"url":"/playground/topics/frontend/when-should-we-use-usereducer-instead-of-usestate"},"/playground/topics/frontend/preserving-and-resetting-state-in-react.md":{"label":"Preserving and Resetting state in React","children":{},"url":"/playground/topics/frontend/preserving-and-resetting-state-in-react"},"/playground/topics/frontend/mixpanel.md":{"label":"Mixpanel","children":{},"url":"/playground/topics/frontend/mixpanel"},"/playground/topics/frontend/validation-with-zod.md":{"label":"Validation with Zod","children":{},"url":"/playground/topics/frontend/validation-with-zod"},"/playground/topics/frontend/invoking-component-functions-in-react.md":{"label":"Invoking component functions in React","children":{},"url":"/playground/topics/frontend/invoking-component-functions-in-react"},"/playground/topics/frontend/webassembly.md":{"label":"Webassembly","children":{},"url":"/playground/topics/frontend/webassembly"},"/playground/topics/frontend/parse-don-t-validate-in-typescript.md":{"label":"Parse, don't validate in TypeScript","children":{},"url":"/playground/topics/frontend/parse-don-t-validate-in-typescript"},"/playground/topics/frontend/sign-in-form-best-practices.md":{"label":"Sign-in Form Best Practices","children":{},"url":"/playground/topics/frontend/sign-in-form-best-practices"},"/playground/topics/frontend/singleton-design-pattern-in-javascript.md":{"label":"Singleton Design Pattern in Javascript","children":{},"url":"/playground/topics/frontend/singleton-design-pattern-in-javascript"},"/playground/topics/frontend/the-best-of-css-tldr.md":{"label":"The Best of CSS TLDR","children":{},"url":"/playground/topics/frontend/the-best-of-css-tldr"},"/playground/topics/frontend/an-introduction-to-atomic-css.md":{"label":"An Introduction to Atomic CSS","children":{},"url":"/playground/topics/frontend/an-introduction-to-atomic-css"},"/playground/topics/frontend/react-fiber.md":{"label":"React Fiber","children":{},"url":"/playground/topics/frontend/react-fiber"},"/playground/topics/frontend/intro-to-indexeddb.md":{"label":"Intro to IndexedDB","children":{},"url":"/playground/topics/frontend/intro-to-indexeddb"},"/playground/topics/frontend/the-fundamental-of-web-performance.md":{"label":"The fundamental of web performance","children":{},"url":"/playground/topics/frontend/the-fundamental-of-web-performance"},"/playground/topics/frontend/wai-aria.md":{"label":"WAI-ARIA","children":{},"url":"/playground/topics/frontend/wai-aria"},"/playground/topics/frontend/build-polymorphic-react-components-with-typescript.md":{"label":"Build polymorphic React components with Typescript","children":{},"url":"/playground/topics/frontend/build-polymorphic-react-components-with-typescript"},"/playground/topics/frontend/prevent-layout-thrashing.md":{"label":"Prevent Layout Thrashing","children":{},"url":"/playground/topics/frontend/prevent-layout-thrashing"},"/playground/topics/frontend/pure-css-parallax.md":{"label":"Pure CSS Parallax","children":{},"url":"/playground/topics/frontend/pure-css-parallax"},"/playground/topics/frontend/css-container-queries.md":{"label":"CSS Container Queries","children":{},"url":"/playground/topics/frontend/css-container-queries"},"/playground/topics/frontend/hsl-color.md":{"label":"HSL Color","children":{},"url":"/playground/topics/frontend/hsl-color"},"/playground/topics/frontend/mitigate-blocking-the-main-thread.md":{"label":"Mitigate blocking the main thread","children":{},"url":"/playground/topics/frontend/mitigate-blocking-the-main-thread"},"/playground/topics/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second.md":{"label":"Dark mode flickers a white background for a fraction of a second","children":{},"url":"/playground/topics/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second"},"/playground/topics/frontend/css-in-js.md":{"label":"CSS in JS","children":{},"url":"/playground/topics/frontend/css-in-js"},"/playground/topics/frontend/why-virtual-dom-is-fast.md":{"label":"Why Virtual DOM is fast?","children":{},"url":"/playground/topics/frontend/why-virtual-dom-is-fast"},"/playground/topics/frontend/why-dom-manipulation-is-slow.md":{"label":"Why DOM manipulation is slow?","children":{},"url":"/playground/topics/frontend/why-dom-manipulation-is-slow"},"/playground/topics/frontend/vitejs-native-modules.md":{"label":"ViteJS native modules","children":{},"url":"/playground/topics/frontend/vitejs-native-modules"},"/playground/topics/frontend/javascript-modules.md":{"label":"JavaScript modules","children":{},"url":"/playground/topics/frontend/javascript-modules"},"/playground/topics/frontend/atomic-design-pattern.md":{"label":"Atomic Design Pattern","children":{},"url":"/playground/topics/frontend/atomic-design-pattern"},"/playground/topics/frontend/focus-trap.md":{"label":"Focus trap","children":{},"url":"/playground/topics/frontend/focus-trap"},"/playground/topics/frontend/html-inert.md":{"label":"HTML inert","children":{},"url":"/playground/topics/frontend/html-inert"},"/playground/topics/frontend/useeffect-double-calls-in-react-18.md":{"label":"useEffect double calls in React 18","children":{},"url":"/playground/topics/frontend/useeffect-double-calls-in-react-18"},"/playground/topics/frontend/react-18.md":{"label":"React 18","children":{},"url":"/playground/topics/frontend/react-18"},"/playground/topics/frontend/remix-versus-nextjs.md":{"label":"Remix Versus Nextjs","children":{},"url":"/playground/topics/frontend/remix-versus-nextjs"},"/playground/topics/frontend/zaplib-post-mortem.md":{"label":"Zaplib post-mortem","children":{},"url":"/playground/topics/frontend/zaplib-post-mortem"},"/playground/topics/frontend/parallelism-in-javascript.md":{"label":"Parallelism in JavaScript","children":{},"url":"/playground/topics/frontend/parallelism-in-javascript"},"/playground/topics/frontend/mpa-spa-and-partial-hydration.md":{"label":"MPA, SPA and Partial Hydration","children":{},"url":"/playground/topics/frontend/mpa-spa-and-partial-hydration"},"/playground/topics/frontend/micro-frontends-microservices-for-frontend-development.md":{"label":"Micro Frontends Microservices For Frontend Development","children":{},"url":"/playground/topics/frontend/micro-frontends-microservices-for-frontend-development"},"/playground/topics/frontend/a-quick-intro-to-webassembly.md":{"label":"A Quick Intro To Webassembly","children":{},"url":"/playground/topics/frontend/a-quick-intro-to-webassembly"},"/playground/topics/frontend/recursively-export-file-pattern-in-javascript-es6-application.md":{"label":"Recursively Export File Pattern In Javascript Es6 Application","children":{},"url":"/playground/topics/frontend/recursively-export-file-pattern-in-javascript-es6-application"},"/playground/topics/frontend/using-correct-html-element-to-increase-website-accessibility.md":{"label":"Using Correct Html Element To Increase Website Accessibility","children":{},"url":"/playground/topics/frontend/using-correct-html-element-to-increase-website-accessibility"},"/playground/topics/frontend/fundamental-end-to-end-frontend-testing-with-cypress.md":{"label":"Fundamental End To End Frontend Testing With Cypress","children":{},"url":"/playground/topics/frontend/fundamental-end-to-end-frontend-testing-with-cypress"},"/playground/topics/frontend/different-ways-to-test-react-application.md":{"label":"Different Ways To Test React Application","children":{},"url":"/playground/topics/frontend/different-ways-to-test-react-application"},"/playground/topics/frontend/hoc-renderprops-and-hook-in-reactjs.md":{"label":"Hoc Renderprops And Hook In Reactjs","children":{},"url":"/playground/topics/frontend/hoc-renderprops-and-hook-in-reactjs"},"/playground/topics/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss.md":{"label":"Remove Unused CSS Styles From Bootstrap Using Purgecss","children":{},"url":"/playground/topics/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss"},"/playground/topics/frontend/be-careful-with-your-code-splitting-setup.md":{"label":"Be Careful With Your Code Splitting Setup","children":{},"url":"/playground/topics/frontend/be-careful-with-your-code-splitting-setup"}},"url":"/playground/topics/frontend"},"/playground/topics/git":{"label":"git","children":{"/playground/topics/git/gitflow-pull-request.md":{"label":"Gitflow Pull Request","children":{},"url":"/playground/topics/git/gitflow-pull-request"},"/playground/topics/git/git-commit-message-convention.md":{"label":"Git Commit Message Convention","children":{},"url":"/playground/topics/git/git-commit-message-convention"}},"url":"/playground/topics/git"},"/playground/topics/golang":{"label":"golang","children":{"/playground/topics/golang/go-for-enterprise":{"label":"go-for-enterprise","children":{"/playground/topics/golang/go-for-enterprise/who-using-golang-in-enterprise.md":{"label":"Who is using Go in enterprise?","children":{},"url":"/playground/topics/golang/go-for-enterprise/who-using-golang-in-enterprise"},"/playground/topics/golang/go-for-enterprise/enterprise-standard-language.md":{"label":"Go as an Enterprise Standard Language","children":{},"url":"/playground/topics/golang/go-for-enterprise/enterprise-standard-language"},"/playground/topics/golang/go-for-enterprise/why-go.md":{"label":"Why Go?","children":{},"url":"/playground/topics/golang/go-for-enterprise/why-go"},"/playground/topics/golang/go-for-enterprise/how-to-use-go-in-enterprise.md":{"label":"How to use Go in the Enterprise","children":{},"url":"/playground/topics/golang/go-for-enterprise/how-to-use-go-in-enterprise"},"/playground/topics/golang/go-for-enterprise/why-enterprise-chose-java.md":{"label":"Why Enterprise Chose Java","children":{},"url":"/playground/topics/golang/go-for-enterprise/why-enterprise-chose-java"},"/playground/topics/golang/go-for-enterprise/when-to-use-golang-in-enterprise.md":{"label":"When to use Go in the Enterprise","children":{},"url":"/playground/topics/golang/go-for-enterprise/when-to-use-golang-in-enterprise"}},"url":"/playground/topics/golang/go-for-enterprise"},"/playground/topics/golang/weekly":{"label":"weekly","children":{"/playground/topics/golang/weekly/dec-13.md":{"label":"#24 Go 1.24 testing/synctest experiment for time and concurrency testing","children":{},"url":"/playground/topics/golang/weekly/dec-13"},"/playground/topics/golang/weekly/dec-06.md":{"label":"#23 Draft Release Notes for Go 1.24 and weak pointers in Go","children":{},"url":"/playground/topics/golang/weekly/dec-06"},"/playground/topics/golang/weekly/nov-29.md":{"label":"#22 GoMLX: ML in Go without Python","children":{},"url":"/playground/topics/golang/weekly/nov-29"},"/playground/topics/golang/weekly/nov-22.md":{"label":"#21 Go sync.Once is Simple","children":{},"url":"/playground/topics/golang/weekly/nov-22"},"/playground/topics/golang/weekly/nov-15.md":{"label":"#20 Go Turns 15","children":{},"url":"/playground/topics/golang/weekly/nov-15"},"/playground/topics/golang/weekly/nov-08.md":{"label":"#19 Writing secure Go code","children":{},"url":"/playground/topics/golang/weekly/nov-08"},"/playground/topics/golang/weekly/nov-01.md":{"label":"#18 Fuzz Testing Go HTTP Services","children":{},"url":"/playground/topics/golang/weekly/nov-01"},"/playground/topics/golang/weekly/oct-25.md":{"label":"#17 Leveraging benchstat Projects in Go benchmark and Go Plan9 memo on 450% speeding up calculations","children":{},"url":"/playground/topics/golang/weekly/oct-25"},"/playground/topics/golang/weekly/oct-18.md":{"label":"#16 Understand sync.Map","children":{},"url":"/playground/topics/golang/weekly/oct-18"},"/playground/topics/golang/weekly/oct-11.md":{"label":"#15 Go embed and Reflect","children":{},"url":"/playground/topics/golang/weekly/oct-11"},"/playground/topics/golang/weekly/oct-04.md":{"label":"#14 Compile-time eval \u0026 SQLite with wazero","children":{},"url":"/playground/topics/golang/weekly/oct-04"},"/playground/topics/golang/weekly/sep-27.md":{"label":"#13 Compiler Quests and Vector Vexations","children":{},"url":"/playground/topics/golang/weekly/sep-27"},"/playground/topics/golang/weekly/sep-20.md":{"label":"#12 CLI Tools for K8s, REST, and Terminals","children":{},"url":"/playground/topics/golang/weekly/sep-20"},"/playground/topics/golang/weekly/sep-13.md":{"label":"#11 Actors, Frameworks, and the Future of Go","children":{},"url":"/playground/topics/golang/weekly/sep-13"},"/playground/topics/golang/weekly/sep-06.md":{"label":"#10 Script, Telemetry","children":{},"url":"/playground/topics/golang/weekly/sep-06"},"/playground/topics/golang/weekly/aug-30.md":{"label":"#9 TinyGo, SQLite vector search, and Permify","children":{},"url":"/playground/topics/golang/weekly/aug-30"},"/playground/topics/golang/weekly/aug-23.md":{"label":"#8 GoNB, kubetrim, and GopherCon UK 2024","children":{},"url":"/playground/topics/golang/weekly/aug-23"},"/playground/topics/golang/weekly/aug-16.md":{"label":"#7 Go 1.23, Websockets, and Structs","children":{},"url":"/playground/topics/golang/weekly/aug-16"},"/playground/topics/golang/weekly/aug-09.md":{"label":"#6 Cogent Core, Russ Cox stepping down","children":{},"url":"/playground/topics/golang/weekly/aug-09"},"/playground/topics/golang/weekly/aug-02.md":{"label":"#5 Go 1.23 features, Memory, Minecraft, and More","children":{},"url":"/playground/topics/golang/weekly/aug-02"},"/playground/topics/golang/weekly/jul-26.md":{"label":"#4 Ethical Hacking, HTTP Requests, Mac App Development","children":{},"url":"/playground/topics/golang/weekly/jul-26"},"/playground/topics/golang/weekly/jul-12.md":{"label":"#3 Generic Collections, Generics Constraints, AI Bot","children":{},"url":"/playground/topics/golang/weekly/jul-12"},"/playground/topics/golang/weekly/jul-05.md":{"label":"#2 Go 1.23 Iterators","children":{},"url":"/playground/topics/golang/weekly/jul-05"},"/playground/topics/golang/weekly/june-27.md":{"label":"#1 eBPF and PGO Optimization Techniques","children":{},"url":"/playground/topics/golang/weekly/june-27"}},"url":"/playground/topics/golang/weekly"},"/playground/topics/golang/go-import.md":{"label":"Go import design: using git repo path","children":{},"url":"/playground/topics/golang/go-import"},"/playground/topics/golang/extension-interface-pattern.md":{"label":"Go extension interface pattern","children":{},"url":"/playground/topics/golang/extension-interface-pattern"},"/playground/topics/golang/go-package.md":{"label":"Package first design","children":{},"url":"/playground/topics/golang/go-package"},"/playground/topics/golang/go-generics-type-safety.md":{"label":"How does Go achieve type safety when it enables generics?","children":{},"url":"/playground/topics/golang/go-generics-type-safety"},"/playground/topics/golang/error-handling-patterns.md":{"label":"Error handling patterns","children":{},"url":"/playground/topics/golang/error-handling-patterns"},"/playground/topics/golang/compute-union-2-finite-automata.md":{"label":"Efficient Union of Finite Automata in Golang: A Practical Approach","children":{},"url":"/playground/topics/golang/compute-union-2-finite-automata"},"/playground/topics/golang/golang-for-high-performance-video-streaming.md":{"label":"Leveraging Golang and WebRTC for High-Performance Video Streaming","children":{},"url":"/playground/topics/golang/golang-for-high-performance-video-streaming"},"/playground/topics/golang/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines.md":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{},"url":"/playground/topics/golang/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines"},"/playground/topics/golang/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq.md":{"label":"Message Queues And Streaming Platforms Eg Kafka Nats Rabbitmq","children":{},"url":"/playground/topics/golang/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq"},"/playground/topics/golang/unit-testing-best-practices-in-golang.md":{"label":"Unit Testing Best Practices In Golang","children":{},"url":"/playground/topics/golang/unit-testing-best-practices-in-golang"},"/playground/topics/golang/profiling-in-go.md":{"label":"Profiling in Go","children":{},"url":"/playground/topics/golang/profiling-in-go"},"/playground/topics/golang/go-in-software-engineering.md":{"label":"Go In Software Engineering","children":{},"url":"/playground/topics/golang/go-in-software-engineering"},"/playground/topics/golang/bunk-license-check.md":{"label":"Bunk License Check","children":{},"url":"/playground/topics/golang/bunk-license-check"},"/playground/topics/golang/go-concurrency.md":{"label":"Go Concurrency","children":{},"url":"/playground/topics/golang/go-concurrency"},"/playground/topics/golang/slice-and-array-in-golang.md":{"label":"Slice And Array In Golang","children":{},"url":"/playground/topics/golang/slice-and-array-in-golang"},"/playground/topics/golang/use-go-selenium-to-crawl-data.md":{"label":"Use Go Selenium To Crawl Data","children":{},"url":"/playground/topics/golang/use-go-selenium-to-crawl-data"},"/playground/topics/golang/connecting-vim-with-golang.md":{"label":"Connecting Vim With Golang","children":{},"url":"/playground/topics/golang/connecting-vim-with-golang"}},"url":"/playground/topics/golang"},"/playground/topics/liquidity":{"label":"liquidity","children":{"/playground/topics/liquidity/an-overview-of-micro-investment-in-real-estate.md":{"label":"An Overview Of Micro Investment In Real Estate","children":{},"url":"/playground/topics/liquidity/an-overview-of-micro-investment-in-real-estate"}},"url":"/playground/topics/liquidity"},"/playground/topics/llm":{"label":"llm","children":{"/playground/topics/llm/§ Building LLM System.md":{"label":"§ Building LLM system","children":{},"url":"/playground/topics/llm/building-llm-system"},"/playground/topics/llm/§ LLM.md":{"label":"§ LLM","children":{},"url":"/playground/topics/llm/llm"},"/playground/topics/llm/model-context-protocol.md":{"label":"Intro to Model Context Protocol","children":{},"url":"/playground/topics/llm/model-context-protocol"},"/playground/topics/llm/quantization-in-llm.md":{"label":"Quantization for large language models","children":{},"url":"/playground/topics/llm/quantization-in-llm"},"/playground/topics/llm/graphrag.md":{"label":"GraphRAG - Building a knowledge graph for RAG system","children":{},"url":"/playground/topics/llm/graphrag"},"/playground/topics/llm/guardrails-in-llm.md":{"label":"Guardrails in llm","children":{},"url":"/playground/topics/llm/guardrails-in-llm"},"/playground/topics/llm/react-in-llm.md":{"label":"ReAct(Reason + Act) in LLM","children":{},"url":"/playground/topics/llm/react-in-llm"},"/playground/topics/llm/model-selection.md":{"label":"Model selection","children":{},"url":"/playground/topics/llm/model-selection"},"/playground/topics/llm/metric-pillar.md":{"label":"Metrics","children":{},"url":"/playground/topics/llm/metric-pillar"},"/playground/topics/llm/trace-pillar.md":{"label":"Tracing","children":{},"url":"/playground/topics/llm/trace-pillar"},"/playground/topics/llm/logs-pillar.md":{"label":"Logging","children":{},"url":"/playground/topics/llm/logs-pillar"},"/playground/topics/llm/observability-in-ai-platforms.md":{"label":"Observability in AI platforms","children":{},"url":"/playground/topics/llm/observability-in-ai-platforms"},"/playground/topics/llm/intent-classification-by-llm.md":{"label":"Intent classification by LLM","children":{},"url":"/playground/topics/llm/intent-classification-by-llm"},"/playground/topics/llm/use-cases-for-llm-applications.md":{"label":"Use cases for LLM applications","children":{},"url":"/playground/topics/llm/use-cases-for-llm-applications"},"/playground/topics/llm/the-rise-of-ai-applications-with-llm.md":{"label":"The rise of AI applications with LLM","children":{},"url":"/playground/topics/llm/the-rise-of-ai-applications-with-llm"},"/playground/topics/llm/evaluation-guideline-for-llm-application.md":{"label":"Evaluation guidelines for LLM applications","children":{},"url":"/playground/topics/llm/evaluation-guideline-for-llm-application"},"/playground/topics/llm/prevent-prompt-injection.md":{"label":"Prevent prompt injection","children":{},"url":"/playground/topics/llm/prevent-prompt-injection"},"/playground/topics/llm/evaluate-chatbot-agent-by-simulated-user.md":{"label":"Evaluate Chatbot Agent by User Simulation","children":{},"url":"/playground/topics/llm/evaluate-chatbot-agent-by-simulated-user"},"/playground/topics/llm/llm-tracing-in-ai-system.md":{"label":"LLM tracing in AI system","children":{},"url":"/playground/topics/llm/llm-tracing-in-ai-system"},"/playground/topics/llm/multi-agent-collaboration-for-task-completion.md":{"label":"Multi-agent collaboration for task completion","children":{},"url":"/playground/topics/llm/multi-agent-collaboration-for-task-completion"},"/playground/topics/llm/caching-with-rag-system.md":{"label":"Evaluating caching in RAG systems","children":{},"url":"/playground/topics/llm/caching-with-rag-system"},"/playground/topics/llm/generative-ui.md":{"label":"What is Generative UI?","children":{},"url":"/playground/topics/llm/generative-ui"},"/playground/topics/llm/hybrid-search.md":{"label":"Evaluating search engine in RAG systems","children":{},"url":"/playground/topics/llm/hybrid-search"},"/playground/topics/llm/re-ranking-in-rag.md":{"label":"Re-ranking in RAG","children":{},"url":"/playground/topics/llm/re-ranking-in-rag"},"/playground/topics/llm/function-calling.md":{"label":"Function calling in AI agents","children":{},"url":"/playground/topics/llm/function-calling"},"/playground/topics/llm/thumbs-up-and-thumbs-down-pattern.md":{"label":"Thumbs up and Thumbs down pattern","children":{},"url":"/playground/topics/llm/thumbs-up-and-thumbs-down-pattern"},"/playground/topics/llm/supervisor-ai-agents.md":{"label":"Building Agent Supervisors to Generate Insights","children":{},"url":"/playground/topics/llm/supervisor-ai-agents"},"/playground/topics/llm/feedback-mechanism.md":{"label":"Design feedback mechanism for LLM applications","children":{},"url":"/playground/topics/llm/feedback-mechanism"},"/playground/topics/llm/proximal-policy-optimization.md":{"label":"Proximal Policy Optimization","children":{},"url":"/playground/topics/llm/proximal-policy-optimization"},"/playground/topics/llm/multimodal-in-rag.md":{"label":"Multimodal in rag","children":{},"url":"/playground/topics/llm/multimodal-in-rag"},"/playground/topics/llm/how-to-talk-to-chatgpt-effectively.md":{"label":"How to talk to ChatGPT effectively","children":{},"url":"/playground/topics/llm/how-to-talk-to-chatgpt-effectively"},"/playground/topics/llm/a-grand-unified-theory-of-the-ai-hype-cycle.md":{"label":"A Grand Unified Theory of the AI Hype Cycle","children":{},"url":"/playground/topics/llm/a-grand-unified-theory-of-the-ai-hype-cycle"},"/playground/topics/llm/history-of-structured-output-for-llms.md":{"label":"History of Structured Outputs for LLMs","children":{},"url":"/playground/topics/llm/history-of-structured-output-for-llms"},"/playground/topics/llm/developing-rapidly-with-generative-ai.md":{"label":"Developing rapidly with Generative AI","children":{},"url":"/playground/topics/llm/developing-rapidly-with-generative-ai"},"/playground/topics/llm/rlhf-with-open-assistant.md":{"label":"RLHF with Open Assistant","children":{},"url":"/playground/topics/llm/rlhf-with-open-assistant"},"/playground/topics/llm/story-map-for-llms.md":{"label":"Story map for LLMs","children":{},"url":"/playground/topics/llm/story-map-for-llms"},"/playground/topics/llm/adversarial-prompting.md":{"label":"Adversarial Prompting in Prompt Engineering","children":{},"url":"/playground/topics/llm/adversarial-prompting"},"/playground/topics/llm/chunking-strategies-to-overcome-context-limitation-in-llm.md":{"label":"Chunking strategies to overcome context limitation in LLM","children":{},"url":"/playground/topics/llm/chunking-strategies-to-overcome-context-limitation-in-llm"},"/playground/topics/llm/dealing-with-long-term-memory-in-ai-chatbot.md":{"label":"Storing Long-Term Memory in ChatGPT Using VectorDB","children":{},"url":"/playground/topics/llm/dealing-with-long-term-memory-in-ai-chatbot"},"/playground/topics/llm/llm-s-accuracy-self-refinement.md":{"label":"LLM's Accuracy - Self Refinement","children":{},"url":"/playground/topics/llm/llm-s-accuracy-self-refinement"},"/playground/topics/llm/reward-model.md":{"label":"Reward Model","children":{},"url":"/playground/topics/llm/reward-model"},"/playground/topics/llm/q-learning.md":{"label":"Q Learning","children":{},"url":"/playground/topics/llm/q-learning"},"/playground/topics/llm/llm-query-caching.md":{"label":"Query Caching for Large Language Models","children":{},"url":"/playground/topics/llm/llm-query-caching"},"/playground/topics/llm/reinforcement-learning.md":{"label":"Introduction to Reinforcement Learning and Its Application with LLMs","children":{},"url":"/playground/topics/llm/reinforcement-learning"},"/playground/topics/llm/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms.md":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{},"url":"/playground/topics/llm/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms"},"/playground/topics/llm/select-vector-database-for-llm.md":{"label":"Select Vector Database for LLM","children":{},"url":"/playground/topics/llm/select-vector-database-for-llm"},"/playground/topics/llm/exploring-machine-learning-approaches-for-fine-tuning-llama-models.md":{"label":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","children":{},"url":"/playground/topics/llm/exploring-machine-learning-approaches-for-fine-tuning-llama-models"},"/playground/topics/llm/build-your-chatbot-with-open-source-large-language-models.md":{"label":"Build your chatbot with open source Large Language Models","children":{},"url":"/playground/topics/llm/build-your-chatbot-with-open-source-large-language-models"},"/playground/topics/llm/working-with-langchain-document-loaders.md":{"label":"Working with langchain document loaders","children":{},"url":"/playground/topics/llm/working-with-langchain-document-loaders"},"/playground/topics/llm/workaround-with-openai-s-token-limit-with-langchain.md":{"label":"Workaround with OpenAI's token limit with Langchain","children":{},"url":"/playground/topics/llm/workaround-with-openai-s-token-limit-with-langchain"}},"url":"/playground/topics/llm"},"/playground/topics/mobile":{"label":"mobile","children":{"/playground/topics/mobile/xpc-services-on-macos-app-using-swift.md":{"label":"Xpc Services On Macos App Using Swift","children":{},"url":"/playground/topics/mobile/xpc-services-on-macos-app-using-swift"},"/playground/topics/mobile/create-circular-text-using-swiftui.md":{"label":"Create Circular Text Using Swiftui","children":{},"url":"/playground/topics/mobile/create-circular-text-using-swiftui"},"/playground/topics/mobile/draw-watch-face-using-swiftui.md":{"label":"Draw Watch Face Using Swiftui","children":{},"url":"/playground/topics/mobile/draw-watch-face-using-swiftui"},"/playground/topics/mobile/swiftui.md":{"label":"Swiftui","children":{},"url":"/playground/topics/mobile/swiftui"},"/playground/topics/mobile/uidynamicanimator.md":{"label":"Uidynamicanimator","children":{},"url":"/playground/topics/mobile/uidynamicanimator"},"/playground/topics/mobile/reproduce-apple-find-me-bottom-menu-view.md":{"label":"Reproduce Apple Find Me Bottom Menu View","children":{},"url":"/playground/topics/mobile/reproduce-apple-find-me-bottom-menu-view"},"/playground/topics/mobile/build-a-passcode-view-with-swift.md":{"label":"Build A Passcode View With Swift","children":{},"url":"/playground/topics/mobile/build-a-passcode-view-with-swift"}},"url":"/playground/topics/mobile"},"/playground/topics/personas":{"label":"personas","children":{"/playground/topics/personas/mbti":{"label":"mbti","children":{"/playground/topics/personas/mbti/ni.md":{"label":"Introverted Intuition (Ni)","children":{},"url":"/playground/topics/personas/mbti/ni"},"/playground/topics/personas/mbti/se.md":{"label":"Extraverted Sensing (Se)","children":{},"url":"/playground/topics/personas/mbti/se"},"/playground/topics/personas/mbti/ocean-model.md":{"label":"OCEAN model","children":{},"url":"/playground/topics/personas/mbti/ocean-model"},"/playground/topics/personas/mbti/fe.md":{"label":"Extraverted Feeling (Fe)","children":{},"url":"/playground/topics/personas/mbti/fe"},"/playground/topics/personas/mbti/history.md":{"label":"The History","children":{},"url":"/playground/topics/personas/mbti/history"},"/playground/topics/personas/mbti/te.md":{"label":"Extraverted Thinking (Te)","children":{},"url":"/playground/topics/personas/mbti/te"},"/playground/topics/personas/mbti/perceiving-judging.md":{"label":"Perceiving vs Judging","children":{},"url":"/playground/topics/personas/mbti/perceiving-judging"},"/playground/topics/personas/mbti/ne.md":{"label":"Extraverted Intuition (Ne)","children":{},"url":"/playground/topics/personas/mbti/ne"},"/playground/topics/personas/mbti/thinking-feeling.md":{"label":"Thinking vs Feeling","children":{},"url":"/playground/topics/personas/mbti/thinking-feeling"},"/playground/topics/personas/mbti/functions.md":{"label":"Cognitive functions stack","children":{},"url":"/playground/topics/personas/mbti/functions"},"/playground/topics/personas/mbti/intuition-sensing.md":{"label":"Intuition vs Sensing","children":{},"url":"/playground/topics/personas/mbti/intuition-sensing"},"/playground/topics/personas/mbti/strategies-for-accurate-typing.md":{"label":"Strategies for Accurate Typing","children":{},"url":"/playground/topics/personas/mbti/strategies-for-accurate-typing"},"/playground/topics/personas/mbti/si.md":{"label":"Introverted Sensing (Si)","children":{},"url":"/playground/topics/personas/mbti/si"},"/playground/topics/personas/mbti/preferences.md":{"label":"8 preferences","children":{},"url":"/playground/topics/personas/mbti/preferences"},"/playground/topics/personas/mbti/fi.md":{"label":"Introverted Feeling (Fi)","children":{},"url":"/playground/topics/personas/mbti/fi"},"/playground/topics/personas/mbti/ti.md":{"label":"Introverted Thinking (Ti)","children":{},"url":"/playground/topics/personas/mbti/ti"},"/playground/topics/personas/mbti/introversion-extraversion.md":{"label":"Introversion vs Extraverion","children":{},"url":"/playground/topics/personas/mbti/introversion-extraversion"}},"url":"/playground/topics/personas/mbti"}},"url":"/playground/topics/personas"},"/playground/topics/pm":{"label":"pm","children":{"/playground/topics/pm/how-to-deal-with-technical-debt-in-scrum.md":{"label":"How to deal with technical debt in Scrum","children":{},"url":"/playground/topics/pm/how-to-deal-with-technical-debt-in-scrum"},"/playground/topics/pm/an-alternative-to-tm.md":{"label":"An Alternative To Tm","children":{},"url":"/playground/topics/pm/an-alternative-to-tm"},"/playground/topics/pm/project-management.md":{"label":"Project Management","children":{},"url":"/playground/topics/pm/project-management"},"/playground/topics/pm/our-daily-standup-format.md":{"label":"Our Daily Standup Format","children":{},"url":"/playground/topics/pm/our-daily-standup-format"},"/playground/topics/pm/definition-of-done.md":{"label":"Definition Of Done","children":{},"url":"/playground/topics/pm/definition-of-done"},"/playground/topics/pm/estimation-in-agile.md":{"label":"Estimation In Agile","children":{},"url":"/playground/topics/pm/estimation-in-agile"}},"url":"/playground/topics/pm"},"/playground/topics/product":{"label":"product","children":{"/playground/topics/product/subscription-pricing-models.md":{"label":"Subscription Pricing Models","children":{},"url":"/playground/topics/product/subscription-pricing-models"},"/playground/topics/product/aarrr-framework-in-a-nutshell.md":{"label":"Aarrr Framework In A Nutshell","children":{},"url":"/playground/topics/product/aarrr-framework-in-a-nutshell"}},"url":"/playground/topics/product"},"/playground/topics/prompt-kit":{"label":"prompt-kit","children":{"/playground/topics/prompt-kit/projects-operations.md":{"label":"Project Operations Copilots","children":{},"url":"/playground/topics/prompt-kit/projects-operations"},"/playground/topics/prompt-kit/team-copilots.md":{"label":"Team Copilots","children":{},"url":"/playground/topics/prompt-kit/team-copilots"}},"url":"/playground/topics/prompt-kit"},"/playground/topics/qa":{"label":"qa","children":{"/playground/topics/qa/qc-onboarding.md":{"label":"Qc Onboarding","children":{},"url":"/playground/topics/qa/qc-onboarding"}},"url":"/playground/topics/qa"},"/playground/topics/rust":{"label":"rust","children":{"/playground/topics/rust/error-handling-in-rust.md":{"label":"Error handling on Rust","children":{},"url":"/playground/topics/rust/error-handling-in-rust"},"/playground/topics/rust/rust-trait.md":{"label":"Rust Trait","children":{},"url":"/playground/topics/rust/rust-trait"},"/playground/topics/rust/playaround-with-rust.md":{"label":"Playaround With Rust","children":{},"url":"/playground/topics/rust/playaround-with-rust"}},"url":"/playground/topics/rust"},"/playground/topics/security":{"label":"security","children":{"/playground/topics/security/a-holistic-guide-to-security.md":{"label":"A Holistic Guide to Security","children":{},"url":"/playground/topics/security/a-holistic-guide-to-security"},"/playground/topics/security/the-key-of-security-mechanisms-in-tackling-cyber-threats.md":{"label":"The Key Of Security Mechanisms In Tackling Cyber Threats","children":{},"url":"/playground/topics/security/the-key-of-security-mechanisms-in-tackling-cyber-threats"},"/playground/topics/security/applied-security-basis.md":{"label":"Applied Security Basis","children":{},"url":"/playground/topics/security/applied-security-basis"}},"url":"/playground/topics/security"},"/playground/topics/solana":{"label":"solana","children":{"/playground/topics/solana/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana.md":{"label":"Dynamic Liquidity Market Maker - a new form of concentrated liquidity AMM on Solana","children":{},"url":"/playground/topics/solana/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana"},"/playground/topics/solana/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana.md":{"label":"Introduce to Solana Token 2022 - new standard to create a token in solana","children":{},"url":"/playground/topics/solana/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana"},"/playground/topics/solana/solana-core-concept.md":{"label":"Solana core concepts","children":{},"url":"/playground/topics/solana/solana-core-concept"},"/playground/topics/solana/metaplex-nft-compression.md":{"label":"Metaplex NFT Compression","children":{},"url":"/playground/topics/solana/metaplex-nft-compression"},"/playground/topics/solana/solana-account.md":{"label":"Solana Account","children":{},"url":"/playground/topics/solana/solana-account"},"/playground/topics/solana/anchor-framework.md":{"label":"Anchor framework","children":{},"url":"/playground/topics/solana/anchor-framework"},"/playground/topics/solana/how-tokens-work-on-solana.md":{"label":"How Tokens Work on Solana","children":{},"url":"/playground/topics/solana/how-tokens-work-on-solana"}},"url":"/playground/topics/solana"},"/playground/topics/writing":{"label":"writing","children":{"/playground/topics/writing/state-explain-link.md":{"label":"State, Explain, Link - An all-purpose writing technique","children":{},"url":"/playground/topics/writing/state-explain-link"}},"url":"/playground/topics/writing"}},"url":"/playground/topics"},"/playground/_index.md":{"label":"Labs Team","children":{},"url":"/playground"},"/playground/schedule.md":{"label":"Labs x Consulting Workflow","children":{},"url":"/playground/schedule"},"/playground/onboarding.md":{"label":"Labs - New Member Onboarding","children":{},"url":"/playground/onboarding"},"/playground/roadmap.md":{"label":"Labs Roadmap (Nov 23 update)","children":{},"url":"/playground/roadmap"}},"url":"/playground"},"/radar":{"label":"radar","children":{"/radar/index":{"label":"index","children":{"/radar/index/README.md":{"label":"Tech Radar","children":{},"url":"/radar/index"},"/radar/index/duckdb.md":{"label":"Duckdb","children":{},"url":"/radar/index/duckdb"}},"url":"/radar/index"},"/radar/README.md":{"label":"Tech radar index","children":{},"url":"/radar"}},"url":"/radar"},"/updates":{"label":"updates","children":{"/updates/build-log":{"label":"build-log","children":{"/updates/build-log/service_monitoring_with_upptime.md":{"label":"Secure and transparent uptime monitoring with Upptime and GitHub secrets","children":{},"url":"/updates/build-log/service_monitoring_with_upptime"},"/updates/build-log/create-slides-with-overleaf.md":{"label":"Create slides with Overleaf and ChatGPT","children":{},"url":"/updates/build-log/create-slides-with-overleaf"},"/updates/build-log/optimize-init-load-time-for-trading-platform.md":{"label":"Optimizing initial load time for a Trading Platform","children":{},"url":"/updates/build-log/optimize-init-load-time-for-trading-platform"},"/updates/build-log/ai-interview-platform-mvp.md":{"label":"Building MVP for AI-driven interview platform","children":{},"url":"/updates/build-log/ai-interview-platform-mvp"},"/updates/build-log/optimizing-ui-for-effective-investment-experience.md":{"label":"Hedge Foundation - Optimizing UI for effective investment experience","children":{},"url":"/updates/build-log/optimizing-ui-for-effective-investment-experience"},"/updates/build-log/implement-binance-future-pnl-analysis-page.md":{"label":"Implement Binance Futures PNL analysis page by Phoenix LiveView","children":{},"url":"/updates/build-log/implement-binance-future-pnl-analysis-page"},"/updates/build-log/migrate-normal-table-to-timescale-table.md":{"label":"Migrate regular tables into TimescaleDB hypertables to improve query performance","children":{},"url":"/updates/build-log/migrate-normal-table-to-timescale-table"},"/updates/build-log/database-hardening-for-trading-platform.md":{"label":"Database hardening for a trading platform","children":{},"url":"/updates/build-log/database-hardening-for-trading-platform"},"/updates/build-log/bitcoin-alt-performance-tracking.md":{"label":"Tracking Bitcoin-Altcoin Performance Indicators in BTC Hedging Strategy","children":{},"url":"/updates/build-log/bitcoin-alt-performance-tracking"},"/updates/build-log/data-archive-and-recovery.md":{"label":"Building a data archive and recovery strategy for high-volume trading system","children":{},"url":"/updates/build-log/data-archive-and-recovery"},"/updates/build-log/persist-history-using-data-snapshot-pattern.md":{"label":"Implementing data snapshot pattern to persist historical data","children":{},"url":"/updates/build-log/persist-history-using-data-snapshot-pattern"},"/updates/build-log/building-data-pipeline-ogif-transcriber.md":{"label":"Building data pipeline for OGIF transcriber","children":{},"url":"/updates/build-log/building-data-pipeline-ogif-transcriber"},"/updates/build-log/building-chatbot-agent-for-project-management-tool.md":{"label":"Building chatbot agent to streamline project management","children":{},"url":"/updates/build-log/building-chatbot-agent-for-project-management-tool"},"/updates/build-log/ai-ruby-travel-assistant-chatbot.md":{"label":"AI-powered Ruby travel assistant","children":{},"url":"/updates/build-log/ai-ruby-travel-assistant-chatbot"},"/updates/build-log/centralized-monitoring-setup-for-trading-platform.md":{"label":"Setup centralized monitoring system for Hedge Foundation trading platform","children":{},"url":"/updates/build-log/centralized-monitoring-setup-for-trading-platform"},"/updates/build-log/enhancing-cryptocurrency-transfer-logger.md":{"label":"Transfer mapping: enhancing loggers for better transparency","children":{},"url":"/updates/build-log/enhancing-cryptocurrency-transfer-logger"},"/updates/build-log/binance-transfer-matching.md":{"label":"Building better Binance transfer tracking","children":{},"url":"/updates/build-log/binance-transfer-matching"},"/updates/build-log/crypto-market-outperform-chart-rendering.md":{"label":"Visualizing crypto market performance: BTC-Alt dynamic indicators in Golang","children":{},"url":"/updates/build-log/crypto-market-outperform-chart-rendering"},"/updates/build-log/reconstructing_trading_pnl_data_pipeline_approach.md":{"label":"Reconstructing historical trading PnL: a data pipeline approach","children":{},"url":"/updates/build-log/reconstructing_trading_pnl_data_pipeline_approach"},"/updates/build-log/ai-powered-monthly-project-reports.md":{"label":"Project reports system: a case study","children":{},"url":"/updates/build-log/ai-powered-monthly-project-reports"}},"url":"/updates/build-log"},"/updates/changelog":{"label":"changelog","children":{"/updates/changelog/2024-10-25-knowledge-base.md":{"label":"Build your knowledge base","children":{},"url":"/updates/changelog/2024-10-25-knowledge-base"},"/updates/changelog/2024-09-13-dwarve-updates-ai-llm.md":{"label":"The Stage of AI and LLM at Dwarves","children":{},"url":"/updates/changelog/2024-09-13-dwarve-updates-ai-llm"},"/updates/changelog/README.md":{"label":"Dwarves Updates","children":{},"url":"/updates/changelog"},"/updates/changelog/2023-09-12-growth-stages.md":{"label":"The Stage of Growth at Dwarves","children":{},"url":"/updates/changelog/2023-09-12-growth-stages"},"/updates/changelog/2022-08-26-the-next-leading-chairs.md":{"label":"The Next Leading Chairs","children":{},"url":"/updates/changelog/2022-08-26-the-next-leading-chairs"},"/updates/changelog/2022-06-26-blockchain-and-data.md":{"label":"The future is blockchain and data","children":{},"url":"/updates/changelog/2022-06-26-blockchain-and-data"},"/updates/changelog/2022-03-31-hiring-stages.md":{"label":"The stages of hiring at Dwarves","children":{},"url":"/updates/changelog/2022-03-31-hiring-stages"},"/updates/changelog/2021-12-30-2021-in-review.md":{"label":"It's a wrap: 2021 in Review","children":{},"url":"/updates/changelog/2021-12-30-2021-in-review"},"/updates/changelog/2021-12-01-engineering-org-structure.md":{"label":"Engineering Organizational Structure","children":{},"url":"/updates/changelog/2021-12-01-engineering-org-structure"},"/updates/changelog/2021-10-31-path-to-growth.md":{"label":"The Path To Growth at Dwarves","children":{},"url":"/updates/changelog/2021-10-31-path-to-growth"},"/updates/changelog/2021-09-29-engineer-performance-review.md":{"label":"Engineer Performance Review","children":{},"url":"/updates/changelog/2021-09-29-engineer-performance-review"},"/updates/changelog/2021-08-23-project-compliance.md":{"label":"Project Compliance","children":{},"url":"/updates/changelog/2021-08-23-project-compliance"},"/updates/changelog/2021-07-11-dalat-office.md":{"label":"Da Lat Office","children":{},"url":"/updates/changelog/2021-07-11-dalat-office"},"/updates/changelog/2021-06-10-dwarves-updates.md":{"label":"Dwarves Updates","children":{},"url":"/updates/changelog/2021-06-10-dwarves-updates"}},"url":"/updates/changelog"},"/updates/forward":{"label":"forward","children":{"/updates/forward/Market report":{"label":"Market report","children":{"/updates/forward/Market report/2024-october.md":{"label":"October 2024","children":{},"url":"/updates/forward/market-report/2024-october"},"/updates/forward/Market report/2024-september.md":{"label":"September 2024","children":{},"url":"/updates/forward/market-report/2024-september"},"/updates/forward/Market report/2024-august.md":{"label":"August 2024","children":{},"url":"/updates/forward/market-report/2024-august"},"/updates/forward/Market report/2024-july.md":{"label":"July 2024","children":{},"url":"/updates/forward/market-report/2024-july"},"/updates/forward/Market report/2024-may.md":{"label":"May 2024","children":{},"url":"/updates/forward/market-report/2024-may"},"/updates/forward/Market report/2024-april.md":{"label":"April 2024","children":{},"url":"/updates/forward/market-report/2024-april"},"/updates/forward/Market report/2024-march.md":{"label":"March 2024","children":{},"url":"/updates/forward/market-report/2024-march"},"/updates/forward/Market report/2024-february.md":{"label":"February 2024","children":{},"url":"/updates/forward/market-report/2024-february"},"/updates/forward/Market report/2024-january.md":{"label":"January 2024","children":{},"url":"/updates/forward/market-report/2024-january"},"/updates/forward/Market report/2023-december.md":{"label":"December 2023","children":{},"url":"/updates/forward/market-report/2023-december"}},"url":"/updates/forward/market-report"},"/updates/forward/market-commentary":{"label":"market-commentary","children":{"/updates/forward/market-commentary/event-takeaways-2nd.md":{"label":"2nd Talks and Takeaways","children":{},"url":"/updates/forward/market-commentary/event-takeaways-2nd"},"/updates/forward/market-commentary/event-takeaways-1st.md":{"label":"1st Talks and Takeaways","children":{},"url":"/updates/forward/market-commentary/event-takeaways-1st"},"/updates/forward/market-commentary/2025-28th-feb.md":{"label":"#9: Bybit Loses $1.5B in Hack, Claude 3.7 Sonnet Drops, and OpenArt Designs Characters","children":{},"url":"/updates/forward/market-commentary/2025-28th-feb"},"/updates/forward/market-commentary/2025-21th-feb.md":{"label":"#8: R1 1776 Goes Open-Source, Cardex Gets Hacked, and Grok-3 Debuts","children":{},"url":"/updates/forward/market-commentary/2025-21th-feb"},"/updates/forward/market-commentary/2025-14th-feb.md":{"label":"#7: 10x AI Cost Reduction, Lyft’s 2026 Robotaxi Milestone, and Solana ETF Buzz","children":{},"url":"/updates/forward/market-commentary/2025-14th-feb"},"/updates/forward/market-commentary/2025-7th-feb.md":{"label":"#6 Trending Products, DeepSeek Wave, and Ethereum Predictions","children":{},"url":"/updates/forward/market-commentary/2025-7th-feb"},"/updates/forward/market-commentary/2025-17th-jan.md":{"label":"#5 VC Trends, Blockchain Breakthroughs, and AI Innovations","children":{},"url":"/updates/forward/market-commentary/2025-17th-jan"},"/updates/forward/market-commentary/2025-10th-jan.md":{"label":"#4 AI Supercomputers, Mini AI PCs, SEA VC","children":{},"url":"/updates/forward/market-commentary/2025-10th-jan"},"/updates/forward/market-commentary/2025-3rd-jan.md":{"label":"#3 AI at CES, Wall Street Boom, Blockchain Trends","children":{},"url":"/updates/forward/market-commentary/2025-3rd-jan"},"/updates/forward/market-commentary/2024-27th-dec.md":{"label":"#2 AI Talent Wars, OpenAI’s New Models, Hyperliquid","children":{},"url":"/updates/forward/market-commentary/2024-27th-dec"},"/updates/forward/market-commentary/2024-13th-dec.md":{"label":"#1 Gemini 2.0, OpenAI’s Sora,  a16z’s Predictions","children":{},"url":"/updates/forward/market-commentary/2024-13th-dec"}},"url":"/updates/forward/market-commentary"},"/updates/forward/product-design":{"label":"product-design","children":{"/updates/forward/product-design/product-design-commentary-20241122.md":{"label":"Product Design Commentary #7: Hyper-personalization - How AI improves user experience personalization","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241122"},"/updates/forward/product-design/product-design-commentary-20241115.md":{"label":"Product Design Commentary #6: AI in Design - Cool ideas and how to make them happen","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241115"},"/updates/forward/product-design/product-design-commentary-20241101.md":{"label":"Product Design Commentary #5: Figma to SwiftUI (functional code) with Claude AI","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241101"},"/updates/forward/product-design/product-design-commentary-20241018.md":{"label":"Product Design Commentary #4: Generative AI UX design patterns","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241018"},"/updates/forward/product-design/product-design-commentary-20241011.md":{"label":"Product Design Commentary #3: The art of prompting in AI-human interaction","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241011"},"/updates/forward/product-design/product-design-commentary-20241004.md":{"label":"Product Design Commentary #2: Unpacking the sparkles icon and AI onboarding challenges","children":{},"url":"/updates/forward/product-design/product-design-commentary-20241004"},"/updates/forward/product-design/product-design-commentary-20240927.md":{"label":"Product Design Commentary #1: New technologies changing UX/UI and product design","children":{},"url":"/updates/forward/product-design/product-design-commentary-20240927"}},"url":"/updates/forward/product-design"},"/updates/forward/vol-01":{"label":"vol-01","children":{"/updates/forward/vol-01/istio.md":{"label":"New Member","children":{},"url":"/updates/forward/vol-01/istio"}},"url":"/updates/forward/vol-01"},"/updates/forward/2025-02.md":{"label":"2024_2025","children":{},"url":"/updates/forward/2025-02"},"/updates/forward/2024-09.md":{"label":"September 2024","children":{},"url":"/updates/forward/2024-09"},"/updates/forward/2023-11.md":{"label":"November 2023","children":{},"url":"/updates/forward/2023-11"},"/updates/forward/2023-10.md":{"label":"October 2023","children":{},"url":"/updates/forward/2023-10"},"/updates/forward/2023-08.md":{"label":"August 2023","children":{},"url":"/updates/forward/2023-08"},"/updates/forward/2023-06.md":{"label":"June 2023","children":{},"url":"/updates/forward/2023-06"},"/updates/forward/2023-05.md":{"label":"May 2023","children":{},"url":"/updates/forward/2023-05"},"/updates/forward/2023-03.md":{"label":"March 2023","children":{},"url":"/updates/forward/2023-03"},"/updates/forward/2023-12.md":{"label":"December 2023","children":{},"url":"/updates/forward/2023-12"},"/updates/forward/2022.md":{"label":"2022","children":{},"url":"/updates/forward/2022"},"/updates/forward/volume-03.md":{"label":"Tech Radar Volume 03","children":{},"url":"/updates/forward/volume-03"},"/updates/forward/volume-02.md":{"label":"Tech Radar Volume 02","children":{},"url":"/updates/forward/volume-02"},"/updates/forward/volume-01.md":{"label":"Tech Radar Volume 01","children":{},"url":"/updates/forward/volume-01"},"/updates/forward/README.md":{"label":"Forward Engineering","children":{},"url":"/updates/forward"}},"url":"/updates/forward"},"/updates/labs-digest":{"label":"labs-digest","children":{"/updates/labs-digest/05.md":{"label":"Labs Weekly Catchup #5","children":{},"url":"/updates/labs-digest/05"},"/updates/labs-digest/04.md":{"label":"Labs Weekly Catchup #4","children":{},"url":"/updates/labs-digest/04"},"/updates/labs-digest/03.md":{"label":"Labs Weekly Catchup #3","children":{},"url":"/updates/labs-digest/03"},"/updates/labs-digest/02.md":{"label":"Labs Weekly Catchup #2","children":{},"url":"/updates/labs-digest/02"},"/updates/labs-digest/01.md":{"label":"Labs Weekly Catchup #1","children":{},"url":"/updates/labs-digest/01"}},"url":"/updates/labs-digest"},"/updates/OGIF":{"label":"OGIF","children":{"/updates/OGIF/41-20250314.md":{"label":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","children":{},"url":"/updates/ogif/41-20250314"},"/updates/OGIF/39-20250207.md":{"label":"#39 Frontend report, DB Scaling, AI Workflow","children":{},"url":"/updates/ogif/39-20250207"},"/updates/OGIF/38-20250117.md":{"label":"#38 Erlang automata, AI Trends, Year-End Awards","children":{},"url":"/updates/ogif/38-20250117"},"/updates/OGIF/37-20241227.md":{"label":"#37 AI Fine-tuning, Data archiving, Datalakes","children":{},"url":"/updates/ogif/37-20241227"},"/updates/OGIF/28-20241018.md":{"label":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","children":{},"url":"/updates/ogif/28-20241018"},"/updates/OGIF/27-20241011.md":{"label":"#27 Go weekly, Frontend, AI UX, Finite Automata","children":{},"url":"/updates/ogif/27-20241011"},"/updates/OGIF/26-20241004.md":{"label":"#26 Design insights, Go tools, Trading app, Chatbots, Essays","children":{},"url":"/updates/ogif/26-20241004"},"/updates/OGIF/25-20240927.md":{"label":"#25 Team updates, Hybrid work, AI insights, Go weekly","children":{},"url":"/updates/ogif/25-20240927"},"/updates/OGIF/24-20240920.md":{"label":"#24 Go weekly, AI workflows, Team AI demo, Figma-UI with Claude","children":{},"url":"/updates/ogif/24-20240920"},"/updates/OGIF/23-20240913.md":{"label":"#23 Go weekly, FE report, Hybrid work, AI agents","children":{},"url":"/updates/ogif/23-20240913"},"/updates/OGIF/22-20240906.md":{"label":"#22 Hybrid work, Tech report, Go weekly, AI demo","children":{},"url":"/updates/ogif/22-20240906"},"/updates/OGIF/21-20240830.md":{"label":"#21 Community engagement, Go weekly, Journey of thought for prompt engineering","children":{},"url":"/updates/ogif/21-20240830"},"/updates/OGIF/20-20240823.md":{"label":"#20 Go weekly, Dynamic objects, Devbox, LLM tracing, Cursor AI","children":{},"url":"/updates/ogif/20-20240823"},"/updates/OGIF/19-20240821.md":{"label":"#19 Go weekly, UI design, File sharing, Dify AI","children":{},"url":"/updates/ogif/19-20240821"},"/updates/OGIF/18-20240809.md":{"label":"#18 Go weekly, RAG, UI, FE updates","children":{},"url":"/updates/ogif/18-20240809"},"/updates/OGIF/17-20240802.md":{"label":"#17 Community Call July, C4 Model, Interview Life in the US","children":{},"url":"/updates/ogif/17-20240802"},"/updates/OGIF/16-20240726.md":{"label":"#16 Go weekly, Dune query, AI voice clone, RAG re-ranking","children":{},"url":"/updates/ogif/16-20240726"},"/updates/OGIF/15-20240719.md":{"label":"#15 AI Supervisors, Local-first Software, Code Completion, Bot Commands","children":{},"url":"/updates/ogif/15-20240719"},"/updates/OGIF/14-20240712.md":{"label":"#14 Generic Collections, Pricing Models, and OGIF Summarizer","children":{},"url":"/updates/ogif/14-20240712"},"/updates/OGIF/13-20240705.md":{"label":"#13 Go Weekly updates, Radix Sort, Human Feedback Mechanism, and effective ChatGPT usage","children":{},"url":"/updates/ogif/13-20240705"},"/updates/OGIF/12-20240628.md":{"label":"#12 June updates, Go Performance, eBPF, PGO, Multimodal RAG","children":{},"url":"/updates/ogif/12-20240628"},"/updates/OGIF/11-20240621.md":{"label":"#11 Design patterns: template method \u0026 visitor, Radix sort, and weekly tech commentary","children":{},"url":"/updates/ogif/11-20240621"},"/updates/OGIF/10-20240614.md":{"label":"#10 Behavioral Patterns and Map Content Organization","children":{},"url":"/updates/ogif/10-20240614"},"/updates/OGIF/9-20240607.md":{"label":"#9 What's next for June and Behavior Design Patterns","children":{},"url":"/updates/ogif/9-20240607"},"/updates/OGIF/7-20240517.md":{"label":"#7 Echelon EXPO, Programming patterns, and Moonlighting","children":{},"url":"/updates/ogif/7-20240517"},"/updates/OGIF/6-20240510.md":{"label":"#6 Factory Pattern, Erlang State Machines, and Trading Process","children":{},"url":"/updates/ogif/6-20240510"},"/updates/OGIF/5-20240503.md":{"label":"#5 Singapore Market Report, C4 Modelling, Memo's Nested Sidebar","children":{},"url":"/updates/ogif/5-20240503"},"/updates/OGIF/4-20240426.md":{"label":"#4 DCA, Devbox","children":{},"url":"/updates/ogif/4-20240426"},"/updates/OGIF/3-20240419.md":{"label":"#3 Generative AI, Tokenomics, and Finance Talks","children":{},"url":"/updates/ogif/3-20240419"},"/updates/OGIF/2-20240412.md":{"label":"#2 Devbox as the new Docker, Security Standards, and Understanding Liquidity","children":{},"url":"/updates/ogif/2-20240412"},"/updates/OGIF/1-20240405.md":{"label":"#1 Markdown Presentations, Research Pipeline, Screenshots How-to","children":{},"url":"/updates/ogif/1-20240405"},"/updates/OGIF/README.md":{"label":"OGIF - Oh God It's Friday","children":{},"url":"/updates/ogif"}},"url":"/updates/ogif"},"/updates/wala":{"label":"wala","children":{"/updates/wala/001-43-factory.md":{"label":"43 Factory","children":{},"url":"/updates/wala/001-43-factory"},"/updates/wala/002-dzs-media.md":{"label":"DZS Media","children":{},"url":"/updates/wala/002-dzs-media"},"/updates/wala/003-sp-group.md":{"label":"SP Group","children":{},"url":"/updates/wala/003-sp-group"},"/updates/wala/README.md":{"label":"WALA","children":{},"url":"/updates/wala"}},"url":"/updates/wala"}},"url":"/updates"}},"pinnedNotes":[{"title":"Notes on our culture","url":"/culture/readme","date":"2024-04-29"},{"title":"OGIF - Oh God It's Friday","url":"/culture/ogif-intro","date":"2023-02-16"},{"title":"§ LLM","url":"/playground/topics/llm/llm","date":""}],"tags":["AI","Australia","ESTJ","Frontend","INTJ","ISTJ","ISTP","LLM","US","UX","UX-UI","Vietnam","a-record","a11y","accessibility","accounting","advisory-locks","agents","aggregation","agile","ai","ai-agents","ai-evaluation","ai-integration","algorithms","amm","analysis","analytic-functions","analytics-platform","analytics-tools","anchor","android","animation","animations","apache-hive","apache-pig","api-mocking","apollo","append-only","apprentice","apprenticeship","architecture","assessment","assets","atomic-css","atomic-design","atomicity","automata","avl-tree","aws","aws-s3","backend","backend-engineer","backlog-grooming","bash","behavior-driven-development","behavior-model","behavior-pattern","behavior-patterns","best-practices","bi-tools","big-data","big-o","binance","binary-search-tree","binary-tree","blockchain","blockchain-bridge","bloom-filter","blue-green-deployement","bounty","brain","brainstom","bramble","breakdown","browser-extension","btc","buckets","budgeting","burn-out","burndown","business","caching","caching-data","career","case-study","cassandra","catchup","cdap","chatgpt","checklist","circuit-breaker","clean-architecture","clean-code","cli","client","client-side","clojure","cname-record","code of conduct","code-generation","cognitive","cognitive-load","collaboration","communication","communications","community","community-contributor","complexity","compliance","component","components","composite-index","concurrency","concurrency-control","console","consulting","containerization","content","contribution","conway","conway-law","copilots","crdt","creational-design-pattern","crypto","cryptocurrency","css","css-in-js","cssom","culture","cybersecurity","dark-mode","data","data-analysis","data-engineering","data-fetching","data-modeling","data-modelling","data-ownership","data-pipeline","data-processing","data-structure","data-structures","data-synchronization","data-types","data-vault","data-warehouse","database","dbt","dcos","ddd","debug","debugger","debugging","decontextualize","defi","delegate","delivery","demo","deployment","deployment-strategy","design","design-pattern","design-principle","design-system","design-thinking","devbox","devops","dfg","diagram","dimension-table","disc","discord","discussion","distributed","distributed-storage","distributed-system","distributed-systems","distribution","dns","docker","documentation","dom","domain","domain-driven-design","dora","dropshipping","duckdb","dwarves","dx","e-commerce","early-stage","earn","ecommerce","elixir","email","employee","enabling-team","energy","engagement","engineer","engineering","engineering-management","engineeringbackend","engineeringdata","engineeringfrontend","engineeringmanagement","engineeringmobile","engineeringqa","english","enhancements","enterprise","entertainment","erlang","error","error-handling","esm","estimation","ethereum","etl","evaluating-tech","evaluation","event","event-sourcing","event-store","evm","explicit-locking","fact-table","feature-flags","federation","feedback","film","finance","finite-automata","finite-state-machine","fintech","fluency","flutter","fnb","fonts","form","forward-engineering","forward-proxy","foundational-topics","founder","foundry","framework","fronten","frontend","frontend-engineer","frontendperformance","fsm","fullstack","function-calling","funding","gang-of-four","generative-ui","generics","git","git-flow","global-state-management","go","go-weekly","goal","golang","gomock","google-cloud","google-data-fusion","google-data-studio","google-dataproc","graphical-notation","graphql","growth","guide","guideline","guidelines","guides","guidline","hadoop","handbook","hash","hdfs","healthcare","hedge-foundation","hiring","history","home","hook","hooks","hospitality","hsl","html","htmx","http","human-resource","hybrid-search","hybrid-working","i-os","icy","iframe","import","index","innovation","instructions","integrative-thinking","intent-classification","interface","internal","internship","intersection-observer","interview","iot","ip-address","iterators","java","javascript","jbtd","jetpack","js","jtbd","k8s","kafka","kernel-programing","knowledge","knowledge-base","kotlin","kubernetes","kubeseal","labs","langchain","language","law","leaderboard","leadership","learning","libcluster","license","life-at-dwarves","life-at-dwarves, ai-developer, hybrid-work","life-at-dwarves, alumni, career-growth","life-at-dwarves, apprenticeship, backend-engineer","life-at-dwarves, backend-engineer, career-change","life-at-dwarves, backend-engineer, community-building","life-at-dwarves, backend-engineer, continuous-learning","life-at-dwarves, backend-engineer, golang","life-at-dwarves, backend-engineer, learning-culture","life-at-dwarves, backend-engineer, mentorship","life-at-dwarves, backend-engineer, personal-development","life-at-dwarves, backend-engineer, teamwork","life-at-dwarves, communication-specialist, remote-work","life-at-dwarves, community-contributor, techie-project","life-at-dwarves, data-engineer, remote-work","life-at-dwarves, devops-engineer, personal-growth","life-at-dwarves, engineer, work-culture","life-at-dwarves, engineering-manager, mentorship","life-at-dwarves, frontend-engineer, community-building","life-at-dwarves, frontend-engineer, community-learning","life-at-dwarves, frontend-engineer, community-member","life-at-dwarves, frontend-engineer, design-engineering","life-at-dwarves, operations, techie-story","life-at-dwarves, product-executive, personal-growth","life-at-dwarves, qa-engineer, mentorship","life-at-dwarves, qa-engineer, quality-standards","life-at-dwarves, senior-engineer, mentorship","life-at-dwarves, software-engineer, engineering-values","life-at-dwarves, software-engineer, growth-mindset","life-at-dwarves, software-engineer, mentorship","life-at-dwarves, ui-designer, design-communication","liquidity","llm","load-balancing","local-first","localstack","locking","log","logging","long-terms","lua","mac-os","machine-learning","machines","macos","maintainability","management","management-knowledge","management-skills","mapreduce","market-report","marketing","marketplace","materialized-view","mbti","mcp","mechanism","meeting","memo","messaging","method","metric","micro-frontend","microservices","migration","migrations","mobile","moc","mock-service-worker","mocking","model","modeling","modules","monitoring","monogodb","multi-column-index","multi-threaded","multisign-wallet","narrative","native-modules","nda","network","networking","newsletter","nextjs","nft","nghenhan","nix","nocode","nosql","note","notefleeting","noteliterature","notepermanent","object-dependency","object-oriented-programming","observability","observer-pattern","ocaml","office-hours","ogif","okr","onboarding","oop","open-source","operation","operations","ops","oracle","organization","oss","overleaf","overview","package","parallelism","pareto","parsing","partitions","partners","partnership","pattern","pattern-matching","patterns","payment","people","performance","performance-review","personalities","personality","personas","pg","philosophy","phoenix-live-view","phrasing","pillar","playbook","pm","po-s","policy","polygon","polymorphic-component","pomodoro","post-message","power-bi","practice","presentation","pricing","principle","principles","process","product","product-design","product-sense","production-traffic","productivity","profiling","programming","progressive-delivery","project","project-management","project-manager","projectmanagement","prompt","prompt-engineering","prompting","proof-of-knowledge","protocol","protocols","psychology","purpose","python","qa-terms","quality","quality-assurance","quant","race","race-condition","radar","radio","rag","random-reads","random-writes","rate-limiting","rd","react","reactjs","readability","real-estate","real-time","real-time-collaboration","recording","redis","reinforcement-learning","reliability","remote","remote-caching","remote-work","render-tree","rendering","repl","report","reporting","requirement","requirements","research","researchqualitative","researchquantitative","resiliency","responsive-design","reward","rfc","ride-hailing","rust","sandbox","sargable-queries","scroll-driven-animations","scrum","sdlc","search","search-engine","secret","security","seo","sequential-reads","sequential-writes","server-component","serverless","service","service-account","shadow-dom","shares","sharing","showcase","skeuomorphism","slide","smart-contract","social-networks","software","software-architecture","software-design","software-modeling","solana","solid","solid-principles","solving-issue","sops","sorted-set","sorting","sql","standardization","stark-net","startup","state","state-machine","state-machine-diagram","state-machines","state-management","state-mangement","stateful","stateless","statistics","stitching","storage","story-telling","storybook","strategy-design-pattern","streaming","subdomain","subscription","summit","supervisor-architecture","swap","swift","swr-infinite","system design","tableau","target-branch","tauri","team","team-building","team-design","team-management","team-structure","team-topologies","teamwork","tech-radar","tech-report","techecosystem","technicaldebt","technique","techniques","template","test","test-cases","testing","testing-type","thinkingintegrative","threejs","time","timescaledb","tip","tips","token","ton","tool","tooling","tracing","trait","transaction","transducers","transformersjs","translation","transparency","travel","tuning-llm","tutorial","typesafe","typescript","ubiquitous-language","ui","uilibraries","unified-api-gateway","updates","upptime","url-redirect","url-rewrite","use-effect","user-experience","ux","ux-research","ux-ui","validation","variable-fonts","vector-database","ventures","vercel","video-streaming","vim","vim-slime","virtual-dom","virtualization","visitor-design-pattern","vitejs","wai-aria","wala","wasm","web","web-api","web-design","web-development-tool","web-performance","web-socket","web3","webassembly","webrtc","websocket","window-functions","wisdom","work","work-adoption","workflow","workshop","writing","writing-test-cases","writting","zero-knowledge","zettelkasten","zk-proof","zk-rollup","zookeeper"],"ogifMemos":[{"content":"\n### What’s OGIF?\n\nOGIF stands for \"Oh God It’s Friday.\" It’s our weekly Friday tradition where members share 10-minute talks. The team hops on Discord to share and discuss diverse topics like software development, engineering patterns, industry trends, finance, entrepreneurship, blockchain, AI, and a mix of other cool stuff. It’s a quick, engaging way to learn something new.\n\nWe value the importance of sharing our work with other people at Dwarves. And it's a chance for people who don't work directly on development to feel included in the process.\n\n### Meet the OGIF memo summarizer\n\nWorking in collaboration with my colleague @innno\\_, we crafted a chatbot using Dify. This bot transcribes YouTube videos and extracts key points from our OGIF sessions. The result? A major boost in our ability to review and reference the knowledge shared during these meetings.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-7.webp)\n\n### How it works?\n\n**YouTube Transcription**: We integrated a YouTube transcription workflow as a function-calling tool for our chatbot.\n\n**Intelligent Summarization**: The chatbot generates structured summaries from the transcribed content using a well-designed prompt.\n\n**Three-Tier Summary Structure**:\n\n1. **Short Summary**: 3-5 key timestamps with brief descriptions of the most important topics.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-1.webp)\n\n2. **Detailed Summary**: Comprehensive timestamps, each with 2-3 bullet points providing in-depth information.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-2.webp)\n\n3. **Languages:** The summary is available in both English and Vietnamese.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-3.webp)\n\n### The prompt\n\nThe core of our summarizer is the prompt we’ve meticulously designed. Here’s what it does:\n\n- Generates structured summaries with accurate timestamps.\n  ![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-4.webp)\n- Creates clickable links to specific points in the video.\n- Offers both a quick overview and detailed insights.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-5.webp)\n\n- Maintains consistent formatting for easy reading.\n- Provides a comprehensive overview of the video content.\n\n![](assets/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing_crafting-ogif-summarize-bot-6.webp)\n\n### Benefits\n\n- **Time-Saving**: Team members can quickly grasp the main points of a session without watching the entire video.\n- **Easy Navigation**: Clickable timestamps for swift access to specific topics of interest.\n- **Knowledge Retention**: The structured summary serves as a reliable reference for future use.\n- **Improved Accessibility**: Makes the session content more accessible to team members who couldn’t attend live.\n\n### What’s next?\n\nWe’re always looking to enhance our OGIF Memo Summarizer. Some future ideas include:\n\n- Integrating automatic tagging for easy topic categorization.\n- Implementing a search function across multiple summaries.\n- Creating a visual timeline of topics discussed over multiple sessions.\n\n### Wrapping up\n\nThe OGIF Memo Summarizer has improved our weekly knowledge-sharing sessions. Using AI, we've made a system that saves time and enhances the value of our Friday office hours.\n\nWe recommend other teams try similar tools to streamline their knowledge-sharing. In software engineering, efficient learning and sharing information can give you a big advantage.\n\nHappy coding and happy sharing.\n","title":"How we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","short_title":"","description":"Introducing the OGIF Memo Summarizer, a chatbot we developed using Dify in collaboration with @innno_. This tool transcribes YouTube videos and extracts key points from our Oh God It’s Friday (OGIF) sessions. By providing both short and detailed summaries in English and Vietnamese, it significantly enhances our ability to review and reference the diverse knowledge shared every Friday.","tags":["ai","ogif","guidelines"],"pinned":false,"draft":false,"hiring":false,"authors":["monotykamary","innno_"],"date":"Wed Jul 10 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/topics/ai/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing.md","slugArray":["playground","topics","ai","how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing"]},{"content":"\n### Topics and Highlights\n\n- **Swap ICY-BTC:** Huy shared updates on the ICY-BTC swap mechanism, explaining the current state and adjustments needed to ensure accurate ICY valuation during swaps.\n- **GitHub BotL:** Thanh introduced a GitHub bot to automate PR reviews, aiming to improve processing speed and consistency in code management.\n- **Memo UI:** The team presented improvements to the Memo user interface, focusing on better data access and user experience.\n- **Agentic: MCP-DB:** Huy discussed the MCP-DB system, highlighting how it handles data storage and retrieval to support agents in automated workflows.\n- **Pocket Turning, Recapable:** Vincent shared progress on the Pocket Turning and Recapable, outlining the completion of core gameplay and next steps.\n- **Funding Rate Arbitrage:**  Antran presented a strategy for funding rate arbitrage across multiple exchanges, addressing technical challenges and execution strategies.\n\n### Vietnamese Transcript\n\n**[05:30]** Hôm nay chắc mình bắt đầu sớm nha. Buổi hôm nay chắc kết hợp với lại anh trong buổi meeting một xíu. Một phần là sẽ làm showcase, cái thứ hai là anh tổng kết một số việc mà bữa trước có trao đổi với mấy anh em á. Cái số hai, cái số ba là mình sẽ bắt đầu cho mấy anh em đăng ký công việc. Hiện tại để mà dễ trước, chắc là mình sẽ để cho Huy Nguyễn đi show mấy cái phần bên Huy trước, liên quan tới ICY một tí, xong rồi show một số cái về tech mà team mình đang làm nè. Để mình có một cái snapshot về chuyện là team tech thì hiện nay như thế nào nhé. Rồi sắp tới thì team mình cần gì, với lại mấy anh em xem contribute được gì vào đó ha.\n\n**[06:35]** Huy, Thành đâu? Nhường sân khấu này nè. Rồi ok, nội dung đầu tiên, chắc là bên ICY Swap trước đi. Mình announce đó, hồi tuần trước, tuần này deploy lên rồi thì giờ những cái khác biệt như thế nào, chắc nhờ Huy đi lại hết mấy series đó.\n\n**[07:29]** Alo, rồi rồi, đã xem màn hình rồi. Thì bây giờ mọi người có thể vào trang ICY Swap để mà swap được rồi. Đây, mình chỉ số liệu nha. Nhưng mà ở trên đây thì nó đang ready hết tất cả mọi thứ rồi. Việc làm duy nhất bây giờ là đang ngồi soát lại mấy cái số ICY á. Tại vì lúc trước vận hành á, thì mình vận hành theo kiểu là mình neo cái giá ICY, nên mình cũng không quan tâm cái lượng lưu thông (circulated) lắm. Nên có mấy trường hợp là mình để vô mấy cái ví của team, hoặc là chuyển qua mấy cái Mochi Balance của em hoặc là của anh Bảo. Thì mấy cái đó đang cần rà soát lại để mà nó ra cái số lưu thông đúng. Tại vì giờ mình sẽ ngồi, cái giá của mình nó sẽ dynamic theo cái pool nên cần ngồi check lại cái đó thì cũng gần xong hết rồi.\n\n**[09:09]** Giờ còn mỗi cái account của anh Bảo là cần kiểm tra lại thôi. Nhớ có đợt là chuyển cho anh Bảo, giờ đang ngồi xem lại cái phần đó rồi cộng trừ lại rồi cắt cái phần đó ra khỏi cái circulated thì số này nó sẽ ra đúng. Còn lại hiện tại muốn swap ủng hộ thì cũng có thể swap được ở trên trang này. Lịch là đang vậy. Em show thử cái list Holder của mình hiện tại cho mấy anh em xem chắc cần biết nhiều hơn xíu. Trước giờ mọi người tham gia không quan tâm nhiều lắm nhưng mà chắc lần này thì mình cần để ý hơn.\n\n**[09:51]** ICY của mình mình deploy ở trên Base, đúng không? Nên khi anh em vào trong cái list Holder, mọi người sẽ thấy được một cái list khoảng tất cả những cái ví nào đang được giữ ICY của team mình, thì là CCK Holder ha. Là một. Rồi thì cái link để mà vô đây chắc Huy share nha. Chứ mọi người lên mà search thì chắc không biết được đâu.\n\nĐầu tiên là anh em cần nắm cái này. Quay qua đoạn này rồi. Anh nghĩ mấy anh em cần quan tâm phần này nhiều hơn xíu. Nó trở thành cái norm của thế giới tech luôn rồi, không cần làm gì mới nữa. Nên anh em nắm được thì sẽ ok hơn.\n\n**[10:33]** ICY của mình hiện đã được list. Trong danh sách này có các ví minter, ví dùng để lập ngân sách cho các hoạt động, và một số ví đang nắm giữ lượng ICY lớn. Các hoạt động liên quan đến staking ICY sẽ được triển khai dần dần trong thời gian tới. Đây là thông tin đầu tiên anh em cần nắm rõ.\n\n**[11:15]** Huy, demo thử luồng swap đi. Có ai có địa chỉ Bitcoin với một ít ICY không? Vincent có ở đây không? Ok, giờ thử swap từ ICY sang Bitcoin. Giá hiện tại được tính theo cơ chế động dựa trên lượng ICY đang lưu hành và pool. Chức năng swap rất đơn giản, chỉ cần điền số lượng, bấm swap là xong.\n\n**[12:27]** Khoan đã, đừng nhập địa chỉ ảo. Ok, vậy là ổn rồi. Khung đầu tiên là ICY như bình thường. Ở dưới thì đang hiển thị đơn vị là satoshi, tức là đơn vị nhỏ nhất của Bitcoin. Khi nhập số lượng vào, nó sẽ tự động chuyển đổi. Tuy nhiên, tỷ giá hiện tại đang bị lệch một chút, khoảng 1.2 thay vì 1.5. Đây chắc là lỗi tính toán nhỏ, chỉnh lại là được.\n\n**[13:28]** Cần có số ICY tối thiểu để swap. Thử nhập 30 ICY xem sao. Refresh lại thử xem có được không.\n\n**[14:43]** Hình như không đủ tiền trong ví rồi. Bạn có ETH trên Base không? Chuyển qua Base và kiểm tra lại xem.\n\n**[15:51]** Không phải lỗi đó đâu. Vấn đề là account chưa được đăng ký nên không thể thực hiện giao dịch. Sẽ fix phần đó sau. Mục tiêu ở đây là giúp mọi người hiểu rõ hơn về cơ chế swap và cách định giá token. Nếu nắm rõ thì sau này sẽ dễ dàng hơn trong việc quản lý tokenomics.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=0LryX12wLbTu3i1m\u0026amp;start=806\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[16:47]** Huy, giải thích nhanh lại cơ chế tính giá đi. Lần trước Quan demo chưa nói kỹ phần đó. Giá của ICY được xác định theo cơ chế minting, nghĩa là giá sẽ không thay đổi mạnh nếu có ai đó swap số lượng lớn. Nó không hoạt động theo kiểu cơ chế tạo lập thị trường tự động (AMM) mà giá sẽ được kiểm soát theo cơ chế minting. Cơ chế này giúp giá duy trì ổn định ngay cả khi có giao dịch lớn.\n\n**[17:43]** Hoàn toàn là nó phụ thuộc vào Bitcoin. Nên nếu giá Bitcoin tăng thì lượng ICY mà anh em đang cầm sẽ tăng về giá trị USD. Còn về cơ chế minting, nhờ Huy giải thích thêm một chút. Nói chung là cơ chế chung của mình trước giờ là mình sẽ cố định giá trị của ICY theo USDC. Anh em không cần quan tâm nhiều, cứ hiểu đơn giản là một ICY tương đương với 1.5 USD.\n\n**[18:37]**Phần đảm bảo này là để giúp team vận hành có thể đảm bảo là tới ngày thì sẽ đổi USDC vào trong contract để mọi người swap. Tỷ giá swap trong contract cũ là cố định ở mức 1.5 ICY, nhưng đó là model cũ. Model mới của mình thì linh hoạt hơn. Nếu anh em đã dùng Uniswap hay các AMM (Automatic Market Maker) khác thì nó cũng tương tự một chút. Ở đây, cơ chế hoạt động là bên dưới có một pool thanh khoản (liquidity pool), trong đó chứa cả ETH và USDC. Tùy vào tình hình của pool lúc đó, tỷ giá sẽ được điều chỉnh dựa trên lượng ETH và USDC trong pool.\n\n**[19:18]** Cơ chế của mình cũng tương tự như vậy. Giá ICY sẽ được quyết định bởi lượng Bitcoin trong pool và lượng ICY đang được lưu hành. Công thức đơn giản thôi: mình có lượng ICY (X), có lượng BTC (Y) trong pool, thì X/Y sẽ ra được giá trị của một ICY tính theo BTC. Công thức này là công thức toán học cơ bản, không có gì phức tạp.\n\n**[19:55]** Do cơ chế hoạt động của mình, sẽ có hai thời điểm làm thay đổi thanh khoản:\n\n1. **Thời điểm đầu tiên** là vào mỗi tháng, team vận hành sẽ đổ thêm BTC vào pool để làm chi phí cho các hoạt động của team. Lúc này giá ICY sẽ tăng lên một chút vì lượng BTC trong pool tăng lên.\n2. **Thời điểm thứ hai** là khi team đẩy thêm ICY vào pool (minting thêm). Khi mint thêm ICY, giá ICY trên thị trường sẽ giảm xuống do lượng ICY trong pool tăng lên.\n\n**[20:35]** Hai trường hợp trên sẽ ảnh hưởng trực tiếp đến giá ICY. Còn nếu giá Bitcoin thay đổi thì giá trị USD của ICY có thể thay đổi, nhưng giá ICY tính theo BTC thì không thay đổi. Market impact từ Bitcoin là yếu tố bên ngoài, không ảnh hưởng trực tiếp đến việc minting hoặc giá trị ICY trong pool.\n\n**[21:12]** Anh em có câu hỏi gì thêm thì đặt câu hỏi, tí nữa sẽ trả lời sau. À, có câu hỏi về việc swap ngược từ BTC về ICY đúng không? Hiện tại thì chưa có chức năng đó. Hiện tại chỉ hỗ trợ swap từ ICY sang BTC thôi, không có chức năng swap ngược lại. Tức là mua vào thì được, nhưng bán ra thì chưa hỗ trợ.\n\n**[21:40]** Cảm ơn Huy. Có gì cần lưu ý thêm không? Cần lưu ý là hiện tại vẫn đang trong giai đoạn thử nghiệm nên có thể có một số trường hợp ngoại lệ. Ví dụ như một số tình huống có thể phát sinh khi swap hoặc thanh khoản chưa đủ. Về cơ bản thì luồng hiện tại vẫn đang hoạt động ổn định.\n\n**[22:00]** Như là số lượng ICY tối thiểu để swap. Vì bản chất là team mình đang cover cái phần phí mà để mà làm gas trên ETH, trên Base và cả trên BTC luôn thì nên đang kiểu đang giới hạn cái số ICY nó swap nhiều tí để mà hạn chế với cái việc mà mọi người swap tầm 1-2 ICY để test á thì nó tốn cái chi phí gas nên đang để tầm trên 20 ICY mới cho mọi người swap trên web.\n\nCái thứ hai là ở cái do cái việc mà mình mint thêm ICY thì nó sẽ làm thay đổi giá thị trường, thì nên em đang disable luôn cái phần mà cơ chế cái ứng lương trước của mình.\n\n**[22:37]** Tức là đồng loạt ứng lương thì nó sẽ ảnh hưởng giá đúng không? Vậy cái lesson learn trong cái này đó là sau đợt này làm thì có vài điểm mà anh đang thấy là bắt đầu team mình đang tập trung vô build những cái tool nó hỗ trợ mình hoạt động. Cũng là một số cái thử nghiệm mới, cũng là một số cái mà hỗ trợ hoạt động thiệt sự. Nhưng mà sau khi xong mấy cái bài này thì nó sẽ ra được một số mấy cái article liên quan thì mấy anh em nếu mà trước đó không có tham gia những cái dự án đó có thể tìm lại những cái bài đó để mà coi được cái game, cái knowledge game từ cái đợt đó là cái gì của mấy anh em làm dự án đó ha.\n\n**[23:24]** Rồi thì trong cái vụ ICY Swap đợt này chắc là được hai ba bài phải không? Dạ, như được ba bài. Còn kiểu viết nhiều thêm thì vẫn có nhiều cái để viết. Ừ, thôi đó cứ thong thả từ từ đi.\n\n**[24:02]** Sau phần của Huy, anh cảm ơn Huy rồi chuyển sang nội dung thứ hai liên quan đến những gì team mình đang làm. Anh Bảo ai nói trước cũng được, nhưng chắc là để Thành nói trước. Thành bảo là em nói trước cũng được, em sẽ gom lại hết để anh cho mọi người biết team đang ở giai đoạn nào. Nhưng anh bảo là để Thành nói trước đi, tại vì đang có người bấm chuông. Rồi anh mời Thành bắt đầu.\n\n**[25:00]** Mọi người, Memo của mình là một trong những cái đợt lớn đợt này, có upgrade format lại cho nhìn nó ok hơn tí. Mình luôn muốn mình tạo những cái map content, những cái thứ mà mình đọc được cái mình up lên đây. Nhưng mà hiện tại cái mô hình đó thật ra nó cũng không có còn quá hiệu quả với chuyện là mấy cái model ra đời nó nén dữ liệu lại, rồi mình query trực tiếp từ đó ra thì nó sẽ hiệu quả hơn.\n\nThì cái point của chuyện là đưa những cái kiến thức mà nó bình thường lên trên Memo thì nó cũng không phù hợp lắm ha. Nên đợt này lúc mà làm lại thì có một cái ý chính để mà muốn nói với anh em đó là Memo hiện tại sẽ được dùng chỉ cho mục đích duy nhất thôi ,  đó là cái knowledge gain mà từ dự án.\n\nCái đó là gần như là những cái mới mà nó xuất phát từ chính cái hoạt động của cái team mình. Gần như trên đây sau này nó sẽ gồm là liên quan tới lĩnh vực gì đó, mình đã làm gì đó trong đó. Nó có nhiều hơn, maybe là sau một giai đoạn thì khi tụi nó train lại cái model thì những cái dữ liệu của mình á thì nó sẽ trở thành một phần của kiến thức chung cho cả cộng đồng.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=MhsFuFFQ5NFKTlYS\u0026amp;start=1556\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[25:39]** Và cái phần này anh nghĩ là nó sẽ giúp ích rất nhiều cho cái chuyện mà mọi người làm kiểu training lại cho AI model sau này, hoặc là mấy cái chuyện mà mình muốn nó có cái việc mà suggestion kiểu tự động ấy.\n\n**[26:24]** Nội dung sẽ trở thành một phần trong mô hình đó hoặc nếu có mấy công cụ tìm kiếm trên internet, thì có thể bài của mình chỉ là một phần nhỏ trong nguồn tài liệu được tham khảo vào thôi, giống như là một phần nhỏ trong citation. Điều này cũng không có vấn đề gì lớn. Nhưng nhìn chung, toàn bộ những nội dung này sẽ gần như trở thành spirit của team.\n\nTrong lần nâng cấp lớn này, có một điểm chính mà Tuấn đã hoàn thành chưa nhỉ? Tuấn ơi, phần liên quan đến việc đồng bộ toàn bộ dữ liệu của team, nhất là về phần nội dung, hiện đang được định hướng như vậy để các thành viên nắm rõ hơn.\n\n**[27:00]** Tức là sau đợt này, các thành viên đang tham gia vào các dự án sẽ có xu hướng ngồi lại với nhau để xem xét kỹ hơn từ những dự án đó, và xác định rõ phần **knowledge gain** (kiến thức thu được) từ chính các dự án đó là gì. Sau đó, team sẽ đưa lên Memo làm nguồn tài liệu nội bộ cho team.\n\nPhần thứ hai là ở cuối mỗi bài sẽ có một phần liên quan đến **group of reading**. Hiện tại phần này vẫn chưa hoàn chỉnh, nhưng ý tưởng là sau khi hoàn thiện, sẽ có thêm phần thông tin tổng hợp về bài viết để người đọc có thể tra cứu và học thêm từ bài viết đó.\n\n**[27:47]** Ngoài ra, tất cả dữ liệu của team được viết ra sẽ được gán định danh ví dụ như **GitHub**, **Discord**, hoặc những kênh nội bộ khác. Dữ liệu này sẽ được upload lên dạng **blockchain storage** trên nền tảng **Arweave (AV)** – một nền tảng lưu trữ phi tập trung. Điều này giúp cho nội dung của team có một định danh rõ ràng và minh bạch.\n\nThêm vào đó, người đọc sẽ có thể xem lại bài viết, đánh giá hoặc để lại phản hồi trực tiếp trên bài viết. Đây là một phần của ý tưởng nâng cấp mới cho trang **Memo** của team.\n\n**[28:39]** Trước đây, team đã có ý định sử dụng Obsidian để quản lý nội dung, nhưng có vẻ như một số thành viên gặp khó khăn trong việc làm quen với công cụ đó. Vì vậy, hiện tại để làm cho mọi thứ đơn giản hơn, team sẽ chuyển sang cơ chế trực tiếp hơn. Cụ thể là thay vì phải làm qua Obsidian, các thành viên có thể submit nội dung trực tiếp vào repository của thư viện chung của team.\n\nCác thành viên chỉ cần đưa nội dung vào và submit trực tiếp qua nền tảng này, không cần phải tuân theo workflow bắt buộc của Obsidian nữa. Nếu ai vẫn muốn dùng Obsidian thì không sao, nhưng nếu không dùng thì cũng không ảnh hưởng gì cả. Đây là thay đổi cơ bản nhất trong hệ thống Memo của team.\n\n**[29:24]** Hiện tại team đang làm một số dự án chính, bao gồm:\n\n1. Bitcoin Swap – đã nhắc tới ở phần trước.\n2. Memo – vừa mới trình bày xong.\n3. Hai dự án nhỏ khác:\n\n- **agentic** – nhóm của Quang và Huy đang phát triển.\n- **github bot** – nhóm của Thành đang thực hiện, hiện đang test thử.\n\nGiờ chắc nhường lại cho Thành để chia sẻ thêm về những nội dung này.\n\n**[30:32]** Dự án này đã được khởi động hơn một tuần và đã chính thức chạy code được hơn một tuần. Mục đích chính của nó là tạo ra một hệ thống nhắc nhở (reminder). Trước đây, team thường gặp tình huống khi tạo pull request (PR), mọi người hay để đó và chờ chạy xong rồi quên luôn việc cần review. Tool này sẽ phục vụ cho việc theo dõi và cập nhật thông tin về các hoạt động hàng ngày trên github hoặc hàng tuần trên các kênh giao tiếp nội bộ của team.\n\n**[31:18]** Hệ thống này được thiết kế dưới dạng một tích hợp đơn giản. Luồng hoạt động cơ bản bao gồm một số use case như: thông báo cho người được assign để review, tương tác với GitHub API, và post thông tin vào các kênh nội bộ như Discord hoặc Slack. Hiện tại, team đang test thử trên Discord. Ngoài ra, team cũng đang thử nghiệm với agentic và một framework mới gọi là **Mastra AI**.\n\nFramework này khác với các tool Python thông thường. Một số thành viên trong team không quen làm việc với Python, nên team muốn thử nghiệm xem liệu sử dụng framework mới này có hiệu quả hơn các giải pháp hiện tại hay không. Framework này hỗ trợ các tính năng như setup môi trường, define các trạng thái để quản lý dữ liệu, và cho phép cấu hình lại tùy theo nhu cầu của team.\n\n**[32:19]** Cấu trúc của hệ thống này có hai phần chính:\n\n1. **Agentic App** – Đây là ứng dụng chính để xử lý các hoạt động của hệ thống.\n2. **Discord App** – Hỗ trợ việc gửi thông báo vào Discord.\n\nNgoài ra, hệ thống còn có một vài component phụ, như workflow để xử lý công việc theo lịch trình, kiểm tra và thông báo cho developer nếu có bất kỳ pull request nào đang chờ được review. Nếu pull request vượt quá một khoảng thời gian nhất định, hệ thống sẽ gửi thông báo để nhắc người thực hiện review.\n\n**[33:12]** Agentic App sẽ expose một vài API cho phép chat và theo dõi trạng thái của các pull request. Khi có một pull request được tạo ra, hệ thống sẽ tự động xác định các điều kiện như trạng thái của pull request (work in progress hay chưa), thời gian tạo pull request, và sẽ gửi thông báo cho người review sau khoảng 30 phút kể từ lúc tạo. Ví dụ: nếu có một pull request cần được review nhưng không có ai assigned hoặc đã quá thời gian xử lý, hệ thống sẽ tự động ping lại người phụ trách.\n\n**[35:02]** Thay vì phải theo dõi thủ công, hệ thống sẽ gắn con agent vào để tự động theo dõi và thông báo thông qua endpoint của hệ thống. Trong phần logic, hệ thống sẽ định nghĩa các điều kiện cụ thể, chẳng hạn như chỉ gửi thông báo nếu pull request được tạo trong vòng 30 phút hoặc đang trong trạng thái work in progress. Nếu pull request được cập nhật hoặc chuyển trạng thái, hệ thống sẽ tự động theo dõi và gửi thông báo cho developer để đảm bảo không bị sót.\n\n**[35:39]** Hệ thống sẽ hoạt động dựa trên code filter thông thường. Ngoài ra, nó sẽ có một số workflow khác như việc gửi thông báo vào cuối ngày để tổng hợp tình trạng của các pull request trên Discord. Hệ thống sẽ tự động gửi thông báo về số lượng pull request đang mở, tình trạng của chúng và trạng thái review hiện tại. Đây là chức năng chính của tool này ,  đóng vai trò như một công cụ reminder.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=Zduog0abeAWXIIM4\u0026amp;start=2107\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[36:24]** Hệ thống cũng có thể tích hợp với các công cụ chat khác. Đơn giản là có thể tạo thêm một command và gửi request tới endpoint của hệ thống. Các request này sẽ được định nghĩa dựa trên schema cụ thể, ví dụ như input là **review ID** hoặc các thông tin khác liên quan đến trạng thái của pull request. Hệ thống sẽ lấy dữ liệu này và hiển thị trên giao diện mà người dùng thường xuyên sử dụng.\n\n**[37:04]** Phần xử lý backend của hệ thống được thực hiện thông qua tool Lippia, một công cụ định dạng dữ liệu JSON thành dạng bảng Markdown table hoặc dạng data binding. Hiện tại team đang test thử hai luồng xử lý này trước khi mở rộng thêm các tính năng khác. Khi hệ thống hoạt động ổn định, các workflow này sẽ được mở cho tất cả các thành viên trong team thử nghiệm và phát triển thêm.\n\n**[38:08]** Hệ thống được thiết kế để mở rộng một cách linh hoạt. Các thành viên trong team có thể tự phát triển và đóng góp các workflow khác nhau. Hệ thống này cho phép xây dựng các tool dưới dạng một đơn vị độc lập (**packaging unit**), sau đó kết hợp các đơn vị này lại để tạo ra các workflow phức tạp hơn. Khi muốn phát hành một workflow mới, các thành viên chỉ cần định nghĩa lại đơn vị cơ bản và tích hợp nó vào hệ thống.\n\nViệc mở rộng các workflow sẽ giúp hệ thống phát triển theo chiều ngang (mở rộng số lượng tính năng), thay vì theo chiều dọc (phát triển tính năng hiện tại). Khi số lượng các workflow tăng lên, hệ thống sẽ càng trở nên linh hoạt và mạnh mẽ hơn.\n\n**[38:54]** Về cơ bản, workflow được coi là lớp ứng dụng (application layer) tương tự như các API data trước đây. Hệ thống này sẽ hoạt động ở cấp độ tool, nhưng người dùng cuối sẽ tương tác với nó qua giao diện của workflow. Hiện tại, vẫn chưa có đơn vị nào triển khai thành công mô hình này ở quy mô lớn. Tuy nhiên, GitHub hiện đã mở rộng API cho các developer tạo các extension và tích hợp chúng trực tiếp vào GitHub.\n\n**[39:40]** Dify đang xây dựng một nền tảng để hỗ trợ các developer phát triển và triển khai các tool và workflow này một cách dễ dàng hơn. Mục tiêu là tạo ra một marketplace để các tool và workflow có thể được phân phối và sử dụng bởi nhiều người dùng khác nhau. Hệ thống này tương tự như một nền tảng mở, cho phép các developer bên thứ ba triển khai các tool và workflow của riêng họ.\n\nTrên nền tảng của Dify đã có khoảng 50 tool khác nhau. Một số tool đã từng được phát hành dưới dạng thử nghiệm, nhưng do chưa có định hướng rõ ràng và thiếu sự hỗ trợ từ cộng đồng, nên chúng chưa đạt được thành công như mong đợi.\n\n**[40:17]** Một số nền tảng trước đây đã thử xây dựng mô hình tương tự nhưng chưa đạt được thành công. Lý do là vì các tool này chỉ được xây dựng dưới dạng form, thiếu khả năng tương tác với dữ liệu bên ngoài và chưa có khả năng kết hợp các workflow phức tạp. Tuy nhiên, Dify đang tập trung vào việc giải quyết các vấn đề này để tạo ra một hệ sinh thái hoàn chỉnh cho các workflow và tool.\n\n**[40:59]** Các công cụ này cũng cho phép người dùng đẩy dữ liệu từ các nguồn bên ngoài vào hệ thống. Người dùng có thể gửi dữ liệu từ các ứng dụng bên ngoài qua các Open Form hoặc API. Dify sẽ tự động xử lý và định dạng dữ liệu để sử dụng trong các workflow của hệ thống.\n\n**[41:56]** Team đang tập trung vào hai hướng phát triển chính:\n\n1. Tiếp tục mở rộng và phát triển các workflow hiện có.\n2. Cải tiến và tối ưu hóa các công cụ hiện tại để hỗ trợ việc triển khai và sử dụng dễ dàng hơn.\n\nHệ thống được xây dựng dựa trên các tiêu chuẩn chung về thiết kế tool và workflow. Công cụ Smithery hiện tại đang đóng vai trò như một Agent để quản lý các workflow. Smithery cũng có thể được sử dụng như một Package Manager để cài đặt và quản lý các tool trong hệ thống.\n\n**[42:53]** Workflow sẽ hoạt động theo cơ chế, nếu một workflow nào đó trở nên phổ biến, mọi người có thể lấy nó về và sử dụng dưới dạng tool. Bản chất của các công cụ này là được thiết kế để phục vụ các domain cụ thể. Ví dụ như một công cụ để tạo file, tìm kiếm hoặc lấy file code chẳng hạn. Nó hoạt động giống như một SDK, tức là một bộ thư viện mà bạn chỉ cần import vào để sử dụng.\n\n**[43:37]** Khi đã tích hợp vào SDK, bạn có thể sử dụng các method sẵn có để thao tác với dữ liệu. Điều này cho phép tích hợp dễ dàng vào các công cụ AI. Hiện tại, chỉ có Cross là hỗ trợ trực tiếp cho các thao tác này. Tuy nhiên, trong tương lai, nó sẽ được chuẩn hóa để các công cụ khác cũng có thể dễ dàng tích hợp. Trường hợp của Manus là một ví dụ. Manus sử dụng rất nhiều tool khác nhau, tuy nhiên khi so sánh với hệ thống agent trong Smithery, về cơ bản chúng là hai lớp hoàn toàn khác nhau.\n\n**[44:15]** Trong hệ thống của Manus, các công cụ được kết hợp lại để tạo ra các workflow tổng quát hơn. Các công cụ này hoạt động ở các lớp khác nhau, trong khi các agent trong Smithery được thiết kế để hoạt động độc lập. Câu hỏi đặt ra là làm thế nào để phân biệt rõ ràng sự khác nhau giữa hệ thống của Manus và hệ thống agent trong Smithery. Có một bài tóm tắt về điều này đã được đăng trong kênh AI Club ,  nội dung chính nói về khả năng suy nghĩ (thinking) và khả năng sử dụng máy tính (computer use).\n\n**[45:09]** Cơ chế của hệ thống Manus là một hệ thống service-oriented. Để kết hợp nhiều tool với nhau trong cùng một workflow, cần phải định nghĩa rõ các bước thực hiện. Ví dụ như bước 1 cần sử dụng tool nào, bước 2 cần sử dụng tool nào, v.v. Điều này đòi hỏi các bước phải được cấu hình cụ thể. Tuy nhiên, hệ thống mới có khả năng suy luận để tự động xác định xem cần sử dụng những công cụ nào để hoàn thành tác vụ. Đây chính là điểm khác biệt giữa hệ thống mới và các hệ thống cũ.\n\n**[45:59]** Cụ thể, hệ thống mới có thể nhận biết được một tác vụ cần sử dụng bao nhiêu công cụ, thực hiện qua các bước nào, và có thể điều chỉnh thứ tự thực hiện một cách thông minh. Đây là một cơ chế đặc biệt và khác biệt so với các hệ thống cũ. Nói cách khác, nó hoạt động như một Supervisor ,  có khả năng suy luận và đưa ra quyết định về thứ tự và phương pháp thực hiện các bước trong workflow.\n\n**[46:35]** Hệ thống Supervisor hoạt động ở lớp cao hơn so với các agent trong  Smithery. Các agent trong  Smithery chỉ đơn giản là các công cụ thực thi một tác vụ cụ thể, trong khi Supervisor có khả năng quản lý và điều phối toàn bộ quá trình thực hiện tác vụ. Việc tích hợp Supervisor cho phép hệ thống hoạt động một cách linh hoạt hơn, đồng thời dễ dàng mở rộng và bổ sung thêm các công cụ mới.\n\n**[47:33]** Mục tiêu của team là hiểu rõ cách hoạt động của hệ thống và nắm được cơ chế điều hành của các workflow. Nếu có thể xác định được cách thức triển khai và quản lý các workflow, thì sẽ có thể chọn lọc và sử dụng các công cụ hiệu quả hơn. Đây là điều mà team đang hướng tới ,  xây dựng một hệ thống có khả năng mở rộng và tối ưu hóa quy trình làm việc.\n\n**[48:24]** Tiếp theo, team sẽ tập trung vào việc xây dựng hệ thống **MCP**. Đây là một hệ thống mới được thiết kế để quản lý dữ liệu và workflow. Team đã tiến hành demo hệ thống này cách đây khoảng hai tuần. Bản chất của hệ thống MCP là xây dựng một agent hoạt động trên nền tảng có sẵn. Người dùng có thể nhanh chóng triển khai và kiểm tra hệ thống thông qua MCP.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=KGQZ4rVPmrc9nMq9\u0026amp;start=2935\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[49:10]** MCP sẽ là một hệ thống hoàn chỉnh, bao gồm một cơ sở dữ liệu (**database**) và một máy chủ (**server**). Điều này cho phép hệ thống hoạt động một cách độc lập và có khả năng xử lý dữ liệu lớn. Khác với các hệ thống cũ, MCP sẽ cho phép người dùng điều chỉnh cấu hình và quản lý dữ liệu dễ dàng hơn.\n\n**[49:58]** Bản chất của MCP là một agent, được định nghĩa theo một cấu trúc input và output cụ thể. Điều này cho phép các hệ thống khác nhau có thể kết nối và tương tác với MCP thông qua các giao thức tiêu chuẩn. Nói cách khác, MCP có thể được tích hợp vào bất kỳ hệ thống nào thông qua các giao thức được định nghĩa sẵn.\n\n**[50:35]** MCP cũng cho phép người dùng quản lý dữ liệu thông qua Knowledge Database, bản chất nó là timescale database, dump hết mọi data về hoạt động của team vào trong đó. Đây là một cơ sở dữ liệu dạng time-series, cho phép ghi nhận các sự kiện theo thời gian thực, ai làm backend sẽ quen dạng event sourcing, event log. Ví dụ: ghi nhận thông tin về các thành viên của team, trạng thái hoạt động của hệ thống, hoặc các sự kiện quan trọng khác.\n\n**[51:13]** Knowledge Database sẽ lưu trữ toàn bộ dữ liệu hoạt động của team, bao gồm các thông tin như ai đã thực hiện tác vụ gì, trạng thái của hệ thống vào từng thời điểm cụ thể, và các thông tin khác liên quan đến hoạt động nội bộ của team. Điều này cho phép team theo dõi và phân tích hiệu suất làm việc, từ đó đưa ra các quyết định điều chỉnh hợp lý.\n\n**[51:51]** Concept của hệ thống sẽ có một thành phần gọi là Landing Zone. Landing Zone có nghĩa là mọi dữ liệu mà mình đang có ,  khoảng mười mấy đến hàng chục bộ dữ liệu (database) ,  sẽ được tập kết vào đây. Trước đây, khoảng ba đến năm năm trước, nếu muốn xây dựng một hệ thống lưu trữ dữ liệu mình sẽ tạo một con bot để thu thập mọi hoạt động của team và đưa vào trong cơ sở dữ liệu của mình.\n\nVới mô hình Meta mới, tất cả các dữ liệu lớn (Big Data) sẽ được dump vào một kho lưu trữ tạm thời dưới dạng file .dat trên S3 hoặc GCS (Google Cloud Storage). Con MCP này sẽ có khả năng đọc trực tiếp từ Landing Zone. Nếu hệ thống thấy rằng dữ liệu trong Landing Zone có giá trị và cần thiết, nó có thể tự động chuyển đổi dữ liệu đó sang dạng Time Series Database (TSDB) để sử dụng lâu dài. Đây chính là end game (kết quả cuối cùng) của hệ thống này.\n\nCòn lại, vấn đề sẽ là xây dựng các Use Case (trường hợp sử dụng) dựa trên các dữ liệu đã được tổ chức trong hệ thống ,  theo hướng mà team mong muốn. Đây là định hướng phát triển quan trọng của hệ thống MCP trong thời gian tới.\n\n**[52:25]** Vậy là hiện tại team sẽ có một hệ thống cơ sở dữ liệu cũ ,  đó là cơ sở dữ liệu dạng table kiểu cũ, nằm ở phần bên dưới của hệ thống (có thể thấy trên diagram với các khối màu xanh dương). Giờ đây, team đang bổ sung thêm hai thành phần mới:\n\n- Thành phần **Landing Zone** ,  nằm trong khối màu vàng phía trên của hệ thống.\n- Thành phần **Time Series Database (TSDB)** ,  được kết nối trực tiếp với các thành phần trong hệ thống cũ để phân tích và khai thác dữ liệu.\n\nTeam đang lưu trữ các dữ liệu thô trong Landing Zone. Về bản chất, việc tập kết dữ liệu trong Landing Zone giống như việc gom quân ,  tập trung tất cả dữ liệu về một chỗ, sau đó mới quyết định cách phân tích và xử lý. Đây là cơ chế giúp hệ thống vận hành linh hoạt hơn và dễ dàng mở rộng khi có thêm dữ liệu mới.\n\n**[53:11]** Điểm đặc biệt của hệ thống này là khả năng tự động chuyển đổi dữ liệu từ Landing Zone sang Time Series Database. Cơ chế này xuất phát từ nhu cầu ngày càng tăng về phân tích dữ liệu cục bộ (local analytics). Đây là xu hướng đang nổi lên trong bối cảnh sự phát triển của AI (Trí tuệ nhân tạo).\n\nSự trỗi dậy của AI đã làm gia tăng nhu cầu về các hệ thống phân tích dữ liệu theo thời gian thực. Khi các dữ liệu thô được tập kết vào Landing Zone, hệ thống sẽ tự động nhận diện dữ liệu có giá trị và chuyển chúng sang TSDB để phân tích chi tiết hơn. Đây là một bước tiến quan trọng trong việc xây dựng hệ thống phân tích dữ liệu hiệu quả và có khả năng thích ứng với những thay đổi của thị trường.\n\n**[53:45]** Hiện tại team đã có thể chạy analytic trực tiếp cho phần dữ liệu được lưu trữ trên local. Hệ thống này cho phép chạy analytic ngay trên dữ liệu Data Lake mà không cần phải chuyển dữ liệu đi xa. Đối với phần dữ liệu trong Landing Zone ,  tức là phần file packet mà Huy đang show trên màn hình ,  đây là phần mà team cần tập trung nghiên cứu thêm. Vấn đề này có liên quan đến text processing, nên mấy anh em cần phải pick up (nắm bắt) chủ đề này. Cái này cũng không khó lắm, chắc học trong vòng nửa ngày là có thể nắm được cơ bản.\n\nPhần Prompt để tìm kiếm và khai thác dữ liệu cũng khá nhanh và đơn giản, không phức tạp. Đây là phần rất đáng để thử nghiệm vì nó liên quan đến cơ chế knowledge discovery (khám phá tri thức) trong hệ thống. Đây là một trong những phần nâng cấp mới mà Huy vừa nhắc tới.\n\n**[54:22]**  Điểm nổi bật nhất của hệ thống trong đợt nâng cấp này chính là **Knowledge Hub**. Đây là nơi mà team sẽ tập trung toàn bộ dữ liệu để phục vụ cho việc phân tích và khai thác tri thức. Knowledge Hub sẽ trở thành một dạng **data pool** chung của toàn team. Bất kỳ ai cũng có thể thêm dữ liệu vào đây, và hệ thống sẽ xử lý, chuyển đổi dữ liệu theo format tiêu chuẩn.\n\nĐiều quan trọng là khi hệ thống đã được thiết lập xong, mọi người trong team sẽ có chung một **protocol** để sử dụng. Các module hoặc component khác nhau sẽ có thể **share (chia sẻ)** chung một cấu trúc dữ liệu và truy cập trực tiếp vào Knowledge Hub. Đây sẽ là nền tảng chung để đồng bộ dữ liệu và xử lý dữ liệu trong nội bộ team.\n\n**[54:58]** Về phần cơ sở dữ liệu (DB), hệ thống sẽ có hai lớp:\n\n- **DB cũ:** Dùng để hỗ trợ các nghiệp vụ hiện có và xử lý các dữ liệu có cấu trúc sẵn.\n- **DB mới:** Được thiết kế để kết nối trực tiếp với **Knowledge Hub** và hỗ trợ phân tích dữ liệu theo thời gian thực.\n\nĐiểm đặc biệt là phần **MCP** sẽ đóng vai trò như một **protocol** để các module khác nhau có thể giao tiếp với nhau. Điều này có nghĩa là bất kỳ dữ liệu nào cần được truy cập hoặc xử lý, chỉ cần đưa vào đúng đường dẫn của hệ thống thì nó sẽ tự động được xử lý theo cấu trúc tiêu chuẩn. Đây là cách để hệ thống đồng nhất dữ liệu và tránh xung đột khi có nhiều nguồn dữ liệu cùng được xử lý.\n\n**[55:43]** Từ giờ, team sẽ cần làm quen với các cơ chế xử lý dữ liệu mới. Mọi người nên dành thời gian để tìm hiểu thêm về các thành phần trong hệ thống mới. Khi các thành phần này hoạt động ổn định, các dự án mới của team sẽ tận dụng các công cụ này để triển khai nhanh hơn và hiệu quả hơn. Đây sẽ là bộ công cụ chính để phục vụ cho các dự án trong tương lai.\n\nHệ thống này có tiềm năng trở thành **requirement** bắt buộc trong các dự án tiếp theo. Nếu bạn muốn bắt kịp với hệ thống mới, hãy bắt đầu từ việc tìm hiểu các nguyên lý cơ bản về MCB và các protocol liên quan.\n\n**[56:40]** Trước đây, khi team triển khai hệ thống trên S3 hoặc GCS (Google Cloud Storage), việc xử lý dữ liệu khá mất thời gian. Tuy nhiên, với cơ chế mới, dữ liệu từ Landing Zone sẽ được xử lý nhanh hơn và dễ dàng hơn.\n\nHệ thống đã được thử nghiệm trên nhiều nền tảng khác nhau, bao gồm **S3** và **GCS**. Tuy nhiên, vì hạ tầng hiện tại của team đang chạy trên **GCS**, nên các dữ liệu từ Landing Zone sẽ được xử lý trên GCS trước. Mặc dù vậy, về mặt kỹ thuật, hệ thống này có thể mở rộng sang các nền tảng khác mà không gặp trở ngại lớn.\n\n**[57:45]** Cơ chế hoạt động của Landing Zone khá đơn giản:\n\n- Các dữ liệu từ nhiều nguồn khác nhau sẽ được tập trung vào Landing Zone.\n- Các dữ liệu này sẽ được lưu dưới dạng **file Parquet** theo từng ngày.\n- Hệ thống có khả năng đọc lại các file này thông qua cơ chế **Time Series Database** (TSDB).\n\nHiện tại, một số file **Parquet** mẫu đã được tạo và đang trong quá trình kiểm tra. Nếu cần, team có thể chạy thử demo trên các dữ liệu mẫu này để kiểm tra tính nhất quán của hệ thống.\n\n**[58:24]** Những hoạt động của team giống như kiểu **AI sub** hoặc **Memo** thì nó cũng được đẩy hết lên đây. Nhiệm vụ của **Landing Zone** là lưu trữ mọi dữ liệu mà team muốn, ai muốn lưu trữ gì thì cứ đẩy hết vào đây rồi sau đó hệ thống sẽ quyết định xử lý dữ liệu đó như thế nào. Hệ thống cũng đã cung cấp một số công cụ để mọi người có thể đẩy dữ liệu lên, ví dụ như là các **API proxy** để forward các sự kiện. Mọi người muốn push thông tin lên Landing Zone thì chỉ cần gọi API là được.\n\nMemo hiện tại đang sử dụng cơ chế này để lấy dữ liệu từ các **nền tảng xã hội** và đồng bộ vào hệ thống. Cơ chế này cũng đã được thử nghiệm thành công. Còn đối với những loại dữ liệu có tính đặc thù như là **Discord messages** hoặc **data từ Basecamp**, team cần phải xây dựng các **crawler** hoặc các **connector** để thu thập dữ liệu. Hiện tại, team đã có một số template sẵn cho những loại dữ liệu này.\n\n**[58:59]** Về hướng phát triển tiếp theo, team sẽ tập trung vào việc khai thác dữ liệu từ Landing Zone. Nếu bạn muốn tham gia vào dự án này, lời khuyên là hãy bắt đầu từ một **vertical cụ thể**. Ví dụ:\n\n- Xác định một **use case** rõ ràng.\n- Tìm hiểu xem **dữ liệu nào** cần cho use case đó.\n- Định nghĩa lại cơ chế khai thác dữ liệu theo hướng **từ trên xuống dưới**.\n\nThay vì kiểu thấy dữ liệu nào hay thì lưu lại, team nên nghĩ theo hướng là **xác định use case trước** rồi mới quyết định lưu trữ dữ liệu. Điều này giúp hệ thống hoạt động một cách có tổ chức và dễ dàng quản lý hơn.\n\nVí dụ cụ thể là nếu có một use case về **Project Nghệ Nhân** thì team sẽ cần tạo một **Git Agent** để thu thập dữ liệu từ Git, sau đó đẩy dữ liệu đó vào **Knowledge Hub** thông qua MCP. Từ đó, hệ thống sẽ định nghĩa các công cụ khai thác dữ liệu cho use case này.\n\n**[1:00:16]** Ngoài ra, team đang phát triển một MCP Server nhỏ. MCP Server này thực chất là một server cơ bản, sử dụng các thành phần kỹ thuật thông thường của hệ thống internet hiện tại. Nó định nghĩa các input và output rõ ràng, cho phép kết nối với nhiều loại giao diện khác nhau.\n\nVí dụ:\n\n- Nếu có một MCP để xử lý dữ liệu từ Slack, team sẽ định nghĩa các API cho từng loại dữ liệu.\n- Nếu cần có các công cụ để đọc dữ liệu từ Google Sheets hoặc phân tích dữ liệu về tình trạng check-in trong tuần, team có thể tạo các MCP tool để xử lý những dữ liệu đó.\n\nMCP sẽ là một thành phần trung gian để đồng bộ và xử lý dữ liệu từ nhiều nguồn khác nhau. Mọi người có thể truy cập các công cụ này từ Editor, Command Line, hoặc bất kỳ giao diện nào khác.\n\n**[1:01:07]** Bản chất của MCP là nó sẽ đóng vai trò như một **API Gateway** để kết nối các công cụ. Nếu bạn cần theo dõi việc check-in hàng tuần của mọi người trong team, bạn có thể tạo một MCP để thu thập dữ liệu từ **Knowledge Hub** và Google Sheets, sau đó so sánh dữ liệu để xem ai đã check-in và ai chưa check-in.\n\nHệ thống hiện tại đang dừng ở mức độ triển khai MCP Server cơ bản. Giao diện hiện tại sử dụng **Command Line** để gọi MCP, nhưng về cơ bản team có thể mở rộng để kết nối với các công cụ khác nhau.\n\n**[1:01:43]** Hệ thống đang tập trung vào việc triển khai cơ chế xác thực (authentication) và phân quyền (authorization).\n\n- Authentication – Xác thực người dùng để truy cập vào hệ thống.\n- Authorization – Phân quyền cho các hoạt động xử lý dữ liệu.\n\nHệ thống đang được sử dụng nội bộ trong team, chưa công khai ra bên ngoài. Nếu bạn muốn sử dụng MCP, bạn sẽ cần nhập vào **private key** để xác thực quyền truy cập.\n\n**[1:02:23]** Về mặt kỹ thuật, MCP có thể mở rộng ra các thành phần khác nhau trong hệ thống. Mọi người có thể tích hợp MCP vào các ứng dụng hiện tại hoặc các công cụ hiện có mà không cần phải viết lại quá nhiều code.\n\nTeam vẫn đang thử nghiệm tính năng này và tập trung vào việc hoàn thiện các phần về bảo mật và quản lý quyền truy cập. Khi hệ thống đã ổn định, mọi người có thể tích hợp MCBP vào các quy trình xử lý dữ liệu hiện có.\n\n**[1:03:00]** Chỉ là đang dừng lại ở đây thôi, chưa xử lý được các bài toán phức tạp về authorization. Sau khi hoàn thành các bước hiện tại thì mới đến việc xử lý các bài toán phức tạp hơn liên quan đến authorization và quyền sử dụng hệ thống. Mọi người có thể tập trung vào các vấn đề cơ bản trước đã.\n\nRồi, cảm ơn Huy nhé. Đây là một trong những phần phát triển kỹ thuật quan trọng của team. Nếu theo dõi các hoạt động trên tech và AI Club, mọi người sẽ nhận ra team đang tiến tới các bước tiếp theo trong quá trình phát triển. Về mặt kỹ thuật, mọi người nên chú ý vào các từ khóa quan trọng mà Huy vừa đề cập. Nếu chưa hiểu rõ thì có thể xem lại bản ghi để nắm được đầy đủ thông tin.\n\n**[1:03:45]** Team core vẫn đang tiếp tục phát triển hệ thống. Yêu cầu tất cả các thành viên tham gia vào dự án để có thể **transfer knowledge** hiệu quả hơn. Dự án này là môi trường để mọi người học hỏi và thực hành.\n\nĐây là cơ hội để các thành viên mới trong team tiếp cận và nắm bắt các khía cạnh kỹ thuật quan trọng. Nếu cảm thấy chưa sẵn sàng thì có thể tham khảo các phần hướng dẫn và tài liệu nội bộ để bắt kịp. Việc training sẽ được thực hiện trong quá trình làm việc chứ không có các buổi training riêng. Đây là môi trường thực hành trực tiếp để vừa làm vừa học.\n\n**[1:04:29]** Bên cạnh việc phát triển hệ thống, team cũng đang thực hiện knowledge transfer từ các dự án đã hoàn thành. Dự kiến cuối tháng sẽ có một buổi tổng hợp lại các bài học rút ra từ các dự án này. Nếu ai chưa thực sự hiểu rõ thì có thể tham khảo hoặc hỏi các thành viên đã làm qua để nắm thêm thông tin.\n\nNếu cảm thấy chưa sẵn sàng hoặc cần thêm thông tin thì có thể hỏi trực tiếp các thành viên trong team. Mọi người có thể ping các thành viên có kinh nghiệm hơn để nhận được sự hỗ trợ.\n\n**[1:05:07]** Team có hai nhóm khác nhau đang hoạt động song song:\n\n- **Team của Tuấn** đang phát triển một số game và ứng dụng nhỏ.\n- **Team build** đang làm việc trên các ứng dụng thử nghiệm để kiểm tra tính khả thi của hệ thống.\n\nCác hoạt động này tương tự với các nhóm **Build Club** và **AI Club** trong team Foundation. Một số sản phẩm đã bắt đầu có **output** tốt. Tuấn và team đang phát triển một trò chơi dựa trên **Turing Machine**.\n\n**[1:06:38]** Trò chơi **Turing Machine** mà team Tuấn phát triển được chuyển thể từ phiên bản board game thành phiên bản trên thiết bị di động. Mục tiêu của trò chơi là đoán một chuỗi gồm **ba số**. Để đoán đúng chuỗi số này, người chơi sẽ nhận được các **clue** (gợi ý).\n\nVí dụ:\n\n- Nếu gợi ý nói rằng “một trong ba số phải lớn hơn 1” → Người chơi có thể nhập số vào và hệ thống sẽ xác định xem đáp án có đúng hay không.\n- Nếu hai số sai nhưng một số đúng thì hệ thống sẽ phản hồi ngay để người chơi có thể tiếp tục điều chỉnh.\n\nLuật chơi khá phức tạp nên có thể gây khó khăn cho người chơi mới. Tuấn và team đang tiếp tục điều chỉnh để trò chơi trở nên dễ tiếp cận hơn mà không mất đi tính thử thách.\n\n**[1:07:23]** Tên trò chơi là [**Pocket Turing**](https://pocket-turing.vercel.app/) bởi vì phiên bản board game gốc của nó liên quan đến các thẻ đục lỗ – giống như cơ chế hoạt động của Turing Machine trong lập trình máy tính. Tuy nhiên, mình đã điều chỉnh và phát triển thêm các yếu tố mới để phù hợp hơn với phiên bản di động.\n\nMÌnh có kế hoạch tinh chỉnh và mở rộng trò chơi trong các phiên bản tiếp theo. Ngoài ra, cũng đang kiểm tra xem có thể triển khai thêm các tính năng thu phí hoặc các tùy chọn nâng cao để tăng khả năng monetize.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=bP3ZjI3af1fVijle\u0026amp;start=3997\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:08:16]** Mình đang thử nghiệm phiên bản beta của trò chơi. Trò chơi đã hoàn thiện về mặt gameplay và người chơi có thể trải nghiệm trọn vẹn các tính năng. Bước tiếp theo là thử nghiệm với nhóm người dùng rộng hơn để thu thập phản hồi và cải thiện sản phẩm.\n\n**[1:09:15]** Mục tiêu tiếp theo là đưa trò chơi vào App Store và Google Play để tiếp cận nhiều người dùng hơn. Trước mắt, team muốn đảm bảo trò chơi hoạt động ổn định và không phát sinh lỗi nghiêm trọng.\n\nTuấn kỳ vọng trò chơi sẽ thu hút được ít nhất **100 người dùng** trả phí trong giai đoạn thử nghiệm đầu tiên. Nếu nhận được phản hồi tích cực  sẽ mở rộng thêm các tính năng mới và cải thiện trải nghiệm người chơi. Mong nhận được phản hồi từ các thành viên khác để có thể điều chỉnh và hoàn thiện sản phẩm tốt hơn. Tuấn đã chia sẻ link tải trò chơi cho các thành viên trong team để mọi người có thể trải nghiệm và đóng góp ý kiến.\n\n**[1:10:13]** Nếu anh em hứng thú với việc build sản phẩm thì giai đoạn này là thời điểm phù hợp để bắt đầu. Trước đây team đã thử nghiệm nhiều lần nhưng lần này là cơ hội tốt để làm bài bản hơn. Việc phát triển các sản phẩm nội bộ không chỉ giúp cải thiện năng lực kỹ thuật mà còn mở ra cơ hội thương mại hóa trong tương lai.\n\nNgoài game của Tuấn, team đang phát triển thêm các công cụ khác. Nếu có ý tưởng hay, anh em có thể đóng góp để cùng xây dựng và thử nghiệm. Cách bán hoặc thương mại hóa sản phẩm thì tính sau, quan trọng là hoàn thiện các tính năng cốt lõi trước.\n\n**[1:10:58]** Tiếp theo là phần của An. An từng làm một tool gọi là **Rec** để tổng hợp thông tin theo dạng giống với hệ thống của **Apple**. Phiên bản 1 của Rec yêu cầu người dùng tự sắp xếp thông tin, còn phiên bản 2 hiện tại đã được tích hợp AI để hỗ trợ sắp xếp tự động.\n\nTuy nhiên, AI vẫn có một số hạn chế trong việc nhận diện nội dung đầy đủ. Đôi khi AI không thể xác định được toàn bộ ngữ cảnh nên kết quả trả về chưa thực sự hoàn hảo. Tuy nhiên, các nội dung quan trọng vẫn được sắp xếp và hiển thị đầy đủ.\n\n**[1:11:56]** Tool này đang trong giai đoạn hoàn thiện, nhưng các chức năng cốt lõi đã ổn định. Hiện tại, team đang tập trung vào việc cải thiện phần giao diện và tối ưu trải nghiệm người dùng. An dự kiến sẽ tiếp tục phát triển thêm các tính năng bổ sung để hỗ trợ người dùng tốt hơn.\n\n**[1:12:51]** Các dự án của team hiện đang ở giai đoạn thử nghiệm và cải tiến. Nếu ai có thắc mắc hoặc góp ý, có thể trực tiếp trao đổi với An hoặc các thành viên khác trong team. Hiện tại, các dự án đã showcase gần hết. Các phần chi tiết hơn sẽ được đề cập vào buổi sau.\n\n**[1:13:57]** Bên đội mình, anh luôn nói về chuyện kiến thức liên quan tới liquidity và game in general, thì anh em thật sự muốn team mình đẩy theo hướng đó một chút. Vì nó có lợi cho gần như là cái life skill luôn, đúng không? Nên anh muốn team mình đi theo hướng đấy trong đợt này. Mấy anh em, đặc biệt là những người hứng thú với trading, tức là lấy data về để tìm kiếm cái Alpha trên đó, Intel trên đó, để ra được những cái market-making dựa trên điều kiện nào đó.\n\n**[1:14:43]** Nó là một cái, hoặc có thể đi xa hơn để làm một luồng rất tuyệt vời. Hình như hiện tại chỉ là ước mơ của anh thôi. An đã làm được một version, anh thấy khá ok. Đây là cơ hội để cho anh em biết trong team đang có những tiến triển như vậy. Đang chạy ha, mời An. Nói chung là game kiếm tiền thôi. Coi tụi nó kiếm tiền sao thì mình làm vậy. Mấy cái thường thường thì có biết một cái gì để thử, nó cũng là dạng **Delta neutral**, đúng không? Thì mình cũng research những thứ đó. Rồi đi build và research xong để có kiến thức ship.\n\n**[1:15:30]** Chơi cái cột này hết thôi, không nhìn tới đâu nữa. Mọi người thấy màn hình terminal chưa? Có thấy chưa? Có thấy rồi, ok, chạy để chạy thử. Chắc phải zoom lên, zoom lên một hai level, hơi nhỏ, rồi ok rồi. Đây là arbitrage để ăn funding free, thì có nhiều thể loại arbitrage. Cái này chỉ là một trong những loại đó thôi, ăn trên chênh lệch phantom giữa các sàn. Đang tập trung vào ba sàn: Binance, OKX ,  thằng OKX này sàn của nó không có nhiều dữ liệu lắm ,  nên em có cái diagram cho cái đó không, An?\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pj13pwqkVdQ?si=IevTgfLbxwcu6MOh\u0026amp;start=4506\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**[1:16:26]** Nghĩ mọi người sẽ hơi khó hình dung. Nhìn cái này chắc không hiểu nó là gì. Có diagram không? Ok, không có vẽ à? Có cái này, to, nhưng là lý thuyết, không cụ thể ra được high level. Không thấy, chắc phải ngồi vẽ lại sơ sơ. Thấy chưa? Chắc nhìn hình của anh đi, hình của anh, biết ngay là cái này luôn. PRL à? Ủa, nó đang chạy lộn, quên, nó đang vào mấy cái socket của…\n\n**[1:17:47]** Tụi nó để lấy real-time data về. Đang lấy dữ liệu từ bao nhiêu account? Ba cái: Binance, Bybit, với cái gì nữa? Ok rồi, init để lấy giá về, đúng không? Lấy giá, lấy funding, lấy phí chưa? Lấy mấy cái data như phí thì đang code theo calculation, chưa xài để lấy về. OKX chắc không có. Mấy cái trade này thì thường chỉ nằm ở hai cái chính: Binance, Bybit. Ừ, setup ok rồi, nó sẽ có mảng thể hiện cái vị nào đang có chênh lệch funding, thì có profit. Em tính được nếu mình vào thì nó sẽ bao nhiêu. PH là số lần thu funding để hòa vốn phí, monitoring cái cao nhất giữa hai sàn. Bước một là lấy chênh lệch funding giữa hai bên, đúng không? Không, em nói là lấy trên ba exchange, so với góc của nó vẫn là exchange net, đúng không? Ừ, exchange net tính sau thôi.\n\n**[1:19:49]** Funding thì giả sử tụi nó thường, trên lập giá thì không có nhiều, kiểu một thằng dương, hai thằng đều dương, hoặc hai thằng đều âm, thì chênh lệch ít hơn. Thật ra mình đặt counter trên cái chênh khác, chỉ là offset giá di chuyển, để không lỗ bởi giá. Trên exchange net, không có chênh lệch đó, mình làm funding lúc nào cũng bằng 0. Vì không có phí swap, thay vì counter trên sàn khác bằng cái đó, mình counter lúc funding bằng 0. Hiện tại chấp nhận ít lợi hơn, nhưng version đầu tiên vậy.\n\n**[1:20:33]** Lấy giá, lấy **funding**, rồi coi có deploy capital thôi, đúng không? Chạy thử chưa? Chưa đổ tiền vô. Ừ, cái này kiểu game scale, cần nhiều tiền mới ăn, vài trăm ngàn thì vô không thấy gì. Hiểu, ok. Về kỹ thuật thì em làm gì? Từ lúc price crash, em làm những gì?\n\n**[1:21:25]** Đầu tiên em research chart trước, coi nó thế nào, có chênh lệch gì không. Xong rồi ship, code hết bằng Cloud 3.7. Phải lấy data sàn trước qua socket, từ API dock của sàn, quăng lên WebSocket client. Sàn nào cũng có dock, lấy về ship lên, tự view được.\n\n**[1:22:15]** Web cho ba sàn xong, có form đầy đủ. Sau đó build chart, giải thích cho nó, build từ từ. Check data, sai thì tự build sample để đảm bảo data đúng. Vì format giữa các sàn khác nhau.\n\n**[1:23:01]** Khác hết, nên cần test data valid mới compare được. Tiếp theo build con để vào lệnh, khi phát hiện thì có thằng đứng ra vào lệnh, watch bot xem lỗ không, làm từ từ, hợp lý. Quá trình hết bao lâu? Một tuần.\n\n**[1:23:58]** Bước tiếp theo của tool này là gì? Em sẽ check data trước, xem sàn nào dễ kiếm tiền, có lời. Quản lý rủi ro, lấy phí structure của sàn. Vì phí ảnh hưởng lớn, phải tính chính xác, đảm bảo lời mới vào lệnh. Có back system không? Có history để backtest không? Có, nhưng không chính xác.\n\n**[1:25:38]** Đây là showcase kỹ thuật, hướng này team tự lập, ok. Anh em showcase game trading, có bước đẩy tiếp, đang trên đường làm cái muốn làm, rất good. Công nghệ, techno house, xài thế nào thôi. Quan trọng nhất là…\n\n**[1:26:19]** Hoạt động team hiện tại vậy nhé. Productivity gần đây bắt đầu sync. Tom comment trước, productivity team giờ bao nhiêu? 2/10 hay 4/10? So với 6-7/10 cần, anh thấy setup tốt rồi.\n\n**[1:27:01]** Bước tiếp theo về mặt catch up cái công nghệ tool link để hỗ trợ mình vận hành đội theo mô hình này, nó đang được improve từ từ lên. Bên thị trường, thị trường **funding** nói chung và những sản phẩm bắt đầu cũng rục rịch quay trở lại. Người ta thấy công nghệ ổn định hơn. Bên **crypto** thì do macro ảnh hưởng nhiều, nhưng cứ có trend nào về tech là sẽ vô cắn thôi, là vậy ha. Anh đang thấy sắp tới tín hiệu để nó resume lại thì đâu đó khoảng 50/50. Trước đó anh nhìn thì cái market rất tệ, kiểu mọi thứ chưa sẵn sàng. Dù có học nhiều, làm nhiều thì cũng không ra kết quả liền.\n\n**[1:27:40]** Nhưng đợt này anh nghĩ mấy anh em sẽ phải có sự yêu cầu về chuyện tham gia mấy cái này ha. Tuần sau chắc nhờ Huy, Tom với Thành thống kê lại, xem ngoại trừ dự án anh em đang làm thì hoạt động tham gia những dự án side project như vậy, anh em nào đang làm gì ha. Đó là thần sau nội dung, thì cũng trao đổi gần hết rồi. Tuần sau còn một số cái core flow tiếp, nhưng chắc cũng không ảnh hưởng quá nhiều tới mọi thứ.\n\n**[1:28:22]** Hôm nay là ngày 14, hy vọng đến cuối tháng này, buổi họp team tiếp theo sẽ show được nhiều progress hơn. Tất cả những thứ mình đang làm rất quan trọng ha. Toàn bộ này đều đang được đưa lên Memo, tụi anh đang sử dụng Memo đó không chỉ để share trên đó không ăn thua.\n\n**[1:29:01]** Shill khắp nơi mấy công ty khác mình biết, bắt đầu mở rộng network ra để xem tìm kiếm user cần thiết. Chuyện là mình biết những thứ này rồi thì làm sao mình mound được khả năng mình profit từ kiến thức của mình, ý là vậy. Ok ha, đó là cái skin mà team đang chạy theo. Tóm lại, thị trường nhận định đang như vậy. Tuần sau mấy anh em sẽ phải đăng ký làm cái registration vô cho những cái phần, với lại Huy, Tom và Thành là những bên mạng bắt buộc.\n\n**[1:29:45]** Còn bên mấy cái hobby club, như kiểu Build hay gì đó, thì anh không yêu cầu cao vào bên đó, ra cái kỹ thuật để apply, nó cũng không quan trọng lắm. Quan trọng là output nhiều hơn. Hai nhóm khác nhau: một nhóm là những phần mà core project mình sẽ làm, tập trung vào làm sao tăng activity, tăng cái **knowledge base** của mọi người; còn cụm kia tập trung vào cái **skill set**, chuyện develop product sao launch, làm sao làm onboarding tốt hơn, user các kiểu. Là một cái nhóm skill set khác.\n\n**[1:30:27]** Đặc biệt là Huy, Huy đang co cho cái việc quay trở lại office để bắt đầu làm shadowing cho chuyện **knowledge transfer**, thì nếu được thì cứ tiếp tục để nó diễn ra. Rồi xem số liệu như thế nào thì report lại cho anh ha. Hopefully khi nào có con số đây thì mấy anh em xem thảo luận tiếp, làm sao setup cái vụ shadowing đó trên mấy cái dự án mà mình, mấy cái site mà mình tham gia, để có cái case share với nhau ha.\n\n**1:31:05** Toàn bộ là vậy. Nếu bây giờ không có gì khác thì chắc mình kết thúc ở đây. Đây có bao nhiêu bạn nhờ? 28 bạn hả? Không, đang bao nhiêu bạn trong con này nhờ? Chuẩn bị spam ICY, có vấn đề để transfer ICY chưa? Để cái này mai mốt lấy acc anh, hoài căng quá. Có vấn đề trên ICY nhờ. Mọi người ra random nha, giật cô hồn nha. Amount 28 thì mình sẽ drop 14 token ICY, entry là 14 rồi. Xin mời, duration là 5 giây. Ok, let’s go. Một ICY hồi nãy là tương đương khoảng 100 Satoshi rồi đó.\n\n**[1:32:09]** Tuần sau lịch vậy nha, mọi người xem phối hợp với nhau để làm việc cho hiệu quả rồi. Bye bye.\n\n---\n\n### English Transcript\n\n**[05:30]** Hello, can you hear me? Oh, okay, it’s fine now. Today, I think we’ll start a bit early. Today’s session will probably combine with the brother in the meeting for a little bit. One part will be to do a showcase, the second part is that the brother will summarize some things that were discussed with the guys previously. The second and third parts are that we’ll start letting the guys register for tasks. For now, to make it easier, I’ll probably let Huy Nguyễn go first to show the parts related to Huy, which involve ICY a little, and then show some tech stuff that our team is currently working on. This will give me a snapshot of how the tech team is doing right now. Then, moving forward, what our team needs and what the guys can contribute to it. Alright, let’s get started.\n\n**[06:35]** Huy, where’s Thành? Let’s give them the stage now. Okay, for the first content, let’s start with ICY Swap. We announced it, last week or this week it was deployed, so now how are the differences, I’ll probably ask Huy to go over that whole series again.\n\n**[07:29]** Hello, alright, I’ve seen the screen already. So now everyone can go to the ICY Swap page to swap. Here, I’ll show the data. But up here, everything is fully ready now. The only thing left to do is that we’re currently reviewing the ICY numbers. Because previously, when we were operating, we operated by pegging the ICY price, so we didn’t really care much about the circulating supply. So there were some cases where we put it into the team’s wallets or transferred it to Mochi Balances for me or for brother Bảo. Those things need to be reviewed again to get the correct circulating supply number. Because now we’ll sit down, and the price will be dynamic based on the pool, so we need to check that again, and it’s almost done.\n\n**[09:09]** Now, the only thing left is brother Bảo’s account that needs to be checked again. I remember there was a time we transferred to brother Bảo, so now we’re reviewing that part, doing the addition and subtraction, and cutting that part out of the circulating supply, then this number will come out correct. For now, if anyone wants to swap to support, they can swap on this page. That’s the current schedule. I’ll show the list of our current Holders so the guys can see, probably need to know a bit more. Up until now, people participated without paying much attention, but this time we need to be more mindful.\n\n**[09:51]** Our ICY is deployed on Base, right? So when the guys go into the Holder list, everyone will see a list of all the wallets currently holding our team’s ICY, which are the CCK Holders. That’s one thing. And then the link to access this, Huy will share it, I guess. Because if people go search for it, they probably won’t find it.\n\nFirst, the guys need to understand this. Moving on to this part now. I think the guys need to pay more attention to this part. It’s become the norm in the tech world already, no need to do anything new anymore. So if the guys grasp this, it’ll be better.\n\n**[10:33]** Our ICY is now listed. In this list, there are minter wallets, wallets used to budget for activities, and some wallets holding large amounts of ICY. Activities related to staking ICY will be rolled out gradually in the coming time. This is the first piece of information the guys need to understand clearly.\n\n**[11:15]** Huy, demo the swap process for us. Does anyone have a Bitcoin address with some ICY? Is Vincent here? Okay, now let’s try swapping from ICY to Bitcoin. The current price is calculated dynamically based on the circulating ICY amount and the pool. The swap function is very simple, just enter the amount, press swap, and it’s done.\n\n**[12:27]** Wait, don’t enter a fake address. Okay, it’s good now. The first frame is ICY as usual. Below it, it’s displaying the unit in satoshi, which is the smallest unit of Bitcoin. When you enter the amount, it will automatically convert. However, the current exchange rate is slightly off, around 1.2 instead of 1.5. This is probably a small calculation error, we can fix it.\n\n**[13:28]** You need a minimum amount of ICY to swap. Try entering 30 ICY and see how it goes. Refresh it and check if it works.\n\n**[14:43]** It seems like there’s not enough money in the wallet. Do you have ETH on Base? Transfer it to Base and check again.\n\n**[15:51]** It’s not that error. The issue is that the account hasn’t been registered, so it can’t perform the transaction. We’ll fix that part later. The goal here is to help everyone understand the swap mechanism and how token pricing works better. If you understand it well, it’ll be easier to manage tokenomics later on.\n\n**[16:47]** Huy, quickly explain the pricing mechanism again. Last time Quan demoed it but didn’t go into detail about that part. The price of ICY is determined by the minting mechanism, meaning the price won’t fluctuate heavily if someone swaps a large amount. It doesn’t operate like an automated market maker (AMM) mechanism; the price will be controlled through the minting mechanism. This mechanism helps keep the price stable even with large transactions.\n\n**[17:43]** It completely depends on Bitcoin. So if Bitcoin’s price goes up, the amount of ICY you guys are holding will increase in USD value. As for the minting mechanism, Huy, explain a bit more. Generally, our overall mechanism so far is that we fix ICY’s value to USDC. You guys don’t need to worry too much, just understand simply that one ICY is equivalent to 1.5 USD.\n\n**[18:37]** This assurance part is to help the operating team ensure that by the deadline, USDC will be added into the contract for everyone to swap. The swap rate in the old contract was fixed at 1.5 ICY, but that was the old model. Our new model is more flexible. If you guys have used Uniswap or other AMMs (Automated Market Makers), it’s somewhat similar. Here, the mechanism works with a liquidity pool underneath, which contains both ETH and USDC. Depending on the pool’s situation at that time, the exchange rate will be adjusted based on the amount of ETH and USDC in the pool.\n\n**[19:18]** Our mechanism works similarly. The price of ICY will be determined by the amount of Bitcoin in the pool and the amount of ICY currently in circulation. The formula is simple: we have the amount of ICY (X), we have the amount of BTC (Y) in the pool, then X/Y will give us the value of one ICY in terms of BTC. This formula is basic mathematics, nothing complicated.\n\n**[19:55]** Due to our operating mechanism, there will be two moments that change liquidity:\n\n1. **The first moment** is every month when the operating team adds more BTC into the pool to cover the costs of the team’s activities. At this point, the price of ICY will increase slightly because the amount of BTC in the pool increases.\n2. **The second moment** is when the team adds more ICY into the pool (minting more). When more ICY is minted, the market price of ICY will decrease because the amount of ICY in the pool increases.\n\n**[20:35]** The two cases above will directly affect the price of ICY. However, if the price of Bitcoin changes, the USD value of ICY might change, but the price of ICY in terms of BTC will not change. The market impact from Bitcoin is an external factor and does not directly affect the minting or the value of ICY in the pool.\n\n**[21:12]** If you guys have any more questions, feel free to ask, and we’ll answer them later. Oh, there’s a question about swapping back from BTC to ICY, right? Currently, that function isn’t available. Right now, we only support swapping from ICY to BTC, not the reverse swap. Meaning you can buy in, but selling out isn’t supported yet.\n\n**[21:40]** Thank you, Huy. Anything else to note? One thing to note is that we’re still in the testing phase, so there might be some exceptional cases. For example, some situations might arise during swaps or when liquidity isn’t sufficient. Fundamentally, though, the current flow is still operating stably.\n\n**[22:00]** Like the minimum ICY amount required to swap. Because essentially, our team is covering the gas fees for transactions on ETH, on Base, and even on BTC, we’re kind of limiting it so that the ICY amount swapped needs to be a bit higher. This is to avoid situations where people swap just 1-2 ICY to test, which would cost gas fees, so we’ve set it at around above 20 ICY to allow swapping on the web.\n\nThe second thing is that since minting more ICY will change the market price, I’ve disabled the part about our previous salary advance mechanism.\n\n**[22:37]** Meaning if everyone advances salaries at the same time, it would affect the price, right? So the lesson learned from this is that after this round, there are a few points I’m noticing. Our team is starting to focus on building tools to support our operations. These are also some new experiments and some things that genuinely support our activities. But after finishing these tasks, we’ll produce some articles related to them. So if any of you didn’t participate in those projects earlier, you can look back at those articles to understand the game, the knowledge gained from that round, and what the guys working on those projects achieved.\n\n**[23:24]** So with this ICY Swap round, we’ll probably get two or three articles, right? Yes, like three articles. And if we want to write more, there’s still plenty to write about. Yeah, alright, take it slow and steady.\n\n**[24:02]** After Huy’s part, I thank Huy and move on to the second topic related to what our team is currently doing. Brother Bảo, whoever wants to go first is fine, but I’ll probably let Thành speak first. Thành said it’s okay for him to go first, he’ll gather everything to let everyone know what stage the team is at. But I said let Thành go first because someone’s ringing the bell. Alright, I invite Thành to start.\n\n**[25:00]** Everyone, our Memo is one of the big things this round, and we’ve upgraded its format to make it look a bit better. We always want to create content maps, things that we can read and upload here. But currently, that model isn’t really that effective anymore because new models compress data, and querying directly from there would be more efficient.\n\nSo the point is that putting ordinary knowledge onto Memo isn’t very suitable anymore. For this round, when reworking it, there’s one main idea I want to tell you all: Memo will now be used for one sole purpose ,  the knowledge gained from projects.\n\nThat’s almost like the new things that come directly from our team’s activities. In the future, it’ll mostly consist of what field it’s related to and what we’ve done in that field. There’s more to it ,  maybe after a period when they retrain the model, our data will become part of the shared knowledge for the whole community.\n\n**[25:39]** And I think this part will be very helpful for things like retraining AI models later or for cases where we want it to provide automatic suggestions.\n\n**[26:24]** The content will become part of that model, or if there are internet search tools, our articles might just be a small part of the referenced materials, like a small piece in a citation. That’s not a big issue. But overall, all this content will pretty much become the spirit of the team.\n\nIn this major upgrade, there’s one key point that Tuấn has completed, right? Tuấn, the part about syncing all the team’s data, especially the content, is currently being directed this way so the members can understand it better.\n\n**[27:00]** Meaning after this round, the members participating in projects will tend to sit down together to review those projects more closely and determine exactly what the **knowledge gain** from those projects is. After that, the team will upload it to Memo as internal reference material for the team.\n\nThe second part is that at the end of each article, there will be a section related to a **group of reading**. This part isn’t fully complete yet, but the idea is that once it’s finished, there will be an additional section summarizing information about the article so readers can look up and learn more from it.\n\n**[27:47]** In addition, all the data written by the team will be tagged with identifiers such as **GitHub**, **Discord**, or other internal channels. This data will be uploaded to a **blockchain storage** form on the **Arweave (AV)** platform ,  a decentralized storage platform. This ensures that the team’s content has a clear and transparent identifier.\n\nOn top of that, readers will be able to review the articles, rate them, or leave feedback directly on the articles. This is part of the new upgrade idea for the team’s **Memo** page.\n\n**[28:39]** Previously, the team intended to use Obsidian to manage content, but it seems some members had difficulty getting familiar with that tool. Therefore, to make things simpler now, the team will switch to a more direct mechanism. Specifically, instead of having to go through Obsidian, members can submit content directly to the repository of the team’s shared library.\n\nMembers just need to input the content and submit it directly through this platform, without having to follow Obsidian’s mandatory workflow anymore. If someone still wants to use Obsidian, that’s fine, but if they don’t, it won’t affect anything. This is the most fundamental change in the team’s Memo system.\n\n**[29:24]** Currently, the team is working on several main projects, including:\n\n1. Bitcoin Swap ,  already mentioned in the previous section.\n2. Memo ,  just presented.\n3. Two smaller projects:\n    - **Agentic** ,  being developed by Quang and Huy’s group.\n    - **GitHub Bot** ,  being worked on by Thành’s group, currently in testing.\n\nNow, I’ll probably hand it over to Thành to share more about these contents.\n\n**[30:32]** This project was started over a week ago and has officially been running code for more than a week. Its main purpose is to create a reminder system. Previously, the team often encountered situations where, after creating a pull request (PR), people would leave it there, wait for it to finish running, and then forget about the need to review it. This tool will serve to track and update information about daily activities on GitHub or weekly activities on the team’s internal communication channels.\n\n**[31:18]** This system is designed as a simple integration. The basic workflow includes several use cases, such as notifying the person assigned to review, interacting with the GitHub API, and posting information to internal channels like Discord or Slack. Currently, the team is testing it on Discord. Additionally, the team is experimenting with Agentic and a new framework called **Mastra AI**.\n\nThis framework is different from typical Python tools. Some team members aren’t familiar with working in Python, so the team wants to test whether using this new framework is more effective than current solutions. The framework supports features like setting up the environment, defining states to manage data, and allowing reconfiguration based on the team’s needs.\n\n**[32:19]** The system’s structure has two main parts:\n\n1. **Agentic App** ,  This is the main application for handling the system’s activities.\n2. **Discord App** ,  This supports sending notifications to Discord.\n\nAdditionally, the system has a few auxiliary components, such as workflows to handle scheduled tasks, check, and notify developers if there are any pull requests waiting for review. If a pull request exceeds a certain amount of time, the system will send a notification to remind the person responsible for reviewing it.\n\n**[33:12]** The Agentic App will expose a few APIs that allow chatting and tracking the status of pull requests. When a pull request is created, the system will automatically identify conditions like the pull request’s status (work in progress or not), the time it was created, and will notify the reviewer after about 30 minutes from the creation time. For example, if a pull request needs review but no one is assigned or it has exceeded the processing time, the system will automatically ping the responsible person again.\n\n**[35:02]** Instead of having to track manually, the system will attach an agent to automatically monitor and notify through the system’s endpoint. In the logic part, the system will define specific conditions, such as only sending notifications if the pull request was created within 30 minutes or is in a work-in-progress state. If the pull request is updated or changes status, the system will automatically track and notify the developer to ensure nothing is missed.\n\n**[35:39]** The system will operate based on standard code filters. Additionally, it will have some other workflows, like sending notifications at the end of the day to summarize the status of pull requests on Discord. The system will automatically send notifications about the number of open pull requests, their statuses, and the current review status. This is the main function of this tool ,  acting as a reminder tool.\n\n**[36:24]** The system can also integrate with other chat tools. It’s simple ,  you can create an additional command and send a request to the system’s endpoint. These requests will be defined based on a specific schema, such as the input being a **review ID** or other information related to the pull request’s status. The system will take this data and display it on the interface that users frequently use.\n\n**[37:04]** The backend processing of the system is handled through the Lippia tool, which formats JSON data into Markdown tables or data-binding formats. Currently, the team is testing these two processing flows before expanding to additional features. Once the system is stable, these workflows will be opened up for all team members to test and further develop.\n\n**[38:08]** The system is designed to scale flexibly. Team members can independently develop and contribute different workflows. This system allows the creation of tools as standalone **packaging units**, which can then be combined to create more complex workflows. When wanting to release a new workflow, members just need to redefine the basic unit and integrate it into the system.\n\nExpanding workflows will help the system grow horizontally (increasing the number of features) rather than vertically (developing existing features). As the number of workflows increases, the system will become more flexible and powerful.\n\n**[38:54]** Fundamentally, workflows are considered the application layer, similar to previous data APIs. This system will operate at the tool level, but end users will interact with it through the workflow interface. Currently, no entity has successfully implemented this model on a large scale. However, GitHub has now expanded its API for developers to create extensions and integrate them directly into GitHub.\n\n**[39:40]** Dify is building a platform to support developers in developing and deploying these tools and workflows more easily. The goal is to create a marketplace where tools and workflows can be distributed and used by various users. This system is similar to an open platform, allowing third-party developers to deploy their own tools and workflows.\n\nOn Dify’s platform, there are already about 50 different tools. Some tools were previously released as experiments, but due to a lack of clear direction and community support, they didn’t achieve the expected success.\n\n**[40:17]** Some platforms in the past tried building similar models but didn’t succeed. The reason is that those tools were only built as forms, lacking the ability to interact with external data and unable to combine complex workflows. However, Dify is focusing on solving these issues to create a complete ecosystem for workflows and tools.\n\n**[40:59]** These tools also allow users to push data from external sources into the system. Users can send data from external applications via Open Forms or APIs. Dify will automatically process and format the data for use in the system’s workflows.\n\n**[41:56]** The team is focusing on two main development directions:\n\n1. Continuing to expand and develop existing workflows.\n2. Improving and optimizing current tools to support easier deployment and use.\n\nThe system is built based on common standards for tool and workflow design. The Smithery tool is currently acting as an Agent to manage workflows. Smithery can also be used as a Package Manager to install and manage tools within the system.\n\n**[42:53]** Workflows will operate on the mechanism that if a workflow becomes popular, people can take it and use it as a tool. The nature of these tools is that they are designed to serve specific domains. For example, a tool for creating files, searching, or retrieving code files. It works like an SDK, meaning a library that you just need to import to use.\n\n**[43:37]** Once integrated into the SDK, you can use the available methods to manipulate data. This allows easy integration into AI tools. Currently, only Cross directly supports these operations. However, in the future, it will be standardized so other tools can also integrate easily. The case of Manus is an example. Manus uses many different tools, but when compared to the agent system in Smithery, they are fundamentally two completely different layers.\n\n**[44:15]** In Manus’s system, tools are combined to create more general workflows. These tools operate at different layers, while agents in Smithery are designed to work independently. The question is how to clearly distinguish the difference between Manus’s system and the agent system in Smithery. There’s a summary article about this posted in the AI Club channel ,  the main content discusses the ability to think (thinking) and the ability to use computers (computer use).\n\n**[45:09]** The mechanism of the Manus system is a service-oriented system. To combine multiple tools into a single workflow, the execution steps need to be clearly defined. For example, step 1 uses which tool, step 2 uses which tool, and so on. This requires the steps to be specifically configured. However, the new system has the ability to reason and automatically determine which tools are needed to complete a task. This is the key difference between the new system and older systems.\n\n**[45:59]** Specifically, the new system can recognize how many tools a task requires, which steps to go through, and can intelligently adjust the execution order. This is a special mechanism and a difference compared to older systems. In other words, it operates like a Supervisor ,  capable of reasoning and making decisions about the order and method of executing steps in a workflow.\n\n**[46:35]** The Supervisor system operates at a higher layer than the agents in Smithery. Agents in Smithery are simply tools that execute a specific task, while the Supervisor has the ability to manage and coordinate the entire task execution process. Integrating the Supervisor allows the system to operate more flexibly while making it easy to expand and add new tools.\n\n**[47:33]** The team’s goal is to understand how the system works and grasp the mechanics of managing workflows. If we can determine how to deploy and manage workflows, we’ll be able to select and use tools more effectively. This is what the team is aiming for ,  building a system capable of scaling and optimizing workflows.\n\n**[48:24]** Next, the team will focus on building the **MCP** system. This is a new system designed to manage data and workflows. The team conducted a demo of this system about two weeks ago. The essence of the MCP system is to build an agent that operates on an existing platform. Users can quickly deploy and test the system through MCP.\n\n**[49:10]** MCP will be a complete system, including a **database** and a **server**. This allows the system to operate independently and handle large amounts of data. Unlike older systems, MCP will allow users to adjust configurations and manage data more easily.\n\n**[49:58]** The essence of MCP is an agent, defined with a specific input and output structure. This allows different systems to connect and interact with MCP through standard protocols. In other words, MCP can be integrated into any system via predefined protocols.\n\n**[50:35]** MCP also allows users to manage data through the Knowledge Database, which is essentially a timescale database where all the team’s activity data is dumped. This is a time-series database that enables recording events in real-time, something backend developers will recognize as event sourcing or event logs. For example, it records information about team members, the system’s operational status, or other significant events.\n\n**[51:13]** The Knowledge Database will store all the team’s activity data, including details like who performed which task, the system’s status at specific times, and other information related to the team’s internal operations. This allows the team to track and analyze work performance, thereby making reasonable adjustment decisions.\n\n**[51:51]** The system’s concept includes a component called the Landing Zone. The Landing Zone means that all the data we currently have ,  about a dozen to tens of datasets (databases) ,  will be centralized here. Three to five years ago, if we wanted to build a data storage system, we’d create a bot to collect all the team’s activities and input them into our database.\n\nWith the new Meta model, all large data (Big Data) will be dumped into a temporary storage in the form of .dat files on S3 or GCS (Google Cloud Storage). The MCP will have the ability to read directly from the Landing Zone. If the system determines that the data in the Landing Zone is valuable and necessary, it can automatically convert that data into a Time Series Database (TSDB) for long-term use. This is the end game (final outcome) of this system.\n\nThe remaining issue will be building Use Cases based on the organized data in the system ,  in the direction the team desires. This is a key development direction for the MCP system in the near future.\n\n**[52:25]** So currently, the team will have an old database system ,  a traditional table-based database located at the bottom of the system (visible in the diagram with blue blocks). Now, the team is adding two new components:\n\n- The **Landing Zone** component ,  located in the yellow block at the top of the system.\n- The **Time Series Database (TSDB)** component ,  directly connected to the old system’s components for data analysis and exploitation.\n\nThe team is storing raw data in the Landing Zone. Essentially, centralizing data in the Landing Zone is like rallying troops ,  gathering all the data in one place before deciding how to analyze and process it. This mechanism makes the system more flexible and easily scalable when new data is added.\n\n**[53:11]** The special feature of this system is its ability to automatically convert data from the Landing Zone to the Time Series Database. This mechanism stems from the growing need for local data analytics. This is an emerging trend in the context of AI (Artificial Intelligence) development.\n\nThe rise of AI has increased the demand for real-time data analysis systems. When raw data is centralized in the Landing Zone, the system will automatically identify valuable data and transfer it to the TSDB for more detailed analysis. This is a significant step forward in building an efficient and adaptable data analysis system to market changes.\n\n**[53:45]** Currently, the team can already run analytics directly on the data stored locally. This system allows running analytics right on the Data Lake without needing to transfer data elsewhere. For the data in the Landing Zone ,  the file packets that Huy is showing on the screen ,  this is the part the team needs to focus on researching further. This issue relates to text processing, so the guys need to pick up this topic. It’s not too difficult; it’ll probably take about half a day to grasp the basics.\n\nThe Prompt for searching and exploiting data is also quite fast and simple, not complicated. This is a part very worth experimenting with because it relates to the knowledge discovery mechanism in the system. This is one of the new upgrades Huy just mentioned.\n\n**[54:22]** The most standout feature of the system in this upgrade is the **Knowledge Hub**. This is where the team will centralize all data to serve analysis and knowledge exploitation. The Knowledge Hub will become a common **data pool** for the entire team. Anyone can add data here, and the system will process and convert the data into a standard format.\n\nThe important thing is that once the system is fully set up, everyone in the team will have a common **protocol** to use. Different modules or components will be able to **share** a common data structure and access the Knowledge Hub directly. This will be the common foundation for syncing and processing data within the team.\n\n**[54:58]** Regarding the database (DB), the system will have two layers:\n\n- **Old DB:** Used to support existing operations and process pre-structured data.\n- **New DB:** Designed to connect directly with the **Knowledge Hub** and support real-time data analysis.\n\nThe special thing is that the **MCP** will act as a **protocol** for different modules to communicate with each other. This means that any data needing access or processing just needs to be fed into the system’s correct pathway, and it will be automatically processed according to the standard structure. This is how the system unifies data and avoids conflicts when multiple data sources are processed simultaneously.\n\n**[55:43]** From now on, the team will need to get familiar with the new data processing mechanisms. Everyone should take the time to learn more about the components in the new system. Once these components are stable, the team’s new projects will leverage these tools to deploy faster and more efficiently. This will be the main toolkit to serve future projects.\n\nThis system has the potential to become a **requirement** for upcoming projects. If you want to keep up with the new system, start by learning the basic principles of MCP and related protocols.\n\n**[56:40]** Previously, when the team deployed systems on S3 or GCS (Google Cloud Storage), data processing took quite a bit of time. However, with the new mechanism, data from the Landing Zone will be processed faster and more easily.\n\nThe system has been tested on various platforms, including **S3** and **GCS**. However, since the team’s current infrastructure runs on **GCS**, the data from the Landing Zone will be processed on GCS first. That said, technically, the system can expand to other platforms without major obstacles.\n\n**[57:45]** The Landing Zone’s operating mechanism is quite simple:\n\n- Data from various sources will be centralized in the Landing Zone.\n- This data will be stored as **Parquet files** by day.\n- The system can read these files back through the **Time Series Database (TSDB)** mechanism.\n\nCurrently, some sample **Parquet** files have been created and are being tested. If needed, the team can run a demo on these sample data sets to check the system’s consistency.\n\n**[58:24]** The team’s activities, like **AI sub** or **Memo**, are also fully pushed up here. The task of the **Landing Zone** is to store all the data the team wants ,  anyone who wants to store something can push it all here, and then the system will decide how to process that data. The system has also provided some tools for people to push data up, such as **API proxies** to forward events. If anyone wants to push information to the Landing Zone, they just need to call the API.\n\nMemo is currently using this mechanism to pull data from **social platforms** and sync it into the system. This mechanism has been successfully tested. For more specific data types like **Discord messages** or **data from Basecamp**, the team needs to build **crawlers** or **connectors** to collect the data. Currently, the team already has some ready-made templates for these data types.\n\n**[58:59]** For the next development direction, the team will focus on exploiting data from the Landing Zone. If you want to join this project, the advice is to start with a specific **vertical**. For example:\n\n- Identify a clear **use case**.\n- Find out **which data** is needed for that use case.\n- Redefine the data exploitation mechanism in a **top-down** approach.\n\nInstead of storing whatever data seems interesting, the team should think in terms of **defining the use case first** and then deciding what data to store. This helps the system operate in an organized and easily manageable way.\n\nA specific example is if there’s a use case about **Project Nghệ Nhân**, the team would need to create a **Git Agent** to collect data from Git, then push that data into the **Knowledge Hub** via MCP. From there, the system would define data exploitation tools for this use case.\n\n**[1:00:16]** Additionally, the team is developing a small MCP Server. This MCP Server is essentially a basic server, using standard technical components of the current internet system. It defines clear inputs and outputs, allowing connection to various interfaces.\n\nFor example:\n\n- If there’s an MCP to process data from Slack, the team will define APIs for each data type.\n- If tools are needed to read data from Google Sheets or analyze weekly check-in status data, the team can create MCP tools to handle that data.\n\nMCP will act as an intermediary component to sync and process data from various sources. Everyone can access these tools from the Editor, Command Line, or any other interface.\n\n**[1:01:07]** The essence of MCP is that it will serve as an **API Gateway** to connect tools. If you need to track everyone’s weekly check-ins in the team, you can create an MCP to collect data from the **Knowledge Hub** and Google Sheets, then compare the data to see who has checked in and who hasn’t.\n\nThe current system is at the stage of deploying a basic MCP Server. The current interface uses the **Command Line** to call MCP, but fundamentally, the team can expand it to connect with various other tools.\n\n**[1:01:43]** The system is focusing on implementing authentication and authorization mechanisms.\n\n- **Authentication** – Verifying users to access the system.\n- **Authorization** – Assigning permissions for data processing activities.\n\nThe system is currently being used internally within the team and has not been made public externally. If you want to use MCP, you’ll need to input a **private key** to authenticate your access rights.\n\n**[1:02:23]** Technically, MCP can expand to different components within the system. Everyone can integrate MCP into existing applications or tools without needing to rewrite too much code.\n\nThe team is still testing this feature and focusing on completing the security and access management parts. Once the system is stable, everyone can integrate MCP into their existing data processing workflows.\n\n**[1:03:00]** It’s just paused here for now; we haven’t tackled the complex authorization problems yet. After completing the current steps, we’ll move on to addressing more complex issues related to authorization and system usage rights. For now, everyone can focus on the basic issues first.\n\nAlright, thank you, Huy. This is one of the important technical development parts for the team. If you follow the activities on the tech and AI Club, you’ll notice the team is moving toward the next steps in the development process. Technically, everyone should pay attention to the key terms Huy just mentioned. If you’re not clear on them, you can review the transcript to get the full information.\n\n**[1:03:45]** The core team is still continuing to develop the system. We request all members to participate in the project so we can **transfer knowledge** more effectively. This project is an environment for everyone to learn and practice.\n\nThis is an opportunity for new team members to get acquainted with and grasp important technical aspects. If you feel unprepared, you can refer to the internal guides and documents to catch up. Training will happen during the work process rather than in separate sessions. This is a hands-on environment where you learn while doing.\n\n**[1:04:29]** Alongside system development, the team is also conducting knowledge transfer from completed projects. We expect to have a session at the end of the month to summarize the lessons learned from these projects. If anyone doesn’t fully understand yet, they can refer to or ask members who’ve worked on them for more information.\n\nIf you feel unprepared or need more details, you can directly ask team members. Everyone can ping more experienced members to get support.\n\n**[1:05:07]** The team has two different groups working in parallel:\n\n- **Tuấn’s team** is developing some games and small applications.\n- **The build team** is working on experimental applications to test the system’s feasibility.\n\nThese activities are similar to the **Build Club** and **AI Club** groups within the Foundation team. Some products have started showing good **output**. Tuấn and his team are developing a game based on the **Turing Machine**.\n\n**[1:06:38]** The **Turing Machine** game that Tuấn’s team is developing is adapted from the board game version into a mobile version. The game’s goal is to guess a sequence of **three numbers**. To guess the correct sequence, players receive **clues**.\n\nFor example:\n\n- If the clue says “one of the three numbers must be greater than 1” → Players can input numbers, and the system will determine if the answer is correct.\n- If two numbers are wrong but one is correct, the system will respond immediately so players can continue adjusting.\n\nThe rules are quite complex, which might be challenging for new players. Tuấn and the team are continuing to tweak it to make the game more accessible without losing its challenge.\n\n**[1:07:23]** The game is called [**Pocket Turing**](https://pocket-turing.vercel.app/) because the original board game version involves punched cards ,  similar to how the Turing Machine works in computer programming. However, I’ve adjusted and added new elements to make it more suitable for the mobile version.\n\nI plan to refine and expand the game in future versions. Additionally, I’m checking if we can implement premium features or advanced options to increase monetization potential.\n\n**[1:08:16]** I’m testing the beta version of the game. The gameplay is complete, and players can fully experience the features. The next step is to test it with a broader user group to gather feedback and improve the product.\n\n**[1:09:15]** The next goal is to bring the game to the App Store and Google Play to reach more users. For now, the team wants to ensure the game runs stably without serious bugs.\n\nTuấn hopes the game will attract at least **100 paying users** in the initial testing phase. If we get positive feedback, we’ll expand with new features and improve the player experience. I’d love to hear feedback from other team members to adjust and perfect the product further. Tuấn has shared the game download link with team members so everyone can try it and provide input.\n\n**[1:10:13]** If you guys are excited about building products, this is a good time to start. The team has experimented many times before, but this is a chance to do it more systematically. Developing internal products not only improves technical skills but also opens up future commercialization opportunities.\n\nBesides Tuấn’s game, the team is working on other tools. If you have any good ideas, feel free to contribute so we can build and test together. How to sell or monetize the products can be figured out later; the priority is completing the core features first.\n\n**[1:10:58]** Next is An’s part. An once made a tool called **Rec** to aggregate information in a format similar to **Apple**’s system. Version 1 of Rec required users to manually organize information, while the current Version 2 has integrated AI to support automatic organization.\n\nHowever, the AI still has some limitations in fully recognizing content. Sometimes it can’t grasp the entire context, so the results aren’t completely perfect. Still, the important content is organized and displayed fully.\n\n**[1:11:56]** This tool is in the refinement stage, but the core functions are stable. Currently, the team is focusing on improving the interface and optimizing the user experience. An plans to continue developing additional features to better support users.\n\n**[1:12:51]** The team’s projects are currently in the testing and improvement phase. If anyone has questions or suggestions, they can directly discuss with An or other team members. For now, we’ve showcased almost all the projects. More detailed parts will be covered in the next session.\n\n**[1:13:57]** On our team’s side, I always talk about the knowledge related to **liquidity** and **game in general**, right? We really want the team to push a little in that direction because it’s beneficial for almost like a **life skill**, you know? So I want our team to head in that direction this time. Especially those of you who are really interested in **trading**, meaning getting data to find the **Alpha** on it, the **Intel** on it, to come up with some **market-making** strategies based on certain conditions or something like that.\n\n**[1:14:43]** It’s one thing, or it could even go further to create a really awesome flow. It feels like it’s just my dream for now. An has already made a version that I think is pretty okay. Just taking this chance to let you guys know that the team has this kind of progress going on. It’s running, right? Let’s invite An. Okay, okay, generally it’s just a money-making game. See how they make money, and we’ll do the same. The usual stuff has a bit of something to test, a bit of it is also in the form of **Delta neutral**, right? So we also research those things there. Then go build and research to have the knowledge to ship it.\n\n**[1:15:30]** Play this column until it’s all used up, that’s it, no looking over there. Do you all see the terminal screen? Do you see it? Yes, okay, let’s run it. Let’s run it. I think we need to zoom in, zoom in one or two levels, it’s still a bit small, okay now. Yeah, this is the arbitrage to eat the **funding free frost**, right? There are many, many types of arbitrage like that. This is just one of those types, which is eating off the difference, the **phantom bin**, between the exchanges. We’re focusing on three exchanges: **Binance**, **OKX** ,  that devil OKX, their exchange doesn’t have too much stuff ,  so, do you have a diagram for that, An?\n\n**[1:16:26]** I think it’ll be a bit hard for everyone to visualize. Looking at this, they won’t understand what it is. Is there one? Okay, no diagram? Oh, there’s this, big one, just theory, nothing concrete comes up at a high level? I guess we’ll have to sketch it roughly again. Do you see it yet? Maybe look at my diagram, yeah, my diagram, so you know it’s this right away. The PRL? Wait, it’s messed up, running wrong, forgot, it’s going into the sockets of…\n\n**[1:17:47]** Those guys to pull **real-time data** back, and it’s currently pulling data from how many accounts? Three accounts ,  **Binance**, **Bybit**, and what? Okay, initialized to get the price back, right? Getting the price, getting the **funding** back, getting the price yet? Getting some data like fees and fee-related data, it’s kind of coded according to that calculation, but it hasn’t been used to fetch yet. OKX probably doesn’t have it. Those trades, okay, don’t have it, so usually it’s just on the two main ones, which are **Binance** and **Bybit**. Yeah, setup is okay, and then it’ll have an array to show which pair has the difference, the difference in **funding**, then it’ll have profit, and I calculate that if we enter, how much it would be. PH is the number, the number of times we collect the **funding** to break even with the fees. It’s monitoring, monitoring the highest between the two exchanges, that’s it. Step one is getting the difference, the difference in **funding** between the two sides, right? Between the two sides that I’m talking about, no. Because I said this is getting from three exchanges, so compared to its angle, it’s still on the exchange net, right? Yeah, exchange net is something calculated later because…\n\n**[1:19:49] Funding**, let’s say normally, on the price setup, they usually don’t have much, like one is positive, two are positive, or two are negative, so the difference is smaller. Actually, we place a counter on the other difference, it’s just the offset, the price movement offset, so it doesn’t lose due to the price. On the exchange net, there won’t be that difference, we make the **funding** always equal to zero, right? Because there’s no swap fee there, and instead of countering on another exchange with that thing, we counter at the moment when the **funding** is also zero. Currently, we accept that it won’t be as profitable, but that’s how the first version is.\n\n**[1:20:33] b**So we get the price, get the top, get the **funding**, then see if it’s just about deploying capital, right? Have you tried running it yet? Not yet, haven’t poured money in. Yeah, this is like a scale game, you need a lot of money to profit, but with just a few hundred or a few thousand, it goes in, and it doesn’t look like much. Got it, got it, okay, understood. But on the technical side, technically, for me to do this, what did I apply from start to finish? From when it crashed, what did I do? Yeah…\n\n**[1:21:25]** First, I researched that chart beforehand, checked how it was, whether it had this or that, all those things. Yeah, got those dots sorted. We’ll ship it for that, yeah, that guy will code everything with code, okay? Using this **Cloud 3.7**, right? First, you have to get the exchange’s data before calculating anything. The main exchanges will pull from sockets, and the setup is, first, on the API docks of the exchanges, right? This one, yeah, then pull their docks back, throw it to this guy, it ships up to the **WebSocket client**. Every exchange has docks, all of them, pull them back, ship them up, and it’ll auto-view.\n\n**[1:22:15]** The web for all three exchanges is done, with forms and everything. After that, we start building it up, yeah, this part, this chart. The first step is probably explaining it to it and stuff, kind of it builds slowly up. Then check the data and all, if it’s wrong, it auto-builds itself. I built it, and it auto, every time there’s something new, it’ll auto-build a sample to check the data before finding it again for us. Cool, to ensure the data is correct or not. Because the thing between the exchanges, the format is different, the data format is…\n\n**[1:23:01]** Different, everything is different, right? So to compare it all into one final form for it to compare, it needs a section to test pulling the data back, ensuring the data is valid, then it starts comparing. That’s the step. Next, it’ll be building things like, next is, yeah, building the guy to place orders. When it detects these, there’ll be a guy standing by to place orders, watching our bot to see if it’s losing or whatever, slowly, reasonably. The whole process took how long? About a week, yeah, cool, huh? So now the next step…\n\n**[1:23:58]** The next step of this tool I’m working on, what’s the next step? I’ll check the data first, check the data to see which one makes money easily, which one is profitable, which one you put money into and it’s all profitable. There’ll be data to check those profits, then manage more of our stuff, like risks and all that, then, yeah, pull all the data back about the fee structure of the exchanges. Because if you use those trusts or whatever, the exchange fees affect the thing a lot, so you need the exact fees, then calculate…\n\n**[1:24:51]** How to ensure it’s profitable in the end before placing orders, right? Next will probably be those steps. Okay, that’s the step, the step of when to place orders, that’s the final thing, right? The rest needs to filter the data first. Is there a back system built? Because I think this data, does it have history or not? Does it? Or is it just at that moment? It does, if you can get the history, it’s backtest history, but I think it’s not accurate, huh? Yeah, not accurate, not there. I don’t think so. The other stuff might have it, but this arbitrage is a bit hard to get accurate.\n\n**[1:25:38]** This is a technical showcase. I think with this direction in the team, independently, our team, regarding this direction, it’s okay. You guys showcasing the trading game have started having steps that the team is pushing forward to do. I think fundamentally, fundamentally, you guys are all on a path, on the way to getting to what you want to do, which is very good. The thing is, with technology, with that tech know-how, how we bring it out and use it, right?\n\n**[1:26:19]** The team’s activities in general are like this, okay? Regarding productivity, it feels like recently everyone has started syncing with each other to a certain degree. But with Tom, Tom is probably out, but Tom had a comment from before when you guys were sitting and chatting. We were thinking, what’s the team’s productivity level right now? How much would Tom rate it? 2/10 or 4/10, huh? If compared to the level we need, you guys at like 6-7/10, the general average, I think we’re in a very good setup right now.\n\n**[1:27:01]** The next step regarding the quality, the technology tool link to support us in operating the team according to this model, it’s being improved slowly but surely. On the market side, the **funding** market in general and the products are starting to stir and come back. People see the technology becoming more stable. In **crypto**, it’s heavily influenced by macro factors, but whenever there’s a tech trend, they’ll jump in and bite, that’s how it is, right? I’m seeing signals for it to resume soon, about 50/50 right now. Before this, I looked at the market, and it was really bad, like everything wasn’t ready yet. Even if you studied a lot and worked a lot, results wouldn’t come immediately.\n\n**[1:27:40]** But this time, I think you guys will need to have some requirements about participating in these things, okay? Next week, I’ll probably ask Huy,Tom and Thành to compile some stats, to see besides the projects you’re working on, what’s the participation in side projects like that, who’s doing what, alright? That’s the follow-up after the content, we’ve discussed almost everything. Next week, there are still some core flow parts left, but they probably won’t affect things too much.\n\n**[1:28:22]** Today is the 14th, I hope by the end of this month, the next team meeting will show more progress. Everything we’re doing is very important, right? Another important thing is we have Sister Minh here, Nicki. Probably past the out time already. All of this is being uploaded to **Memo**, and we’re using that **Memo** not just to share on it ,  that’s not enough ,  but the channels we’re working on are being sent out…\n\n**[1:29:01]** To everywhere, to other companies we know, starting to expand the network to look for necessary users. The thing is, we know these things already, so how do we mound our ability to profit from our knowledge? That’s the idea, okay? That’s the skin the team is following. In summary, the market assessment is like this. Next week, you guys will need to register for those parts, and Huy, Tom, and Thành are the mandatory segments.\n\n**[1:29:45]** As for the hobby clubs, like Build or something, I don’t have high demands there, producing technical stuff to apply, it’s not that important. The output matters more. Two different groups: one group is the core project parts we’ll work on, focusing on how to increase activity, increase everyone’s **knowledge base**; the other group focuses on the **skill set**, how to develop products for launch, how to do onboarding better, users and all that. It’s a different skill set group. You guys next week jump in and start thinking, especially…\n\n**[1:30:27]** Especially Huy, Huy is co-handling the return to the office to start shadowing for **knowledge transfer**. If it works, just keep it going, then see how the numbers look and report back to me, okay? Hopefully, when we have the numbers, you guys discuss further, figure out how to set up that shadowing on the projects, the sites we’re involved in, to have cases to share with each other, right? Like Sister An, finishing this in a week is super solid, doing everything herself, using new workflows and all, it’s great…\n\n**[1:31:05]** Alright, you guys, that’s the whole thing. If there’s nothing else now, we’ll probably end here. How many people are here? 28 people, huh? No, how many in this call right now? Preparing to spam ICY, is there an issue with transferring ICY yet? Everyone go random, grab it like ghosts, okay? Amount is 28, so we’ll drop 14 ICY tokens, entry is 14 already. Go ahead, duration is 5 seconds. Okay, let’s go. One ICY earlier was about 100 Satoshi already.\n\n**[1:32:09]** Now starting, don’t know when the boss updates the multiplier price, just estimate it for now. Today’s early, next time seeing Bitcoin, it looks cool. Happy Weekend, bye bye everyone.\n","title":"OGIF Office Hours #41 - ICY-BTC Swap, GitHub Bot, MCP-DB, Pocket Turing, Recapable, and Arbitrage Strategy","short_title":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","description":"In OGIF 41, the team covered key updates on the ICY-BTC swap, GitHub bot automation, MCP-DB system for agent workflows, and progress on the Pocket Turning and Recapable projects and we also shared insights into funding rate arbitrage strategies.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Thu Mar 20 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/41-20250314.md","slugArray":["updates","ogif","41-20250314"]},{"content":"\n### Topics and Highlights\n\n- **Team check-ins \u0026 workflow**: Kicked off with roll-call vibes, planning speed-run topics, and assigning tasks to Hải, Cường, and Tom. Encouraged quick 10-minute concept pitches.\n- **Frontend updates**: Hải’s January report covered React 19 and Next.js 15.1, spotlighting the View Transition API for smoother stage animations and Deno Deploy’s new server-side rendering support.\n- **Tooling \u0026 libraries**: Explored Transformer for running Python models in JS, Neon’s switch from Webpack to a lighter setup with better hot reloads, and HTMX’s rise with logic-in-HTML simplicity.\n- **Database design practices**: Cường recapped scaling databases with business growth, emphasizing DBA roles, migrations, CI systems, and versioning for managing schema changes and avoiding API breaks.\n- **AI-driven development**: Tom showcased a full-cycle approach, leveraging AI for rapid planning, task breakdowns, and proposals.\n- **Skillset spotlight**: Highlighted team strengths, security/performance (Thành), user/data flow (Tom), and how to align them with proposals, from MVP to real-time app concepts.\n- **Process optimization**: Detailed Tom’s AI-assisted workflow: extracting insights, crafting prompts, validating concepts, and scaling tasks with 90% accuracy, plus burning questions for client rapport.\n- **Q\u0026A \u0026 next steps**: Wrapped with open questions, a nod to future Tom-led sessions, and a promise to refine skills like real-time handling and proposal structuring.\n\n### Vietnamese transcript\n\n**[00:00]** Bắt đầu thôi nào. Chào mấy anh em, cảm ơn đã đợi. Thành với Cường đâu rồi? Cường có lên phòng chưa? Thấy đăng ký thứ Sáu mà giờ lên đây rồi, đúng không? Tuần này Thành đâu rồi? À, lên rồi, đứng đây nè. Tuấn, Tom lên stage luôn nha. \n\n**[04:51]** Đang xem mấy cái bài, tự nhiên cái link này Tom ơi, đẹp chưa? Để anh sửa lại. Ngày hôm nay 186 giao dịch, 1 user, 30 ICY member như cũ, 5 cái inactive, 1482 giả mạo. Hai channel chat nhiều nhất, ba channel chat nhiều nhất, mấy người chat nhiều nhất là ai? Ờ, tiêu rồi! Còn ai nữa không? Hôm nay thiếu ai không? Có hai chủ đề cũ: một cái là \"run and report\". Sáng nay anh post link lên rồi, chắc vậy, để kiểm tra lại. Cái thứ hai là bài design của Cường, anh chưa biết nội dung.\n\n**[06:03]** Bài này là cái gì vậy, ngồi nghe mà chẳng hiểu gì luôn. Bài số ba là nối tiếp cái series hôm trước, mấy anh em viết xong, làm xong, giờ nó thành hình cụ thể rồi. Qua 3 tháng thì team cũng có vài cập nhật mới, hướng đi này rõ ràng hơn chút. Hệ thống thấy cũ rồi, tí anh forward link cho mọi người đọc trước qua email.\n\n**[07:10]** Đăng ký dùng thử đi, tí nữa vào xem. Plan là vậy. Chắc ship bài của Hải trước, rồi tới bài của Cường, rồi tới bài của Tom, mấy phần Tom làm đó. Nội dung hôm nay chắc vậy. Anh em xem thử còn thiếu ai không, hay thấy ngắn quá, có gì liên quan nữa không? Ai thiếu vậy? Thành lên chưa? Ờ, đệ Thành đỉnh quá, hết việc để làm rồi. Anh cũng nghĩ vậy.\n\n**[08:55]**  Đợi chút nha, đợi đủ người rồi tụi mình speed-run mấy chủ đề này. Chủ đề cũng đơn giản thôi. Anh em cố gắng tóm gọn bài của mình, nói concept, idea trong 10 phút thôi, đừng dài quá, để dành thời gian cho buổi kia. Nếu cần hơn 10 phút thì nói dài hơn chút, vậy nha. Tuần sau có lịch lên văn phòng, tuần này check-in bình thường thôi. \n\n**[09:57]** Tuần sau dựa trên danh sách đăng ký, anh sẽ đề xuất với Huy Nguyễn làm trò điểm danh cho đủ mặt. Thành policy luôn rồi. Tuần sau làm điểm danh cho đông đủ. Đoạn tiếp theo thì mấy dự án cũ giờ gần xong hết rồi. Giờ dep blockchain với AI là vua của mọi nghề, anh em nào muốn làm trực tiếp thì phải lên kế hoạch cái đó. Có ai trùng gì không, hay còn ý gì nữa không?\n\n**[10:58]** Chắc bắt đầu với bài của Hải trước nha. Hải ơi, mời em trình bày. Dạ, mọi người thấy hình của em rồi đúng không? Em tóm tắt Frontend report tháng 1. Tháng 12 năm ngoái, React 19 release đi kèm với nó là thằng Next.JS 15.1 cũng tung ra một phiên bản mới.\n\n**[12:07]** Để hỗ trợ cả thằng Next.JS lẫn thằng React 19 luôn. Bên Reactthì em thấy nó đang làm một cái API khá hay, gọi là View Transition. Browser giờ đã có API View Transition này rồi, nhưng trước đây thì React chưa hỗ trợ. Một số thư viện đã viết và dùng cái API của bên kia, nhưng khi đưa lên React thì gặp vài vấn đề về performance. Ờ, tụi nó đang đợi API này từ React để hỗ trợ tốt hơn, giúp giải quyết vấn đề performance rõ ràng hơn. \n\n**[12:48]** API này dùng để làm animation khi chuyển giữa hai stage của trang web. Ví dụ như anh kéo xuống dưới đây, nó sẽ như ví dụ bên dưới này, cái stage đầu tiên là box nằm trên, stage thứ hai thì box nằm dưới. Thay vì chuyển stage mà nó nhảy thẳng xuống luôn, thì View Transition này hỗ trợ mình tạo hiệu ứng animation, nhảy qua nhảy lại các kiểu. Tương tự, với mấy cái như hình ảnh, nó cũng tạo hiệu ứng animation. \n\n**[13:28]** Khi chuyển đổi hình ảnh, thay vì chỉ nhảy sang hình khác ngay lập tức. Dạ, cái API này vẫn đang trong giai đoạn thử nghiệm thôi. Phải dùng phiên bản thử nghiệm thì mình mới xài được. Nhưng nó hứa hẹn sẽ tăng performance khi sử dụng. Vì trước đây, thằng Motion cũng đã hỗ trợ rồi, nhưng chỉ trong môi trường thuần thôi. Còn nếu lên thì nó gặp vài vấn đề performance, tại vì nó phải xử lý cả trước và sau khi set.\n\n**[14:07]** Cho cái phần này, bên SCS thì có mấy thứ như thằng Deno Deploy. Lúc trước nó chỉ hỗ trợ deploy static site thôi, nhưng giờ nó đã hỗ trợ hoàn toàn để deploy cả thằng Next.JS luôn, kể cả server-side rendering. Giờ mình có thể dùng Deno thay thế, hòa chung được, để deploy một ứng dụng NS. Dạ, cái này vẫn chưa có gì để nói hết. Còn cái thư viện Transformer Z này cũng khá hay. Bản chất của nó là đang biến mấy cái model. \n\n**[15:03]** Bản chất của nó là đang biến mấy cái model viết bằng Python lên thành JS, để mình có thể chạy trực tiếp mấy cái model này trên trình duyệt luôn, không cần qua API hay ngôn ngữ Python gì hết. Như trong bài này, nó chạy được cái sentiment testing. Ví dụ như positive hay negative, hoặc là object detection, như phát hiện con mèo. Bản chất thì em nghĩ mấy model khác, mấy cái pipeline khác, vẫn chạy được, miễn là nó được hỗ trợ bởi thư viện này.\n\n**[18:33]** Bọn em buộc phải hỗ trợ kiểu dù có mạng hay không, data vẫn phải lưu được hết. Sau đó chọn cách lưu xuống IndexedDB, rồi khi có kết nối trở lại, mới đẩy data lên server. Kiểu như vậy. Ở dưới đây nó có hướng dẫn step-by-step để xử lý. Làm vậy thì sẽ gặp vài vấn đề, như list data bị fail khi sync chẳng hạn. Nó chỉ ra một số cách để giải quyết mấy vấn đề đó.\n\n**[19:22]** Kiểu như vậy. An mới post link gì đó à? Zero là con gì? An mới bảo gì kìa, có liên quan không? Bữa trước thấy Lập, cũng bảo cái vụ \"local first\", chắc giống vậy đúng không? Mọi người chung bài toán, thi nhau đi giải. Tiếp theo, bên Win thì có nhắc. Bài này có update chút, giờ nó support thằng đó luôn rồi. Lúc trước Node.js thì phải có command line để combine thằng Typescript ra js mới chạy được. Còn giờ nó chạy trực tiếp luôn.\n\n**[20:01]** Như nó chạy bằng cái command line, load file luôn. Theo em thấy, còn một bài nữa về anh dec này, kể về chuyện các dependency ở bên MBM. Nó cứ ra version mới hoài, kiểu mỗi version lại kèm theo mấy cái breaking change. Ổng nói làm vậy khá cực, muốn update version nhưng sợ app không theo kịp. Không phải lúc nào cũng có thời gian để xử lý hết. Nên ổng không thích thằng React lắm, chọn hướng khác. Ổng bảo thằng này sẽ ổn định hơn, ít bị thay đổi như vậy. Ổng ưu tiên thằng này hơn. Thằng HTMX thì cũng nổi lên đang đứng top 1.\n\n**[21:07]** Dạ, còn một bài cuối nhanh về thằng Neon. Thằng này cung cấp dịch vụ về database. Nó vừa chuyển từ Webpack sang cái khác. Trong quá trình đó, nó gặp vài vấn đề, nhận ra một số hạn chế của Webpack. Như là nó không support tốt, có một danh sách dài những khó khăn ngay đây. Nhưng kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái mới ổn hơn Webpack. Thứ nhất, nó ít lỗi hơn, reliable hơn thằng Webpack. Thứ hai, config của nó đơn giản hơn. Như nó nói, chỉ cần mười mấy, hai mươi cái plugin của Webpack là làm cho nó nhẹ hơn nhiều. Em cũng không biết tại sao nó để vậy.\n\n**[22:03]** Nhưng mà cái kết quả cuối cùng sau khi chuyển thì nó cảm thấy cái hot reload của nó ok hơn thằng Webpack. Nó ít kiểm khi bị full reload hơn thằng Webpack. Thứ hai là config của nó, nó simple hơn. Như nó nói là nó cỡ mười mấy, hai mươi cái plugin của Webpack gì đó, nó làm cho cái của nó nhẹ hơn nhiều.\n\n**[23:01]** Bài này nó chủ yếu là nói về những cái khó khăn và những cái kết quả cuối cùng khi mà nó chuyển từ Webpack sang cái kia. Dạ, vậy là cái của mấy anh em đang thay đổi à? Đang chuyển qua từ cái Webpack chuyển qua cái con kia là một đúng không? Cái React ở trên kia thì sao?\n\n**[23:50]** Chuyển qua HTMX hả? Là hai rồi, còn gì khác nữa không? Xài con Deno à? Với lại TP hả? TP thành main framework hả? Ừ, dạ, cho nó rồi. Còn mấy bài khác thì mọi người có thể đọc thêm trong cái này. Dạ, cái gì nhờ Hải post lại cái link nhé? Cảm ơn Hải, cảm ơn mấy anh em đã cho cái reply. HTMX nó là cái gì mà tại sao lại được chọn vậy? HTML nhưng mà có logic trong đó hả? Kiểu nó sẽ thêm một số thằng trực tiếp vô cái HTML, rồi dùng để trực tiếp ông lại chê nhau thôi. Cái trò này từ thời Backbone.js với lại Knockout.js. \n\n**[25:06]** Đây cả chục năm, giờ mới làm y chang vậy mà. Anh em có câu hỏi gì không? Cho một phút comment thêm. Có gì cần update thêm không? Có gì nhờ Hải post link vô, cho vô ngoài random hay vô group chat nhé. Mời bạn tiếp theo. Mời Cường đi nhanh qua chủ đề về database design. Dạ, bắt đầu luôn. Tiết học lịch sử hả? Cái này, cái bài mấy cái practice này là có từ 2017 rồi.\n\n**[26:17]** Em chỉ recap lại thôi à? Tổng kết hả? Tổng kết cái kỹ năng thiết kế dữ liệu, tip entity hả? Dạ, không, không hẳn là quản lý dữ liệu. Kiểu mấy cái practice để mà mình handle mấy cái kiến thức trong quá trình mình phát triển, mình grow cái database của mình lên. Dạ, em xin vô luôn. Database với lại cái hệ thống mà mình phát triển thì lúc nào cũng đi đôi với nhau. Khi mà phần mềm của mình scale up để bắt kịp cái business demand, thì mình bắt buộc phải scale up cái database của mình lên để quản lý số lượng lớn các.\n\n**[26:51]** Dữ liệu trải qua từng năm. Ví dụ như từ 2015, Amazon mới có khoảng 50 triệu dữ liệu, thì bắt đầu tới 2020 đã phát triển lên tới mức phải handle 200 triệu dữ liệu. Vậy tại sao cần phải có những cái practice này? Khi mà cái database của mình có tới cả trăm hoặc cả ngàn schema, thì cái management system như SQL Server hay mấy cái hệ thống quản lý dữ liệu khác, mình nhìn vào sơ đồ schema, table hay data thì không thể biết hết được tất cả.\n\n**[27:27]** Các cái context. Tại sao những cái change này đã được apply vào trong hệ thống? Để đúc kết ra được thì sẽ có một vài practice. Bắt buộc phải có sự kết hợp giữa con người và hệ thống để quản lý các kiến thức này. Tất cả những cái này chỉ là practice, không bao gồm việc lựa chọn hệ thống quản lý database hay thiết kế database schema. Nó bao gồm cách mà mình chia sẻ kiến thức database, lưu trữ những kiến thức này. Và khi những cái database change được boost lên thì sẽ có một hệ thống riêng để quản lý mấy cái change này, như continuous integration và những cái tương tự.\n\n**[28:02]** Đó là những cái change này sẽ bắt buộc phải follow một vài refactoring rules. Về no-sharing thì bình thường trong tổ chức của mình sẽ có một người gọi là DBA. Người này sẽ quản lý cũng như phải chia sẻ tất cả kiến thức và các sự thay đổi của database được apply vào hệ thống. Ví dụ, nếu mình có nhiều team dev, dev 1 khi phát triển phần mềm A, dev 2 quản lý phần mềm B, thì cả hai khi push change lên database của hệ thống sẽ phải hỏi qua người DBA. DBA này sẽ verify từng change xem nó có tác dụng gì, để quyết định cái change đó có make sense với database chính hay không.\n\n**[28:34]** Khi từng dev push cái database của mình lên, thì dev này sẽ verify với hệ thống chính để xem các API gọi đến database có bị ảnh hưởng gì không. Sau đó sẽ đánh giá cái change này có cần thiết không. Nếu cái change này ảnh hưởng quá lớn đến hệ thống, thì người DBA có thể reject cái change đó, bắt người dev phải update, refactor hoặc chỉnh sửa lại cho hợp lý. Khi cái change đã được approve, thì người DBA sẽ phải document lại rằng cái change này có ý nghĩa gì, tại sao cần cái change đó, rồi post một cái migration lên cho database master bắt đầu cập nhật.\n\n**[29:14]** Những cái dữ liệu này còn phải được lưu trữ ở một chỗ nào đó mà tất cả mọi người đều dễ dàng truy cập và tìm kiếm để biết tại sao những thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào một cái repository, giống như một coding project. Cái repository này chứa tất cả database artifact, bao gồm script chạy database, credential login, configuration, và mức độ dung lượng tối đa mà các instance này có thể quản lý, cũng như các documentation của hệ thống. Cái repository này cũng tương tự như một coding project, sẽ được quản lý bởi một version control.\n\n**[29:51]** Cũng như là tìm kiếm để biết được là tại sao những cái thay đổi này cần thiết. Tất cả những thay đổi này sẽ được bỏ vào trong một cái repository giống như một coding project vậy. Mọi người có gì hỏi thêm không?\n\n**[30:39]** Để mọi người có thể check, cũng như kiểm tra các cái change, context và history của những thay đổi này trong hệ thống, thì mỗi lần thay đổi, người push cái migration này sẽ tạo một cái pull request và thêm description. Description này giải thích tại sao cần cái change này, nó cần thiết ra sao, và những hệ thống nào sẽ bị ảnh hưởng bởi cái change đó. Người review, đa số là các dev của những API mà cái change này tác động trực tiếp tới, sẽ vào xem xét.\n\n**[31:14]** Sau khi những thay đổi này được merge vào nhánh master, sẽ có versioning để mình có thể rollback hoặc deploy các version này vào từng hệ thống để dev, testing, và cuối cùng là đưa lên production. Khi mà mình có nhiều dev instance giữa các version, thì lúc dev từng hệ thống riêng, mình sẽ phải checkout ra từ một instance của master database để sử dụng cho việc development. Như vậy, khi thay đổi gì đó hoặc migration một cái mới, mình không ảnh hưởng trực tiếp tới cái database chính.\n\n**[31:52]** Khi đó, mình cần có một hệ thống CI. Mỗi khi thay đổi gì trong instance mà mình dev, mình có thể dễ dàng verify xem cái change này có break master database hay không. Đồng thời, khi ai đó push một cái change mới lên master database, mình sẽ được thông báo về schema thay đổi hoặc resource conflict trước khi làm chậm tiến độ dev. Khi boost một thay đổi trên database, những thay đổi này bao gồm mấy bước như sau: thay đổi một cái database schema. \n\n**[32:25]** Khi push một thay đổi, mình phải tạo một migration script lên database đó. Sau khi script này được merge, mình phải đổi database access code để API có thể dùng cái change mới đó. Đối với những database change như thêm một column mới, thì có thể không nhất thiết phải thay đổi access layer của API khi change này được push lên. Vì một số API không cần dùng tới cột mới đó. Ví dụ, mình có bảng user với name và address, một service mới cần thêm field birthday vào bảng user, thì các service cũ như service gom nhóm user theo address không cần thay đổi gì trong API để tích hợp cái change mới này.\n\n**[33:07]** Đối với những change ảnh hưởng lớn, như giới thiệu một non-null value hay tách bảng, thì tất cả service phụ thuộc vào nó cần phải đổi data access layer để tránh lỗi. Ví dụ như bảng user vừa nãy, nếu tách bảng user ra, thì service nào dùng bảng đó phải thay đổi toàn bộ access layer để không bị lỗi. Ngoài ra, có thể dùng một cái gọi là transition interface để dần dần apply các thay đổi mới, rồi boost cái change đó mà không làm crash API cũ.\n\n**[33:45]** Sau khi đã refactor và apply change lên master database, mình còn phải notify tất cả các service dùng database này để tránh break mấy cái API đó. Đồng thời, mọi người có thể contact nhau để resolve config khi thay đổi master database. Về phần recap, trong quá trình develop một software, khi phần mềm phát triển thì bắt buộc database của mình cũng phải phát triển theo. Để mọi người đều nắm được thông tin và context của từng cái change trong database này, cần vận dụng tất cả kiến thức để chia sẻ và sắp xếp kiến thức của mình.\n\n**[34:32]** Đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian, mọi người resource conflict giữa các cái database change. Bài này thấy nó có giá trị ở chỗ góc nhìn. Chắc là giống như góc nhìn dev, nhưng mà nó đứng góc nhìn về chuyện là thay đổi đối tượng làm việc chính.\n\n**[35:21]** Thông tin và cũng như context của những từng cái change bên trong database này thì cần phải vận dụng tất cả những kiến thức để mà know sharing cũng như là sắp xếp các cái kiến thức của mình và đồng thời là tất cả những cái change này đều phải release tường tận để mà tránh các cái conflict thời gian mọi người resource conflict giữa các cái database change. Hết rồi. Mọi người có gì hỏi thêm không?\n\n**[35:58]** Là không phải codebase mà là cái database đúng không? Theo hướng đó nhiều. Cứ nghe tới đoạn này thấy hơi meta quá, kiểu hệ thống lớn chắc mới quan tâm, còn hệ thống như hiện tại thì hơi khó áp dụng hả? Khoảng hệ thống cỡ 20 table là thấy hơi lâu lâu, nhìn vô cũng hơi chóng mặt rồi. Đúng rồi. Vậy cái này liên quan tới chuyện documentation, quản lý versioning, với cả làm monitoring. Không phải version monitoring, mà là notification cho mấy cái team khác đúng không? Dạ, vậy nó còn ít lắm, nhưng mà đúng rồi. Mấy cái này đưa vô thì hợp lý, vì có góc nhìn.\n\n**[36:50]** Quản trị data trước tới mấy cái kia. Logic thì logic ở đây, mai mốt data chạy rồi. Anh em có hỏi gì Cường không? Không thì sẽ kết thúc ở đây. Bài này có giá trị về góc nhìn. Nghĩ mấy anh em khi làm backend mà muốn làm giàu thì sẽ phải theo dự án suốt đời. Dự án càng lâu thì nghĩa là dự án càng có tiền. Thấy vậy, đi được với dự án càng lâu thì về bản chất nó sẽ ok. Nhưng mà thường dev thì nó sẽ lười. Dev thấy cái gì mà làm lâu quá thì bị chán, hành vi rất là lạ. \n\n**[37:40]** Trước khi qua bài tiếp theo, để đóng góp cho buổi hôm nay, một cái keyword của tuần này, trong quá trình đi ngồi đọng lại, có một keyword mới, mới học được. Từ mới dành cho những bạn chưa biết, giống như anh chưa biết. Đây là kiểu hôm nay lên, có một trường phái tên là Luddism. Luddism là một cái chữ xuất thân từ thế kỷ 19, khi cuộc cách mạng công nghiệp diễn ra. Ngành những ngành liên quan tới dệt may được tự động hóa, thì cái đó tức là những người theo có cửa, họ tên là Luddites sao á, mới đi đốt mấy cái máy đó.\n\n**[38:34]** Mấy cái máy đó cướp việc của mình, cướp chén cơm của mình, nên họ đi phá mấy máy đó. Thành ra cái này trở thành một trường phái Luddism. Tức là tầng lớp working class đi chống lại xu hướng hiện đại hóa. Rồi chữ khóa tiếp theo đi sâu tiếp thì sẽ ra Neo-Luddism, với lại cái thằng Luddism ngay đây. Cái gì anh em đọc thêm nhé, thấy khá là relevant với mình sắp tới. Theo những dự đoán mà hôm trước.\n\n**[39:18]** Mình ngồi nói với nhau á, thì sắp tới chắc sẽ nhiều người dậy lắm. Ở trên Reddit thì nó có một cái bài cách đây 2 năm, có cái làn sóng Neo-Luddism mới sẽ xuất hiện. Giờ lên thấy cũng nhiều lắm ha. Thì cố gắng, góc nhìn anh thì cố gắng, anh em không nên, không nên theo trường phái này. Hồi phát triển sẽ đi tiếp, không nên chống lại bánh xe lịch sử. Rồi có luôn cái subreddit tên là Luddism luôn, nói từ Luddism luôn. Không chỉ nói về automation, mà nói về đủ thứ trả lại công nghệ trên đời. Cảm giác lạc lỏng, cảm giác thế này thế kia. Đây là keyword khá là thú vị, ha, anh em.\n\n**[40:08]** Không bị dính vào đây ha. Rồi cái số hai nữa là có cái liên quan đến cái này. Thằng vừa rồi mới ngồi, mới ngồi tìm ra này, đó là U.S. geopolitical. Có một góc nhìn về chuyện nước Mỹ phát triển như thế nào. Anh nghiên cứu về thị trường vốn, có cái dòng tiền đầu tư nó chảy đâu, nên vô tình lọt vô cái chủ đề này. Đây là chủ đề thứ hai, thấy cũng khá thú vị. Maybe anh em sẽ quan tâm. Chủ đề này liên quan tới macro economy. Thì ra nó được, từ cái trị nó chuyển qua thành macro economy.\n\n**[40:56]** Nước Mỹ sẽ có xu hướng có hai phái thôi. Một là isolationism, tức là cô lập hóa. Một chữ khác thể dùng cái đó, tính làm đây ha. Thì trong cái movement này, nó nói gì? Nước Mỹ sẽ có xu thế là nó co mình lại, không deploy mấy cái resource đi khắp nơi để giao thương nữa, mà gom cái đó về, đứng đó phòng thủ. Đây là cái cụm thứ nhất. Hiện tại, tất cả những tin tức mình thấy được á, thì nó đang trong cái đó, protectionism hoặc là isolationism. Cụm này, hướng thứ hai mình thấy là globalization.\n\n**[41:41]** Globalization thì những cái sáng chế, những cái công việc sẽ tập trung vào chuyện trading với nhau nhiều hơn, giao thương nhiều hơn. Nước này nước nọ quăng những cái đó đi khắp nơi. Mỹ sẽ có xu hướng là out ra ngoài, những anh chị em theo cái phái đó. Những cái nước theo cái phái đó cũng sẽ có xu hướng cởi mở hơn, chạy khắp nơi. Thì nó là cái tình trạng trong trạng thái mà nó diễn ra từ report, từ năm 45 tới gần đây, thì đã đi thành những cái cụm nhỏ. \n\n**[42:20]** Trong giai đoạn sau chiến tranh với Nhật, đi nút cho Nhật hai cú xong rồi, thì giúp Nhật với Đức sau Chiến tranh. Nó sẽ giúp tái thiết lại, thì bắt đầu nó deploy, nó globalization theo hướng đó. Đó là cái phase ban đầu. Nó bắt cái đoạn đó, đến khi mà Nhật mạnh quá rồi phải không, thì bắt đầu sẽ bị nerf lại bằng một số sự kiện nhất định. Ở đây có sự kiện này, với cả sự kiện tên là VIA này, ra thông tin hơn. Nhưng cơ bản là vậy. Thì idea chính là gì? Idea chính là đang có cái xu hướng học từ lịch sử trước đây.\n\n**[43:09]** Từ cái Great Depression năm 1930 cho tới giờ, hiện nay 2020, có một cái nước Mỹ đang trở lại với trường phái protectionism. Sẽ dẫn đến tất cả những nước khác cũng sẽ đi theo cái này. Ai cũng sẽ là dân tộc mình là cái chính. Thì cái chuyện mà mình nhảy khắp nơi sẽ ít lại hơn, so với giai đoạn này. Đường đỏ là đường Trung Quốc nè, đường màu này là đường của Nga nè. Nga sau năm 91 cũng được buff xong rồi, nó đi, nó quất Crimea, cái bị nerf lại. Hiện đang tới Trung Quốc. \n\n**[43:54]** Mà cái này sẽ ảnh hưởng gì? Tới thế sẽ ảnh hưởng là thị trường thì nó sẽ khó khăn. Theo cái hướng nó sẽ favor một số nước nhất định. Không biết Việt Nam, Việt Nam hiện nay trong top 4 mấy cái nước có delta import-export với Mỹ vẫn cao, nhưng mà vẫn được buff. Không biết có được ăn nhậu gì không, nhưng về cơ bản thì mọi người sẽ chạy chậm với tiền mình hơn. Thì hai cái hướng chính nè. Một hướng là công nghệ nó ra, nó replay liên tục để trường hợp mà cái cụm từ này lại được gọi tên lần nữa. \n\n**[44:35]** Với cả cái xu thế về kinh tế toàn cầu đang dày, anh đáng là nó sẽ đi kèm với cái gì mình từng nói với nhau. Thị trường càng ngày càng khó tính, được proven qua cái này ha. Dễ dàng thấy với mình thì mình sẽ phải behave như thế nào. Mời Tom lên show hàng những kỹ năng của Tom. Anh nghĩ là mấy anh em trong team sẽ cần đấy. Anh em trong team mình sẽ cần những kỹ năng mà Tom nó được từ cái làm việc với ai ha. \n\n**[45:31]** Trong team mình hiện tại á, có một số cái mình không nói về hướng phát triển của software nữa nha. Cái đó thì nói với nhau suốt rồi. Nhưng mà trong quá trình làm việc với Tôm, anh nhận ra Tôm có một kỹ năng rất hay. Đó là gần như nguyên cái life cycle của chuyện làm phần mềm, một mình Tôm gần như dùng khả năng viết code, tự động hóa bằng tool, tự mình viết agent luôn. Thì gần như cả quá trìn h từ dev ban đầu, capture cái insight dự án, xong rồi lên planning.\n\n**[46:07]** Cả các thứ, Tom xử lý rất OK. Nên hiện tại anh muốn em show một tí [âm nhạc] về cái approach của em trong quá trình làm việc. Khi em nhận được đề bài cho tới lúc em đến cái planning của em, nó như thế nào, em đã làm ra sao? À, OK, chắc để em share screen. Hy vọng không có gì nhạy cảm. Anh nghĩ là mình lấy luôn cái đề bài mà tí nữa mình sẽ đi sâu, sẵn đó. Ô, đề bài chơi cái đó luôn đó. Mình đang không biết là cái gì đó, mình cũng chưa đi sâu luôn. Thì giờ cái phong hợp nhất đang gần như là zero.\n\n**[46:55]** Nó có một cái ví dụ về đề bài thôi đấy, cho tới lúc mà em đến cái kia như nào. OK, để em share screen, tìm lại cái chỗ đó, đúng không? OK, thông thường, logic phía em là như thế nào? Mình có data, mình muốn gỡ ra những ý của cái data này. Nếu mình có mấy cái ảnh này, thì ví dụ em sẽ cởi hết chỗ này, sau đó extract ra. Cái app này nó có những cái gì mình sẽ phải để ý. OK, sau đó là những direction mình muốn cho nó, giải thích cho mình. Vì luôn luôn là có thể mọi người xem cái app này.\n\n**[47:59]** Có thể là Airbnb, hoặc là dạng app cho personal trainer và lifestyle trainer. Thì ví dụ ở đây, em muốn tìm kiếm kiểu \"What the hell\", thì trước tiên em sẽ sắp xếp một cái prompt. Một là, nếu context là bây giờ em just about to have a meeting with client that asks us to improve their user experience. Sau đó là ý context của bên ngoài, rồi context của bên mình là \"I have some idea of what they may want\". Câu hỏi là có cần input luôn cả cái brief của cái đưa mình không? Ở đây không có. Sau đó là, this chính là cái này.\n\n**[49:08]** Nó sẽ là objective, adjective, context. This is the email they sent to us. Sau đó, em muốn cái vision chính là \"What is the vision, goals, and objectives for them asking us to help improve?\". Từ cái này, em sẽ sinh ra một số context em dùng để gửi lại cho bên phía AI. Thực tế thì cái này nó chắc em làm rồi chứ? Ờ, cái đứng ra là model nào cũng được, nhưng thinking model sẽ giúp mình kéo ra những góc nhìn mà mình không phát hiện ra. Những thinking model rất là siêu về mấy cái đấy.\n\n**[50:15]** Thì cũng hơi functional, user app-centric. Từ cái context này, em sẽ biết app nó là gì, sau đó hình dung cái vision họ đang muốn cần là cái gì. À, chính là có cái gì chi tiết hơn về user, user experience. Thì như vậy, em sẽ hỏi câu hỏi là \"What images, what bọn này muốn?\". Bọn này không muốn gì đâu, bọn này đang muốn là sẽ clone cái app này, chứ không phải là improve cái app này đâu. Cái app đã có sẵn rồi, và giờ nó muốn clone lại, mirroring đúng không?\n\n**[51:07]** Cái này là một cái đã có sẵn, lại mình làm. Với cái chuyển trường hợp này thì sẽ sinh ra một số câu hỏi như \"Are there ways their app is extending to? What are your thoughts?\". Sau đó, dần dần em xây dựng một cái picture. Từ cái picture này, sẽ sinh ra một cái prompt model cuối cùng để gửi cho bên phía làm cho mình. Ví dụ là tiếp tục về một cái dự án đã proven model shortcomings. Như vậy mình sẽ có một số cái mình phải chú ý. Chú ý bên phía mình sẽ phải thử bằng tay những cái gì nhỉ? \n\n**[52:15]** Chi tiết về concept validation này. Chính là cái gì nó work rồi dùng cái đó thôi. Concept như vậy thì em sẽ làm một cái prompt là \"Give me a proposal to pass on what I learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what the story is đằng sau họ đang muốn cái gì, and what they want us to consult, develop?\". \n\n**[53:23]** Về cái chuyện proposal này nó sẽ ra dạng như thế nào,  sau đó từ cái này, vì mỗi thứ mình dùng với AI nó sẽ có reference sẵn rồi, em sẽ copy một cái reference mình có sẵn. Là cái proposal đã làm sẵn, ví dụ trước là cái này. Sau đó là mình sẽ copy cái proposal của bên phía chẳng hạn đi, nhân đi, đi này đi nhỉ. Hình như là hình như internet đâu á? À, chắc copy nhầm này. Đúng ra là mọi người có thể ra cái này, hoặc là download. Use reference to create the proposal, or just in case, don’t take elements. \n\n**[55:16]** From but do follow the proposal format để adapt to what we learned and what they wanting to meet the trust. Sau đó, luôn luôn là mình sẽ expect cái proposal này nó không ổn định. Nó sẽ ổn định lúc mình bỏ thêm những idea, những idea mình thấy là mình có thể involve bản thân mình vào. Vì do mình đang xem khía cạnh của họ là dạng như thế này, thì bên phía mình sẽ làm được cái gì? Ví dụ skillset bên phía em khuyến khích là giỏi về user experience, user flow, data flow. Trong này mình có Mirror được cái app và optimize cái data flow, user flow chẳng hạn.\n\n**[56:10]** Hoặc là bên phía anh Thành là optimize về security và performance. Làm như thế nào để apply đúng cái project proposal này? Thêm về mấy cái kiểu good-to-have: performance và security. Nếu là dạng MVP thì mấy cái này sẽ không consider mấy chuyện hack. Là những cái design liên quan với data. Ví dụ bên phía em thì hay thiết kế data dạng là temporal state, event store, hoặc là thiết kế uniform.\n\n**[56:51]** Như thế nào để apply đúng kỹ năng của mình trên computer science về cái này? Cho nó không phải đơn giản quá, nhưng sẽ simplify, maintain cho cái chuyện cái app này nó đưa ra. Nếu mà trên đây với cái tham khảo này đi, bây giờ sẽ tới kiểu anh sẽ cần mấy cái để chốt được cái deal đúng không? Mình sẽ phải cần những câu hỏi để hỏi xem với bọn đó như thế nào. Giống như con open deal, mình open book á. Mà mình nói đi thì phải cần mấy câu hỏi đấy nữa, kèm với chuyện gần như phải suggest được cái lịch làm việc, cái milestone làm việc tiếp theo giữa mình với bọn đó.\n\n**[57:28]** Cần mấy câu hỏi đấy nữa, kèm với chuyện là gần như phải. Em làm như nào? Building rapport, sau đó là xem về burning questions chúng nó. Thì nếu mình có chuyên về nghề của mình, thì mình sẽ suy ra mấy cái câu hỏi cũng không khó lắm. Nhưng nếu mình thấy là mình hơi bị stuck, mình có cái block gì đó, thì mình sẽ nhờ AI cho hỏi mấy cái question. \n\n**[58:14]** \"So we haven’t met with this partner yet, with this client yet, but we want to make a deal with them. What should I do to help build rapport and meet the three burning questions I need to get this deal off the ground and solve any technical concerns?\". Thì cái này là good start, mình sẽ dùng cái này cho bên phía AI suy ra một số câu cho mình. Sau đó mình sẽ dựa trên cái này suy ra thêm. Nếu mình có suy ra thêm thì mình sẽ bổ sung thêm ở trên proposal và add thêm cũng realistic thôi. Không phải riêng bên phía Gemini, nhưng có một số app như Claude hoặc là ChatGPT, mình sẽ phải làm như thế nào. \n\n**[59:12]** Những cái due diligence mình sẽ phải làm như thế nào? Những cái burning question, ví dụ ở trên này mình không có context của trước, thì dùng đi. Nó kiểu như thế ngoài đó. Mình muốn đặt mấy cái goal như vậy, đứng ra là ở trên cái proposal đầu tiên, mình đang hơi nghi ngờ là mirroring là tại sao họ mirror? Nó sẽ hở ra ở trong cái intent của cái proposal đầu tiên mình xây dựng cho họ. Nên là nó sẽ liên quan với cái này. Lúc mình có thêm không nhất thiết. \n\n**[59:50]** Sẽ dùng luôn cái này, nhưng từ cái này em sẽ suy ra là, à, maybe góc nhìn về handling real-time thì sao? Maybe bên phía họ thì không phải real-time, nó sẽ kiểu như booking appointment app. Và nếu mình ghi về dạng real-time, họ có muốn đi hướng vision đó không? Để đem ra consult xem là họ muốn cái app nó kiểu đẹp hơn, ổn hơn, hay là họ muốn cái mới hơn, hoặc kiểu risky hơn? Nó sẽ là mấy cái step mình hỏi, mình chém, để xem họ reply như thế nào thôi. Và nó không có hại.\n\n**[01:00:34]** Vì nó cũng là câu hỏi hợp lý mà. Rồi, ví dụ như bước tiếp theo dev này nó hit đi, thì sau đó cái đoạn mà lên to-do rồi, kể mọi thứ thì như nào? Dạ, dạ, nó cứ hình dung. Em có một số cái cứ hình dung là cái điều này đã OK rồi. Sau đó em bỏ sung cái technical direction mình đồng ý để đi tiếp theo với họ. Ví dụ là real-time đi, \"We think they want something like this, but are open to the idea of a more real-time something like Grab, Uber for the personal trainer\". Trước tiên em sẽ xây dựng cái Technical proposal.\n\n**[01:01:33]** Như chắc không cần đâu, thông thường em sẽ xây dựng cái đó để làm rõ góc nhìn. Nhưng từ khía cạnh này, thì ví dụ là \"Help me create tasks for frontend, backend\". Tại vì cái đoạn giữa mà Tôm em sẽ figure out ra tất cả mấy cái diagram, flow, rồi tất cả mọi thứ. Phải chốt cái đấy trước, mới base cái đấy bắt đầu làm cái breakdown đúng không? Nên để đơn giản hóa hôm nay mình sẽ nhờ bên phía AI suy ra luôn.\n\n**[01:02:11]** Cái này nó là một cái góc sơ sơ, nhưng mình sẽ bổ sung thêm là \"We are planning to use Timescale and RxJS to do the sync and part real-time features of the app. We are most comfortable with React for frontend, and our house mostly uses all this in mind. Create and format tasks with description, user story, and acceptance criteria\". Mình sẽ nhờ bên phía AI viết giúp mình cái này luôn. Sau đó, nếu mình dùng thì mình sẽ copy cái copy epic là cái gì, copy story là cái gì, copy cái story, sau đó bỏ xuống cái criteria.\n\n**[01:03:44]** Cái này thì bên phía em thì làm thêm cho về cũng là cho bản thân. Vì ở đây đang là story, giải thích cái story, xử lý cái story. Lúc mình đến technical, technical nó chỉ cần confirm là nó có đạt đúng tiêu chí của story không. Vì nếu story đó nó tồn tại chung với cái vision của họ, coi như mình làm thành công bên phía họ rồi phải? Nhưng mà dự như cái sườn này là bắt đầu scale lên được một cái chất.\n\n**[01:04:23]** Chờ cho tất cả những cái liên quan cho backend. Thông thường trong technical proposal hoặc là cái context, em sẽ bỏ xuống thêm boilerplate, những cái code mình đã dùng rồi, những cái concept mình muốn apply ở trên cái app này. Với goal chính là goal của mình dựa trên goal của họ. Copy bên phía họ thì nếu có cái lúc có cái đấy xong, sau đó xây dựng mấy cái test này, thì sẽ có đầy đủ để mình breakdown đúng cái task mình cần thiết nhất. Ờ, đúng là nó sẽ độ chính xác tầm 90 phần trăm, nhưng 10 phần trăm còn lại nó sẽ bị thừa.\n\n**[01:05:02]** Nhưng mà đỡ hơn là mình bắt đầu ở chỗ kiểu zero đúng không? Rồi, chắc tới đây thôi. Giờ Tom anh bắt đầu có con, với lại khách hàng thật rồi. Tí nữa giao hết cho Tom nhé. Nay chốt tới đây thôi bạn ơi. Đây là nghĩa là bước đầu tiên để show được quá trình làm phần mềm á. Nếu mà mình có một kỹ năng mềm tốt, với lại capture được cái domain và tất cả quá trình làm việc á, có thể leverage AI rất là nhiều để mà quá trình làm ra một. \n\n**[01:05:39]** Người ban đầu lúc trước, một cái quá trình như vậy sẽ tốn khoảng 2 ngày, 3 ngày, 4 ngày gì đấy. Giờ quá trình làm xong, soạn rồi, vẽ diagram rồi, present cái idea, những hệ thống kiểu cũ á, nó nhanh rất là nhiều ha. Nên khi xong là đây là một cái skill trong team mình, Tom đang ở mức độ này. Ờ, mà Tôm đang tự tin là nó đang khoảng bao nhiêu phần trăm hả anh? Anh không rõ lắm. Mà anh nghĩ chắc đâu đó, chắc sẽ trên 50 phần trăm ha, trên 50 bé hơn 90. Hy vọng là những cái bước về sau thì sẽ có những buổi sau.\n\n**[01:06:21]** Mình lại làm thêm vài buổi với Tom. Còn giờ chắc là tạm thời dừng ở đây. Các câu hỏi có liên quan thì anh em sẽ hỏi sau. Giờ anh đây. Bye bye, hẹn gặp lại mấy anh em nhé.\n\n---\n\n### English transcript\n\n**[00:00]** Let’s get started. Hey everyone, thanks for waiting. Where are Thành and Cường? Has Cường joined the room yet? I saw he registered for Friday, but he’s up here now, right? Where’s Thành this week? Oh, he’s here, standing right there. Tuấn, Tom, hop on stage now.\n\n**[04:51]** We’re going through some articles, and suddenly this link, Tom, isn’t it great? Let me fix it. Today’s stats: 186 transactions, 1 user, 30 ICY members as usual, 5 inactive, 1482 fakes. Which are the top two most active chat channels? The top three? Who’s chatting the most? Oh, we’re in trouble! Anyone else around? Are we missing someone today? There are two old topics: one is “run and report”, I posted the link this morning, I think, let me double-check. The second is Cường’s design piece; I don’t know the details yet.\n\n**[06:03]** What’s this one about? I’m sitting here listening and totally lost. The third piece follows up on the series from before you guys wrote it, worked on it, and now it’s taken solid shape. After three months, the team’s got some small updates, and this direction’s getting a bit clearer. The system feels outdated, though; I’ll forward a link later for everyone to review via email.\n\n**[07:10]** Sign up and try it out, we’ll dive in later. That’s the plan. We’ll probably ship Hải’s piece first, then Cường’s, then Tom’s the parts Tom worked on. That’s today’s content, I think. Guys, check if anyone’s missing or if it feels too short. Anything else related we should add? Who’s not here? Has Thành joined yet? Oh, bro Thành’s on fire out of work to do. I think so too.\n\n**[08:55]** Hold on a sec, let’s wait till everyone’s here, then we’ll speed-run these topics. They’re pretty straightforward. Try to sum up your piece of concept, idea, n 10 minutes max. Don’t go overboard so we can save time for the other session. If you need more than 10, stretch it a bit, alright? Next week’s the office schedule; this week’s just regular check-in.\n\n**[09:57]** Next week, based on the sign-up list, I’ll suggest to Huy Nguyễn we do a roll-call game to get everyone in. It’s basically policy now, full attendance next week. The next part: those old projects are nearly wrapped up. Now it’s all about deploying blockchain and AI, they’re the kings of the game. Anyone wanting to work on them directly needs to plan it out. Any duplicates? Any more ideas?\n\n**[10:58]** Guess we’ll start with Hải’s piece first. Hải, go ahead and present! Uh, everyone can see my visuals, right? I’ll summarize the frontend report for January. Last December, React 19 dropped, and alongside it, Next.js 15.1 rolled out a new version too.\n\n**[12:07]**\n\nTo support both Next.js and React 19. On the React side, I see they’re working on a pretty cool API called View Transition. Browsers already have this View Transition API, but React didn’t support it before. Some libraries have built on that external API, but when integrated into React, they hit a few performance snags. Yeah, they’re waiting for React’s version of this API to improve support and tackle those performance issues more cleanly.\n\n**[12:48]** This API’s for animating transitions between two stages of a webpage. Like, if you scroll down here, it’s like this example below. The first stage has the box up top, the second stage has it below. Instead of the stage just jumping straight down, View Transition helps us create an animation effect, sliding back and forth smoothly. Same deal with images, it adds animation effects too.\n\n**[13:28]** When switching images, instead of instantly jumping to the next one. Yeah, this API’s still in the experimental phase. You’ve got to use the experimental version to try it out. But it promises a performance boost when implemented. Before, Motion supported this, but only in a vanilla environment. When scaled up, it ran into some performance hiccups because it had to handle pre- and post-set states.\n\n**[14:07]** On this front, over at SCS, there’s stuff like Deno Deploy. It used to only support static site deployment, but now it fully supports deploying Next.js too, including server-side rendering. Now we can use Deno as a replacement, blending it in to deploy an NS app. Yeah, nothing much to say on that yet. Then there’s this Transformer Z library, pretty neat. At its core, it’s about converting models.\n\n**[15:03]** At its core, it’s about converting models written in Python into JavaScript, so we can run these models directly in the browser without needing APIs or Python itself. Like in this article, it can handle sentiment testing say, positive or negative or object detection, like spotting a cat. Essentially, I think other models or pipelines can work too, as long as they’re supported by this library.\n\n**[18:33]** We had to support a setup where, network or not, all data still gets saved. So we chose to store it in IndexedDB, then push it to the server once the connection’s back. That’s the gist of it. Down here, it’s got step-by-step instructions for handling it. Doing it this way runs into a few issues, like data lists failing during sync, for example. It points out some ways to tackle those problems.\n\n**[19:22]** Something like that. Did An just post a link or something? What’s Zero? What did An just say related or not? The other day, I saw Lập mention this “local first” thing probably the same deal, right? Everyone’s tackling the same problem, racing to solve it. Next up, there’s a mention from the Win side. This one’s got an update, it supports that thing now. Before, with Node.js, you had to use a command line to compile TypeScript into JS to run it. Now it runs directly.\n\n**[20:01]** Like, it runs straight from the command line, loading the file as-is. From what I see, there’s another piece about this dev guy, talking about dependencies at MBM. They keep dropping new versions, and each one comes with breaking changes. He says it’s a pain, wants to update versions but worries the app can’t keep up. There’s not always time to fix everything. So he’s not big on React, went a different route. He says this one’s more stable, less prone to constant shifts. He prefers it over the others. Meanwhile, HTMX is popping off, sitting at number one.\n\n**[21:07]** Yeah, one last quick bit about Neon. This one’s a database service provider. They just switched from Webpack to something else. During the process, they hit some snags and realized Webpack’s got limitations. Like, it doesn’t support things well, there’s a long list of issues right here. But the end result after switching? They feel the new setup beats Webpack. First, it’s got fewer bugs, more reliable than Webpack. Second, its config is simpler. They say with just a dozen or two Webpack plugins, it makes their setup way lighter. I’m not sure why they went with that.\n\n**[22:03]** But the final outcome after the switch is they think its hot reload is better than Webpack’s. It triggers fewer checks during full reloads compared to Webpack. Second, its config is simpler. Like they said, with about a dozen or twenty Webpack plugins or so, it keeps their setup much lighter.\n\n**[23:01]** This piece mostly covers the challenges and the final results of switching from Webpack to that other thing. So, does that mean what we’re working on is shifting too? Are we moving from Webpack to this new one as well? What about that React stuff up there?\n\n**[23:50]** Switching to HTMX, huh? That’s two now., what else is there? Using Deno? And TP, is that the main framework now? Yeah, alright, it’s in. For the other articles, you guys can check them out in here. Uh, what was it—Hải, can you repost that link? Thanks, Hải, and thanks, everyone, for the replies. What’s HTMX, and why’d it get picked? HTML with logic baked in? Like, it injects some stuff straight into the HTML and uses it to handle things directly, no fuss. This trick goes back to Backbone.js and Knockout.js days. \n\n**[25:06]** A decade ago, and now they’re doing it the same way again. Any questions, guys? One minute for extra comments. Anything need updating? If there’s something, Hải, toss the link in random channel or group chat, whatever. Next up. Let’s move quick to Cường’s topic on database design. Alright, starting now. History lesson, huh? This stuff, these practices, they’ve been around since 2017.\n\n**[26:17]** Just a recap, right? Summing it up? Summing up data design skills, entity tips? Nah, not exactly data management. More like practices for handling the knowledge as we build and scale up our database. Alright, I’ll dive in. The database and the system we’re developing always go hand in hand. When our software scales up to meet business demands, we’ve got no choice but to scale the database too, to manage a huge amount of data over the years.\n\n**[26:51]** Take Amazon: in 2015, they had about 50 million data points, then by 2020, it grew to needing to handle 200 million. So why do we need these practices? When your database hits hundreds or thousands of schemas, management systems like SQL Server or other data management tools, you look at the schema diagrams, tables, or data, and you can’t possibly grasp it all.\n\n**[27:27]**\n\nThe context why were these changes applied to the system? To boil it down, there are a few practices. It’s gotta be a combo of people and systems to manage this knowledge. All of this is just practices not about picking a database management system or designing the schema itself. It’s about how we share database knowledge, store that knowledge. And when database changes get rolled out, there’s a separate system to manage those changes like continuous integration and stuff like that.\n\n**[28:02]** Those changes have to follow a few refactoring rules. Regarding no-sharing, we usually have someone called a DBA in our organization. This person manages and shares all the knowledge and changes applied to the database within the system. For example, if we’ve got multiple dev teams, say Dev 1 working on Software A and Dev 2 on Software B, both need to check with the DBA when pushing changes to the system’s database. The DBA verifies each change to see what it does and decides if it makes sense for the main database.\n\n**[28:34]** When a dev pushes their database changes, they verify with the main system to check if the APIs calling the database are affected. Then they assess whether the change is necessary. If it impacts the system too heavily, the DBA might reject it and ask the dev to update, refactor, or adjust it to fit better. Once the change is approved, the DBA documents what it means, why it’s needed, and posts a migration for the master database to start updating.\n\n**[29:14]** That data also needs to be stored somewhere everyone can easily access and search, so they understand why these changes matter. All these changes go into a repository, much like a coding project. This repository holds all the database artifacts, including scripts to run the database, login credentials, configurations, and the maximum capacity these instances can handle, plus system documentation. It’s similar to a coding project and gets managed with version control.\n\n**[29:51]** And searchable, so we know why these changes are necessary. All those changes get stored in a repository, just like a coding project. Any questions, guys?\n\n**[30:39]** So everyone can check and review the changes, their context, and history in the system, each time a change happens, the person pushing the migration creates a pull request with a description. This description explains why the change is needed, how essential it is, and which systems it’ll affect. The reviewers, mostly devs from the APIs directly impacted by this change, step in to take a look.\n\n**[31:14]** After those changes get merged into the master branch, there’s versioning so we can rollback or deploy these versions to individual systems for development, testing, and finally production. When we’ve got multiple dev instances across versions, and we’re working on separate systems, we have to check out from an instance of the master database for development use. That way, when we tweak something or add a new migration, it doesn’t directly mess with the main database.\n\n**[31:52]** At that point, we need a CI system. Whenever we change something in the instance we’re developing on, we can easily verify if the change breaks the master database. Plus, when someone pushes a new change to the master database, we get notified about schema updates or resource conflicts before it slows down our dev progress. When rolling out a database change, it involves a few steps, like modifying a database schema.\n\n**[32:25]** When pushing a change, we have to create a migration script for that database. Once the script’s merged, we update the database access code so the API can use the new change. For database changes like adding a new column, it might not always require tweaking the API’s access layer when the change goes live, since some APIs don’t need to touch that new column. For instance, if we’ve got a user table with name and address, and a new service needs to add a birthday field to the user table, older services, like one grouping users by address, don’t need API changes to integrate this new update.\n\n**[33:07]** For changes with big impacts, like introducing a non-null value or splitting a table, all dependent services need to update their data access layer to avoid errors. Take that user table from earlier, for example. If we split the user table, every service using it has to overhaul its access layer to prevent bugs. Alternatively, we could use something called a transition interface to gradually apply the new changes and roll them out without crashing the old APIs.\n\n**[33:45]** After refactoring and applying the change to the master database, we still need to notify all services using this database to prevent breaking those APIs. At the same time, folks can coordinate to resolve config issues when the master database changes. For a recap, during software development, as the software grows, the database has to grow too. To keep everyone in the loop about the info and context of each database change, we need to leverage all our knowledge to share and organize it effectively.\n\n**[34:32]** Also, all these changes have to be released thoroughly to avoid timing conflicts or resource clashes between database updates. This piece has value in its perspective. It’s probably like a dev’s viewpoint, but it focuses on shifting the main object of work.\n\n**[35:21]** The info and context of each change within this database require us to use all our knowledge for knowledge sharing and to organize it well. Plus, all these changes must be released thoroughly to avoid timing conflicts or resource clashes among database updates. That’s it. Any questions, guys?\n\n**[35:58]** It’s not about the codebase but the database, right? Leaning heavily that way. Hearing this part feels a bit meta, like it’s more relevant to big systems. For systems like ours now, it’s kinda tough to apply, huh? A system with about 20 tables already feels a bit sluggish, and looking at it gets overwhelming. Exactly. So this ties into documentation, managing versioning, and monitoring. Not version monitoring, but notifications for other teams, right? Yeah, it’s still limited, but spot on. Bringing this in makes sense because of the perspective.\n\n**[36:50]** Data management comes before the other stuff. The logic’s here, and tomorrow the data will run. Any questions for Cường, guys? If not, we’ll wrap up here. This piece has value in its perspective. Thinking about it, for backend devs wanting to strike it rich, you’ve got to stick with projects long-term. The longer the project, the more money it’s got. That’s how it seems. Sticking with a long-running project is solid at its core, but devs usually get lazy. When something drags on too long, they get bored, and their behavior turns weird.\n\n**[37:40]** Before moving to the next piece, to contribute to today’s session, here’s a keyword of the week. While reflecting on stuff, I picked up a new one, a fresh term for those who don’t know yet, like me. Today I came across this school of thought called Luddism. Luddism’s a word from the 19th century, tied to the Industrial Revolution. Industries like textiles got automated, and that ticked off some folks, called Luddites or something, who went and smashed those machines.\n\n**[38:34]** Those machines stole their jobs, their livelihoods, so they wrecked them. That turned into a movement called Luddism, where the working class pushed back against modernization trends. The next keyword digging deeper is Neo-Luddism, alongside this Luddism stuff right here. Check it out if you want, guys. It feels pretty relevant to what’s coming up for us, based on predictions from the other day.\n\n**[39:18]** When we were chatting, we figured there’d be a lot of pushback soon. On Reddit, there’s a post from two years back about a new Neo-Luddism wave popping up. Now it’s everywhere up there, huh? So, from my angle, I’d say we shouldn’t jump on this bandwagon. Progress keeps moving forward, and we shouldn’t fight the wheel of history. There’s even a subreddit called Luddism, diving straight into it. Not just about automation, but all sorts of tech pushback, feelings of being lost or out of place. Pretty interesting keyword, right, guys?\n\n**[40:08]** Not getting stuck in that, huh? Then there’s a second thing tied to this. I just sat down and dug into it recently, and it’s U.S. geopolitics. There’s a perspective on how the U.S. is evolving. I was researching capital markets, tracking where investment money flows, and stumbled into this topic. It’s the second theme, pretty interesting. Maybe you guys will care about it. This ties into macroeconomics. Turns out it stems from that angle and shifts into macroeconomics.\n\n**[40:56]** The U.S. is trending toward just two camps. One is isolationism, meaning pulling back. There’s another word we could use for it, figuring that out here. So, in this movement, what’s it saying? The U.S. will likely shrink inward, stop spreading resources everywhere for trade, and gather them up to hunker down defensively. That’s the first cluster. Right now, from all the news I’m seeing, it’s leaning that way, protectionism or isolationism. This cluster aside, the second direction I see is globalization.\n\n**[41:41]** With globalization, innovations and jobs focus more on trading with each other, boosting cross-border commerce. Nations toss stuff all over the place. The U.S. would trend outward, along with allies in that camp. Countries following that path would also open up more, moving freely everywhere. That’s the state of things, based on reports from 1945 up to recently, breaking into smaller phases.\n\n**[42:20]** In the post-war phase with Japan, after hitting them hard twice, they helped Japan and Germany rebuild after the war. That kicked off globalization in that direction. It’s the initial phase. It started there, and when Japan got too strong, right? It got dialed back by certain events. There’s this event here, plus one called VIA, with more details out there. But that’s the basics. So what’s the main idea? The core idea is there’s a trend we’re learning from history.\n\n**[43:09]** From the Great Depression in 1930 up to now, 2020, there’s a sense the U.S. is swinging back to protectionism. That’ll pull other countries along too. Everyone’s putting their own nation first. So, hopping around everywhere will slow down compared to this phase. The red line’s China, this colored line’s Russia. Russia got a boost after ’91, went for Crimea, then got dialed back. Now it’s China’s turn.\n\n**[43:54]** So how’ll this affect things? Globally, it’ll mean tougher markets. It’ll favor certain countries in that direction. Not sure about Vietnam. Vietnam’s in the top four for import-export delta with the U.S., still getting a boost. Not sure if we’ll cash in big, but generally, folks will move slower with their money. Two main paths here. One is tech keeps pumping out stuff, replaying constantly, so this term might pop up again.\n\n**[44:35]** Plus, the global economic trend’s getting thick. I reckon it ties into what we’ve talked about. Markets are getting pickier, proven by this, huh? Easy to see how we’ll need to adapt. Let’s get Tom up to showcase some skills. I think the team could use them. Our crew needs the skills Tom’s picked up from working with whoever.\n\n**[45:31]** In our team right now, there’s some stuff I won’t dive into about software dev trends. We’ve hashed that out plenty. But working with Tom, I noticed he’s got a slick skill. Almost the whole software dev life cycle, Tom handles solo, using coding chops and automating with tools, even writing his own agents. From initial dev to capturing project insights, then planning it out.\n\n**[46:07]** Everything, Tom nails it. So I want him to show a bit [music] about his approach during work. From getting the brief to reaching your planning stage, how’s it go, what’ve you done? Oh, cool, I’ll share my screen then. Hope there’s nothing sensitive. I figure we’ll grab the brief we’ll dive into soon, right there. Yeah, let’s roll with that one. We don’t even know what it is yet, haven’t dug in. So the starting point’s basically zero.\n\n**[46:55]** It’s just got a sample brief, up to when I get to that part. Alright, I’ll share my screen, find that spot, yeah? Cool, so usually my logic’s like this. We’ve got data, and I want to unpack its key points. If we’ve got these images, say, I’ll strip it all down, then extract stuff. What’s this app got that we need to watch? Okay, then it’s the directions I want it to take, explaining it for me. Cause it’s always possible folks see this app—\n\n**[47:59]** As maybe Airbnb, or some personal trainer and lifestyle trainer app. So here, I’m trying to figure out “What the hell,” right? First, I’d set up a prompt. Say the context is I’m about to meet a client asking us to improve their user experience. Then there’s their external context, and ours is “I’ve got some guesses on what they might want.” Question is, do we need to input our whole brief too? Not here. Then it’s this, the main bit.\n\n**[49:08]** It’s objective, adjective, context. This is the email they sent us. Then I want the core vision, “What’s the vision, goals, and objectives for them asking us to help improve?” From that, I’ll generate some context to send back to the AI side. In reality, I’ve probably done this already, huh? Yeah, any model works, but a thinking model helps pull out perspectives we miss. Those thinking models are ace at that stuff.\n\n**[50:15]** So it’s kinda functional, user app-centric. From this context, I’ll figure out what the app is, then picture the vision they’re after. Oh, it’s really about something more detailed on users, user experience. So I’d ask, “What images, what do these guys want?” They don’t want much, they’re looking to clone this app, not improve it. The app’s already there, and now they want to mirror it, right?\n\n**[51:07]** It’s an existing thing we’re redoing. With this shift, it sparks questions like, “Are there ways their app’s extending? What’re your thoughts?” Then I gradually build a picture. From that picture, I’ll craft a final prompt model to send to our side’s team. Like, moving forward on a proven model’s shortcomings. That way, we’ve got stuff to watch out for. We’ll need to manually test what, exactly?\n\n**[52:15]** Details on this concept validation. It’s just using what already works. With that concept, I’d make a prompt like, “Give me a proposal to pass on what I’ve learned about this client, their vision, goals, and objectives, and help me consolidate a direction to create a proposal. This proposal ideally isolates and connects dots: what’s the story behind what they want, and what they want us to consult, develop?”\n\n**[53:23]** On how this proposal will shape up, after that, since each AI tool we use has ready references, I’ll copy an existing one we’ve got. A pre-made proposal, say from before, like this. Then we’d copy a proposal from their side, maybe, duplicate it, tweak it here and there. Wait, internet’s out? Oh, probably copied the wrong thing. Should be, you guys can pull this up or download it. Use the reference to create the proposal, or just in case, don’t lift elements.\n\n**[55:16]** From it, but follow the proposal format to adapt to what we’ve learned and what they’re aiming for to build trust. After that, we always expect this proposal won’t be stable. It’ll firm up once we toss in ideas, ideas I think we can bring ourselves into. Since we’re seeing their angle like this, what can our side deliver? For example, my skillset leans toward excelling at user experience, user flow, data flow. Here, we can mirror the app and optimize its data flow, user flow, stuff like that.\n\n**[56:10]** Or, from anh Thành’s side, it’s optimizing for security and performance. How do we apply this correctly to the project proposal? Adding in some good-to-have stuff like performance and security. If it’s an MVP, we wouldn’t consider hacking concerns much. It’s more about designs tied to data. For example, my side often designs data with temporal state, event store, or uniform patterns.\n\n**[56:51]** How do we apply our computer science skills to this properly? Not overly simple, but simplifying and maintaining what this app delivers. If we go with this and the reference here, now it’s like anh needs some stuff to lock in the deal, right? We’ll need questions to figure out how it works with them. Like an open deal, all cards on the table. To get there, we need those questions, plus we’ve got to suggest a work schedule and next milestones between us and them.\n\n**[57:28]** Need those questions, along with the fact we’ve pretty much got to do it. How do I handle it? Building rapport, then digging into their burning questions. If we’re sharp in our craft, coming up with questions isn’t too hard. But if I feel stuck, hitting some block, I’d ask AI for question ideas.\n\n**[58:14]** “So we haven’t met with this partner yet, this client yet, but we want to make a deal with them. What should I do to help build rapport and find the three burning questions I need to kick this deal off and address any technical concerns?” That’s a solid start. I’d use it to have the AI spit out some questions for us. Then I’d build on that. If I come up with more, I’d add them to the proposal, keeping it realistic. Not just Gemini, but apps like Claude or ChatGPT, how do we approach it?\n\n**[59:12]** How do we handle the due diligence? For burning questions, say we’ve got no prior context up here, just use it. It’s like that out there. I want to set goals like that, starting with the first proposal. I’m a bit skeptical about mirroring, why do they want to mirror? It’ll show in the intent of the initial proposal we built for them. So it ties into this. When we’ve got more, it’s not always set.\n\n**[59:50]** I’d use this as is, but from here I’d figure, maybe a real-time handling angle? Perhaps their side isn’t real-time, more like a booking appointment app. If we pitch real-time, do they want that vision? To consult on whether they want the app prettier, stabler, or newer, riskier even? It’s steps where we ask, throw stuff out, see how they reply. No harm in it.\n\n**[01:00:34]** Cause it’s a fair question anyway. Say this dev step lands, then what’s next with the to-do list and laying it all out? Yeah, yeah, it’s like picturing it. I’ve got some stuff I picture as already sorted. Then I flesh out the technical direction we agree on to move forward with them. Like real-time, “We think they want something like this, but are open to a more real-time thing, like Grab or Uber for personal trainers.” First, I’d draft the technical proposal.\n\n**[01:01:33]** Probably not needed though, usually I’d build that to clarify the angle. But from this view, it’s like, “Help me create tasks for frontend, backend.” Cause in that middle stretch, Tôm here figures out all the diagrams, flows, everything. Gotta lock that down first, then base the breakdown on it, right? So to simplify today, we’ll have AI churn it out.\n\n**[01:02:11]** It’s a rough angle, but we’ll add, “We’re plannin","title":"OGIF Office Hours #39 - Frontend updates, Database scaling, AI workflow, and Macro insights","short_title":"#39 Frontend report, DB Scaling, AI Workflow","description":"In OGIF 39, the team explored React 19 and Deno Deploy updates, database scaling with CI and migrations, Tom’s AI-driven dev workflow, and macro insights on protectionism and globalization trends.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Wed Feb 12 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/39-20250207.md","slugArray":["updates","ogif","39-20250207"]},{"content":"\n### Topics and Highlights\n\n- **Erlang automata part 2:** Minh Lưu shared a practical breakdown of Erlang’s gen_statemachine vs gen_server, using a TCP-to-Redis connection module as a case study, implemented in Elixir for clarity.\n- **AI \u0026 market landscape**: Minh Lê recapped AI agent products on Solana, emerging macro trends in Southeast Asia’s tech investment (e.g. challenger banks, neobanks), and how Y Combinator and a16z are repositioning around fintech and stablecoins.\n- **Research direction**: The team discussed how recent AI and Web3 shifts align with internal priorities, reinforcing a focus on high-signal innovation zones and how to build for impact in 2025.\n- **Dwarves of the Year \u0026 Team Awards**: Celebrated contributors for embodying the AMA model, sharing AI research, and supporting team growth. Awards also highlighted excellence in consulting, performance, and community engagement.\n- **Hybrid work shift**: Announced a post-Tết operational update, ending remote-by-default in favor of 3-days-a-week in-office commitment for Saigon, Hanoi, and Danang hubs. Emphasis on adaptability in a shifting tech market.\n- **Operation updates**: Confirmed no referral bonus for 2025. 13th-month salary was processed based on average pay. Noted that profit-sharing would be paused due to cautious market outlook.\n- **Looking ahead**: The team reflected on the importance of physical collaboration, the benefits of dense information zones, and emerging builder trends post-layoffs, from build-in-public to token-based MVPs.\n\n### Vietnamese transcript\n\n**[01:29]** Ok anh em. Lâu quá mới lên sân khấu. Lịch trình hôm nay theo kế hoạch của anh Thành là có bốn bài như thường lệ. Thấy nhắc bốn bài như mọi khi, nhưng tí nữa có tổng kết nên anh chắc chen thêm một bài.\n\n**[07:48]** Xíu đổi lịch trình nha anh Thành. Lịch cũ thì bên đó làm bài tiếp tục series Engineering, cái mà bên nghệ nhân đã chuẩn bị và trình bày. Nhưng anh thấy tạm thời dời lại. Chắc để tuần sau hoặc qua Tết.\n\n**[08:09]** Gộp thành combo, từ từ giới thiệu cho anh em sau đợt hai tháng trước. Giờ mấy topic mở rộng thế nào, làm gì tiếp thì để sau. Lên phần Biên nha, phần hai của Minh Lưu về Erlang. Bữa trước cậu ấy làm bài rồi, giờ tính review lại ý kiến mọi người.\n\n**[08:32]** Buổi trước mọi người thấy sao? Có còn nhớ gì không? Bài này 50-50. Bài Minh Lê thì chắc lấy bài Minh Lê lên xem thử.\n\n**[08:53]** Lần này, bữa trước thấy anh em team bắt đầu viết, nhìn rất ok. Cuối cùng đăng hôm nay. Nên mình có hai, hoặc hai rưỡi topic cho mọi thứ. Thiếu mấy đứa rồi. Anh đâu rồi? Mấy nhóc kia đâu, biến hết rồi?\n\n**[09:34]** Lên nào, lên tổng kết tí. Em ơi, anh đâu rồi? Đây nè. Rồi, đủ mặt, đủ mấy nhóc vô xem. Ừ, Tuấn, Tuấn đi chơi về vui không? Mua gì không? Tuấn trốn trong văn phòng rồi đúng không? Vui quá ha. Nếu đổi lịch thì làm bài Minh Lê trước nha. Hai tuần trước.\n\n**[10:32]** Thấy mấy bài đã public rồi. Anh muốn nghe trực tiếp, xem chất lượng bài market report thế nào. Mọi người xem thử luôn, anh chen vô tí về thị trường. Tiếp theo là tổng kết, có vài chính sách muốn thông báo.\n\n**[10:54]** Sắp tới có update nhỏ, rồi sau đó tổng kết năm. Inno viết bài rồi. Ngồi đọc chung xem sao. Rồi tới bài cuối của Minh Lưu. Chắc làm luôn, bài trước cũng là Minh Lưu. \n\n**[11:22]** Bài trước của Minh Lưu nói về cái gì? Finite state machine trong Erlang đúng không? Hôm trước anh em còn nhớ gì không? Đánh giá sao, cho rating một phút. Còn nhớ nói gì không? Độ tập trung thế nào?\n\n**[11:57]** Bài tháng trước hả? Minh Lưu làm về finite state machine. Còn nhớ nó quan trọng sao không? Nếu nhớ thì sao dùng Erlang khi Elixir cũng làm được? Huy Nho có bài đó không? Huy Nho có tham gia không? Anh thấy bên nhóm của em có nhắc tới làm state machine khá nhiều.\n\n**[12:26]** Huy có góp mặt không vậy? Bên chỗ Minh Lưu là bên Yolo, tụi nó đang xài cái đó cho mấy dự án bên này, đúng không? Còn bên em thì ở Ascenda, cả hai chỗ đều có xài rồi. Làm cái framework, rồi làm cái controller, tới cả stage controller nữa, đúng không? Bên mình thì đang thử mấy cái đó, kiểu như là làm sao để tối ưu hóa được cái luồng xử lý state trong mấy dự án hiện tại.\n\n**[12:56]** Chuyển kiểu round-robin đúng không? Bên này thì define mấy cái state trước, rồi cái state nó sẽ set một cái context, kiểu như là enter state hay state hiện tại là gì, để bên ngoài chỉ cần switch qua lại giữa các state thôi. Còn bên Yolo thì em chưa biết rõ lắm, em chưa xem kỹ bài của Minh Lưu. \n\n**[13:19]** Chắc được ha? Ok, có mấy người mới nhảy vô luôn kìa, đông vui rồi đây. Vậy thì chắc cho Minh Lưu lên trước đi thôi. Minh Lưu chuẩn bị đi nha, Minh Lê ngồi đợi xíu. Lên nào, Minh Lưu, nhanh lên, nói một chút thôi cũng được, vì cái này cũng quan trọng, anh em cần hiểu rõ hơn về hướng đi này.\n\n**[13:41]** Em có 10 phút thôi, 10 phút là ổn. Bài này nối tiếp cái trước đó, tí nữa em sẽ nhắc lại tại sao cái này nó quan trọng với team mình. Em muốn anh em thấm cái này thật kỹ, vì đây là một kỹ thuật không dùng nhiều trong Erlang, nên nó khá đặc biệt. Mấy cái này bên mình làm xong thì có thể áp dụng qua mấy dự án khác nữa.\n\n**[14:12]** Order Minh Lưu nha, em có slide sẵn đây rồi. Để em review lại bài trước một chút cho anh em nhớ. Trong đó có cái behavior, tức là một pattern để mình define một cái module. Nó sẽ set sẵn một số hook, rồi mình dựa vào đó để xử lý các trường hợp cụ thể trong code, kiểu như là chuẩn bị trước một bộ khung cho mấy cái chức năng chính.\n\n**[14:49]** Mình sẽ implement mấy cái hook đó. Hai cái phổ biến nhất là gen_server và gen_statemachine. Đa số module thì viết dưới dạng server là được rồi, đặc biệt với mấy cái state machine đơn giản thì gen_server đủ sức đáp ứng. Nhưng mà tùy trường hợp thì mình mới chọn cái nào, không phải lúc nào cũng xài bừa được đâu.\n\n**[15:14]** Gen ở đây là viết tắt của generic, tức là generic server đó. Mình chỉ nên dùng gen_statemachine khi thật sự cần mấy tính năng nâng cao, ví dụ như insert event—tự chèn một event vào trong stage—or là cần trigger một cái action cụ thể khi switch giữa các stage. Cái này thì gen_server không làm được tốt bằng, nên phải cân nhắc kỹ.\n\n**[15:36]** Khi chuyển từ stage này sang stage khác mà mình muốn nó tự động chạy một action sẵn, thì cái đó gọi là state entry. Hoặc là mấy cái liên quan tới timeout, kiểu như set thời gian chờ giữa các bước. Trong thực tế gen_statemachine thường được dùng khi mình muốn implement một cái gì đó persistent, tức là cần giữ trạng thái lâu dài.\n\n**[15:59]** Ví dụ như TCP connection chẳng hạn, người ta hay dùng state machine để xử lý mấy cái này. Khi mình đã có một set các state cố định và cái connection đó cần persistent, thì lúc đó gen_statemachine là lựa chọn hợp lý nhất. Còn với mấy trường hợp bình thường khác thì gen_server là đủ rồi, không cần phức tạp quá. Hôm nay em sẽ code một ví dụ cụ thể để anh em hình dung rõ hơn.\n\n**[16:24]** Như anh Minh có hỏi bữa trước, tại sao phải dùng Erlang trong khi Elixir cũng làm được cái này? Thì đúng là vậy thật, behavior trong Erlang thì bên Elixir cũng gọi lên được và implement tương tự luôn. Nên hôm nay em quyết định làm ví dụ bằng Elixir để anh em dễ so sánh, xem cái nào tiện hơn trong trường hợp này.\n\n**[16:49]** Code bằng Elixir thì nhìn nó trực quan hơn một chút. Có slide không nhỉ? Slide về gen này đây, để em cho anh em xem thêm chi tiết, biết rõ hơn cách nó hoạt động. Ok, em chèn slide vô luôn. Hôm nay mình sẽ làm một module để connect TCP tới Redis server, giữ nó đơn giản thôi, chỉ có hai stage để dễ hiểu.\n\n**[17:34]** Hai cái stage thôi, tức là nó sẽ là một module nằm giữa user và Redis server. Module này sẽ implement phương thức để connect tới đó và làm trong suốt quá trình connect tới server. Ví dụ, khi process của mình bắt đầu một connection tới server thì tất cả request từ user tới nó sẽ trả về trạng thái disconnect hết. User không biết process thật sự mình gọi tới server ra sao, chỉ thấy qua process của mình thôi. Nên nó có hai stage: thứ nhất, khi khởi động process lên thì nó ở trạng thái disconnect.\n\n**[18:21]** Mình sẽ thiết lập một connection tới Redis server; sau khi connection thành công thì chuyển sang trạng thái connect. Còn nếu connection bị lỗi hay gặp vấn đề gì đó mà đứt, thì mình quay lại trạng thái disconnect và cố gắng restart lại connection. Những request, tức event request từ client, sẽ nhận dưới dạng event. Event có thể tới lúc disconnect hoặc connected; khi tới lúc disconnect thì mình chỉ đơn giản trả về cho client trạng thái là disconnect.\n\n**[18:41]** Connection thì những request, tức event request từ client, sẽ được nhận dưới dạng một event. Event này có thể tới lúc disconnect hoặc connected. Khi tới lúc disconnect thì mình chỉ đơn giản trả về cho client trạng thái disconnect. Còn nếu tới lúc connected thì mình gửi request đó lên Redis server, lấy data trả về cho client. Tại sao cần dùng state machine? Vì mình cần mấy tính năng như insert event mà em vừa nói. Đây, mọi người thấy không, để em zoom lên.\n\n**[19:39]** Trước tiên, mình implement behavior trên state machine, define cái data trong behavior này. Nó có hai thứ: State là stage thể hiện trạng thái của module, và Data chứa dữ liệu khi state chuyển đổi, mang theo một số data để xử lý trong module. Data gồm host, port để connect tới Redis DB, và request là map chứa danh sách ID của client cùng key là ID để biết trả về cho client nào. Khi start process này lên, việc đầu tiên là connect tới Redis server, chạy trong background và trả về trạng thái disconnect cho user.\n\n**[21:04]** Trước tiên, mình define callback mode gồm hai thứ: State function—tức đặt tên hàm trong module theo tên stage, ví dụ stage disconnect thì module tự chạy vào hàm disconnect để xử lý dựa trên tên và số tham số—và state enter, tí em giải thích sau. Bắt đầu bằng hàm init, mình trả về next_event, tức module tự insert event internal_connect với data là host, port để connect tới Redis server, rồi trả ngay trạng thái disconnect. Khi trả về disconnect, nó chạy vào hàm disconnect để xử lý. Hàm disconnect với internal_connect sẽ mở TCP socket bằng gen_tcp tới Redis server, bắt đầu thiết lập connection trong background.\n\n**[21:49] C**ái state enter thì tí em sẽ giải thích sau, ý nghĩa của state function là như vậy. Xong rồi mình bắt đầu bằng hàm init như hồi nãy em nói. Trong init thì làm gì? Đơn giản là khi bắt đầu hàm, mình dùng một term là next_event. Next_event là gì? Nó sẽ tự insert, tức module này tự chèn một event vào trong tay nó. Event này là internal_connect, dùng với data là host và port để connect tới Redis server, rồi trả về ngay lập tức trạng thái disconnect.\n\n**[22:43]** Khi trả về trạng thái disconnect như vậy thì việc đầu tiên là nó chạy vào hàm disconnect để xử lý. Hàm disconnect với internal_connect này làm gì? Thứ nhất, nó mở một TCP socket bằng gen_tcp tới Redis server. Rồi mình chợt thấy hơi nhiều chi tiết quá, em gộp mấy hàm lại thử được không anh em? Anh chen vô cho anh coi mấy chỗ define hai cái stage của nó, coi hơi rối quá rồi. Chỗ nào define hai trạng thái đâu đây? Mọi người thấy đó, nó có cái list ở kia, mình có hai trạng thái thôi ha.\n\n**[23:48]** Ở đây là hai trạng thái, bốn cái overlap function của disconnect và connect. Minh có bốn overload của connect, còn hai trạng thái thì bình thường thôi. Bên Elixir thì nó overload dựa trên param list đưa vào, nó biết chạy cái nào. Chỗ thứ hai là chỗ nào chuyển trạng thái đâu? Ờ, nó nằm trong từng hàm. \n\n**[24:42]** Ví dụ hàm connect này, khi connect thành công thì nó trả về một atom next_stage, chuyển trạng thái sang connect ngay đây, cùng với data đó. Hiểu rồi, đây là chỗ gọi để chuyển từ state này sang state kia. Nhưng mà với mỗi overload function vậy, em gọi implicit thế này thì có đúng không ta? Thường mấy cái này sẽ có dạng controller để quản lý, chứ gọi chi tiết kiểu này nhìn hơi rối. Có 10 stage trong đây mà define xong gọi thì quản lý khùng luôn.\n\n**[25:01]**\n\nCái ngay chỗ này nè, ngay backend của mình á. Vì state function thì mình có một cái gọi là handle_event function. Khi chuyển sang xài mode như thế này, mình chỉ cần define một function là handle_event thôi. Ví dụ vậy thôi, mình chỉ cần define xong là quản lý hết tất cả state trong cái function đó. Nếu muốn kiểu cách như vậy, nó cho mình lựa chọn giữa việc define function theo state hoặc define một handle_event function. \n\n**[26:14]** Giờ kêu hình đâu, Bảo Hân Trần có nhìn vô cái này, có xài cái này bao giờ chưa? Để trực quan thì em có copy một cái thư viện connection, cũng tới TCP đây. Nó viết bằng gen_server, không xài state machine, theo kiểu button như vậy. Connect fail thì nó back off, rồi retry, cũng quản lý user trong một cái map. Nhưng nó hơi dài hơn tí, phải implement lại mấy thứ có sẵn ở bên này, như timeout chẳng hạn, nó phải tự implement lại hết. Rồi câu hỏi là chị hỏi em cũng mới học, mới làm cái này. Mình biết cái này lâu rồi đúng không, nhưng lúc tiếp cận thì thấy sao? Tại sao nó quan trọng? Nó define một số cách giúp mình làm tiện hơn, anh. Ví dụ như xử lý timeout, nó define dựa trên data mình trả về.\n\n**[27:47]** Ví dụ thế này đây. Khi connection của mình bị drop, mình chạy vô hàm disconnect để xử lý, nó cho mình trả về một atom timeout cộng với thời gian. Sau khoảng thời gian đó, nó tự động chạy vô đây xử lý cho mình. Ừ, đây, nó giúp mình làm mấy cái đó tiện hơn. Thay vì viết bằng gen_server cũng được, nhưng sẽ dài hơn. Ok, ok, cảm ơn Minh. Minh Trần có hỏi gì không? Ai nữa? \n\n**[29:14]** Nhờ Minh Trần code cái này nào, code Elixir nữa, nhờ anh biết có đụng tới đây chưa. Rồi đặt câu hỏi nha, phổ biến nhanh cho anh em: cái này nó như vậy, sự khác biệt cơ bản trong lập trình của nhóm mình, trong tất cả các ngôn ngữ lập trình hiện đại hiện tại thì không có ngôn ngữ nào có sẵn thư viện để quản lý state machine như con Erlang. Erlang là con duy nhất sinh ra để chạy mấy cái hệ máy tự động không xuất phát từ góc nhìn làm server đứng đó đợi gọi tới gọi lui theo mô hình client-server từ đầu.\n\n**[29:30]** Con này sẵn đúng không? Với cái vụ mà nó build-in trong đây thì lúc em lập trình, nó ra được hai thứ. Thứ nhất là nó ép cả đội học ngôn ngữ này, có tooling này sẽ hình thành một mental model, một mô hình tư duy giống nhau. Gặp đúng trường hợp thì mình moi cái tool ra xài, suy nghĩ giải bài toán theo cách này.\n\n**[30:07]** Lúc làm mấy cái model như C4 á, hồi đó cũng đụng tới đây thôi đúng không? Nó cung cấp cái tool cho mình. Thứ hai là nó unify tư duy lại với nhau. Tất nhiên gen_server thì vẫn code kiểu bình thường, anh em xài vẫn ok, không sao hết. Nhưng điểm đặc biệt của Erlang là có cái này.\n\n**[31:13]** Khi team thấm rồi, quản lý code base dưới dạng state machine, chuyển state vòng vòng mấy cái object, thì nó giúp quản lý source tốt hơn. Tránh việc code implicit, đẩy code đi vòng vòng, hồi không biết mình chuyển qua đâu. Mình tập trung vào logic, vào góc nhìn nhiều hơn là lo cái data bên dưới nó thế nào. Chứ không thì lỗi xảy ra rất nhiều trong code base lớn, không nhìn theo kiểu này là lỗi đầy ra, phải đứng ra làm lại hết. Đó là lý do con này đặc biệt, với Erlang thì đặc biệt vậy thôi. Về Elixir, mấy cái thằng kia không build thư viện này lên, nên xài vẫn phải chọt trực tiếp trên Erlang.\n\n**[32:08]** Ừ, thôi đó là tóm tắt nhanh bài của Minh Lưu. Nếu Minh Lưu làm bài tiếp theo, anh đề nghị làm cái gì phức tạp hơn xíu, thực tế hơn xíu. Bài kia có hai stage nhìn còn đơn giản. Hoặc kiếm mấy cái clip open source của tụi nó, quản lý state nhiều hơn. Có cái ví dụ của một nhóm nào đó, xài quản lý mấy WebSocket, nhiều stage lắm. Chỗ đó cũng có xài một phần gen_statemachine, một file lên tới ngàn dòng. Lúc gộp hàm lại thì thấy cấu trúc rõ ràng, không phải kiểu implement đẩy qua đẩy lại lung tung, có một con manager đứng quản lý. Anh em lâu lâu mất não quay lại nhìn vẫn dễ chịu hơn là nhìn mấy hàm tự đặt tên, nhìn mệt lắm.\n\n**[34:18]** Cái chuyện hình thành mental model chung của một nhóm cực kỳ quan trọng. Cảm ơn Minh Lưu, mời Minh Lê. Dạ, để em coi em có bao nhiêu phút? 10 phút không? Ừ, 10 phút, tại em đang có bốn bài rồi hả? Lúc trước bên team consulting có ra ý tưởng viết series hàng tuần, mỗi tuần một bài tổng hợp thông tin liên quan tới bên test mình. Nhưng nó không sâu về test quá, chỉ chung chung. Bài đầu tiên em viết từ giữa tháng 12 năm ngoái, cover lại chuyện đợt Google ra Gemini 2.0, rồi Open AI ra model chain video.\n\n**[34:38]** Mấy cái hình ảnh của nó cho người bệnh, hoặc mấy người khỏe mạnh đeo vào, như mấy cái đồng hồ anh em mình đeo để theo dõi nhịp tim ấy. Rồi bên consumer tập trung vào mấy cái bí mật, như mấy cái để render ra video giống con Sora. Ở trên crypto thì nó cũng gắn AI vào fintech, gaming, infrastructure này nọ.\n\n**[35:21]** Bên Y Combinator cũng tương tự, họ kêu gắn AI vào mọi chủ đề hiện tại. Họ đang tập trung bảo mọi người nghiên cứu stablecoin. Lúc trước thì kêu làm Bitcoin với Ethereum để thanh toán, nhưng sau một thời gian thử nghiệm trên thị trường, họ thấy mấy cái đó không hợp lý. Nên giờ họ chuyển qua nghiên cứu hướng stablecoin, kiểu đủ thị phần để mấy công ty bỏ tiền nghiên cứu, rồi build giải pháp thanh toán.\n\n**[36:21]** Ở đây em nói sơ về một product, một cái AI agent platform trên Solana. Họ khẳng định đây là nền tảng cho mình tạo mấy con AI agent. Mấy con AI agent này giúp quản lý ví, tạo coin, tự trade, nói chung là tự động hóa mấy thứ mà dân crypto với DeFi hay làm. Product này giúp người ta làm vậy dễ hơn với sự hỗ trợ của AI. Đó là bài thứ nhất, qua bài thứ hai. Bài thứ hai trong bốn bài, nhiều lắm. Giờ câu hỏi chính là qua bốn bài tháng vừa rồi, em nghĩ cái gì benefit team mình?\n\n**[37:13]** Ừ, em nghĩ chắc mình đang đi đúng hướng, tập trung vào mấy công nghệ AI, làm quen với agent, blockchain này nọ. Nó đang phát triển rất mạnh, đang bùng nổ thị trường, rất hot. Sắp tới chắc cũng có nhiều product tập trung vào đó. Còn mấy mảng như Y Combinator hay a16z đề xuất thì hơi vượt xa tầm với của thị trường mình, nên cũng khó nhảy vào. Nhưng tuần vừa rồi em coi một cái thống kê sơ về nguồn tiền chảy ra chảy vào ở Đông Nam Á, quý cuối 2024 vừa rồi, thì đây là top mấy ngành đang được đổ tiền vào nhiều.\n\n**[38:23]** Ngành thứ nhất là challenger banks. Nó khác ngân hàng truyền thống, tập trung hoàn toàn vào ngân hàng số. Ở Việt Nam mình hình như có một cái ngân hàng số, nhưng không được định nghĩa là challenger bank hoàn toàn. Như mấy thằng Timo này nọ, nó được gọi là neobank nhiều hơn. Challenger bank là kiểu có giấy phép hoạt động như ngân hàng thật, tự đưa ra sản phẩm mới trong ngân hàng số của nó. \n\n**[40:08]** Còn neobank ở Việt Nam thường có ngân hàng truyền thống đứng sau, bật giấy phép, không tự phát hành sản phẩm ngân hàng được. Ở Đông Nam Á, mấy VC đang đầu tư mạnh vào challenger banks, vì họ nghĩ nó giải quyết vấn đề tiếp cận tài chính cho mấy vùng không có điều kiện, hơn là crypto.\n\n**[40:30]** Nghe nói em có kiểm tra lương bổng, mấy con số lương phải trả cho ngành IT ở các nước Đông Nam Á. Bên Philippines đang có giá cả khá cạnh tranh, dân số thì đông, giáo dục cũng đang phát triển ổn. Ngành nhân lực IT của họ đông lắm, làm cho mấy công ty nước ngoài, tiếng Anh thì rất tốt, mà giá lại cạnh tranh hơn so với mấy nước Đông Nam Á như Việt Nam mình.\n\n**[41:26]** Việt Nam mình vừa rồi quý 4 bị giảm đầu tư khá nhiều, tới hơn 80% luôn, còn Philippines thì tăng mạnh, phát triển lắm. Họ đang đẩy mạnh thương mại điện tử, mấy cái digital, giống kiểu mình cách đây 3-4 năm trước. Giờ họ được đầu tư nhiều. Ok, interesting, kéo lên trên tí. Vậy mấy phần trên anh thấy toàn liên quan tới tiền, đúng không? Banking, ngoại tệ, rồi finance, toàn dính tới tài chính. Đông Nam Á chắc đang đầu tư mạnh vào mấy ngành này.\n\n**[42:39]** Nếu vậy, tuần tới mấy bữa nữa ngồi xem tiếp, soát thử danh sách tiềm năng, coi mấy cái ecosystem map hay system hiện tại của tụi nó, tìm được không? Dạ, để em kiếm thử, mấy report thường có mấy cái đó. Ừ, ok. Còn gì nữa ngoài cái này không? Thấy tuần này là tuần Giáng sinh, ít tin tức, cũng chán. Em có để mấy product blockchain mới, đang được người ta đổ tiền vào nhiều, lock vốn nhiều. Blockchain hả? Cho anh xem thử coi. Liquid rồi, ok, phút phút, tụi nó share trong kênh chat rồi, ok lắm.\n\n**[44:19]** Nếu vậy có hướng này. Thật ra để viết bài này, anh thấy có góc nhìn thế này. Nếu được, mấy anh em ngồi xem, em có thể trả lời theo góc nhìn: slogan của team mình từ đầu là “empower innovation”. Tức là mình tìm mấy cái innovation đang xảy ra, để có cơ hội làm chung với tụi nó, hoặc hỗ trợ, thậm chí invest luôn thì quá đẹp. Cái interest lớn nhất khi anh em làm cái này là tìm xem innovation đang ở đâu. Thay vì điểm tin chung chung, em có thể chỉ rõ mấy điểm nhịp của market, chỗ này chỗ kia, innovation sẽ diễn ra ở đó. Trong đó, business model hay financial model là gì, trả lời đúng trọng tâm của mình, cũng là kiến thức dễ ping bài toán kinh doanh.\n\n**[47:25]** Coi thử hướng đó nha. Ok, cảm ơn Minh nhiều. Trước giờ mình thiếu cái đó, kiến thức trôi nổi, không tập trung. Góc nhìn này sẽ giúp team hiểu rõ, khi soát dự án cứ nhìn kiểu: chỗ nào sale, chỗ nào có tiền. Cách nói khác là innovation đang ở đâu, mọi người làm gì mới ở đó. Ngành software của mình, chỗ nào đang build mới thì mình mới có cơ hội nhảy vào review. Còn mấy cái bùng nổ quá thì chỉ làm retainer, dán dự án, chán lắm. Ok, chắc hết phần này. Giờ tổng kết năm nhanh xíu. Mấy anh em ra ngoài offline là xong. Câu hỏi nhanh: Huy với Thành chuẩn bị mấy phần rồi, out hết chưa? Chắc hết rồi. \n\n**[47:50]**  Năm nay thị trường thay đổi nhiều quá, mấy cái assumption trước đây nó thay đổi hết, cũng không biết đâu mà lần. Nhân sự mình cũng có sự chuyển dịch tương đối, đúng không?\n\n**[48:53]** Nhiều thứ định hướng của mình cũng shift đi, từ mô hình cũ tập trung vào enterprise, nhưng AI ra đời thì wipe hết thị trường luôn, đủ trò hết. Định hướng sắp tới mấy em nhìn chung thấy thị trường đang chuyển dịch theo hướng tiếp cận tài chính, blockchain. Nói chung AI đang hot, nhưng tính ứng dụng vào doanh nghiệp chưa nhiều lắm, đang bùng, mọi người thi nhau bùng thôi. Còn hướng chính vẫn là đánh về tài chính.\n\n**[49:55]** Hướng đó thì vô tình Engineering của mình đi research cái đó, học cái đó, đang diễn ra nhiều. Nên Year-end hiện nay công bố luôn nhờ, có bao nhiêu giải? Năm nay team có bốn giải, còn một giải trong phần community. Vẫn có một giải cao nhất thuộc dạng kiểu vầy cơ. Còn mấy giải kia thì dạng honorable mention. Tất cả giải thưởng này rút ra là để vinh danh mấy bạn high performer trong năm vừa rồi qua các mảng chính của team mình. Chắc để công bố luôn, bảy giải từ bốn giải là gì?\n\n**[51:05]** Một giải đội hả? Ba giải còn lại gì? Giải thứ nhất cho bạn execute cái model AMA của mình tốt nhất vừa rồi. Giải thứ hai thuộc về giải thí sinh cho team lead, bạn nào dẫn dắt team mình ok nhất, được number one. Giải thứ ba thì cho bạn perform tốt nhất ở bên phần consulting. Project có hai performance được anh em trong team đánh giá cao và khách hàng đánh giá cao vừa rồi. Giải thứ ba thì cho tập thể team nào đó perform cộng, có hai đầu: một là perform để ghép được phần inhousing, hai là nâng cao độ tin nhiệm và upsell thêm cho team đó.\n\n**[53:38]** Giải cuối thì cho community, mấy bạn supporter không thuộc official team nhưng có setting với mình, tham gia hoạt động, sharing cá nhân. Ok, vậy là bốn cái đúng không? Developer of the Year tức là mô hình MMA: meaning, mastery, autonomy. Giải thứ hai dành cho hướng style research, là giải động team lead. Giải thứ ba là consulting, ba giải cá nhân và một giải đồng đội là dự án thấy có impact nhất. Mấy dự án khác không impact bằng thì không tính sau nhé. Cuối cùng là giải community, mà community ở đây đâu có ai đâu nhờ? Năm vừa rồi thấy ít mà, phải không? Ừ, thật ra là có bạn làm việc với team mình khoảng mấy tháng thôi.\n\n**[54:26]** Rồi, chắc làm cái công bố nhanh thôi, mấy anh em in-out cái đó cho dễ, ok không? Chứ anh làm cho chuẩn thì lâu lắm, rồi tính tiếp. Cho xin cái reaction cái nào, zalo hay slack để mình capture lại nhờ. Hai con VIP kêu rồi, ok. Rồi, good. Giải thứ hai nhờ Thành công bố cái gì hay giải gì nhờ, cần soát tin rồi check, ok.\n\n**[55:58]** Cho phút lead xin cái reaction. Ủa, còn setting này là cá nhân hả? Đúng rồi. Ok, có thể điểm sơ qua được không? Trước giờ performance mình thấy cũng như mấy năm trước thôi, không biết cụ thể sao, mời. Ừ, vừa rồi thì chắc tí nữa Huy sẽ cho comment chi tiết hơn. Nhưng đánh giá tổng quan mà Thành xem được của Phúc vừa rồi thì thực ra trước giờ, mấy năm trước Phúc đa phần làm internal project với consulting, chủ yếu đợt này qua làm bên team Yolo.\n\n**[57:21]** Thực ra với role nó hơi yếu đúng không, so với expectation dạng depth của backend các thứ. Trước đó role của Phúc focus chủ yếu liên quan đến iOS thôi. Ban đầu thì ông này bên đó cũng skeptical về Phúc, nhưng thấy bảo adapt rất ok. Trong đâu đó năm sáu tháng, anh em bên đó chắc đánh giá ưng nhất, mà nghe nói cũng khó tính. Chi tiết thì chắc Huy có comment chính xác hơn, vì trong project của mình thì Huy chọn mà. Mắt em comment là sao, anh này comment về em mà. Thật ra em nghĩ mấy bạn làm thì cũng tương đối nhiều, mặt kháng giá, lập này nọ.\n\n**[58:46]** Nhưng em nghĩ chỗ anh Thành chọn Phúc là vì thấy sự vừa expectation của mọi người tí. Lúc trước Phúc cũng có làm dự án, nhưng khá im, không giao tiếp vào xong thôi. Dù tài năng rất tốt, nhưng trong năm nay thì có nhiều cái vượt xa mức đó. Ví dụ thành quả là mấy cái style khác, loop rồi, đi mấy cái style JavaScript, TypeScript các thứ, dù lúc trước không liên quan lắm. Rồi còn liên quan đến việc bắt đầu có mấy cái cần soát với khách hàng, deadline kiểu này không ổn. Đỉnh nhất là vừa rồi, chỗ bảo muốn làm cái này, nhưng Phúc bảo không kịp đâu, từ từ làm. Đó là dấu hiệu ban đầu, rồi cũng bắt đầu có việc trọn Phúc cho mấy cái phát triển vừa bực hơn, ghê hơn, chứ không hẳn là nhất. Nhưng expectation của Phúc có vẻ tốt nhất. Ok, đúng là Superman ha. Phúc có lên phát biểu không?\n\n**[61:03]** Chế độ này ngồi sao được, em gắn headphone vô. Em cũng thấy hơi bất ngờ, tự nhiên được cái này. Đang bắt xe xuống cái từ thiện, em cũng không biết nói gì, mà thấy có nhiều hết nghe rồi. Ô, Ngọc Thành vô nè, lâu mới gặp Ngọc Thành, nghe Phúc ơi. Thức nó sao giờ, nó mute luôn rồi. Ở ngoài đường thì hiểu cảm nghĩ là giải thứ hai, em nghĩ cái này có xứng đáng không, thấy hơi ngại, chào đúng không, khó quá. Đủ để anh nhận hội. Dạ, đúng rồi. Giải tiếp theo nhờ Huy, xin mời cho team, thì vô cho bên key team của bên Yolo. Ok, team Yolo tức là như nào?\n\n**[62:28]** Là Yolo đang nghĩ nó là một cái tiêu biểu nhá. Ừ, đồng ý. Giải cuối cùng rồi, còn lại thì trao bạn này. Cho ai cũng cần đạt thì cũng có khoảng thời gian đâu đó 2-3 tháng làm intern với mình, chủ yếu là em research liên quan đến mấy thứ liên quan đến phần mềm.\n\n**[63:20]** Thực ra trước đó, Đạt cũng có trước khi join mình làm intern thì cũng tương đối active trên server mình rồi. Kể cả sau khi kết thúc intern, vẫn tiếp tục với mấy cái về build trên AI. Những bài share thì được mấy anh em phía core fork, hướng lại tương đối nhiều. Một tiêu biểu mà bên cộng đồng mình đang muốn khai thác, và Đạt chắc nên phát biểu tí. Đạt ơi, team có dành phần quà community dành cho em, giờ đang thế nào, đang ở đâu rồi?\n\n**[64:26]** Em đang ngoài đường, chưa tới giờ hết đi họp hết trơn rồi à? Em cảm ơn mọi người đã dành thời gian cho em. Phần lớn thì anh Tom với anh Thành giúp em rất nhiều trong câu chuyện onboarding với cả share lại cho team sao cho ok, kiểu có thể delivery được kiến thức cho mọi người. Còn gì nữa không? Dạ, chắc không, em nghĩ như vậy. Cảm ơn Đạt đã dành thời gian ha.\n\n**[65:11]** Ở style với mấy anh em, việc mình đọc cái gì hay thì chia sẻ cho mọi người, rất là appreciate mấy phần của Đạt. Even là hiện tại, mấy anh em thấy trong hoạt động chia sẻ kiến thức trong team, cái đó là cái chính. Tại hướng đi innovation nó cứ thay đổi suốt, mình kiếm được người đồng điệu, cơ hội làm việc trực tiếp thì chưa đến. Nhưng về cái chung, thấy những thứ thú vị, mình ngồi chia sẻ với nhau, đó là cái rất đáng quý ha. Cảm ơn Đạt. Chắc tới giải cuối cùng, Thành ơi, rồi sau đó mình kết thúc.\n\n**[66:14]** Giải Developer of the Year dành cho bé Biên đây. Anh em có in-out giùm cái. Biên ơi, bên đâu rồi? Xin cho biết lý do đề cử là như nào, xin mời. Từ từ anh em, ừ, rồi. Năm ngoái, năm trước nữa thì anh em đều biết, mọi người đâu đó mình có post một bài viết muốn execute theo model là AMA, viết tắt của mastery, autonomy, với cả meaning. Những cái để giải đáp thì thấy chủ yếu quan sát là bạn nào trong team execute được model đấy tốt nhất trong khoảng 1 năm trở lại.\n\n**[68:23]** Cái gì đấy, một cái mà team đang muốn triển khai trong giai đoạn mà AI có thể giúp mình làm phần unit work, rồi solution với design system các thứ. Vừa rồi nó quan trọng hơn đấy. Biên là một trong những anh em ở đâu đó mà đang nghĩ là execute mấy cái gọi là ninh với core goal của team rõ ràng nhất. Mọi người rất ưng khi làm việc với Biên vừa rồi, thì đó là mấy cái chính.\n\n**[69:18]** Mời Biên cho vài lời, xong rồi mình qua phần cuối cùng nhờ. Em ơi, theo comment của anh Thành thấy sao? Cảm ơn mọi người ơi, nếu có thay đổi gì thì cũng không sao rồi. Kết thúc ở đây nhé, phần của Biên dậy xong nhé.\n\n**[70:21]** Giờ tí nữa trước khi mấy anh em họp mình ra quán hết năm với nhau, thì có vài cái mới để định hướng lại sau Tết. Buổi này là buổi gặp tuần như cuối rồi nha, tuần sau đâu có gặp đâu, đúng không? Tại mình giảm thời lượng xuống còn hai tuần một lần. Mấy cái topic đang share với nhau từ cái idea đầu là mình fit mấy cái keyword để ngồi coi tiếp, học tiếp, tìm hiểu những cái mới để phát triển bản thân. Hiện tại mình giảm thời lượng họp xuống còn hai tuần một buổi, thì buổi cuối tháng 1 rồi, chặp sau Tết mới bắt đầu có buổi khác ha.\n\n**[71:49]** Hiện tại sau Tết, định hướng của team mình, mấy cái dự án mà tụi anh, phần Minh Lê á, nó vơi ra về chuyện innovation diễn ra ở những lĩnh vực như vậy. Bản thân mấy anh em trong team management cũng thấy cái đó, thật ra dự án như vậy đang dần về, mình đang build, làm hết trơn rồi. Cả những sản phẩm cũng theo hướng đó. Như vậy là sau Tết có một pha rất thú vị về chuyện làm startup, rồi build public cái trend mà sau khi nhiều engineer bị layoff. Có cái là mấy bạn đó chuyển qua tự build cho bản thân, tự kinh doanh, tham gia hội build public á, lo một cái token hoặc triển khai một quan alpha. Tự nhiên anh thấy cái flow ra, chuyện bốn thứ đó có điểm chung rất thú vị, vô tình hướng team đi theo hướng mà anh nghĩ từ đây đến giữa năm sẽ đẩy nhiều hơn, trên vực tài chính in general.\n\n**[73:07]** Nếu add cái framework đó vô thì gần như nó là một button repeatable, mình vừa có thể làm consulting trên đó, vừa apply hết kiến thức engineering vốn trước giờ khi làm mấy sản phẩm bình thường. Sản phẩm trước giờ chỉ làm trên dataset là list hay array thôi, giờ mình sẽ làm nhiều thứ khác, giải bài toán scale lớn hơn, làm application nhiều hơn được ha. Nếu muốn tự detect, tự lo cái trend của mình cho thị trường đầu cuối thì vẫn được luôn. Cái hướng rất thú vị, chắc để sau Tết, mấy buổi họp kế tiếp sẽ bắt đầu tiết lộ cho anh em. Với định hướng đó thì kỳ vọng mọi người cũng theo đó, thường định hướng của team sẽ quyết định tới nhân sự rất nhiều.\n\n**[74:28]** Nửa là sẽ đổi chính cho đầu năm sau. Với suy nghĩ như vậy, hiện nay anh muốn tạm thời quên đi cái chuyện team mình là team làm việc remote default nữa. Tức là không còn default là vô thì sẽ bị remote với nhau. Anh mong muốn sau mấy buổi qua, team mình sẽ setup lại sao cho mình sẽ sắp xếp lại cái loop của mình tí, với mấy dự án hiện nay đang hơi chểnh mảng, chưa biết giải quyết thế nào.\n\n**[75:15]** Toàn bộ dự án hơi cảm giác nhẹ nhàng á, anh đang gắn cái mác đó là retainer rồi check, cứ vô trỏng ngồi bình thường bình thường. Thì muốn có một cái policy công bố với anh em: từ sau Tết, mọi người sẽ phải đảm bảo lên văn phòng, với các bạn đang ở Sài Gòn nhé. Ở Hub Sài Gòn, mình sẽ không còn set cái chuyện làm việc remote default nữa, mà sẽ làm việc theo hub được ha.\n\n**[76:07]** Hai cái hub official đang có, even là ba cái: Sài Gòn, Đà Nẵng, Hà Nội, thì sẽ phải kiếm chỗ ngồi lại với nhau, resume lại physical connection. Anh em sẽ phải commit 3 ngày một tuần. Hub Long An thì sao? Hub Chân Giang mấy đó thua mấy đó thua mấy nơi. Trong giai đoạn vừa rồi, cảm giác khi mọi người làm remote thì có cái alone zone rất ok. Alone zone sẽ work khi kiến thức đã sẵn, không thay đổi.\n\n**[77:05]** Nhưng khi thị trường thay đổi thì mức độ trao đổi thông tin với nhau, nơi nào diễn ra càng nhiều thì nhân sự ở đó sẽ dễ thích nghi và phát triển hơn. Vì vậy, mấy anh em đang làm remote có khả năng rất cao là sẽ bị tụt lại, chưa biết sao. Có tuyển lại đó, anh sẽ post lại requirement sau cho chỗ Tân Nhật hả, Tân Nhật đúng không? Sẽ sắp xếp lại. Mấy anh em giai đoạn vừa rồi làm remote, khi thị trường không thay đổi về kiến thức thì có alone zone để tập trung làm việc rất thoải mái. Nhưng khi thị trường đổi tí, nơi nào thông tin diễn ra nhiều hơn thì anh em sẽ phát triển dễ thích nghi hơn.\n\n**[77:43]** Đó là lý do tại sao nó diễn ra như hiện tại. Hoặc mọi người phải cực kỳ active trên online, hoặc phải gặp nhau offline, đó là cái buộc ha. Với policy đó, anh em sẽ có 1 tháng để xem thử thứ của mình như nào. Với cái đà thị trường chuyển dịch, event là Meta mới layoff, mới bị magaling, không update thêm đống người, 50% của đó, 3600 người, bỏ hết. Tất cả doanh nghiệp đều rất skeptical về chuyện phát triển cái gì mới, nên cơ hội làm remote hoàn toàn đang hạn chế dần.\n\n**[78:35]** Đang không request mấy anh em thay đổi liền, nhưng anh đang có kế tiếp trước về chuyện như vậy. Trước nhất, anh em đang ở Hà Nội và Sài Gòn sẽ phải đảm bảo ngồi với nhau. Anh sẽ cố gắng sắp xếp team theo khu vực để mọi người resume lại cái của mình ha.\n\n**[79:29]** Đó là 3 ngày/tuần. Trước mắt giữa mấy anh em với nhau thì chỗ anh Thành sẽ… Anh Thành từ khoảng giữa năm thôi, không, anh Thành sẽ relocate lại về Sài Gòn là một. Huy Nguyễn thì đang run cái office ở Sài Gòn rồi. Mấy bạn hay lên office Sài Gòn không vấn đề gì lắm. Hiện tại sẽ test trước với cái đó, nó là policy chính thức luôn sau Tết nhé. Với mấy bạn mà tụi anh biết là đang ở Sài Gòn thì sẽ require cái đó ha.\n\n**[80:14]** Đó là thay đổi chính nhất trong vận hành. Chuyện thứ hai có thay đổi nữa là năm nay sẽ không có ref sharing. Những năm trước thì mình có chương trình ref sharing, năm nay chỉ có lương tháng 13 thôi. Cách đây khoảng tiếng rưỡi là anh đã gửi lệnh đi rồi, giờ mấy anh em đã nhận được lương tháng 13 của mình rồi. Nó là trung bình lương 12 tháng thôi, bạn nào vô trước thì theo tháng, đủ là đủ. Còn sharing năm nay thì không có.\n\n**[81:26]** Một chương trình khác, sharing là khi doanh thu giữ mức cũ hoặc cao hơn, thì thường anh sẽ chích ra bao nhiêu phần trăm trong doanh thu để chia lại theo mức seniority của bạn trong team. 8 năm thì khác, 5 năm thì số khác, mà năm nay sẽ không có cái đó ha. Toàn bộ điểm tin nhanh cơ bản của team sẽ có những thay đổi đó. Nếu anh em quan tâm hơn thì chỗ Inno có gửi một bài lên trên kia rồi. Mấy nay đọc hết chưa nhờ? Đây, mọi người nhìn màn hình nhé. Có bài điểm qua, anh thấy việc team vận hành hiện nay đang rất tốt, mọi thứ đã vào rượt với nhau hết rồi.\n\n**[82:41]** Bây giờ chỉ có lựa chọn thị trường nào, cơ hội nào để có biến động lớn, đột biến về thu nhập thôi, đó là cái chính. Còn với mô hình, cách vận hành hiện tại thì anh nghĩ rất happy với những gì đang diễn ra nhé. Nếu không còn gì khác thì chúc anh em tối nay ăn tối nhẹ nhàng, tình cảm với nhau ở hai địa chỉ. Hẹn gặp lại sau Tết với kế hoạch chi tiết hơn, recruitment mới hơn, cũng như mở ra lĩnh vực mới về tài chính. Anh nghĩ cơ hội đột biến tài chính cũng sẽ nhiều đó ha.\n\n---\n\n### English transcript\n\n**[01:29]** Alright, folks. It’s been a while since we were on stage. According to Thành’s plan, there are four presentations today as usual. I heard Phát mention four, just like every other time. But since there’s going to be a recap later, I think Thành wants to squeeze in one more.\n\n**[07:48]** Let’s shift the schedule a bit, Thành. Originally, the other team was going to continue the Engineering series the one the artisan group had prepared and presented. But I think for now, we’ll postpone that. Maybe next week or after Tết.\n\n**[08:09]** We’ll bundle it into a combo session and gradually introduce it to everyone, especially after that two-month stretch. As for the expanded topics and what’s next, we’ll save that for later. Let’s move to Biên’s part this is the second part of Minh Lưu’s talk on Erlang. He already did one last time, and now we want to review and get everyone’s thoughts.\n\n**[08:32]** What did you think of the last session? Do you still remember it? This time the vote is kind of split. I guess we’ll bring up Minh Lê’s talk to check it out.\n\n**[08:53]** Last time, I saw a few folks in the team starting to write things up it looked pretty solid. In the end, it got published today. So we’ll probably have two, maybe two and a half topics in total for everything. A few people are still missing. Where’s Anh? Where are the others? Everyone’s disappeared?\n\n**[09:34]** Alright, come on up. Let’s do a quick recap. Hey, where’s Anh? There he is. Okay, looks like we’ve got everyone. Let’s get the others in too. Tuấn, you back from your trip? Did you buy anything? You’re hiding in the office, right? Must’ve had fun. If we’re changing the schedule, then let’s start with Minh Lê’s session first. That was two weeks ago.\n\n**[10:32]** I saw a few posts already went live. I’d like to hear it directly and get a sense of the market report quality. Let’s let everyone see it too. I’ll also chime in a bit with the market section. After that, we’ll do the wrap-up. There are a few policy updates I want to announce.\n\n**[10:54]** There’s going to be a small update coming up. Then we’ll do a year-end wrap-up. Inno already wrote a post for that. Let’s read it together and see how it looks. After that, we’ll get to the final presentation by Minh Lưu. I think it’s the same one Minh Lưu did last time.\n\n**[11:22]** What was Minh Lưu’s last session about again? Wasn’t it about finite state machines in Erlang? Do you still remember anything from that? How would you rate it, just a quick take, like in one minute. Do you remember what it covered? How focused it was?\n\n**[11:57]** That was last month, right? Minh Lưu did a talk on finite state machines. Do you remember how important that was? If so, why use Erlang for it when Elixir can do the same thing? Did Huy Nho work on that one? Did he join the session? I remember your team mentioning a lot about building state machines.\n\n**[12:26]** Was Huy involved? Minh Lưu’s team Yolo is using that stuff in their projects, right? And on your side, at Ascenda, I think both teams have already adopted it. They’re building a framework, then the controller, and even a stage controller, right? On our side, we’re also experimenting with those ideas trying to figure out how to optimize state processing flow in our current projects.\n\n**[12:56]** Like using round-robin mode, right? On this side, we define the states first. Then each state sets a context, like whether it’s entering a state or which state it currently is, so the outside logic just switches between them. As for the Yolo side, I’m not too sure, I haven’t read Minh Lưu’s write-up in detail yet.\n\n**[13:19]** That should be good, right? A few more folks just jumped in nice. Okay, let’s have Minh Lưu go first. Minh Lê, hang tight for a bit. Come on up, Minh Lưu. Just share a bit, it’s important. The team needs to understand this direction better.\n\n**[13:41]** You’ve got 10 minutes. That should be enough. This session continues from the last one. In a moment, I’ll explain again why this matters for our team. I really want everyone to understand this deeply. It’s a technique not widely used in Erlang, which makes it a bit special. Once we figure this out, we can apply it to other projects too.\n\n**[14:12]** Alright, Minh Lưu, you’re up. I’ve got the slides ready. I’ll review a bit from the previous session so everyone can recall it better. That one included a behavior pattern, basically a way to define a module. You predefine some hooks, then handle different scenarios based on those. It’s kind of like a scaffold for core logic.\n\n**[14:49]** Then you implement those hooks. The two most common are gen_server and gen_statemachine. For most modules, you can just write a server. Especially for simple state machines, gen_server is usually enough. But depending on the use case, you have to be careful, don’t just use them interchangeably.\n\n**[15:14]** “Gen” stands for “generic” generic server. You should only use gen_statemachine when you really need more advanced features. For example, inserting events manually into a stage, or triggering a specific action when transitioning between stages. Those are things gen_server doesn’t handle well.\n\n**[15:36]** If you want to automatically run an action when entering a new stage, that’s called a state entry. Or when you want to set timeouts between steps. In practice, gen_statemachine is used when you’re building something persistent—something that needs to hold state over time.\n\n**[15:59]** Like a TCP connection, for instance. State machines are commonly used for that. When you’ve already defined a fixed set of states and need the connection to persist, gen_statemachine is a solid choice. For simpler stuff, gen_server is enough—no need to complicate it. Today, I’ll walk you through an actual code example so it’s easier to understand.\n\n**[16:24]** Like Minh asked last time why use Erlang when Elixir can already handle this? And yeah, that’s true. You can implement behavior in Elixir the same way you do in Erlang. So today, I’ll do the example in Elixir so everyone can compare and see which approach is more practical.\n\n**[16:49]** Elixir code looks a bit more readable. Do we have the slides? Here’s the one on gen, let me show you more detail so you can see how it works. Okay, I’ll insert the slides now. We’ll build a module that connects to a Redis server over TCP. I’ll keep it simple, just two stages so it’s easier to follow.\n\n**[17:34]** Two stages only. This module sits between the user and the Redis server. It implements methods to establish and maintain the connection. For example, when the process starts a connection, all user requests will return a “disconnected” status. The user has no idea what the actual connection state is they only see what the proxy process returns. So, two stages: when the process starts, it’s in “disconnected”.\n\n**[18:21]** It’ll try connecting to Redis. If the connection succeeds, it switches to “connected”. If the connection fails or drops, it goes back to “disconnected” and retries. The incoming requests client event requests are received as events. These can arrive in either disconnected or connected state. If disconnected, we just return that state to the client.\n\n**[18:41]** If connected, we forward the request to Redis, get the data, and return it to the client. Why a state machine? Because we need features like insert event, like I mentioned. Here let me zoom in so you can see.\n\n**[19:39]** First, we implement the behavior in the state machine and define the data structure. It has two parts: State represents the current stage of the module, and Data carries information between transitions—like host, port for connecting to Redis DB, and a map of client requests with client IDs. When the process starts, the first step is to connect to Redis in the background, while returning “disconnected” to the user.\n\n**[21:04]** First, we define the callback mode with two things: the state function which means naming the functions in the module according to the stage name, for example, if the stage is disconnect, then the module will automatically run the disconnect function to handle it based on the name and arity and the state enter, which I’ll explain in a bit. We start with the init function, which returns next_event, meaning the module will automatically insert an internal event called internal_connect, with data being the host and port to connect to the Redis server, and then it immediately returns the disconnect state. When it returns disconnect, it jumps into the disconnect function to handle things. The disconnect function with internal_connect will open a TCP socket using gen_tcp to connect to the Redis server and start setting up the connection in the background.\n\n**[21:49]** The part about state enter, I’ll explain that in a bit. That’s what the state function means. After that, we begin with the init function like I mentioned earlier. What does it do? It’s simple: at the start of the function, we use a term called next_event. What’s next_event? It means the module will insert like automatically insert an event into its own queue. This event is internal_connect, using the data host and port to connect to the Redis server, and then immediately return the state disconnect.\n\n**[22:43]** When it returns the disconnect state like that, the first thing it does is run the disconnect function to handle the logic. What does disconnect with internal_connect do? First, it opens a TCP socket using gen_tcp to connect to the Redis server. But then I realized there are too many details, so I thought maybe I should try combining a few of the functions. You mind if I jump in and show you the part where it defines the two stages? That part looks a bit messy. Where exactly are the two states defined? You can see here it has a list over there, and we just have two states.\n\n**[23:48]** Here are the two states, and four overlapping functions between disconnect and connect. Minh has four overloads of connect, but only two states, which is normal. In Elixir, function overloading works based on the parameter list, so it knows which one to run. What about the part where the state transition happens? Oh right, it’s inside each function.\n\n**[24:42]** For example, in this connect function, when the connection is successful, it returns an atom next_stage, which switches the state to connect right there, along with the data. Got it, that’s the part that triggers the state transition. But with each overload like that, calling it implicitly like this is that okay? Normally, stuff like this should have a controller to manage it. Calling each one like this feels a bit messy. If there are 10 stages and we define and call them all this way, it becomes impossible to manage.\n\n**[25:01]** Right at this part here, in our backend. Since we’re using state functions, there’s something called a handle_event function. When we switch to this mode, we only need to define a single function called handle_event. Just one function to manage all the states in there. If you want to write it that way, the system lets you choose between defining per-state functions or defining one handle_event function.\n\n**[26:14]** Now where’s the diagram, have you looked into this or used it before? For a clearer picture, I copied a connection library that also connects to TCP. It’s written using gen_server, doesn’t use a state machine. It’s built kind of like a button logic. If the connection fails, it backs off and retries. It also manages users inside a map. But it’s a bit longer, you have to reimplement some of the built-in things like timeout, for example. It doesn’t come built-in so you have to code that manually. And to answer the earlier question yes, I just recently learned and started working with this. I’ve known about it for a while, but only recently started applying it. So how did it feel when I actually got into it? Why is it important? Because it defines some structured ways that make things more convenient. For example, for handling timeouts, it defines that based on the data we return.\n\n**[27:47]** Here’s an example. When your connection drops, it jumps into the disconnect function to handle it, and you can return an atom timeout along with a time value. After that time passes, it’ll automatically jump into this function to process it for you. Yeah, so it helps make these things more convenient. You could write it using gen_server too, but that would be a lot longer. Okay, okay, thanks Minh. Minh Trần, any questions? Anyone else?\n\n**[29:14]** Let’s have Minh Trần code this, and code it in Elixir. Since you know it already, have you touched this part before? And ask questions to help spread the knowledge to the team. So here’s how it works: the fundamental difference in programming with our team is that among all the modern programming languages right now, none of them have a built-in library for state machine management like Erlang. Erlang is the only one that was born for running these kinds of automatic systems, not from the traditional client-server mindset where a server just sits there waiting for requests.\n\n**[29:30]** This is built-in, right? And with this kind of built-in feature, when you write code, two things happen. First, it forces the whole team to learn this language, and once they start using the tooling, it shapes a shared mental model a way of thinking that everyone can follow. When you encounter the right use case, you pull out the right tool and solve the problem by thinking about it this way.\n\n**[30:07]** Back when we were doing things like the C4 model, we were already touching this stuff, weren’t we? It gives you the tooling. The second benefit is that it unifies everyone’s mindset. Of course, you can still code using gen_server the traditional way, that’s totally fine. No problem with that. But what makes Erlang special is that it has this built in.\n\n**[31:13]** Once the team really gets this, managing a codebase through state machines, switching between states and objects, helps you keep the codebase much more maintainable. You avoid writing implicit code, bouncing it around in different places, not knowing where the transitions are actually happening. Instead, you can focus more on the logic and the structure, rather than getting caught up in the underlying data. Otherwise, in big codebases, bugs will pile up everywhere, and if you don’t look at it with this kind of mindset, it gets messy fast. That’s why this thing is special. For Erlang, this is what makes it stand out. In Elixir, there’s no official library built like this, so you still have to call into Erlang directly.\n\n**[32:08]** Alright, that was a quick wrap-up of Minh Lưu’s talk. If he’s going to do a follow-up session, I’d recommend something a bit more complex, something more realistic. The last example only had two stages, which still looked simple. Maybe look for some open-source code from other teams, ones that manage more complex state. There’s this example I saw from another group that manages WebSocket state—it had a ton of stages. That example also used gen_statemachine, and the file was over a thousand lines. When they consolidated the functions, the structure became clear it wasn’t just code pushing back and forth randomly. There was a dedicated manager handling everything. So even if you lose track and come back later, it’s still easier to read than trying to understand a bunch of functions with custom names. That gets exhausting fast.\n\n**[34:18]** Building a shared mental model within a team is super important. Thanks, Minh Lưu. Now over to Minh Lê.\n\nYeah, let me check how many minutes I’ve got. Ten minutes, right? Okay, ten minutes. I think I have four posts already?\n\nBack then, the consulting team had the idea of doing a weekly series—one post per week summarizing info related to our testing side. But it wasn’t too deep on testing more of a general roundup. The first one I wrote was back in mid-December last year. It covered the release of Google’s Gemini 2.0 and OpenAI’s model that generates video chains.\n\n**[34:38]** They had visuals showing it applied to patients, or healthy people wearing something like smartwatches to track their heart rates like what we wear. In the consumer space, the focus was more on secret tech, like rendering video similar to Sora. On the crypto side, they’re also embedding AI into fintech, gaming, infrastructure, and so on.\n\n**[35:21]** Same goes for Y Combinator. They’re also saying AI should be applied to everything right now. At the moment, they’re encouraging research into stablecoins. In the past, they were promoting Bitcoin and Ethereum for payments. But after testing it on the market, they realized it wasn’t a good fit. So now they’ve shifted to studying stablecoins—aiming for enough market share that companies would invest money into R\u0026D and build payment solutions.\n\n**[36:21]** Here I’ll briefly mention a product an AI agent platform on Solana. They’re positioning it as a platform to build AI agents. These agents help manage wallets, generate tokens, trade automatically basically automating the kind of stuff crypto and DeFi users normally do. This product helps people get it done easier with AI support.\n\nThat was the first post. Moving on to the second. Out of the four, there are a lot. Now the real question is out of the four posts from last month, what do I think actually benefits our team?\n\n**[37:13]** Yeah I think we’re on the right path now focusing on AI tech getting used to agent stuff blockchain and all that it’s growing fast booming really hot right now probably gonna see a lot of products diving into that soon the other tracks like what Y Combinator or a16z suggest might be a bit out of reach for our market so harder to jump in but last week I saw this report about money flow in and out of Southeast Asia for Q4 2024 these were the top sectors getting capital.\n\n**[38:23]** First one’s challenger banks different from traditional banks fully digital I think in Vietnam we’ve got one that’s sorta digital but it’s not really what they define as a challenger bank like Timo they usually call that a neobank challenger banks are the ones that have a banking license they can issue their own digital banking products.\n\n**[40:08]** Neobanks in Vietnam usually sit under a traditional bank’s license so they can’t release their own banking products in Southeast Asia VCs are pouring money into challenger banks because they think it solves the access to finance problem in underserved areas more so than crypto.\n\n**[40:30]** I checked salary data too for IT in Southeast Asia Philippines is super competitive big population education’s solid IT workforce is huge working with foreign companies English is good pricing better than Vietnam even.\n\n**[41:26]** Vietnam saw a huge drop in Q4 over 80% down meanwhile Philippines is booming ecom’s growing fast all the digital stuff kinda like where we were three or four years ago now they’re getting the investment okay interesting scroll up a bit everything above sounds like it’s tied to money right banking currency finance heavy focus on finance seems like that’s where SEA is betting big.\n\n**[42:39]** If that’s the case maybe next week we can sit down and go through some potential lists try finding those ecosystem maps or system overviews from them can we find that yeah I’ll look for it most reports have those yeah okay anything else besides that this week’s quiet it’s Christmas not much news kinda dull I did bookmark a few new blockchain products they’re getting heavy capital inflows and locked funds blockchain huh show me Liquid yeah hold on they shared it in the chat already it’s solid.\n\n**[44:19]** If that’s the direction then for writing this post I think there’s a good angle if you can frame it this way our team’s slogan from the start has been empower innovation meaning we look for where innovation is happening to find ways to work with them support them even invest if we can biggest value from doing this is spotting where innovation is instead of broad headlines we can pinpoint where the market’s pulsing here and there that’s where innovation’s happening inside that what’s the business model financial model answer those clearly it lines up with our core direction and helps us analyze business cases better too.\n\n**[47:25]** Try that angle yeah thanks Minh we’ve been missing that knowledge has been kinda scattered this perspective helps when we review projects just ask where’s the sale where’s the money another way to ask is where’s innovation happening who’s building something new there’s where we can jump in and give it a look the stuff that already exploded we just retain and patch it up boring stuff alright that’s it for this part now quick year-end wrap up everyone’s going offline after this quick one Huy and Thành got your sections ready done already looks like it.\n\n**[47:50]** This year the market shifted so much all the assumptions from before are out the window even our own people have moved around a bit right\n\n**[48:53]** Our direction’s also shifted we used to focus on enterprise but then AI came and wiped out the whole thing now there’s chaos heading into finance and blockchain is where the flow is AI’s booming yeah but actual enterprise use still low everyone’s just chasing hype main trend is still around finance.\n\n**[49:55]** Turns out our engineering team’s been researching and learning those exact topics so for year-end announcements how many awards do we have team got four awards plus one from community there’s one top-level award too others are kind of honorable mentions these are to recognize high performers across key areas of our team okay let’s announce them seven awards based on four main ones right.\n\n**[51:05]** One team award then three others first one’s for the person who executed the AMA model best second one’s team lead of the year whoever led their squad the strongest third one’s for top consulting performer two projects that got great feedback both from team and from the client third one’s also a team award either for successful inhousing or trust-building and upselling for that team.\n\n**[53:38]** Final award goes to the community folks supporters who aren’t official team members but still worked with us joined activities shared stuff so that’s four right Developer of the Year is based on MMA model meaning mastery autonomy second is research-oriented team lead third is consulting three individual awards one team award for the most impactful project others that didn’t have impact won’t be mentioned last one’s the community award and wait who’s even in the community this year felt pretty quiet right yeah someone did work with us a few months though.\n\n**[54:26]** Alright let’s do a quick announcement let people in-out easily okay if I make it formal it’ll take too long so let’s roll with it someone react on Zalo or Slack so we can screenshot it two VIPs already confirmed good okay second award Thành you got that one what was it again check the notes yeah.\n\n**[55:58]** Lead give us a quick reaction wait is this one individual yeah okay can we do a brief overview so far performance looks about the same as previous years not sure on the details go ahead yeah Huy might add more in a bit from what Thành saw and how Phúc did honestly in previous years Phúc mostly worked on internal and consulting this year moved to team Yolo.\n\n**[57:21]** Honestly his role was kinda light compared to backend expectations before he focused mostly on iOS at first folks were skeptical about him but heard he adapted well in like five six months the team over there seemed pretty happy with him and they’re known to be tough Huy probably has more accurate comments since it was his project wait is that your feedback Huy he’s talking about me well I think he handled quite a lot pushed back where needed made suggestions.\n\n**[58:46]** I think Thành picked Phúc because he met people’s expectations before he’d finish projects but stay quiet didn’t really engage his skills were always good but this year went beyond that like picking up new styles getting into loops exploring JavaScript TypeScript stuff he hadn’t touched before plus started getting into deadlines client timelines and pushing back like recently someone asked if he could finish fast and Phúc said nope not gonna make it let’s do it slower that’s a good sign and then he started getting assigned tougher development tracks not necessarily best overall but his trajectory’s impressive alright Superman Phúc are you coming up to say something.\n\n**[55:58]** Lead, give us a quick reaction. Wait, this one’s personal, right? Yeah. Okay, can we go over it briefly? So far performance seems like previous years, not sure on the details, over to you. Right, Huy might give a more detailed comment later. But based on what Thành saw about Phúc this past year, actually in previous years, Phúc mostly worked on internal projects and consulting. This time he moved over to the Yolo team.\n\n**[57:21]** Honestly, the role was kind of weak compared to backend depth expectations. Before that, Phúc was mainly focused on iOS. At first the team over there was skeptical about him, but apparently he adapted really well. In five or six months, the team there seemed to rate him highly, and I heard they’re pretty tough. Huy probably has the more accurate comments, since it was his project. Hey, this guy’s commenting about me. Actually I think those who worked on this contributed a lot, handling pressure, estimates, all that.\n\n**[58:46]** But I think Thành picked Phúc because he matched expectations well. He’d done projects before but stayed quiet, didn’t communicate much, just got it done. Super talented, but this year he went way beyond that. For example, picked up new styles, loops, worked with JavaScript, TypeScript, even though before he wasn’t really involved with those. He also started reviewing stuff with clients, catching issues with deadlines. The best was when someone wanted to rush something and Phúc said no way, it won’t make it, take it slow. That was a good sign, and then he started being trusted with tougher, more intense features. Not necessarily the best, but probably the most solid expectations-wise. Alright, Superman Phúc. You coming up to speak?\n\n**[61:03]** How am I supposed to sit like this, I just plugged in my headphones. I was pretty surprised, honestly. Was catching a ride to go do charity stuff and didn’t expect this. Don’t really know what to say, but seems like a lot of people heard already. Oh hey, Ngọc Thành just joined, long time no see. Phúc, can you hear us? He’s muted, I guess. I’m outside right now, so this is the second award. Do I think I deserve it? A bit shy to say, but honored. Yes, that’s right. Next award is from Huy, let’s go ahead and present it to the Yolo core team. Okay, Yolo team meaning?\n\n**[62:28]** Yolo’s considered a standout team. Yeah, agreed. This is the last award, the rest goes to this person. This one goes to someone who spent about 2–3 months interning with us, mostly researching software-related stuff.\n\n**[63:20]** Actually, Đạt had already been pretty active on our server even before interning. Even after the internship ended, he kept contributing to AI builds. The posts he shared got forked and reshared quite a bit by core team members. A great example of someone we want to support in the community. Đạt should probably say something. Đạt, the community award is yours. Where are you now?\n\n**[64:26]** I’m outside right now, still on the way. Everyone’s done with meetings already? Thanks so much for the time. Honestly, Tom and Thành helped me a lot during onboarding and sharing stuff back to the team in a way that could be understood and used. Anything else? No, I think that’s it. Thanks again for your time.\n\n**[65:11]** The way you’ve been sharing what you read with everyone is super appreciated, Đạt. Even now, in terms of internal knowledge sharing, that’s been one of the most valuable things. Because innovation is always shifting, and we haven’t had a chance to work closely together yet. But in the bigger picture, sharing cool ideas with each other like that is really meaningful. Thanks, Đạt. Now, final award, Thành. After that we’ll wrap up.\n\n**[66:14]** Developer of the Year award goes to Biên. Everyone, give a quick in-out reaction. Biên, where are you? Please share the reason behind the nomination, go ahead. Alright, hold on everyone, okay. Last year and the year before, as you all know, we had this idea posted in the group we wanted to execute based on the AMA model: mastery, autonomy, and meaning. The way we determine the winner is by observing who in the team executed that model the best over the past year.\n\n**[68:23]** This is something the team really wants to roll out, especially now that AI can help with unit work, solutioning, design systems, all that stuff. It’s become even more important lately. Biên’s been one of those folks who really seems to embody the core goals of the team most clearly. Everyone really enjoyed working with Biên recently, so that’s basically the main point.\n\n**[69:18]** Biên, please say a few words, then we’ll move to the final part. What do you think, based on Thành’s comments? Thanks everyone. Even if things change, that’s alright. Let’s wrap this up here, that’s it for Biên’s part.\n\n**[70:21]** Later, before we all head out to wrap up the year together, we’ve got a few new things to reorient after Tết. This is basically the final weekly sync, right? Next week we’re not meeting anymore, since we’re cutting the frequency down to once every two weeks. The topics we’ve been sharing starting from keywords and ideas are for us to keep learning and exploring to develop ourselves. Since the syncs are now biweekly, the next one will be the final one in January. After Tết, we’ll pick it up again.\n\n**[71:49]** After Tết, the direction of our team and the projects we’ve been building especially the ones Minh Lê’s been involved in are leaning more into innovation in those specific fields. Even the management team sees that. Those types of projects are starting to come in, and we’ve been building them. Our products are heading that way too. So after Tết, there will be a very interesting phase of startup-style building and joining the build-in-public trend, especially after many engineers were laid off. Some of them have started building on their own, launching businesses, participating in build-in-public communities, launching a token, or deploying alpha features. I’ve noticed this natural flow of four themes coming together, and it’s led our team in a direction I think will continue through mid-year—focused more on the finance sector in general.\n\n**[73:07]** If we plug that framework in, it’s almost like a repeatable button. We can do consulting based on that, and at the same time apply all the engineering knowledge we’ve built up from working on regular products. Products used to just be about datasets lists, arrays. Now we’re moving toward more complex problem-solving, scaling, and building real applications. If you want to self-detect and manage your own trend in the end market, that’s also possible. It’s a really interesting direction, and probably after Tết, we’ll start revealing more during future syncs. With that direction in mind, the team’s strategy will influence personnel decisions a lot too.\n\n**[74:28]** Some changes will kick in at the start of next year. With that in mind, I want to temporarily move away from the idea that our team is default-remote. Meaning, it won’t be assumed that everyone just works remotely by default anymore. After the last few syncs, I’m hoping we can reset our rhythm a bit. Some of our current projects feel a bit scattered, and we’re not quite sure how to handle that yet.\n\n**[75:15]** The whole project vibe feels a bit too casual. I’ve been labeling it as retainer work just show up and coast. So I want to announce a new policy: after Tết, everyone in Saigon needs to start working at the office again. We’re removing the remote-default setup for the Saigon Hub. Instead, we’ll move to a hub-based working model.\n\n**[76:07]** We have two, even three, official hubs: Saigon, Danang, and Hanoi. Everyone should find a place to sit together again, rebuild that physical connection. You’ll be expected to show up 3 days a week. Yeah, those are a bit different. Over the past stretch, working remotely has been great for the “alone zone.” Alone zone works when knowledge is stable and not changing.\n\n**[77:05]** But when the market shifts, the teams that communicate the most are the ones who adapt and grow the fastest. So if you’re working remotely, there’s a high chance of falling behind. We’re even thinking of reopening some hiring, I’ll repost the requirements soon. We’ll restructure things. Remote worked great when the knowledge was stable, but now that things are moving, you need those dense information zones to evolve faster.\n\n**[77:43]** That’s why things are shifting the way they are. Either you need to be extremely active online or you meet people offline. That’s the hard rule. With this policy, you’ll have one month to figure out what your setup looks like. Given the way the market is moving, Meta just laid off another batch, didn’t update the rest, dropped 3,600 people everyone’s skeptical about building new stuff. So fully remote opportunities are starting to shrink.\n\n**[78:35]** We’re not asking anyone to change overnight, but this is the direction. First, everyone in Hanoi and Saigon should start sitting together again. I’ll try to arrange teams by region so you can get back into your flow.\n\n**[79:29]** So that’s 3 days per week. At the moment, with our current members—Thành, for example he’ll relocate to Saigon sometime mid-year. Huy Nguyễn is already running the Saigon office. Folks who’ve been going in regularly shouldn’t have any issue. We’ll test this out first, and it’ll become the official policy after Tết. Everyone we know who’s currently based in Saigon will be required to follow it.\n\n**[80:14]** That’s the main change in how we operate. The second change is: there will be no ref-sharing this year. In the past, we had a ref-sharing program, but this year there’s only the 13th-month salary. About 90 minutes ago I pushed the button, so everyone should have received it by now. It’s calculated as the average of your past 12 months’ salary. For newer folks, it’s prorated by month. But no extra sharing this year.\n\n**[81:26]** The sharing program usually kicked in when revenue stayed stable or grew. In those cases, I’d take a percentage of revenue and distribute it based on team seniority. Someone with 8 years would get more than someone with 5, etc. But this year, there won’t be any of that. That’s a quick summary of the core changes in how the team’s operating. If anyone wants more info, Inno posted an article earlier have you all read it? Alright, take a look at the screen. There’s a post there. I think the way we’re running things now is going really well. Everything’s clicking together.\n\n**[82:41]** Now it’s just a matter of choosing the right market and spotting opportunities that can trigger major income jumps that’s the real goal. With our current model and the way we operate, I’m really happy with how things are going. If there’s nothing else, enjoy dinner tonight with the team at either of the two meetup spots. We’ll reconnect after Tết with more detailed plans, new recruitment, and new directions especially around the finance sector. I think there’ll be a lot of breakout financial opportunities ahead.","title":"OGIF Office Hours #38 - Erlang automata p2, market report, DOTY, Year end celebrations ","short_title":"#38 Erlang automata, AI Trends, Year-End Awards","description":"In OGIF 38, the team explored Erlang automata, AI and fintech market shifts in Southeast Asia, year-end reflections with team awards, and a new hybrid work direction for 2025.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sun Jan 19 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/38-20250117.md","slugArray":["updates","ogif","38-20250117"]},{"content":"\n### Topics and Highlights\n\n- **Session setup \u0026 check-in**: Kicked off with a casual vibe, confirming no all-hands this week and setting up three talks. Phát skipped his slot, and the team troubleshooted screen sharing for demos.\n- **AI fine-tuning overview**: Explored fine-tuning vs. retraining, using a doctor’s note example to show how fine-tuning embeds knowledge while retraining leans on token-heavy prompts.\n- **Fine-tuning demo**: Showcased a Duty 40 Mini fine-tuning job on Open AI (~4800 tokens), comparing pre- and post-tuning results, with a nod to local vs. hosted model trade-offs.\n- **Data archiving essentials**: Biên broke down archiving vs. backup for apps with 50K-1M daily transactions, focusing on metadata, cloud storage, and recovery to optimize query performance.\n- **Archiving tools \u0026 Q\u0026A**: Highlighted tools like AWS, Google Cloud, and Timescale, plus hot/warm/cold storage options (Azure, Backblaze), with audience questions on scheduling and platform quirks.\n- **Datalake foundations**: An traced datalakes from 1980s databases to today’s cloud systems, contrasting warehouse ETL (structured) with datalake ELT (raw data) workflows.\n- **Notion’s datalake scaling**: Detailed Notion’s growth to 96 instances and 400+ shards by 2023, shifting from warehouse to datalake with Debezium CDC, Kafka, Hudi, and S3 for analytics.\n- **Interactive wrap-up**: Fielded questions on datalake vs. replication, async processing, and external data handling (e.g., social media), ending with reflections on big data skillset.\n\n### Vietnamese transcript\n\n**[00:00]** Hình như tuần sau mới có all-hand. Không thấy ai tạo event gì hết, chắc tuần này cứ bình thường thôi nhá. Anh em kiểm tra xem màn hình sharing có vấn đề gì không. Hay lên luôn nhỉ? Hôm nay chắc có ba bài thôi đâu đó. Phát vừa bảo tuần này cậu không có gì mới, chắc skip hôm nay rồi. Anh em thử share màn hình cá nhân xem sao nào. Xem trước được không?\n\n**[10:42]** Theo lịch chắc anh nhỉ, để em lên trước nhá. Fine-tuning này, chủ đề này không mới lắm đâu. Bài này chỉ là 100.5 thôi, không phải 101, nên chỉ giới thiệu sơ sơ, chưa đi sâu được đâu. Tại em cũng mù mờ lắm, nên chắc giới thiệu sơ vậy thôi. Hôm nay em giới thiệu bài fine-tuning. Đây là agenda của bài này nè.\n\n**[12:24]** Introduction là nếu mọi người dùng AI, chắc có nghe tới khái niệm fine-tuning rồi. AI có mấy mô hình đa số được fit vào dữ liệu từ một ngày nào đó, với mấy cái data privacy hoặc data của domain riêng. Dữ liệu này không xuất hiện trong knowledge của mô hình nền tảng (foundation model). Để mô hình có được kiến thức đó, người ta thường dùng retraining, đúng không? Nhưng còn một cách khác gọi là fine-tuning. Cuối bài, em sẽ so sánh hai cách này, xem lúc nào nên dùng cái nào, lúc nào không.\n\n**[13:08]** Trước mắt, cứ hiểu fine-tuning như cách để mở rộng kiến thức cho mô hình nền tảng vậy. Fine-tuning là gì? Hiểu đơn giản là mọi người retrain lại mô hình, lấy một mô hình nền, đưa vào một dataset gì đó để fine-tune, nghĩa là retrain lại nó. Sau khi fine-tune xong, ta được một mô hình đã điều chỉnh, gọi là fine-tuned model.\n\n**[13:44]** Tại sao fine-tuning mang được kiến thức mới? Hiểu đơn giản là trong một AI model, kiến thức được lưu qua mấy cái mạng nơ-ron. Fine-tuning sẽ cập nhật các weight, các thông số của mạng nơ-ron đó, để nó phù hợp với kiến thức mới. Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi.\n\n**[14:19]** Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi. Khi fine-tune mô hình trong thực tế, không phải chỉ ném dataset vào rồi retrain là xong. Đúng là ra một mô hình fine-tuned, nhưng không biết cái mô hình sau retrain này có tốt hay không. Em sẽ giới thiệu một workflow mà bên ngoài thường dùng để fine-tune.\n\n**[14:59]** Cái flow này, nó gồm nhiều bước như thế này. Mọi người có thể chia thành hai cụm, hai cụm nhá. Cái flow này, em sẽ chia thành hai cụm. Em đi qua cụm một trước. Cụm đầu tiên là cụm ở bên trái, hiểu đơn giản là một base model ban đầu. Sau đó, mọi người có một dataset mới, một cái gì đó mới, mọi người quăng vô, fine-tune nó. Rồi nó ra một model, mọi người sẽ supervise nó, có nghĩa là mọi người retrain nó dưới kiểu là retrain nó. \n\n**[15:38]** Sẽ cho nó thêm kiến thức, với input này thì output sẽ ra như này. Nó sẽ ra một cái gọi là supervised fine-tuning. Sau đó, để em đi qua phần tiếp. OK, cái fine-tuning này là retrain on data, nghĩa là sẽ cho một cặp input-output trong dataset để nó học. Nó sẽ học được những kiến thức mới đó. Để bước này hoàn hảo, dataset phải được clean. Nó phải clean, không được lẫn với những cái khác. Nghĩa là nó phải specific cho cái domain mà mình muốn train nó.\n\n**[16:31]** Quay lại hình này, sau khi mọi người có một cái model đã được retrain xong, mọi người mang lên production, mọi người dùng, đúng không? Lúc này, bên ngoài sẽ sử dụng một cái system gọi là human feedback. Kiểu như là response của model này có làm bạn hài lòng không, chấm từ 1 tới N sao, kiểu vậy á. Mọi người sẽ collect cái data đó. Nó nằm ở bước này, mọi người sẽ thu thập human feedback từ cái retrained model của mọi người.\n\n**[17:06]** Dựa vào cái feedback đó, mọi người gọi bước này hơi tốn tài nguyên chút. Mọi người sẽ phải retrain một cái model riêng. Cái model này dùng để đánh giá xem response này được chấm bao nhiêu điểm. Kế đó, mọi người tới bước thứ ba, bước cuối. Ở bước này, mọi người sẽ dùng thuật toán như reinforcement learning để kết hợp với cái retrained model và cái reward model của mọi người.\n\n**[17:46]** Mọi người retrain, mọi người lại fine-tune cái model một lần nữa. Nó sẽ ra cho mọi người một cái gọi là model tối ưu. Cái vòng lặp này cứ tiếp tục, tiếp tục mãi. Mọi người có cái model đã retrain xong, thu thập human feedback, rồi kết hợp ba cái đó để retrain cái model thêm lần nữa. Càng ngày, cái model sẽ càng ok hơn với những gì mà mình muốn. Đó là cái flow mà em thấy bên ngoài, trong production, người ta hay dùng. \n\n**[18:24]** Trong quá trình fine-tuning, ,ọi người sẽ thường nghe tới khái niệm gọi là catastrophic forgetting. Nghĩa là sao? Nghĩa là khi mọi người retrain kiến thức mới vào, nó sẽ làm giảm performance với những kiến thức cũ. Tại sao chuyện này xảy ra? Như em đã nói, kiến thức của một model dựa vào mấy cái weight, dựa vào kiến trúc của cái model đó và những tham số của nó. Tham số dynamic trong một model là mấy cái weight. Khi mọi người retrain, mấy cái weight này thay đổi, đúng không?\n\n**[19:00]** Khi nó thay đổi, có phải là kiến thức cũ sẽ bị giảm bớt độ chính xác đi không? Nếu trong dataset của mọi người có nhiều dữ liệu bị overfitting, nghĩa là dataset của mọi người quá đúng, quá đúng trong cái dataset đó. Khi một người quăng cái gì mới vào, nó sẽ sai với những cái cũ đi. Người ta gọi đó là overfitting, nghĩa là nó bị fold in quá mức vào những cái training data. Khi gặp data mới, nó sẽ giảm performance. \n\n**[19:42]** Nên lúc này, bên ngoài người ta sử dụng một kỹ thuật gọi là parameter-efficient fine-tuning, gọi là PEFT. Nó có nhiều cách, nhiều kỹ thuật trong method này, như LoRA này kia. Nhưng trung quy, đa số bọn họ không phải update hết tất cả các weight trong cái model đó. Bọn họ sẽ chỉ đóng băng những layer nào không cần thiết. Họ sẽ đóng băng mấy cái layer không cần thiết, rồi chỉ update một số lượng nhất định các weight thôi. Để tránh trường hợp kiến thức cũ bị mất đi quá nhiều. Đó là cái cơ bản. Còn sâu hơn về mấy cái algorithm đằng sau, mọi người có thể tự tìm hiểu. \n\n**[20:27]** Quay lại câu hỏi lúc ban đầu, ha, nó với retraining khác nhau thế nào, nên dùng cái nào? Có cái bảng đây, mọi người có thể dễ dàng nhận ra. Retraining là dữ liệu phụ thuộc vào database của mọi người. Cứ quăng vào, quăng vào, lúc nào data cũng được update liên tục. Còn fine-tuning là mọi người retrain lại model, nên lúc nào data cũng chỉ ở cái chỗ mà mọi người đã retrain thôi. \n\n**[21:16]** Kế tiếp là customize and learning style. Nghĩa là cái retraining, mục đích của nó là cho mình một cái knowledge base để mình lấy mấy cái knowledge base đó ra tham chiếu, sử dụng. Còn fine-tuning thì sao? Nó upgrade cái não của model lên, để nó có sẵn cái knowledge đó luôn. Còn mấy cái ở dưới thì chắc mọi người tự tìm hiểu tiếp ha.\n\n **[21:57]** Em có một cái ví dụ như vậy. Ví dụ như là mọi người muốn làm một cái system để giải thích những cái note của bác sĩ, đúng không? Những cái note của bác sĩ, mọi người có thể biết là những cái note của bác sĩ nó có rất là nhiều từ chuyên ngành. Và những từ chuyên ngành đó nó còn viết tắt, viết kiểu luộm thuộm nữa. \n\n**[22:44]** Nếu mọi người sử dụng fine-tuning á, mọi người sẽ cho nó học hết tất cả những cái kiến thức luộm thuộm, những cái shorthand, những cái handwriting đó của bác sĩ. Nên khi mọi người input một cái note của bác sĩ vô, nó sẽ trả lời được rất đúng. Còn nếu mọi người dùng retraining á, khi mọi người input một cái note của bác sĩ vô, nó sẽ kiếm được những cái relevant data, mang ra đọc. Nhưng bản chất là cái model nó không hiểu được những từ đó, nên nó cũng sẽ không đưa cho mọi người một câu trả lời chính xác.\n\n**[23:16]** Mọi người có thể hiểu như này: fine-tuning là mình nhờ một bác sĩ đọc một cái note của bác sĩ. Còn retraining là mọi người đưa cho một người có kiến thức rất rộng đọc một cái note của bác sĩ. Người đó có thể kiến thức rất rộng, nhưng về mấy cái chuyên ngành, mấy cái chuyên ngành thật sự, thì nó không đủ sâu như của một bác sĩ thực thụ. Nên độ chính xác sẽ không cao.\n\n**[23:57]** Thứ hai, mọi người có thể nói, bây giờ với retraining, mình dùng một cái system prompt để list hết mấy cái shorthand của bác sĩ ra trong system prompt, nó sẽ tự hiểu thôi. Nhưng làm vậy, mọi người sẽ bị tốn token, đúng không? Tại vì khi mọi người dùng retraining, mọi người lấy hết cái retraining data ra, quăng một cái knowledge retraining vô, lại cộng thêm đống cái zero-shot, mấy cái description, mấy cái đi kèm theo nó trong một cái prompt á, thì nó rất tốn token.\n\n**[24:34]** Và khi ở trong một cái long conversation với một cái model, nó đâu phải chỉ dựa vào câu hỏi của mình đâu. Nó sẽ dựa vào tất cả các cuộc trò chuyện từ trước tới giờ của mình mà nó trả lời cho mình. Lúc này, nó sẽ dẫn tới trường hợp là nó bị limit bởi token. Đó là cái drawback khi sử dụng retraining, là nó sẽ tốn token. Tại vì mọi người cần token để chạy cái system prompt của mọi người nữa. Còn fine-tuning, bản chất là model nó đã có kiến thức rồi, nên không cần phải có system prompt.\n\n**[25:14]** Đó là sơ qua về fine-tuning. Chắc có cái demo cho mọi người xem sẽ rõ hơn ha. Bây giờ em sẽ fine-tune một cái model là Duty 40 Mini ha. Em có một cái dataset như này. Ừ, như này thì mỗi thứ nó sẽ có một cái system như retraining, rồi user hỏi cái này thì muốn nó trả lời vậy, đúng không? Em cộng 10 cái, 10 record trong cái dataset này, em sẽ fine-tune nó.\n\n**[26:13]** Trước khi fine-tune, em sẽ cho nó chạy qua một đoạn code để em estimate được. Tại vì em dùng Open AI, nên sẽ tốn tiền. Nên mình sẽ tính được estimate là nó sẽ charge mình bao nhiêu. Em dùng xong, khúc cuối nó sẽ kiểu, tầm khoảng 4800, sắp xỉ 4800 token. Cái này chỉ là tham khảo thôi, nhưng em thấy nó cũng đúng. Sau đó, em sẽ upload cái file data này lên Open AI. Nó sẽ cho em cái file ở trên cái Open AI của em.\n\n**[27:03]** Rồi em sẽ training nó. Em sẽ tạo một cái fine-tuning job. Lúc này, ở trên Open AI, nó sẽ chạy một cái job này. Mọi người có thể lên đây, mọi người đọc, mọi người quan sát. Nó sẽ không trả kết quả liền, nó sẽ tạo một cái job để pending ra đó, để trên Open AI nó fine-tune cho mình. Trong lúc chờ, mình có thể theo dõi quá trình của nó như thế nào. Sau khi xong đâu rồi, nó sẽ thông báo cho mình. Mình cứ stamp cái câu này, cứ check cái câu này để coi nó đã hoàn thành hay chưa. Mình đọc ở cái chỗ đó.\n\n**[27:58]** Sau khi xong, nó sẽ cho mình mấy cái result status. Sau khi fine-tune xong, với cùng một câu hỏi, ví dụ đây là cái câu hỏi em sử dụng, em dùng câu hỏi này. Cái câu hỏi này gần giống với một cái record trong đống dataset của em. Sau khi em chạy, nó sẽ trả lời như vậy. Nhưng trước khi fine-tune, em dùng một cái model bình thường nha, model bình thường thì nó sẽ trả lời kiểu vậy.\n\n**[28:49]** Có nghĩa là em fine-tune thì nó đã thành công. Đó là cái cách em sử dụng Open AI để fine-tune một cái model. Demo của em tới đây thôi. Anh em có câu hỏi gì không? Đúng rồi, cái này demo em xài tuning chứ để tự fine-tune bằng local mà xịn xịn thì chắc không đủ đồ. Dạ, đồ ngon nhõ thôi. Thực ra có mấy cái model trước, tô nó trên LoRA các thứ, cũng có thể demo được. Nhưng tô không, bài này easy, bài này kiểu một...\n\n**[29:47]** Lẻ tẻ trầm mấy á. Đúng, chắc cũng ổn mà. Nói chung, những đội enterprise hay không muốn tốn thời gian xây dựng GPU thì sẽ dùng cách này. Diagram GPT hồi trước, GPT-4o Mini ra thì fine-tuning đã miễn phí, dùng cái này cũng tiện lợi cho họ. Cái cửa hàng demo cho anh em là sử dụng một cái như kiểu service ấy. Open AI cung cấp service fine-tuning, đưa lên mấy cái model của nó luôn. Mình pick mấy cái model, chắc là pick model mini á. Chắc chi phí nó không cao lắm.\n\n**[30:37]** Đấy cũng là một cách. Nhưng vấn đề thực ra là mình vẫn không phải người own cái model đấy. Bản chất là vẫn host ở trên server của họ. Còn có một cách khác là tự build server và tự running. Trường hợp hôm nay đã khác. Anh em xem, hôm qua em có thử một cái model có 3 billion parameter thôi. Nhưng nó chạy hai ba tiếng, nó chưa xong đâu anh. Thực ra bài này, cái version nó đơn giản hơn một cái bây giờ, nhỏ hơn của bài trước. đầu.\n\n**[31:26]** Nó là một cái full flow liên quan đến gì ta, reinforcement feedback. Ý là cái em giới thiệu ở bên ngoài production á, là khi người ta tuning á. Người ta không phải chỉ fine-tune xong là dùng liền, người ta phải đánh giá lại coi nó có đúng không. Người ta phải cho nó vô cái cycle để càng cải tiến cái fine-tuned model nữa, kiểu vậy. Đây là một cái flow như vậy. Bản chất nó cũng model thôi, đâu có gì đâu. Quan trọng là mọi người biết được những cái cost để đánh giá cái approach thôi.\n\n**[32:05]** Tại ra nó vẫn là bài toán accuracy, đúng không? Mình chọn cách nào để làm cái output nó chính xác hơn. Những cái method như retraining hay fine-tuning, nó sẽ có những nhược điểm khác nhau. Và thực ra kể cả fine-tuning, nó cũng có nhiều method fine-tuning khác nhau. Chắc là cần đi sâu hơn để xác định mấy cái đó. Cái này vẫn hơi general. Chắc vậy, Hoàng. Nếu có điều kiện thì chắc đi sâu hơn tí nữa.\n\n**[32:48]** Sâu hơn theo kiểu là có mấy cái method liên quan đến phần retraining các thứ. Sử dụng fine-tuning method á, có một số cái method nó tương đối tiết kiệm về mặt tài nguyên. Tất nhiên, nó sẽ đánh đổi với một số thứ khác, kiểu vậy. Giới thiệu cái đó để anh em xem thử đâu đó. Mọi người hỏi, Đạt hỏi là khi nào cần fine-tuning. Nói là fine-tuning cần khi mà mọi người muốn nó có một cái kiến thức, một cái specific topic nào đó. Mọi người có thể cân nhắc sử dụng fine-tuning.\n\n**[33:39]** Nhưng trong tất cả trường hợp, em thấy bên ngoài, đa số mọi người sẽ prefer dùng retraining. Tại vì nó dễ và tốn ít tài nguyên hơn. Nhưng một số trường hợp như lúc này, cái ví dụ em nói về cái note của bác sĩ á, suppose là nên dùng tuning. Rồi tùy cái kiến trúc, tùy cái mình chia system của mình ra nhiều system nhỏ, system nhỏ nó như thế nào nữa, tùy. Có thể có một vài cái use case như kiểu chúng nó muốn host mấy cái model bé bé, model bé chẳng hạn.\n\n**[34:18]** Chỉ kiểu dành để làm một cái task cụ thể thôi. Ví dụ như phân tích thời tiết, độ ẩm các thứ để perform cái action nào đấy. Ví dụ như thay đổi cái theme của điện thoại hay để chỉ action nào đấy chẳng hạn. Có thể retrain cái model bé bé để chỉ cần làm chuyện đó thôi, không cần phải cần network các thứ gì cả. Chắc vậy. Từ giờ chắc là Biên ha? \n\n**[36:13]** Dụng cái và build cái recovery process cho nó. Chi tiết như nào thì nó sẽ có một vài phần chính. Trước tiên là cái lý do mà mình cần cái kỹ thuật này và so sánh nó với một cái quen thuộc hơn là backup. Sau đó là đi vào việc để mình build và những cái mình cần để ý, những cái gì. Đầu tiên là trên thực tế, thường có những tổ chức, những công ty mà chạy những cái app với lưu lượng dữ liệu cao á. Ví dụ như giao dịch chứng khoán này nọ. Như em ví dụ này là kiểu 50.000 transaction.\n\n**[37:12]** Như em ví dụ này là kiểu 50.000 mỗi ngày là ít á, kiểu vọt lên 500.000, triệu transaction mỗi ngày. Sau khoảng thời gian, cái lượng data nó sẽ phồng lên rất rất lớn, ảnh hưởng đến cái việc mà mình query data và ảnh hưởng đến cái trải nghiệm người dùng. Trong những cái data đó, sẽ có những cái data mà dùng rồi thì nó rất ít được access lại nữa. Ví dụ như lịch sử trên 7 năm trước chẳng hạn. Nó sẽ dẫn đến một cái vấn đề, làm sao để mình giải quyết cái đống data đó. Nên mình mới dùng cái kỹ thuật là data archiving.\n\n**[38:14]** Nó sẽ có những cái lợi ích để counter lại những chuyện bên trên. Đầu tiên là cái data mà mình sử dụng, mà nó set liên tục á, query ghi đọc liên tục á, thì nó thường tốn chi phí cao. Mình sẽ dùng cái kỹ thuật này, mình sẽ đem data của mình bỏ qua một cái chỗ khác, chi phí rẻ hơn, access ít hơn. Từ đó, nó sẽ làm tăng được cái performance của app của mình trong việc query hay aggregate data các thứ.\n\n**[39:07]** Về mặt pháp lý hay reusable, những cái data đó nó sẽ được bảo vệ an toàn, không bị ảnh hưởng bởi những yếu tố bên ngoài. Để sau này khi mình dùng lại, mình có thể lấy ra dùng được. Như mọi người hay nói, mọi người sẽ liên tưởng đến cái data backup, thường dùng trong việc restore data, restore cái system hay app nếu có lỗi xảy ra. Mà hai thằng này, nó sẽ khác nhau ở chỗ là data backup á, nó sẽ dùng cho cái việc hotfix cái system nhiều hơn. Còn cái thằng archiving...\n\n**[39:59]** Data archiving thì nó hướng về cái việc lưu trữ data một cách lâu dài. Nó sẽ có cái chi tiết so sánh như này. Để mình đi build một cái architecture, một cái system để archive data, xong rồi dùng nó để recovery lúc mình cần thì sẽ làm như sau. Mọi người thấy, nó sẽ có ba cái note chính. Thứ nhất là mình lưu data lại, mình dùng metadata để interact với những cái data đó, rồi mình bỏ lên một chỗ, ví dụ như những cái cloud-based service, cloud storage service, để mình lưu trữ cái data đó.\n\n**[40:51]** Về chi tiết, để lưu trữ cái dữ liệu á, đầu tiên mình phải xác định những cái dữ liệu cần được lưu trữ. Phải phân tích xem dữ liệu nào hay được sử dụng, dữ liệu nào không được sử dụng, ít được truy cập. Sẽ có nhiều công cụ để mình làm những cái đó. Ví dụ như phân tích từ business requirement, hoặc từ các công cụ phân tích, mấy cái công cụ phân tích á. Từ đó, mình mới biết cái data nào là cần, cái nào có thể đem đi archive lại.\n\n**[42:05]** Sau đó, mình sẽ gói nó lại, dùng một vài biện pháp như vector hóa nó, encode nó, rồi dùng checksum các thứ để đảm bảo cái data nó sẽ đúng. Sau này, khi mình sử dụng lại, mình truy cập lại một cách nhanh chóng. Tại vì những cái database này, nó gói lại ở một cái storage khác với cái mình hay set, nên mình cần phải lưu lại cái metadata của nó. Ví dụ như lưu theo tháng ha, hoặc lưu theo account, để sau mình query lại thì dễ hơn.\n\n**[43:07]** Sau khi archive xong, mình muốn sử dụng lại á, thì vừa đây là cái ví dụ em để recovery. Mình sẽ tận dụng những cái metadata lúc nãy, mình search lại những cái block data mà mình cần, rồi đưa về cái môi trường tính toán lại nó khi cần thiết. Cái này nó có lợi ích là khi mình làm những chuyện này, nó sẽ không tác động đến cái data production của cái ứng dụng đang chạy. Mình có thể làm song song được. Mình muốn làm gì với nó thì làm, không chọc ngoáy vào trong cái production, sẽ đảm bảo an toàn được.\n\n**[43:51]** Cho cái trải nghiệm người dùng, như sản phẩm của mình đó. Nói đến đây, có một vài cái practice cho việc sử dụng, xây dựng cái hệ thống này. Nó cũng đơn giản lắm nhỉ. Mình sẽ phải review lại những cái policy mà mình đặt ra để cái hệ thống này chạy, xem data nó có trọn vẹn hay không. Mình sẽ automation những cái step của cái process này. Hiện tại cũng có nhiều tool hỗ trợ mình rồi, ví dụ như AWS, hay Google, đều có những cái như...\n\n**[45:14]** Google Cloud chẳng hạn. Mình chỉ cần viết những cái đơn giản để đẩy lên trên đó thôi. Và mình không thể thiếu cái monitoring để xem data này có hoạt động tốt hay không. Xong rồi, có những kỹ thuật khác như checksum này nọ, để đảm bảo data của mình luôn trọn vẹn. Khi mình cần, cũng sẽ có những chiến lược như schedule trước cái data. Tại vì những cái data này nó tồn tại lâu, nó cũng sẽ lớn, cũng sẽ phồng lên trên cái storage, cái cloud storage mà mình dùng để lưu trữ nó.\n\n**[46:05]** Nên sẽ có những chiến lược như khi nào cần thì phải schedule trước, bao nhiêu thời gian đó để nó replicate data cho mình chẳng hạn. Kế của em chỉ như vậy thôi. Lý thuyết kiểu để giải quyết cái mục đích cuối cùng là nói mọi người về việc giải quyết những cái data tồn động lâu dài, nhưng không sử dụng đến nhiều trong cái hệ thống mà mình build thôi. Ví dụ như bên ngân hàng chẳng hạn, sẽ có kiểu user trade, trade của user nó lên đến cả trăm triệu record chẳng hạn.\n\n**[46:45]** Sau này, nó sẽ lên nữa. Tức là query những cái data gần thôi, nhưng nó cũng rất tốn thời gian, kiểu vậy. Đó là những cái mà em nói hôm nay, hết. Mọi người có hỏi gì không? Khi mà store data, zip data, là mình sẽ zip một cái đoạn fragment trong quá khứ mà nó không sử dụng data đấy cho mục đích hiện tại, đúng không? Dạ, đúng rồi, đúng rồi. Đồng ý, việc em sẽ phải xóa. Khi xong, em phải xóa cái đó, đúng rồi. Nên mình sẽ có những cái load lại để tính toán khi cần.\n\n**[47:41]** Nên mình mới có mấy cái kiểu để mình làm nó an toàn. Mình có hỏi kìa. Em chưa biết cái cơm của Thỏ có biết cái này không, so sánh được không? Đứng ra là Timescale, nó có cơ chế move chunk. Ví dụ là mình compression như bình thường thôi. Thêm về cái vụ là mình có hot, warm, và cold storage. Ví dụ mình backup hàng tuần thì để trên hot storage của Azure. Nếu là cũ quá, ví dụ 2, 3, 4, 5 năm, thì để trên cold storage của Azure, hoặc là Backblaze. Nó sẽ có riêng cái dịch vụ cho mình move cái chất data đó.\n\n**[48:53]** Đúng cái vị trí object hoặc block storage, mình tương tác với Timescale để đảm bảo lúc mình cần tiết kiệm tiền với data cũ. Có thể tiết kiệm được, vốn có thể query, với trade-off là mình sẽ query hơi chậm với data hơi cũ thôi. Dạ, cái em hiểu là để tùy vào cái platform mình dùng để build ha anh. Ví dụ như bên Microsoft thì cũng sẽ có những cái tùy vào thời gian của database, hoặc tùy vào tuổi thọ của data, hay dung lượng này nọ, thì sẽ có những cái level khác nhau.\n\n**[49:40]** Ví dụ nó sẽ có delay, hay bình thường vẫn access, hay delay cho những cái mà không dùng một thời gian lâu nữa. Cái đó là để mình cụ thể trên từng tool thôi. Còn chung chung, nó là anh đang cái này làm gì thì đứng ra là Timescale thì phù hợp cho cái kiểu pattern này, cho về time series. Bên phía Azure thì họ làm cho nó phù hợp với status, hơi giống như Timescale, nhưng nó kiểu giúp mình partition và shard đúng theo kiểu mình mong muốn.  \n\n**[50:39]** Mỗi một cái nó sẽ có ưu điểm, nhược điểm riêng. Với AWS thì đứng ra là với cái dịch vụ này thì phải coi chừng cái hardware cho lưu cái data này, nó có ổn định không. Ví dụ bên phía Azure cold storage thì nó dùng đĩa, đĩa gì ta, đĩa hơi khá đặc trưng. Phải dùng cái máy laser để in vào trong đó. Nên query rất nhanh, nhưng insert thì cũng hơi chậm, kiểu insert một đống cũng mất vài phút. Vì phải có một cái laser cứng để in ở trên đó, không có virtualization layer. \n\n**[51:23]** Mỗi một service và mỗi một cái kiểu tool mình dùng cho compress và lưu trữ sẽ có ưu điểm, nhược điểm riêng, theo cái platform mình subscribe. Dạ, đúng rồi. Cái này không chỉ là mấy cái tool kiểu như AWS hay Google service. Nó là kiểu mình cũng có thể cân nhắc cho cái business của mình nữa. Nên cái này kiểu chung chung thôi. Còn từng platform, nó sẽ dùng những kỹ thuật khác nhau. Mục đích chung cuối cùng là để giải quyết cái vấn đề data nó lớn lên, nhưng ảnh hưởng đến cái việc mình query, mình nó chạy thôi\n\n**[52:16]** Nhiều cách giải quyết cho câu chuyện optimize query, đúng không? Khi mà vấn đề là do data quá lớn, thì có một vài cách. Cách của biên là một cách, tức là sẽ có một phần data mình đang không xài đến, thì ta cắt đi ra, lưu đâu đấy. Về sau mà có cần đến past data thì insert lại xài sau. Còn mình để đâu đó tầm bao nhiêu phần trăm data hiện tại, đủ để xài mục đích hiện tại, query đi nó nhanh hơn. \n\n**[52:59]** Còn một số cách khác thì xài thằng tooling, có một số kiểu database hay kiểu như Timescale, thì nó sẽ optimize luôn cho chuyện query với lượng data lớn lớn. Em nghĩ là bên dưới thì nó cũng sẽ tự động kiểu nó buff lên đâu đó, nó giữ giúp mình thôi, đúng không anh? Nên mình tỉ mỉ bên dưới, mình dùng là interface thôi. Cái bên dưới thì gần gần như nhau, như các em ta. Cảm ơn biên, vậy thôi. Chắc bài cuối của An, không biết có liên quan không. Không biết còn liên quan một tí gì đến cái cộng đồng viên không.\n\n**[54:00]** Chắc có thì chắc cũng nói sơ sơ thôi, cũng không nhiều cái. Cũng gần giống như bài của Biên, nhưng use case cũng gần giống á. Nó mở rộng ra tí thôi. Tí rồi thì bài này là nói sơ về cái datalake với lại cái use case của thằng Notion. Mình nói cái datalake trước. Datalake thì chắc mọi người nghe miết rồi, xưa giờ cũng hơi lâu rồi đó. Mình nhìn lại cái quá trình phát triển của tụi datalake này, coi là mình đang đi tới đâu.\n\n**[54:54]** Thật ra từ lúc bắt đầu, hồi tầm 1980 gì đó, là thời đại của thằng database, mấy thằng database warehouse, mấy cái mà mình đang xài hiện tại á. Về table các kiểu, tạo table rồi xử lý data. Sau này, tới cái đợt tầm năm 2000 các kiểu, tụi mấy thằng big tech bắt đầu thu thập data nhiều á. Rồi nó tận dụng mấy data đó, thì mới sinh ra mấy thằng để giải quyết vấn đề lưu trữ data và xử lý data trên dữ liệu lớn. Như là mấy dữ liệu lưu theo dạng file đồ á. Mấy cái này, mấy thuật ngữ như là cái MapReduce này nè.\n\n**[55:44]** Hình như trong cái memo của mình có một bài về MapReduce. Nếu mọi người không biết thì có thể search lại, tìm đọc thử xem cái MapReduce hồi xưa nó làm cái gì. Nó là cái tiền thân của tuổi. Sau này nó tích hợp vô thôi, giờ không xài nữa, nhưng chắc là nó tích hợp sẵn hết rồi. Sau cái thời gian phát triển của thằng này, mới bắt đầu 2010, thì mới đẻ ra, trước 2010 tí, đẻ ra khái niệm về datalake, big data, cloud, là cái internal data warehouse á, trên cloud á. Nó cloud thôi.\n\n**[56:28]** Sau này, cái đợt bây giờ á, thì nó bắt đầu phát triển hơn nữa, là về cái lake và datamart. Lake chắc bản chất là kết hợp giữa mấy cái của tụi datalake và cái warehouse thôi, để rồi đặt thành cái house. Như là mấy thằng như thằng Datadog, nó đang làm sao không biết, nhưng mình chắc là đang nói về cái này hơi đi sau thời đại tí. Để tập trung vào, chắc mình coi sơ một cái data architecture chung chung trước. Cái này, bữa cái bài của Tom có đăng, cũng có một cái diagram. Nó cũng tinh gọn hơn cái này, tinh gọn hơn tí, là cũng về cái data đi qua mấy cái layer, là processing rồi mới tới thằng gì đó. \n\n**[57:20]** Cái này nó sẽ thể hiện rõ hơn tí, là trong một cái datalake, mình sẽ lưu những loại data gì. So với thằng data warehouse, mình chỉ lưu mấy thằng structured data thôi, hoặc là mấy cái như lưu table data clean hết rồi. Còn thằng datalake này thì nó raw data, nó sẽ cả structured, unstructured, semi-structured data luôn. Nó sẽ lưu dạng raw, sau đó nó mới xử lý data, transform data, rồi nó quăng qua cho cái đám bên BI analytics, hoặc là quăng vô cái warehouse khác để chứa cái data đã được process rồi á.\n\n**[58:18]** Còn cái layer mà analytics sandbox này, thì nó là một cái layer để cho tụi data scientist, hoặc mấy thằng mà cần dùng cái raw data, process data, mà nó không ảnh hưởng tới cái process chính. Bên đây á, thì nó sẽ làm việc trên cái sandbox này để xử lý data cho tụi mấy thằng đó, mấy thằng cần raw data, nhưng không ảnh hưởng trực tiếp tới cái ruồng chính. Cái giống như hồi nãy Biên có nói á, có làm á đó, là nó sẽ lấy data, rồi nó lưu ở đâu đó để sử dụng sau này, hoặc để process gì đó không biết, nhưng mà nó không muốn ảnh hưởng tới process chính của cái app, thì nó sẽ là cái đống này.\n\n**[59:09]** Ở chỗ này, mọi người thấy là mình có khái niệm là cái ETL á, là extract, transform, và load. Bên cái warehouse xưa giờ mình làm á, nó sẽ là extract, transform, và load, nó đi theo thứ tự đó luôn. Nhưng trong cái này, mình sẽ thấy rõ là cái thằng datalake á, nó sẽ là extract và load trước. Rồi sau khi nào cần á, nó bắt đầu process data, là transform. Transform sẽ đứng sau, load sẽ đứng trước. Đó là cái khác biệt giữa hai thằng.\n\n**[59:52]** Đây là chỗ so sánh khác biệt giữa thằng data warehouse và datalake thôi. Đó là dữ liệu bên warehouse, nó được clean, structured, organized thành cái table. Còn thằng này thì nó lưu dạng file, raw data các thứ, semi-structured rồi đó, CSV hoặc mấy cái JSON. Cái process nó cũng sẽ khác nhau giữa thằng lake và lake này. Truy vấn thì thằng warehouse sẽ truy vấn bằng SQL, còn kia thì xử lý trực tiếp trên cái dữ liệu luôn. Mấy thằng hỗ trợ xử lý trực tiếp dữ liệu, như thằng Spark đó, thì nó sẽ hỗ trợ mấy cái đó. Nói qua về cái thằng Notion.\n\n**[01:00:46]** Datalake thì cái use case của thằng Notion, mọi người biết là Notion mình xài cũng hơi nhiều rồi đó. Hồi xưa, nó cũng đi từ từ thôi. Mấy cái tổ chức, mấy cái block hồi xưa, nó tổ chức thì cũng kiểu data bình thường, giống như mình, là mấy cái app nhỏ nhỏ. Mấy cái block của nó bắt đầu tăng dần. Block của nó được hiểu là mấy cái gì, rồi nó sẽ bao gồm cái title trong trong đó. Nó sẽ gọi là block. Số lượng block của nó tăng lên liên tục theo ngày giờ.\n\n**[01:01:35]** Gì đó thì bắt đầu sau này, nó phình ra, nó sẽ bắt đầu sử dụng mấy cái kỹ thuật như là sharding, sharding xưa. Như nhớ có bài của Hải Vũ có xe gì đó, nó scale horizontally. Nó bắt đầu tách ra sharding này nọ, rồi mấy cái instance. Trong giai đoạn từ 2021 đến 2023, nó sẽ có 32 instance. Mỗi instance sẽ có 15 cái shard. Rồi từ 2023 trở đi á, nó bắt đầu chia lại, nó lại tăng lên. Số lượng tăng lên nữa, thì đó là 96 cái instance. Và mỗi cái instance, nó sẽ là 5 cái shard. Nhân lên tầm 400 mấy á, bốn trăm mấy.\n\n**[01:02:27]** Để mà xử lý thì lúc này, nó hơi to, đúng không? Khi mà data nó bắt đầu to lên á, nó sẽ có những nhu cầu. Sau này sẽ có những nhu cầu về cái analytics, hoặc là mấy cái về làm bên machine learning á, tập dữ liệu này nọ, mẹo mẹo rồi. Nó sẽ bắt đầu setup một cái data warehouse architecture của nó. Cái này là cái tiền thân trước khi setup cái datalake. Nó sẽ làm data warehouse để xử lý data. Cái luồng cơ bản của nó setup để thu thập data, mấy cái về thay đổi data của mấy cái block trong từng shard.\n\n**[01:03:21]** Nó sẽ sử dụng cái file transfer để nó ingest mấy cái data từ mấy shard này nè. Nó đổ về cái gì, rồi nó gộp mấy thằng đó lại thành một cái single database to. Cái này sẽ gặp khó khăn trong việc là nãy mình nói, nó đang có khoảng bốn trăm mấy cái shard, đúng không? Nó sẽ gặp khó khăn trong việc là quản lý bốn trăm mấy connection thằng này. Xong rồi mấy cái khó khăn trong việc scaling. Số lượng data thay đổi trong mỗi cái block của thằng Notion, nó xảy ra thường xuyên và nó rất nặng, sẽ...\n\n**[01:04:13]** Gây khó khăn trong việc đọc ghi trong cái table to này. Sau đó, nó mới bắt đầu setup một cái internal datalake của nó. Cái internal datalake này, có note là nó sẽ không thay thế thằng này hoàn toàn, mà nó chỉ sử dụng cái mới thôi. Còn cái này, nó vẫn tận dụng trong một vài tác vụ, kiểu nhẹ hơn, cho mấy cái table thay đổi data không có nặng lắm. Với lại nó cần cái gì. Còn thằng này, nó expect cái luồng này á, là nó sẽ đánh những cái data nó cần để cho những mục đích mà analytics hoặc là machine learning.\n\n**[01:05:08]** Data nó có thể chấp nhận cái độ trễ là vài tiếng, vài phút, tiếng gì đó. Nó sẽ sử dụng cái data trong đây. Cái lượng setup thì cũng đơn giản thôi. Nó sẽ sử dụng cái thằng Debezium CDC này nè. Nó là cái capture data change á, để nó watch cái thằng database này, bắn về Kafka. Sau khi nó bắn cái đống event data change về Kafka, thì có một thằng bên đây là Hudi hay gì đó, nó lấy event đó, nó quăng về thằng S3. Rồi bắt đầu từ thằng này, thằng nào muốn sử dụng thì vô đây, nó lấy về, nó setup tiếp, xài data warehouse hoặc xài mấy cái chủ đích về shard gì đó, thì vô đây nó lấy, nó xài.\n\n**[01:05:51]** Cái đó là cái thật ra, cái case Notion. Chắc là có thể xài thằng này thử. Vì nó cũng là cái thằng đứng ở ngoài, nó watch vô cái đống đó. Nếu mà xài AWS hay retraining á, sẽ xài cái một là cái thằng Redshift hay gì quên rồi. Nó sẽ watch thằng đó, những thay đổi trên cái database, xong rồi nó sẽ lưu hết về trong một cái bucket hay cái gì đó. Xong rồi từ đó, mình bắt đầu xử lý sau. Cái luồng bên này là có thể sử dụng cái này. Hồi nãy setup một cái demo, nhưng mà có vẻ hơi fail rồi.\n\n**[01:06:51]** Tại vì nó chưa có được cái thằng server, nên là nó fail. Để sau đi, rồi chắc chỉ có như đó. Với lại có cái kiểu góc nhìn đó, là cái process này nè. Là cái process mà chắc tụi enterprise, nó sẽ có thể áp dụng. Nó là process kiểu chung chung mà đa số tụi enterprise sau này, em nghĩ là nó có thể. Nhu cầu của nó khi mà cái data lớn lên á, thì cái nhu cầu của nó cũng sẽ đi theo hướng này thôi. Đó là nó cần data, thu thập data để làm cái gì đó, và không ảnh hưởng tới cái luồng chính.\n\n**[01:07:52]** Mình thì xưa giờ toàn focus vào cái việc làm việc với mấy cái model AI. Nhưng mà mình nghĩ là sau này, mình cũng cần cái skill set gì đó để mình biết cách xử lý những data như thế này, tụi mà nó data lớn hơn kiểu vậy. Cho xin lỗi cái, anh nào đây? Anh đang nhìn nhận cái process này, thì nó khác gì với chuyện là mình replicate cái database của mình ra một instance khác để phục vụ chuyện retraining ấy anh? Là tại vì ở đây, đứng ra là ý ở đây, thật ra là kiểu em đang sinh giống như kiểu sinh data sang một...\n\n**[01:08:54]** Cái shard khác, đúng không? Data warehouse, đúng không? Và sử dụng upload kit process cho những cái tác vụ mà nó không, kiểu mình làm mình làm async được ấy, chứ không cần phải trực tiếp trên nguồn data chính. Câu hỏi là đối với cả mấy cái model dạng như sharding hay sử dụng master-slave ấy, thì sao không theo kiểu cứ duplicate cái database của mình ra thôi? Duplicate data thì nó vẫn chỉ là một cái data warehouse ở dưới dạng table ha. Còn thật ra cái này, nó chỉ là cái process, nghĩa là một process cho database thôi\n\n**[01:09:40]**\n\n. Nó có thể có những cái event khác. Như là ví dụ, mình sẽ có nhiều cái external data, không hẳn là mình chỉ có một cái battery, database không. Ví dụ mình có mấy cái capture như là từ social media, hoặc là mấy cái tụm lum la nào đó, chả biết. Nhưng mà nó có thể là nhiều loại data khác nhau, gom về, quăng qua thằng này. Thằng Hudi bạn này, nó sẽ là thằng chịu trách nhiệm xử lý cái raw data đó, để nó quăng vào cái thằng S3 này. Nó lưu...\n\n**[01:10:23]**\n\nMọi thứ dưới cái đống này. Nó vô luôn, mọi thứ về dạng file gì đó, gom hết vô đây, để bắt đầu sau này, mấy thằng ngoài sao n mới có cái slot để xử lý. Thật ra tụi nó cũng có một câu hỏi là tại sao không dùng mấy thằng database như MySQL hay PostgreSQL á? Nó sẽ có mấy cái... Tại sao phải sử dụng cái thằng capture data change mà không sử dụng mấy thằng đó? Mấy thằng đó, nó có cơ chế để streaming mấy cái event change của nó luôn. Mấy thằng đó, event stream, nó thường sẽ stream trực tiếp từ database này qua database khác.\n\n**[01:11:09]**\n\nCòn thằng này, nó sẽ là capture cái event đó và đưa đâu cũng được. Vì là nếu mình không có thằng Kafka này ở đây á, thì mình cần một service nào đó, mình cần cái real-time data xử lý liền luôn á, mình không cần phải vô Kafka. Cái thằng CDC này vẫn có thể bypass qua đó được, kiểu kiểu vậy, chứ không hẳn là từ database sang database kiểu như vậy. Ta cũng có nhìn ý là kiểu, thấy là nếu mà theo góc nhìn về operation chẳng hạn ấy. Tất nhiên nếu mà có multiple datasource và sử dụng những cái partition tool các thứ, nó khác nhau ấy.\n\n**[01:11:50]**\n\nDatabase khác nhau, thì cái này cũng sẽ cả, thật ra là gom nó lại vào một cái datalake, sao cho một số cái tác vụ, nó cụ thể thôi ấy. Thực ra là một số team, như kiểu team AI hay team về mặt làm report, hay data, thì người ta cũng sẽ chỉ cần work trên cái data warehouse này thôi, kiểu vậy. Hoặc là có extend cho bên nào khác nữa, thì cũng sẽ make sense, phân vùng data riêng cho từng cái team riêng, đúng không? Có họ thêm cái vụ mà cái button, cái button ETL bên database bình thường với cả bên datalake, thì nó sẽ là ELT, đúng không?\n\n**[01:12:39]**\n\nĐúng rồi, ELT, anh sẽ hiểu là mình extract, nghĩa là mình tìm đúng file, đúng không? Mình load cái file đấy lên đây, và transform nó thành dạng kiểu structured data ha. Ý là nó sẽ transform, nó chỉ là cái action, nó xảy ra ở sau khi mình có raw data rồi. Còn ETL, nghĩa là extract là sẽ lấy data từ cái đám data source á. Xong rồi nó sẽ có cái quá trình log thẳng vào cái raw data, thẳng vào mấy cái gì đó của mình. Nó gọi là cái raw landing, cái layer raw landing. Xong rồi mình mới có cái gọi là transform. \n\n**[01:13:34]** Sau đó, sau cái landing sẽ có transform để xử lý data, thì nó sẽ ra sau. Còn cái thằng kia là nó extract xong, rồi transform, nó mới quăng thẳng vào cái warehouse, đó là cái database của mình. Hay anh em có hỏi gì không? Rồi, cảm ơn An, đúng rồi. Anh em nhé, rồi bye anh em, mỗi tuần vui vẻ.\n\n---\n\n### English transcript\n\n**[00:00]** It seems like the all-hands meeting is next week. I don’t see anyone creating any events, so this week will probably just be normal, right? Guys, check if there’s any issue with screen sharing. Should we just start? Today, I think we’ll have about three presentations. Phát just said he doesn’t have anything new this week, so he’ll probably skip today. Guys, try sharing your personal screens and see how it goes. Can we preview it first?\n\n**[10:42]** According to the schedule, it’s probably you, right, bro? Let me go first then. This fine-tuning topic, it’s not really that new. This presentation is just 100.5, not 101, so it’s only a brief intro, not going deep into it yet. Honestly, I’m pretty clueless about it too, so I’ll just give a quick overview. Today, I’ll present about fine-tuning. Here’s the agenda for this session.\n\n**[12:24]** The introduction is, if you guys use AI, you’ve probably heard of the concept of fine-tuning. AI has these models that are mostly fitted to data from some specific day, with stuff like data privacy or data from a particular domain. That data doesn’t show up in the knowledge of the foundation model. To get that knowledge into the model, people usually use retraining, right? But there’s another way called fine-tuning. At the end of this, I’ll compare these two methods, looking at when to use which one and when not to.\n\n**[13:08]** For now, just think of fine-tuning as a way to expand the knowledge of a foundation model. What is fine-tuning? Simply put, you retrain the model. You take a foundation model, feed it a dataset to fine-tune it, meaning you retrain it. After fine-tuning, you get an adjusted model, called a fine-tuned model.\n\n**[13:44]** Why does fine-tuning bring in new knowledge? Simply put, in an AI model, knowledge is stored through neural networks. Fine-tuning updates the weights, the parameters of that neural network, to fit the new knowledge. When you throw new knowledge in, those weights change, and at that point, the model has updated its knowledge.\n\n**[14:19]** When you throw new knowledge in, the weights change, and at that point, the model has updated its knowledge. When fine-tuning a model in practice, it’s not just about throwing a dataset in and retraining it. Sure, you get a fine-tuned model, but you don’t know if that retrained model is any good. I’ll introduce a workflow that people outside commonly use for fine-tuning.\n\n**[14:59]** This flow, it’s got several steps like this. You can split it into two clusters, two clusters, alright? This flow, I’ll divide it into two clusters. I’ll go through the first cluster first. The first cluster is the one on the left, simply understood as a base model to start with. Then, you have a new dataset, something new, you throw it in, fine-tune it. Then it produces a model, and you supervise it, meaning you retrain it in a way that’s like retraining it.\n\n**[15:38]** It’ll add more knowledge, with this input, the output will come out like this. It results in something called supervised fine-tuning. After that, let me move to the next part. Alright, this fine-tuning is retraining on data, meaning you give it a pair of input-output in the dataset for it to learn. It’ll pick up that new knowledge. For this step to be perfect, the dataset has to be clean. It has to be clean, not mixed with other stuff. Meaning it has to be specific to the domain we want to train it on.\n\n**[16:31]**\n\nBack to this diagram, after you have a model that’s been retrained, you bring it to production, you use it, right? At this point, people outside use a system called human feedback. It’s like, does the response from this model satisfy you? Rate it from 1 to N stars, something like that. You’ll collect that data. It’s part of this step—you’ll gather human feedback from that retrained model of yours.\n\n**[17:06]** Based on that feedback, people call this step a bit resource-intensive. You’ll have to retrain a separate model. That model is used to evaluate how many points this response gets. Next, you move to the third step, the final one. In this step, you’ll use algorithms like reinforcement learning to combine it with the retrained model and your reward model.\n\n**[17:46]** You retrain, you fine-tune the model one more time. It’ll give you something called an optimized model. This loop keeps going, going forever. You have a retrained model, collect human feedback, then combine those three things to retrain the model again. The more you do it, the better the model gets at what we want. That’s the flow I’ve seen people use out there in production.\n\n**[18:24]** During the fine-tuning process, you’ll often hear about a concept called catastrophic forgetting. What does that mean? It means when you retrain with new knowledge, it reduces performance on the old knowledge. Why does this happen? As I said, a model’s knowledge depends on its weights, its architecture, and its parameters. The dynamic parameters in a model are those weights. When you retrain, those weights change, right?\n\n**[19:00]** When they change, doesn’t that mean the old knowledge gets less accurate? If your dataset has a lot of overfitting data, meaning your dataset is too perfect, too perfect within that dataset, then when someone throws something new in, it’ll mess up the old stuff. They call that overfitting, meaning it’s folded in too much to the training data. When it encounters new data, performance drops.\n\n**[19:42]** So at this point, people out there use a technique called parameter-efficient fine-tuning, or PEFT. It has lots of methods, techniques within this approach, like LoRA and stuff. But generally, most of them don’t update all the weights in the model. They’ll just freeze the layers that aren’t necessary. They freeze those unneeded layers and only update a certain number of weights. That’s to avoid losing too much of the old knowledge. That’s the basic idea. For deeper stuff about the algorithms behind it, you can look it up yourselves.\n\n**[20:27]** Back to the question from the start, how’s it different from retraining, and which should we use? There’s a table here, you can easily see it. Retraining depends on your database. You keep throwing stuff in, throwing stuff in, and the data gets updated constantly. But with fine-tuning, you retrain the model, so the data stays only where you retrained it.\n\n**[21:16]** Next is customize and learning style. Meaning, retraining’s purpose is to give us a knowledge base that we can pull from to reference and use. But fine-tuning? It upgrades the model’s brain, so it has that knowledge built-in already. The stuff below that, you guys can probably look into it more yourselves, yeah?\n\n**[21:57]** I’ve got an example like this. For instance, say you want to build a system to explain doctors’ notes, right? Doctors’ notes, as you might know, have tons of technical terms. And those technical terms are often abbreviated, written all sloppy too.\n\n**[22:44]** If you guys use fine-tuning, you’ll make it learn all that messy knowledge, the shorthand stuff, the handwriting stuff from doctors. So when you input a doctor’s note, it’ll give you a really accurate answer. But if you use retraining, when you input a doctor’s note, it’ll find the relevant data and pull it up to read. But the thing is, the model doesn’t actually understand those terms, so it won’t give you an accurate answer either.\n\n**[23:16]** You can think of it like this: fine-tuning is like asking a doctor to read a doctor’s note. Retraining is like giving it to someone with really broad knowledge to read a doctor’s note. That person might know a ton, but when it comes to the specialized stuff, the real technical terms, they don’t have the depth of an actual doctor. So the accuracy won’t be high.\n\n**[23:57]** Second, you might say, alright, with retraining, we can use a system prompt to list out all the doctor’s shorthand in the system prompt, and it’ll figure it out on its own. But doing that, you’ll end up using a lot of tokens, right? Because when you use retraining, you pull out all the retraining data, throw in a retraining knowledge base, plus a bunch of zero-shot stuff, descriptions, and whatever else goes with it in a prompt, that takes up a ton of tokens.\n\n**[24:34]** And when you’re in a long conversation with a model, it’s not like it only relies on your question. It bases its answers on all the conversations you’ve had with it from the start. At that point, it leads to a situation where it’s limited by tokens. That’s the drawback of using retraining—it eats up tokens. Because you need tokens to run your system prompt too. But with fine-tuning, the thing is, the model already has the knowledge, so you don’t need a system prompt.\n\n**[25:14]** That’s a quick rundown on fine-tuning. Probably having a demo for you guys to see would make it clearer, yeah? Now I’ll fine-tune a model called Duty 40 Mini. I’ve got a dataset like this. Yup, like this, each thing has a system like retraining, then the user asks this and wants it to answer like that, right? I’ve got 10 things, 10 records in this dataset, and I’ll fine-tune it.\n\n**[26:13]** Before fine-tuning, I’ll run it through a piece of code so I can estimate it. Since I’m using Open AI, it’ll cost money. So we’ll calculate an estimate of how much it’ll charge me. After I run it, at the end it’s like, around 4800, close to 4800 tokens. This is just a reference, but I think it’s pretty accurate. Then I’ll upload this data file to Open AI. It’ll give me the file up on my Open AI account.\n\n**[27:03]** Then I’ll train it. I’ll create a fine-tuning job. At this point, on Open AI, it’ll run this job. You guys can go up here, read it, check it out. It won’t give results right away, it’ll create a job and leave it pending there, so Open AI can fine-tune it for us. While waiting, we can track how the process is going. Once it’s done, it’ll notify us. We just keep stamping this sentence, checking this sentence to see if it’s finished or not. We read it from that spot.\n\n**[27:58]** Once it’s done, it’ll give us some result statuses. After fine-tuning, with the same question. For example, this is the question I used, I used this question, it’s pretty close to one of the records in my dataset pile. After I run it, it’ll answer like this. But before fine-tuning, I used a normal model, just a regular model, and it answered like that.\n\n**[28:49]** Meaning, when I fine-tuned it, it worked. That’s how I used Open AI to fine-tune a model. That’s it for my demo. Any questions, guys? Yeah, for this demo, I used tuning, but to do fine-tuning locally with something fancy, I probably don’t have the gear. Yup, just small-time gear. Actually, with some earlier models, I ran them on LoRA and stuff, and they could’ve been demoed too. But I didn’t, this one’s easy, it’s like a basic one.\n\n**[29:47]** A bit scattered and slow, huh? Yeah, it’s probably fine though. Generally, enterprise teams or those who don’t want to spend time building GPUs will use this method. Back with Diagram GPT, when GPT-4o Mini came out, fine-tuning was free, so using this was pretty convenient for them. The demo shop for you guys is using something like a service. Open AI provides a fine-tuning service, putting it right up on their models. We pick some models probably the mini ones, I guess. The cost probably isn’t too high.\n\n**[30:37]** That’s one way to do it. But the issue is, we’re still not the ones owning that model. The thing is, it’s still hosted on their server. There’s another way, like building your own server and running it yourself. Today’s case was different. Check it out, guys, yesterday I tried a model with just 3 billion parameters. But it ran for two or three hours and still wasn’t done, bro. Actually, this one, its version is simpler than what we have now, smaller than the previous one from earlier.\n\n**[31:26]** It’s a full flow related to what was it reinforcement feedback. The point is, what I introduced about production out there is when people tune stuff. They don’t just fine-tune it and use it right away, they have to evaluate it again to see if it’s right. They put it into a cycle to keep improving the fine-tuned model, something like that. This is one of those flows. At its core, it’s just a model, nothing special. The key is you guys knowing the costs to judge the approach.\n\n**[32:05]** Because it’s still about accuracy, right? We pick a method to make the output more accurate. Methods like retraining or fine-tuning they’ve got different downsides. And honestly, even with fine-tuning, there are lots of different fine-tuning methods. We’d probably need to dig deeper to figure those out. This is still kinda general. Probably so, Hoàng. If we’ve got the chance, we should dive a bit deeper.\n\n**[32:48]** Deeper in the sense of looking at methods related to retraining and stuff. With fine-tuning methods, some of them are pretty resource-efficient. Of course, there’s a trade-off with some other stuff, like that. I’m introducing this so you guys can check it out somewhere. People asked—Đạt asked when we need fine-tuning. I’d say fine-tuning is needed when you want it to have knowledge on a specific topic. You can consider using fine-tuning then.\n\n**[33:39]** But in all cases, from what I’ve seen out there, most people prefer retraining. Because it’s easier and uses fewer resources. But in some cases, like right now, the example I gave about doctors’ notes, I’d suppose tuning is better. Then it depends on the architecture, how we split our system into smaller systems, what those smaller systems are like it varies. There might be some use cases where they want to host small models, tiny ones maybe.\n\n**[34:18]** Just for doing a specific task. Like analyzing weather, humidity, stuff like that, to perform some action. For example, changing your phone’s theme or triggering some action or whatever. You could retrain a small model just for that, no need for a network or anything fancy. Probably like that. From now on, it’s Biên’s turn, yeah?\n\n**[36:13]** Using it and building a recovery process for it. How it works in detail, it’s got a few main parts. First is the reason we need this technique and comparing it to something more familiar like backup. Then it’s about how we build it and the things we need to watch out for, what stuff. To start, in reality, there are often organizations, companies running apps with high data traffic. Like stock trading stuff, for example. My example here is something like 50,000 transactions.\n\n**[37:12]** My example is like 50,000 a day is low it could shoot up to 500,000, a million transactions a day. After a while, that data volume swells up huge, affecting how we query data and impacting the user experience. In that data, there’s stuff that, once used, barely gets accessed again. Like history from over 7 years ago, for instance. That leads to a problem how do we deal with that pile of data? So we use a technique called data archiving.\n\n**[38:14]** It’s got benefits to counter those issues up there. First off, the data we use, the stuff that’s constantly being set, queried, read, and written all the time, it usually costs a lot. With this technique, we take our data and move it somewhere else, somewhere cheaper with less access. That way, it boosts our app’s performance when querying or aggregating data and such.\n\n**[39:07]** In terms of legal stuff or reusability, that data will be kept safe, not affected by external factors. So later, when we need to use it again, we can pull it out and use it. As people often say, you’ll think of data backup, which is usually used to restore data, restore the system, or the app if something goes wrong. But these two things are different in that data backup is more for hotfixing the system. As for archiving data archiving focuses on storing data long-term.\n\n**[39:59]** It has a detailed comparison like this. To build an architecture, a system to archive data, and then use it for recovery when we need it, here’s how it works. You guys see, it has three main notes. First, we store the data, we use metadata to interact with that data, then we put it somewhere, like cloud-based services or cloud storage services, to keep that data stored.\n\n**[40:51]** In detail, to store the data, first we have to figure out which data needs to be stored. We need to analyze which data gets used a lot, which doesn’t get used, or gets accessed rarely. There are lots of tools to help us do that. For example, analyzing from business requirements or using analytics tools, those analytics tools. From there, we figure out which data is necessary, which can be archived.\n\n**[42:05]** After that, we’ll package it up, using a few methods like vectorizing it, encoding it, then using checksums and stuff to make sure the data stays correct. Later, when we use it again, we can access it quickly. Because these databases are packaged in a storage different from what we usually set, we need to save its metadata. For instance, store it by month, yeah, or by account, so it’s easier to query later.\n\n**[43:07]** After archiving, when we want to use it again, just now I gave an example for recovery. We’ll use that metadata from earlier, search for the data blocks we need, then bring it back to the computing environment when necessary. The benefit here is that when we do this stuff, it doesn’t mess with the production data of the running app. We can do it in parallel. Whatever we want to do with it, we do, without poking around in production, so it keeps things safe.\n\n**[43:51]** For the user experience, like our product. Speaking of this, there are a few practices for using and building this system. It’s pretty simple, right? We’ll have to review the policies we set up for this system to run, check if the data stays intact. We’ll automate the steps of this process. Nowadays, there are plenty of tools supporting us already, like AWS or Google, they’ve got stuff like...\n\n**[45:14]** Google Cloud, for example. We just need to write some simple stuff to push it up there. And we can’t skip monitoring to see if this data is working well or not. Then, there are other techniques like checksums and such, to ensure our data always stays intact. When we need it, there’ll also be strategies like scheduling the data beforehand. Because this data sticks around for a long time, it’ll grow big, it’ll swell up in the storage, the cloud storage we use to keep it.\n\n**[46:05]** So there’ll be strategies like, when we need it, we have to schedule in advance, how much time it’ll take to replicate the data for us, for instance. That’s my plan, that’s it. The theory is kind of to address the ultimate goal of explaining to you guys about handling data that sticks around long-term but isn’t used much in the system we build. Like in banking, for example, there’s stuff like user trades, user trades hitting hundreds of millions of records or something.\n\n**[46:45]** Later on, it’ll grow even more. Meaning querying just the recent data, but it still takes a ton of time, something like that. That’s what I talked about today, done. Any questions, guys? When we store data, zip data, it’s like we zip up a fragment from the past that doesn’t use that data for current purposes, right? Yup, exactly, exactly. Agreed, I’ll have to delete it. Once it’s done, I’ve got to delete that, yeah. So we’ll have ways to reload it for calculations when needed.\n\n**[47:41]** That’s why we’ve got these methods to keep it safe. I’ve got a question over here. I don’t know if Thỏ’s crew knows about this, can we compare it? Standing out is Timescale, it’s got a chunk-moving mechanism. For example, we compress it like normal. Plus, there’s this thing about having hot, warm, and cold storage. Like, if we back up weekly, it goes on Azure’s hot storage. If it’s too old, say 2, 3, 4, 5 years, then it’s on Azure’s cold storage or Backblaze. It’s got a separate service for us to move that data stuff.\n\n**[48:53]** Right to the object or block storage spot, we interact with Timescale to make sure we save money with old data when we need to. It can save costs, it can still query, with the trade-off being that querying is a bit slow with older data. Yup, what I get is it depends on the platform we use to build, right, bro? For example, with Microsoft, it’ll depend on the database’s timing or the data’s lifespan or capacity and stuff, so there’ll be different levels.\n\n**[49:40]** For example, there’ll be delays, or normal access still, or delays for stuff that hasn’t been used in a long time. That’s for us to specify on each tool. But generally, it’s like, bro, what’s this doing? Standing out is Timescale, it fits this kind of pattern, for time series stuff. On Azure’s side, they make it fit the status, kinda like Timescale, but it helps us partition and shard the way we want.\n\n**[50:39]** Each one has its own pros and cons. With AWS, standing out is that with this service, you’ve got to watch the hardware storing this data, whether it’s stable or not. For example, Azure’s cold storage uses disks, what kind of disks, some pretty unique ones. They’ve got to use a laser machine to burn it in there. So querying is super fast, but inserting is kinda slow, like inserting a bunch takes a few minutes. Because it needs a hard laser to burn it on there, no virtualization layer.\n\n**[51:23]** Each service and each type of tool we use for compressing and storing has its own pros and cons, depending on the platform we subscribe to. Yup, exactly. This isn’t just about tools like AWS or Google services. It’s like we can also weigh it for our business too. So this is kinda general. Each platform uses different techniques. The ultimate common goal is to tackle the issue of data growing big but affecting how we query, how it runs.\n\n**[52:16]** Lots of ways to solve the query optimization problem, right? When the issue is that the data’s too big, there are a few approaches. Biên’s way is one approach, meaning there’s a chunk of data we’re not using, so we cut it out, store it somewhere. Later, if we need past data, we insert it back to use it. For now, we keep some percentage of current data, enough for current purposes, so querying is faster.\n\n**[52:59]** Other ways use tooling, some database types, like Timescale, optimize querying for huge data right off the bat. I think underneath, it kinda auto-buffs it somewhere, holds it for us, right, bro? So we fuss over the details underneath, we just use the interface. Underneath, it’s pretty much the same, like us kids. Thanks, Biên, that’s it. Probably An’s last piece, not sure if it’s related. Not sure if it ties a bit to the community stuff.\n\n**[54:00]** If there is, it’s probably just a quick rundown, not a lot. Pretty similar to Biên’s piece, but the use case is kinda close too. It expands a bit more. Alright, so this piece is a quick talk about datalakes and Notion’s use case. Let’s talk datalakes first. Datalakes, you guys have probably heard about them tons, been around for a while now. Let’s look back at how these datalakes evolved, see where we’re at.\n\n**[54:54]** Actually, from the start, around 1980 or something, it was the era of databases, database warehouses, the stuff we’re using now. Table stuff, creating tables and processing data. Later, around the 2000s or so, the big tech folks started collecting tons of data. They used that data, so new stuff popped up to handle storing and processing data on big datasets. Like data stored as files and such. These things, terms like MapReduce, for instance.\n\n**[55:44]** I think in my memo, there’s an article on MapReduce. If you guys don’t know, you can search it up, check it out to see what MapReduce did back then. It was the ancestor of this era. Later, it just got integrated in, not used standalone anymore, but it’s probably all built-in now. After that development phase, around 2010, it started giving birth, a bit before 2010, to concepts like datalakes, big data, cloud, internal data warehouses on the cloud. It’s just cloud stuff.\n\n**[56:28]** Now, these days, it’s evolving further, into lakes and datamarts. Lakes are probably just a mix of datalake stuff and warehouses, then turned into a house. Like Datadog or whatever they’re doing, I don’t know, but we’re probably talking about this a bit behind the times. To focus in, let’s take a quick look at a general data architecture first. This one, Tom’s piece the other day posted it, had a diagram too. It’s a bit more streamlined than this, a bit more concise, about data going through layers, processing then to some other thing.\n\n**[57:20]** This one shows it a bit clearer, about what kinds of data we store in a datalake. Compared to a data warehouse, we only store structured data, or stuff like table data that’s all cleaned up. But this datalake, it’s raw data, it’ll handle structured, unstructured, semi-structured data all together. It stores it raw, then it processes the data, transforms it, and tosses it over to the BI analytics crew or into another warehouse to hold the processed data.\n\n**[58:18]** Then there’s this analytics sandbox layer, which is a layer for data scientists or folks who need to use raw data, process data, without messing with the main process. Over here, they’ll work on this sandbox to handle data for those guys, the ones who need raw data but don’t directly affect the main flow. It’s like what Biên said earlier, doing that stuff, taking data and storing it somewhere for later use or to process something, I don’t know, but it doesn’t want to mess with the app’s main process, so it’s this pile.\n\n**[59:09]** Here, you guys see we’ve got this concept called ETL, extract, transform, and load. With warehouses, what we’ve done so far is extract, transform, and load, it follows that order straight up. But in this one, you’ll clearly see the datalake does extract and load first. Then when it’s needed, it starts processing the data, that’s transform. Transform comes after, load comes before. That’s the difference between the two.\n\n**[59:52]** This is just the spot comparing the differences between data warehouses and datalakes. With warehouses, the data is cleaned, structured, organized into tables. But this one stores it as files, raw data and stuff, semi-structured already, CSV or JSON files. The processing is different between this lake and that lake too. Querying, the warehouse uses SQL, while over there, it processes directly on the data itself. Tools that support direct data processing, like Spark, handle that stuff. Moving on to Notion.\n\n**[01:00:46]** For datalakes, the use case of Notion, you guys know we’ve been using Notion quite a bit already. Back in the day, it started slow. The organizations, the blocks from before, they were organized like normal data, just like us, small apps. Its blocks started growing gradually. Blocks are understood as what, and they’ll include the title in there. They call it a block. The number of blocks keeps increasing constantly by the day and hour.\n\n**[01:01:35]** Something like that, then later it started swelling up, and it began using techniques like sharding, old-school sharding. Like I remember Hải Vũ’s article mentioning something about it, scaling horizontally. It started splitting into sharding and stuff, then instances. From 2021 to 2023, it had 32 instances. Each instance had 15 shards. Then from 2023 onward, it started splitting again, increasing even more. The number went up again, so that’s 96 instances. And each instance had 5 shards. Multiply that, it’s around 400-something, four hundred and some.\n\n**[01:02:27]** To handle that, at this point it’s pretty big, right? When the data starts getting big, it’ll have some needs. Later on, it’ll have needs for analytics or stuff related to machine learning, datasets and tricks and all that. It started setting up its own data warehouse architecture. This was the precursor before setting up the datalake. It did a data warehouse to process data. The basic flow it set up was to collect data, stuff about data changes in the blocks across each shard.\n\n**[01:03:21]** It used file transfers to ingest the data from these shards here. It dumped it into something, then merged those things into one big single database. This ran into issues because, like I said earlier, it’s got about four hundred-something shards, right? It struggled with managing four hundred-something connections for this thing. Plus the scaling challenges. The amount of data changing in each block of Notion happens often and is super heavy, so it made reading and writing in this big table tough.\n\n**[01:04:13]** After that, it started setting up its own internal datalake. This internal datalake has a note that it won’t completely replace this one, it just uses the new stuff. The old one, it still uses for some tasks, lighter ones, for tables where data changes aren’t too heavy. And it needs something. But this one, it expects this flow to tag the data it needs for purposes like analytics or machine learning.\n\n**[01:05:08]** The data can handle a delay of a few hours, a few minutes, something like that. It’ll use the data in here. The setup amount is pretty simple. It uses this thing, Debezium CDC, you know. It’s the capture data change thing, to watch this database and shoot it over to Kafka. After it shoots that pile of event data changes to Kafka, there’s a thing over here, Hudi or something, that grabs those events and tosses them to S3. Then from this point, whoever wants to use it goes in here, grabs it, sets it up further, uses it for data warehouses or some shard-related purposes, they take it from here and use it.\n\n**[01:05:51]** That’s actually the Notion case. We could probably try using this thing. Because it’s also one of those that stands outside, watching that pile. If we use AWS or retraining, it’d use something like Redshift or whatever, I forget. It’d watch that, the changes on the database, then save it all into a bucket or something. From there, we start processing afterward. This flow here could use that. Earlier, I set up a demo, but it kinda flopped.\n\n**[01:06:51]** Because it didn’t have the server yet, so it failed. Let’s leave it for later, probably just that much for now. Plus there’s this perspective, this process here. It’s a process that enterprise folks could probably apply. It’s a kinda general process that most enterprises later on, I think, might use. Their needs, when the data grows big, will probably head in this direction anyway. It’s that they need data, collect data to do something, without messing with the main flow.\n\n**[01:07:52]** For us, so far we’ve always focused on working with AI models. But I think later on, we’ll also need some skill set to know how to handle data like this, stuff where the data’s bigger, you know. Sorry, which bro is this? You’re looking at this process, how’s it different from us replicating our database to another instance for retraining purposes, bro? Because here, standing out, the point is, it’s like I’m kinda generating data to another different shard, right?\n\n**[01:08:54]** And using an upload kit process for tasks that don’t, like, we can do async, you know, instead of needing to work directly on the main data source. The question is, for all those models like sharding or using master-slave setups, why not just duplicate our database? Duplicating data, it’s still just a data warehouse in table form, yeah. But actually this, it’s just a process, meaning a process for the database.\n\n**[01:09:40]** It could have other events. Like, for example, we’ll have lots of external data, not just one battery, a database, you know. Say we’ve got captures from social media or some random messy stuff, who knows. But it could be lots of different data types, gathered up, tossed to this thing. This Hudi buddy here, it’s the one responsible for processing that raw data, to throw it into this S3 thing. It stores everything under this pile.\n\n**[01:10:23]** It goes right in, everything in some file format, all dumped in here, so later on, the outside folks have a slot to process it. Actually, they had a question too, why not use databases like MySQL or PostgreSQL? They’ve got their own... Why use this capture data change thing instead of those? Those ones, they’ve got mechanisms to stream their event changes already. With them, event streams usually stream straight from one database to another.\n\n**[01:11:09]** But this one, it captures that event and sends it wherever. Because if we don’t have this Kafka here, we’d need some service, we’d need real-time data processed right away, without going through Kafka. This CDC thing can still bypass to there, kinda like that, not exactly from database to database like that. We also noticed something, like, it feels like, from an operations angle, for instance. Of course, if there are multiple datasources and we use partition tools and stuff, they’re different.\n\n**[01:11:50]** Different databases, so this will also, actually, bundle it into a datalake, so some tasks are specific, you know. Actually, some teams, like the AI team or the reporting team or data folks, they’d probably just need to work on this data warehouse, like that. Or if it extends to other sides too, it’d make sense, splitting data zones for each team separately, right? They added this thing about the button, the ETL button in regular databases versus datalakes, it’d be ELT, right?\n\n**[01:12:39]** Yup, ELT, you’d get it as extract, meaning we find the right file, right? We load that file up here and transform it into something like structured data, yeah. The idea is it transforms, it’s just an action, it happens after we’ve got the raw data. But ETL means extract is pulling data from the data source pile. Then it’s got a process to log straight into the raw data, straight into some stuff of ours. They call it raw landing, the raw landing layer. Then we’ve got what’s called transform.\n\n**[01:13:34]** After that, after the landing, there’s transform to process the data, so it comes out later. But the other one extracts, then transforms, then tosses it straight into the warehouse, that’s our database. Any questions, guys? Alright, thanks An, yup. See you, guys, bye, have a fun week","title":"OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion","short_title":"#37 AI Fine-tuning, Data archiving, Datalakes","description":"In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q\u0026A to boost our technical skills.","tags":["office-hours","ogif","discord"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_"],"date":"Sun Dec 29 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"updates/ogif/37-20241227.md","slugArray":["updates","ogif","37-20241227"]}],"newMemos":[{"content":"\nAt [Company/Community Name], we’re obsessed with working smarter, not harder. This month, we’re tapping into the power of AI to speed up our processes—and we need YOUR ideas to make it happen! Whether it’s automating writing, deploying AI agents, or streamlining workflows, share your proposal and earn *15 icy* for every approved idea.\n\nHere’s How It Works:\n\n- **What We’re Looking For**: Creative ways to use AI to accelerate tasks—like writing, research, or operations. We want proposals, guidelines, or practices that save time and boost efficiency. (See an example below!)\n- **How to Submit**: Send your idea to [submission link/form/channel] with a brief explanation. No idea’s too small!\n- **Reward**: Earn *15 icy* per approved submission added to our AI-powered playbook. Standout ideas could score up to 25 icy!\n- **Review Process**: Our team reviews weekly and will let you know if your idea’s a go.\n- **What’s Next**: Winning ideas get implemented, and you’ll be credited community-wide!\n\nWhy Join In?\n\n- Earn *icy* tokens to [use in community perks, trade, etc.—add token value context].\n- Help us harness AI to work faster and smarter.\n- Open to everyone—bring your A(I)-game!\n\nExample Submission:\n“Use an AI writing assistant (like Grok) to draft initial posts, memos, or emails in seconds. Team members can edit the output, cutting writing time by 50%.”\n\nReady to speed things up? Submit your AI idea at [link] and start earning icy today!\n\n---\n\nproductivity is one of our study focus.\n\nintro: ref to handbook \u003e how-we-work \u003e ## Pitching ideas\n\naccepted content\n\n- write a rfc\n- build a supporting tool\n-\n\nhow to submit\n\n- show a workflow image\n\nreward\n\nList of proposal\n\nList of articles\n","title":"Productivity","short_title":"","description":"","tags":["earn","productivity"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/000-productivity.md","slugArray":["earn","000-productivity"]},{"content":"","title":"Software Quality","short_title":"","description":"","tags":["earn","quality"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/001-quality.md","slugArray":["earn","001-quality"]},{"content":"","title":"Open Source","short_title":"","description":"","tags":["earn","open-source"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Apr 03 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"earn/002-open-source.md","slugArray":["earn","002-open-source"]}],"teamMemos":[{"content":"\n## Record \u0026 reward sharing at Dwarves\nIf you're part of the Dwarves community, you'll notice our culture of continuous learning and knowledge sharing. This culture isn’t just about individual growth; it’s also about building a strong, connected community.\n\nWe believe learning is key to success and our goal is to create an environment where it’s prioritized, valued, and rewarded, allowing everyone to grow. \n\nTo support this, we’ve organized a variety of engaging events and activities that encourage and recognize knowledge sharing. Additionally, we allocate a portion of our profits to acknowledge these contributions with awards.\n\n## Our approach to learning and growth\nLearning goes hand in hand with a growth mindset. In our fast-moving industry, staying ahead means picking up new knowledge all the time. The more knowledge you share, the more rewards you earn. It’s that simple.\n\n## Monthly pool of up to 2500 ICY for recognized contributions\nWe’ve set aside a monthly pool of **2500 ICY (around $4000)** to reward those who contribute valuable insights. **70%** of this pool is dedicated to those who actively share knowledge across the community. \n\nAdditionally, we place extra value on contributions in key areas like **AI/LLM**, **Golang**, **Software Architecture**, and **Blockchain**. If you’ve got expertise in these fields, your insights will be especially appreciated and may earn you more ICY.\n\nCheck out the [**🧊・earn-icy**](https://discord.com/channels/462663954813157376/1006198672486309908/1239502938918096960) channel to see how we distribute our ICY and get involved.\n\n## How you can participate and make a contribution\nIf you have something worth sharing? Drop useful links in our research channels such as [**💻・tech**](https://discord.com/channels/462663954813157376/810481888619135046/1281086341995565057), [**💡・til**](https://discord.com/channels/462663954813157376/1001883339046797342/1281097209072320615) to get recognized. Got a topic in mind? Present it in our OGIFs or submit a note to our [**memo**](https://memo.d.foundation/). Community members are welcome to participate too.\n\nWe also love open-source work, especially building tools that boost our productivity. If you’re into that, join the force at [**🦄・build**](https://discord.com/channels/462663954813157376/1280726623414390805/1280791483280261161) and start cooking up something great.\n\nWe hope this push will keep our learning culture strong and moving forward.\n\nHappy coding and sharing!","title":"Record and reward sharing at Dwarves","short_title":"","description":"Discover how the Dwarves community fosters a culture of continuous learning and knowledge sharing. With a monthly reward pool of up to 2500 ICY, contributors are recognized for sharing valuable insights, especially in areas like AI/LLM, Golang, and more. Get involved and grow with us.","tags":["reward","team","community"],"pinned":false,"draft":false,"hiring":false,"authors":["innno_","thanh","monotykamary"],"date":"Thu Sep 05 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/notes/misc/record-reward-sharing-culture.md","slugArray":["playground","notes","misc","record-reward-sharing-culture"]},{"content":"\n**A Junior Backend Engineer recounts her journey at Dwarves Foundation, from being attracted by their distinctive logo to finding satisfaction in remote work and learning the important lesson that seeking help from supportive team members is key to professional growth.**\n\n![Cat Nguyen - Junior Backend Engineer at Dwarves](assets/notion-image-1744012268919-ye4mt.webp)\n\nMy journey with Dwarves began with a glance at their logo during my senior year at Bach Khoa University. While browsing through tech job listings on the university's website, Dwarves' distinctive red logo caught my eye amidst a sea of white logo backgrounds. I read the JD, found it aligned with my aspirations, aced the test, sailed through interviews, and here I am.\n\nThough I've only been with Dwarves for just over a year, my job satisfaction is a solid 10/10. Thanks to remote work, I save 2-3 hours on commuting, granting me more time for fitness, knowledge updates, and quality moments with my parents.\n\nIn my year with Dwarves, I've worked on two projects: Console Labs and another client project. Starting at Console, I received big support from **Khoi** and **Tuan Dao**. Even with seemingly simple questions, they patiently explained and provided me with reading materials. I did appreciate it.\n\nAnd the most memorable experience was tackling the client project, where the difficulty level soared, delving into domains I hadn't touched before. For two weeks in a row, I worked tirelessly until 11PM to complete difficult tasks. As a newcomer, I hesitated to seek guidance initially, only turning to **Bien Vo** for the toughest queries. Then **Thanh Pham** caught up and assigned **Hieu Phan** as my mentor. Lucky me!\n\nDespite the steep learning curve, I realized a crucial lesson: when in doubt, reach out to Dwarves Team for support; their unwavering support propels both personal and professional growth.\n","title":"#22 Cat Nguyen on team support","short_title":"Cat Nguyen","description":"Cat Nguyen shares her experience as a Junior Backend Engineer at Dwarves, highlighting the supportive team culture and how asking for help accelerated her growth","tags":["life-at-dwarves, backend-engineer, teamwork"],"pinned":false,"draft":false,"hiring":false,"authors":[],"date":"2023-11-27","filePath":"careers/life/2023-11-27-22-cat-nguyen.md","slugArray":["careers","life","2023-11-27-22-cat-nguyen"]},{"content":"\nAt Dwarves, *the core team live and breathe growth*. We believe that growth is our universal language, and we're always striving to improve ourselves, both personally and professionally. It's not just a job for us; it's a way of life. We're hustlers with an extra hat, always pushing ourselves to be better and earn more.\n\nEvery time we hire, we're looking for like-minded individuals who share our growth mindset. We want to be with people who are willing to challenge themselves and reach new heights.\n\nEverything we do is centered around growth discussions and opportunity-seeking as a team. We don't believe in playing too much the work-life balance card; instead, we're always on the lookout for new opportunities to elevate.\n\n### **70/50**\n\nOur unspoken culture at Dwarves is called [70/50](https://github.com/dwarvesf/handbook/blob/master/what-we-value.md#7050), and it's all about growth and opportunity. We don't live by a 9-5 schedule; we're committed to putting in an extra 20-50% effort to grow ourselves after our day job is done. It's not mandatory, but it's encouraged. The extra effort we put in increases our chances of reaching new heights, and it's rewarded when the things we build hit the market and bring value to our customers.\n\nAs a company that's purely based on technological know-how, we spend our free time on hobby projects or exploring new technologies. It's not just about building our skills; it's about developing our passion for growth.\n\n\u003e We believe that NOT all people can get along with these ideas, but who know if the like-minded will find this place home.\n\n### Retaining people\n\nWe don't believe in retaining people by creating a non-challenging environment or looking back. We look to the future, always striving to be better and achieve more. When I sit down with managers from other companies, they often struggle with how to retain employees. But at Dwarves, we don't have that problem because we focus on growth, and that keeps us all around for the right reasons.\n\nSo, if you're looking for a place where growth is the norm, come join us at Dwarves. You'll be part of a team that's always hungry for more and committed to reaching new heights. We believe that growth is our universal language, and we can't wait to share it with you.\n\n![](assets/growth-is-our-universal-language_aa45d5d9f6a7dc8c1d87e835a8be0f87_md5.webp)\n","title":"Growth Is Our Universal Language","short_title":"","description":"","tags":["people","teamwork","culture"],"pinned":false,"draft":false,"hiring":false,"authors":["tieubao"],"date":"Thu Feb 16 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"culture/growth-is-our-universal-language.md","slugArray":["culture","growth-is-our-universal-language"]}],"changelogMemos":[],"hiringMemos":[]},"__N_SSG":true},"page":"/","query":{},"buildId":"6RUfDf25HpYZ1MIctalND","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>