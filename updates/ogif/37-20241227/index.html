<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion</title><meta property="title" content="OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion" data-next-head=""/><meta property="og:title" content="OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion" data-next-head=""/><meta name="description" content="In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q&amp;A to boost our technical skills." data-next-head=""/><meta property="og:description" content="In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q&amp;A to boost our technical skills." data-next-head=""/><meta name="keywords" content="office-hours, ogif, discord" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:site_name" content="Dwarves Memo" data-next-head=""/><link rel="icon" type="image/x-icon" href="{{ $favicon.Permalink }}" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><link rel="alternate" type="application/rss+xml" title="OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion - RSS Feed" href="/feed.xml" data-next-head=""/><link rel="alternate" type="application/atom+xml" title="OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion - Atom Feed" href="/atom.xml" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com" data-next-head=""/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous" data-next-head=""/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/669cdda7f755610e.css" as="style"/><link rel="preload" href="/_next/static/css/db96805030821792.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=Public+Sans:ital,wght@0,100..900;1,100..900&amp;family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&amp;display=swap" rel="stylesheet" data-next-head=""/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&amp;display=swap" rel="stylesheet" data-next-head=""/><link rel="stylesheet" href="/_next/static/css/669cdda7f755610e.css" data-n-g=""/><link rel="stylesheet" href="/_next/static/css/db96805030821792.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-e3db99e50f47eac1.js" defer=""></script><script src="/_next/static/chunks/framework-6e85e635ddcbf499.js" defer=""></script><script src="/_next/static/chunks/main-d8ff59cf99127f56.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c18a2b88d5866408.js" defer=""></script><script src="/_next/static/chunks/09244f9f-b7c50d32ae94dc35.js" defer=""></script><script src="/_next/static/chunks/8336-e98b59feaeebd03c.js" defer=""></script><script src="/_next/static/chunks/8771-a626ff6fb55a2f2d.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-4b0170d1861d0308.js" defer=""></script><script src="/_next/static/vgOJ-EFVJTcloGB4AsrPT/_buildManifest.js" defer=""></script><script src="/_next/static/vgOJ-EFVJTcloGB4AsrPT/_ssgManifest.js" defer=""></script><style id="__jsx-a10783270b45f6c2">[data-rk] .ju367v1g{font-weight:500}[data-rk] .ju367v1h{font-weight:500}[data-rk] .ju367v1i{font-weight:600}</style></head><body class="min-h-screen antialiased"><script>
              (function() {
                // Get saved theme or default to system preference
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const savedTheme = localStorage.getItem('theme');
                
                // Default to system preference if no saved preference
                const theme = (savedTheme === 'light' || savedTheme === 'dark') 
                  ? savedTheme 
                  : (prefersDark ? 'dark' : 'light');
                
                // Apply theme
                if (theme === 'dark') {
                  document.documentElement.classList.add('dark');
                  document.documentElement.setAttribute('data-theme', 'dark');
                } else {
                  document.documentElement.classList.remove('dark');
                  document.documentElement.setAttribute('data-theme', 'light');
                }
              })();
            </script><div id="__next"><div data-rk=""><style>[data-rk]{--rk-blurs-modalOverlay:small;--rk-fonts-body:var(--font-sans);--rk-radii-actionButton:var(--radius);--rk-radii-connectButton:var(--radius);--rk-radii-menuButton:var(--radius);--rk-radii-modal:var(--radius);--rk-radii-modalMobile:var(--radius);--rk-colors-accentColor:var(--primary);--rk-colors-accentColorForeground:var(--primary-foreground);--rk-colors-actionButtonBorder:transparent;--rk-colors-actionButtonBorderMobile:transparent;--rk-colors-actionButtonSecondaryBackground:var(--muted);--rk-colors-closeButton:var(--muted-foreground);--rk-colors-closeButtonBackground:var(--muted);--rk-colors-connectButtonBackground:var(--muted);--rk-colors-connectButtonBackgroundError:var(--primary);--rk-colors-connectButtonInnerBackground:var(--background);--rk-colors-connectButtonText:var(--muted-foreground);--rk-colors-connectButtonTextError:var(--primary-foreground);--rk-colors-connectionIndicator:var(--primary);--rk-colors-downloadBottomCardBackground:linear-gradient(126deg, rgba(255, 255, 255, 0) 9.49%, rgba(171, 171, 171, 0.04) 71.04%), #FFFFFF;--rk-colors-downloadTopCardBackground:linear-gradient(126deg, rgba(171, 171, 171, 0.2) 9.49%, rgba(255, 255, 255, 0) 71.04%), #FFFFFF;--rk-colors-error:var(--primary);--rk-colors-generalBorder:var(--border);--rk-colors-generalBorderDim:var(--border);--rk-colors-menuItemBackground:var(--muted);--rk-colors-modalBackdrop:color-mix(in oklab, var(--color-black) 50%, transparent);--rk-colors-modalBackground:var(--background);--rk-colors-modalBorder:var(--border);--rk-colors-modalText:var(--foreground);--rk-colors-modalTextDim:var(--muted-foreground);--rk-colors-modalTextSecondary:var(--muted-foreground);--rk-colors-profileAction:var(--muted);--rk-colors-profileActionHover:var(--accent);--rk-colors-profileForeground:var(--foreground);--rk-colors-selectedOptionBorder:var(--primary);--rk-colors-standby:var(--primary);--rk-shadows-connectButton:0px 4px 12px rgba(0, 0, 0, 0.1);--rk-shadows-dialog:0px 8px 32px rgba(0, 0, 0, 0.32);--rk-shadows-profileDetailsAction:0px 2px 6px rgba(37, 41, 46, 0.04);--rk-shadows-selectedOption:0px 2px 6px rgba(0, 0, 0, 0.24);--rk-shadows-selectedWallet:0px 2px 6px rgba(0, 0, 0, 0.12);--rk-shadows-walletLogo:0px 2px 16px rgba(0, 0, 0, 0.16);}</style><div class="bg-background border-border w-sidebar-mobile xl:w-sidebar fixed top-0 left-0 z-40 flex h-full flex-col border-r pt-4 pb-12 font-sans transition-transform duration-300 ease-in-out translate-x-[-100%] xl:translate-x-0 "><a class="mx-4 flex h-10 items-center gap-2 px-2 xl:mx-0 xl:justify-center" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6.25 w-6 min-w-6"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="inline-block font-sans text-xs leading-tight font-bold tracking-tight uppercase xl:hidden">Dwarves<br/>Memo</span></a><nav class="flex flex-1 flex-col p-4 xl:items-center xl:px-2"><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-0" data-state="closed" data-slot="tooltip-trigger" href="/"><div class="p-2"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6"><g id="Type=home, Fill=false"><path id="home" d="M6 18.9997H9.34625V13.0575H14.6538V18.9997H18V9.99972L12 5.48047L6 9.99972V18.9997ZM4.5 20.4997V9.24972L12 3.60547L19.5 9.24972V20.4997H13.1538V14.5575H10.8463V20.4997H4.5Z" fill="currentColor"></path></g></svg></div><span class="ml-3 inline-block xl:hidden">Home</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-1" data-state="closed" data-slot="tooltip-trigger" href="/consulting/"><div class="p-2"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6"><g id="Type=consulting, Fill=false"><path id="handshake" d="M11.7893 19.971C11.8879 19.971 11.9883 19.9479 12.0903 19.9017C12.1921 19.8556 12.2725 19.803 12.3315 19.744L20.3295 11.746C20.5552 11.5203 20.7251 11.2792 20.8393 11.0227C20.9533 10.7664 21.0103 10.4972 21.0103 10.2152C21.0103 9.9229 20.9533 9.64148 20.8393 9.37098C20.7251 9.10032 20.5552 8.85732 20.3295 8.64198L16.3295 4.64198C16.1142 4.41632 15.8809 4.25123 15.6295 4.14673C15.3784 4.0424 15.1066 3.99023 14.8143 3.99023C14.5323 3.99023 14.2614 4.0424 14.0018 4.14673C13.7421 4.25123 13.5027 4.41632 13.2835 4.64198L12.7103 5.21523L14.5603 7.08048C14.7846 7.29465 14.9504 7.5389 15.0575 7.81323C15.1645 8.08757 15.218 8.37215 15.218 8.66698C15.218 9.27732 15.0142 9.78632 14.6065 10.194C14.1989 10.6017 13.6899 10.8055 13.0795 10.8055C12.7847 10.8055 12.4991 10.7567 12.2228 10.6592C11.9466 10.5619 11.7014 10.4062 11.4873 10.192L9.59302 8.31323L5.24703 12.6592C5.17136 12.7349 5.11461 12.8196 5.07677 12.9132C5.03894 13.0067 5.02003 13.1029 5.02003 13.2017C5.02003 13.3862 5.08286 13.5439 5.20852 13.6747C5.33419 13.8056 5.48927 13.871 5.67377 13.871C5.77261 13.871 5.87294 13.8479 5.97477 13.8017C6.07677 13.7556 6.15727 13.703 6.21627 13.644L9.50077 10.3595L10.5545 11.4132L7.28527 14.6977C7.20977 14.7734 7.15311 14.8581 7.11527 14.9517C7.07744 15.0452 7.05852 15.1414 7.05852 15.2402C7.05852 15.4184 7.12294 15.5719 7.25178 15.7007C7.38061 15.8296 7.53411 15.894 7.71227 15.894C7.81111 15.894 7.91144 15.8709 8.01327 15.8247C8.11527 15.7786 8.19569 15.726 8.25452 15.667L11.6545 12.2825L12.7085 13.3362L9.32377 16.7362C9.25461 16.7952 9.19952 16.8757 9.15852 16.9775C9.11752 17.0795 9.09702 17.1798 9.09702 17.2785C9.09702 17.4568 9.16144 17.6104 9.29027 17.7392C9.41911 17.8681 9.57261 17.9325 9.75077 17.9325C9.84944 17.9325 9.94561 17.9136 10.0393 17.8757C10.1328 17.8379 10.2174 17.7812 10.293 17.7055L13.693 14.321L14.747 15.3747L11.347 18.7747C11.2714 18.8504 11.2146 18.9382 11.1768 19.0382C11.1389 19.1382 11.12 19.2344 11.12 19.3267C11.12 19.5112 11.1886 19.6647 11.3258 19.7872C11.4629 19.9097 11.6174 19.971 11.7893 19.971ZM11.7738 21.4707C11.2084 21.4707 10.7155 21.2747 10.295 20.8825C9.87452 20.4902 9.65469 20.0017 9.63552 19.417C9.06886 19.3785 8.59544 19.1772 8.21527 18.8132C7.83511 18.4491 7.62902 17.9708 7.59702 17.3785C7.00469 17.3402 6.52586 17.1332 6.16052 16.7575C5.79502 16.3818 5.59944 15.9093 5.57377 15.34C4.97894 15.3017 4.48794 15.086 4.10077 14.693C3.71361 14.3 3.52003 13.8029 3.52003 13.2017C3.52003 12.9069 3.57611 12.6181 3.68827 12.3352C3.80044 12.0526 3.96361 11.8042 4.17777 11.5902L9.59302 6.19023L12.522 9.11899C12.5809 9.18815 12.658 9.24332 12.7535 9.28448C12.8492 9.32548 12.9528 9.34598 13.0643 9.34598C13.2463 9.34598 13.4033 9.28573 13.5353 9.16523C13.6674 9.04473 13.7335 8.88698 13.7335 8.69198C13.7335 8.58048 13.713 8.47698 13.672 8.38148C13.6309 8.28598 13.5757 8.20873 13.5065 8.14973L9.99877 4.64198C9.78344 4.41632 9.54852 4.25123 9.29402 4.14673C9.03953 4.0424 8.76611 3.99023 8.47377 3.99023C8.19177 3.99023 7.92419 4.0424 7.67102 4.14673C7.41769 4.25123 7.17827 4.41632 6.95277 4.64198L3.66802 7.94198C3.48602 8.12398 3.33702 8.33907 3.22102 8.58723C3.10502 8.83523 3.03677 9.08815 3.01627 9.34598C2.99561 9.55882 3.00519 9.7694 3.04502 9.97773C3.08469 10.1861 3.15452 10.3819 3.25452 10.5652L2.15077 11.669C1.92511 11.3433 1.75527 10.9786 1.64127 10.5747C1.52711 10.1709 1.48027 9.76132 1.50077 9.34598C1.52127 8.88565 1.62511 8.44107 1.81227 8.01223C1.99944 7.5834 2.26161 7.2004 2.59877 6.86323L5.87378 3.58823C6.24827 3.22407 6.65569 2.95007 7.09602 2.76623C7.53636 2.58223 7.99886 2.49023 8.48352 2.49023C8.96802 2.49023 9.42886 2.58223 9.86602 2.76623C10.3034 2.95007 10.704 3.22407 11.068 3.58823L11.6413 4.16124L12.2143 3.58823C12.5886 3.22407 12.9944 2.95007 13.4315 2.76623C13.8687 2.58223 14.3296 2.49023 14.8143 2.49023C15.2989 2.49023 15.7614 2.58223 16.2018 2.76623C16.6421 2.95007 17.0444 3.22407 17.4085 3.58823L21.3835 7.56323C21.7475 7.9274 22.0264 8.34182 22.22 8.80649C22.4135 9.27115 22.5103 9.74582 22.5103 10.2305C22.5103 10.7152 22.4135 11.1761 22.22 11.6132C22.0264 12.0504 21.7475 12.451 21.3835 12.815L13.3853 20.7977C13.1648 21.0182 12.9164 21.1856 12.6403 21.2997C12.3639 21.4137 12.0751 21.4707 11.7738 21.4707Z" fill="currentColor"></path></g></svg></div><span class="ml-3 inline-block xl:hidden">Consulting</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-2" data-state="closed" data-slot="tooltip-trigger" href="/earn/"><div class="p-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" class="h-6 w-6"><g id="Type=earn, Fill=false"><path id="emoji_events" d="M7.40514 20.7734V19.2734H10.9629V15.8464C10.1142 15.6759 9.36422 15.3189 8.71289 14.7754C8.06156 14.2318 7.59872 13.5542 7.32439 12.7427C6.17056 12.6055 5.19689 12.1158 4.40339 11.2734C3.60972 10.4311 3.21289 9.4311 3.21289 8.27344V7.27344C3.21289 6.86444 3.36064 6.51219 3.65614 6.21669C3.95164 5.92119 4.30389 5.77344 4.71289 5.77344H7.03989V3.77344H16.3859V5.77344H18.7129C19.1219 5.77344 19.4741 5.92119 19.7696 6.21669C20.0651 6.51219 20.2129 6.86444 20.2129 7.27344V8.27344C20.2129 9.4311 19.8161 10.4311 19.0224 11.2734C18.2289 12.1158 17.2552 12.6055 16.1014 12.7427C15.8271 13.5542 15.3642 14.2318 14.7129 14.7754C14.0616 15.3189 13.3116 15.6759 12.4629 15.8464V19.2734H16.0206V20.7734H7.40514ZM7.03989 11.1312V7.27344H4.71289V8.27344C4.71289 8.97094 4.93147 9.58344 5.36864 10.1109C5.80581 10.6384 6.36289 10.9785 7.03989 11.1312ZM11.7129 14.4082C12.5911 14.4082 13.3362 14.102 13.9484 13.4897C14.5606 12.8775 14.8666 12.1324 14.8666 11.2542V5.27344H8.55914V11.2542C8.55914 12.1324 8.86522 12.8775 9.47739 13.4897C10.0896 14.102 10.8347 14.4082 11.7129 14.4082ZM16.3859 11.1312C17.0629 10.9785 17.62 10.6384 18.0571 10.1109C18.4943 9.58344 18.7129 8.97094 18.7129 8.27344V7.27344H16.3859V11.1312Z" fill="currentColor"></path></g></svg></div><span class="ml-3 inline-block xl:hidden">Earn</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-3" data-state="closed" data-slot="tooltip-trigger" href="/careers/hiring/"><div class="p-2"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6"><g id="Type=hiring, Fill=false"><path id="diversity_4" d="M19.2578 8.47559C19.5411 8.47559 19.7786 8.37975 19.9703 8.18809C20.162 7.99642 20.2578 7.75892 20.2578 7.47559C20.2578 7.19225 20.162 6.95475 19.9703 6.76309C19.7786 6.57142 19.5411 6.47559 19.2578 6.47559C18.9745 6.47559 18.737 6.57142 18.5453 6.76309C18.3536 6.95475 18.2578 7.19225 18.2578 7.47559C18.2578 7.75892 18.3536 7.99642 18.5453 8.18809C18.737 8.37975 18.9745 8.47559 19.2578 8.47559ZM19.2571 9.97559C18.5486 9.97559 17.9549 9.73625 17.4761 9.25759C16.9972 8.77875 16.7578 8.1855 16.7578 7.47784C16.7578 6.784 16.9972 6.1935 17.4761 5.70634C17.9547 5.21917 18.5479 4.97559 19.2556 4.97559C19.9494 4.97559 20.5399 5.21875 21.0271 5.70509C21.5142 6.19125 21.7578 6.78167 21.7578 7.47634C21.7578 8.18484 21.5146 8.7785 21.0283 9.25734C20.5421 9.73617 19.9517 9.97559 19.2571 9.97559ZM12.2578 7.47559C12.6745 7.47559 13.0286 7.33392 13.3203 7.05059C13.612 6.76725 13.7578 6.40892 13.7578 5.97559C13.7578 5.55892 13.612 5.20475 13.3203 4.91309C13.0286 4.62142 12.6745 4.47559 12.2578 4.47559C11.8245 4.47559 11.4661 4.62142 11.1828 4.91309C10.8995 5.20475 10.7578 5.55892 10.7578 5.97559C10.7578 6.40892 10.8995 6.76725 11.1828 7.05059C11.4661 7.33392 11.8245 7.47559 12.2578 7.47559ZM12.2583 8.97559C11.4156 8.97559 10.7049 8.68659 10.1261 8.10859C9.54723 7.53059 9.25781 6.82067 9.25781 5.97884C9.25781 5.151 9.54681 4.4435 10.1248 3.85634C10.7028 3.26917 11.4128 2.97559 12.2548 2.97559C13.0825 2.97559 13.7899 3.26842 14.3771 3.85409C14.9642 4.43959 15.2578 5.14659 15.2578 5.97509C15.2578 6.81775 14.965 7.5285 14.3793 8.10734C13.7938 8.68617 13.0868 8.97559 12.2583 8.97559ZM7.98856 13.3506C7.98856 13.9019 8.26165 14.5238 8.80781 15.2161C9.35398 15.9083 10.4431 16.9954 12.0751 18.4776L12.2578 18.6411L12.4598 18.4486C14.0265 17.0319 15.096 15.9697 15.6683 15.2618C16.2408 14.554 16.5271 13.9169 16.5271 13.3506C16.5271 12.8923 16.3761 12.5036 16.0743 12.1846C15.7726 11.8658 15.4005 11.7063 14.9578 11.7063C14.6853 11.7063 14.4275 11.7659 14.1843 11.8851C13.941 12.0044 13.7366 12.1686 13.5713 12.3776L12.4578 13.7063H12.0426L10.9193 12.3776C10.754 12.1686 10.5511 12.0044 10.3108 11.8851C10.0703 11.7659 9.81931 11.7063 9.55781 11.7063C9.09515 11.7063 8.7179 11.8658 8.42606 12.1846C8.1344 12.5036 7.98856 12.8923 7.98856 13.3506ZM6.48856 13.3506C6.48856 12.5314 6.7774 11.8026 7.35506 11.1641C7.93256 10.5256 8.66681 10.2063 9.55781 10.2063C10.0616 10.2063 10.5382 10.323 10.9876 10.5563C11.4369 10.7897 11.8186 11.0999 12.1328 11.4871L12.2578 11.6313L12.3828 11.4776C12.7033 11.0968 13.0841 10.7897 13.5251 10.5563C13.9661 10.323 14.4436 10.2063 14.9578 10.2063C15.8425 10.2063 16.5751 10.5285 17.1558 11.1728C17.7366 11.8173 18.0271 12.5433 18.0271 13.3506C18.0271 14.1826 17.7142 15.0166 17.0886 15.8526C16.4629 16.6884 15.2129 17.9634 13.3386 19.6776L12.2578 20.6678L11.1963 19.7063C9.27581 17.95 8.01106 16.6613 7.40206 15.8401C6.79306 15.0189 6.48856 14.1891 6.48856 13.3506ZM5.25781 8.47559C5.54115 8.47559 5.77865 8.37975 5.97031 8.18809C6.16198 7.99642 6.25781 7.75892 6.25781 7.47559C6.25781 7.19225 6.16198 6.95475 5.97031 6.76309C5.77865 6.57142 5.54115 6.47559 5.25781 6.47559C4.97448 6.47559 4.73698 6.57142 4.54531 6.76309C4.35365 6.95475 4.25781 7.19225 4.25781 7.47559C4.25781 7.75892 4.35365 7.99642 4.54531 8.18809C4.73698 8.37975 4.97448 8.47559 5.25781 8.47559ZM12.3328 21.4756V19.9756H20.2578V13.2833C20.2578 13.2 20.2274 13.1278 20.1666 13.0668C20.1056 13.006 20.0334 12.9756 19.9501 12.9756H17.3463V11.4756H19.9501C20.4472 11.4756 20.8728 11.6526 21.2268 12.0066C21.5808 12.3606 21.7578 12.7862 21.7578 13.2833V21.4756H12.3328ZM4.25781 19.9756H12.3328V21.4756H2.75781V13.2858C2.75781 12.7855 2.93106 12.3587 3.27756 12.0053C3.62406 11.6522 4.0534 11.4756 4.56556 11.4756H7.16931V12.9756H4.56556C4.47573 12.9756 4.40198 13.006 4.34431 13.0668C4.28665 13.1278 4.25781 13.2 4.25781 13.2833V19.9756ZM5.25706 9.97559C4.54856 9.97559 3.9549 9.73625 3.47606 9.25759C2.99723 8.77875 2.75781 8.1855 2.75781 7.47784C2.75781 6.784 2.99723 6.1935 3.47606 5.70634C3.95473 5.21917 4.5479 4.97559 5.25556 4.97559C5.9494 4.97559 6.5399 5.21875 7.02706 5.70509C7.51423 6.19125 7.75781 6.78167 7.75781 7.47634C7.75781 8.18484 7.51465 8.7785 7.02831 9.25734C6.54215 9.73617 5.95173 9.97559 5.25706 9.97559Z" fill="currentColor"></path></g></svg></div><span class="ml-3 inline-block xl:hidden">Hiring</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start" id="sidebar-item-4" data-state="closed" data-slot="tooltip-trigger" href="/updates/digest/"><div class="p-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" class="h-6 w-6"><path d="M5.12806 21.3701C4.6229 21.3701 4.19531 21.1951 3.84531 20.8451C3.49531 20.4951 3.32031 20.0676 3.32031 19.5624V6.17788C3.32031 5.67272 3.49531 5.24513 3.84531 4.89513C4.19531 4.54513 4.6229 4.37013 5.12806 4.37013H6.51256V2.25488H8.05106V4.37013H15.6281V2.25488H17.1281V4.37013H18.5126C19.0177 4.37013 19.4453 4.54513 19.7953 4.89513C20.1453 5.24513 20.3203 5.67272 20.3203 6.17788V19.5624C20.3203 20.0676 20.1453 20.4951 19.7953 20.8451C19.4453 21.1951 19.0177 21.3701 18.5126 21.3701H5.12806ZM5.12806 19.8701H18.5126C18.5896 19.8701 18.6601 19.8381 18.7241 19.7739C18.7882 19.7099 18.8203 19.6394 18.8203 19.5624V10.1779H4.82031V19.5624C4.82031 19.6394 4.8524 19.7099 4.91656 19.7739C4.98056 19.8381 5.05106 19.8701 5.12806 19.8701ZM4.82031 8.67788H18.8203V6.17788C18.8203 6.10088 18.7882 6.03038 18.7241 5.96638C18.6601 5.90222 18.5896 5.87013 18.5126 5.87013H5.12806C5.05106 5.87013 4.98056 5.90222 4.91656 5.96638C4.8524 6.03038 4.82031 6.10088 4.82031 6.17788V8.67788Z" fill="currentColor"></path></svg></div><span class="ml-3 inline-block xl:hidden">Digest</span></a><a class="hover:bg-muted dark:hover:bg-muted flex items-center rounded-lg text-sm font-medium transition-colors md:justify-start text-primary" id="sidebar-item-5" data-state="closed" data-slot="tooltip-trigger" href="/updates/ogif/"><div class="p-2"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6"><g id="Type=ogif, Fill=false"><path id="psychology" d="M6.77148 21.8418V17.811C5.82148 16.9444 5.08398 15.9511 4.55898 14.8313C4.03398 13.7113 3.77148 12.5458 3.77148 11.3348C3.77148 8.97563 4.5979 6.97038 6.25073 5.31905C7.9034 3.66755 9.91032 2.8418 12.2715 2.8418C14.2138 2.8418 15.9507 3.4223 17.482 4.5833C19.0135 5.74413 20.0081 7.25021 20.4657 9.10155L21.634 13.716C21.709 14.0012 21.6561 14.2601 21.4752 14.4928C21.2946 14.7255 21.0536 14.8418 20.7522 14.8418H18.7715V18.034C18.7715 18.5312 18.5945 18.9568 18.2405 19.3108C17.8865 19.6648 17.4609 19.8418 16.9637 19.8418H14.7715V21.8418H13.2715V18.3418H16.9637C17.0536 18.3418 17.1273 18.313 17.185 18.2553C17.2427 18.1976 17.2715 18.1239 17.2715 18.034V13.3418H19.9715L19.0215 9.4668C18.6382 7.9438 17.8183 6.70888 16.562 5.76205C15.3055 4.81521 13.8753 4.3418 12.2715 4.3418C10.3382 4.3418 8.68815 5.01771 7.32148 6.36955C5.95482 7.72138 5.27148 9.3653 5.27148 11.3013C5.27148 12.2998 5.47565 13.2483 5.88398 14.1468C6.29232 15.0453 6.87148 15.844 7.62148 16.5428L8.27148 17.1418V21.8418H6.77148ZM11.4927 15.1303H13.0502L13.1617 14.0438C13.3592 13.9938 13.5442 13.921 13.7167 13.8255C13.8891 13.7299 14.0362 13.6115 14.158 13.4705L15.135 13.9225L15.9137 12.5975L15.0485 11.9533C15.1202 11.7495 15.156 11.5456 15.156 11.3418C15.156 11.138 15.1202 10.9341 15.0485 10.7303L15.9137 10.086L15.135 8.76105L14.158 9.21305C14.0362 9.07205 13.8891 8.95371 13.7167 8.85805C13.5442 8.76255 13.3592 8.6898 13.1617 8.6398L13.0502 7.5533H11.4927L11.3812 8.6398C11.1837 8.6898 10.9987 8.76255 10.8262 8.85805C10.6539 8.95371 10.5068 9.07205 10.385 9.21305L9.40798 8.76105L8.62923 10.086L9.49448 10.7303C9.42282 10.9341 9.38698 11.138 9.38698 11.3418C9.38698 11.5456 9.42282 11.7495 9.49448 11.9533L8.62923 12.5975L9.40798 13.9225L10.385 13.4705C10.5068 13.6115 10.6539 13.7299 10.8262 13.8255C10.9987 13.921 11.1837 13.9938 11.3812 14.0438L11.4927 15.1303ZM12.271 13.0245C11.8033 13.0245 11.4061 12.8609 11.0792 12.5335C10.7522 12.2062 10.5887 11.8088 10.5887 11.3413C10.5887 10.8736 10.7524 10.4764 11.0797 10.1495C11.4071 9.82255 11.8045 9.65905 12.272 9.65905C12.7397 9.65905 13.1369 9.82271 13.4637 10.15C13.7907 10.4774 13.9542 10.8748 13.9542 11.3423C13.9542 11.81 13.7906 12.2072 13.4632 12.534C13.1359 12.861 12.7385 13.0245 12.271 13.0245Z" fill="currentColor"></path></g></svg></div><span class="ml-3 inline-block xl:hidden">OGIFs</span></a></nav><div class="mx-4 border-t pt-1 xl:mx-2"><div class="flex items-center justify-between gap-3 p-2 xl:justify-center"><button class="flex cursor-pointer items-center justify-center hover:opacity-80"><svg viewBox="0 0 20 20" width="24" height="24"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></button><span class="inline-block flex-1 shrink-0 text-sm leading-6 font-medium xl:hidden">Night mode</span><button class="bg-border flex h-5 w-9 cursor-pointer items-center justify-center rounded-full py-0.5 pr-4.5 pl-0.5 hover:opacity-95 xl:hidden"><div class="text-foreground-light rounded-full bg-white p-0.5"><svg viewBox="0 0 20 20" width="12" height="12"><path d="M16.667 12.3249L17.3564 12.6202C17.4795 12.3329 17.4115 11.9994 17.1857 11.7832C16.96 11.567 16.6239 11.5135 16.3421 11.6489L16.667 12.3249ZM8.19804 2.3999L8.79449 2.85459C8.9845 2.60535 8.99949 2.26424 8.83208 1.99928C8.66467 1.73433 8.35016 1.60141 8.04348 1.666L8.19804 2.3999ZM13.6635 12.2548C10.3006 12.2548 7.60587 9.59905 7.60587 6.36135L6.10587 6.36135C6.10587 10.4618 9.50689 13.7548 13.6635 13.7548L13.6635 12.2548ZM16.3421 11.6489C15.5358 12.0364 14.6271 12.2548 13.6635 12.2548L13.6635 13.7548C14.8559 13.7548 15.9863 13.4841 16.9918 13.0009L16.3421 11.6489ZM15.9776 12.0295C14.9688 14.384 12.579 16.0499 9.77963 16.0499L9.77963 17.5499C13.1836 17.5499 16.1131 15.5222 17.3564 12.6202L15.9776 12.0295ZM9.77963 16.0499C6.05539 16.0499 3.06774 13.1083 3.06774 9.51796L1.56774 9.51796C1.56774 13.971 5.26169 17.5499 9.77963 17.5499L9.77963 16.0499ZM3.06774 9.51796C3.06774 6.3999 5.31884 3.77274 8.3526 3.1338L8.04348 1.666C4.35439 2.44295 1.56774 5.65176 1.56774 9.51796L3.06774 9.51796ZM7.60587 6.36135C7.60587 5.04819 8.0465 3.83578 8.79449 2.85459L7.60159 1.94521C6.66318 3.17619 6.10587 4.70542 6.10587 6.36135L7.60587 6.36135Z" fill="currentColor"></path><path d="M13.9357 2.46517C13.5852 2.2404 13.1672 2.64169 13.4007 2.97822L13.8173 3.57826C13.9864 3.82156 14.0766 4.10745 14.0766 4.3999C14.0766 4.69235 13.9864 4.97825 13.8173 5.22154L13.4007 5.82158C13.1672 6.15811 13.5858 6.55941 13.9364 6.33463L14.5607 5.93461C14.8141 5.77233 15.1119 5.68573 15.4165 5.68573C15.7211 5.68573 16.0189 5.77233 16.2723 5.93461L16.8973 6.33463C17.2478 6.55941 17.6658 6.15811 17.4317 5.82158L17.015 5.22154C16.846 4.97825 16.7558 4.69235 16.7558 4.3999C16.7558 4.10745 16.846 3.82156 17.015 3.57826L17.4317 2.97822C17.6658 2.64169 17.2478 2.2404 16.8966 2.46517L16.2723 2.8652C16.0189 3.02747 15.7211 3.11407 15.4165 3.11407C15.1119 3.11407 14.8141 3.02747 14.5607 2.8652L13.9357 2.46517Z" fill="currentColor" fill-opacity="0.25"></path></svg></div></button></div></div></div><div class="bg-background text-foreground relative flex h-screen font-sans transition-colors "><div id="sidebar" class="bg-background-secondary xl:w-directory-width h-[calc(100svh-32px)] w-0 flex-col pt-10 pb-2 text-sm leading-normal xl:pr-3 xl:ml-sidebar translate-0 transition duration-100 ease-in-out z-2 overflow-y-auto reading:opacity-0 reading:translate-x-[-10%] reading:pr-0 reading:w-0 reading:ml-0"><div class=""><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/pinned/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Pinned Notes</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium text-muted-foreground pl-2" href="/culture/"><span>Notes on our culture</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium text-muted-foreground pl-2" href="/culture/ogif-intro/"><span>OGIF - Oh God It&#x27;s Friday</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out"><path d="m6 9 6 6 6-6"></path></svg><span>Home</span></a><div class="m-0 w-full pl-1"><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/careers/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Careers</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/consulting/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Consulting</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/culture/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Culture</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/earn/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Earn</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/fund/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Fund</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/handbook/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Handbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/opensource/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Opensource</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playbook/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playbook</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/playground/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Playground</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/radar/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Radar</span></a></div><div class="relative flex flex-col before:bg-border before:absolute before:top-0 before:left-[7px] before:h-full before:w-[1px] before:content-[&#x27;&#x27;] after:bg-border pl-3 after:absolute after:top-1/2 after:left-[7px] after:h-full after:w-[1px] after:origin-center after:-translate-y-1/2 after:scale-y-0 after:transition-transform after:duration-300 after:ease-in-out after:content-[&#x27;&#x27;]"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/updates/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Updates</span></a></div></div></div><div class="relative flex flex-col"><a class="flex cursor-pointer items-center gap-1 p-1.25 text-left text-xs leading-normal font-medium" href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down stroke-muted-foreground transition-all duration-300 ease-in-out -rotate-90"><path d="m6 9 6 6 6-6"></path></svg><span>Popular Tags</span></a></div></div></div><div class="relative flex flex-1 flex-col overflow-y-auto"><header class="bg-background/95 supports-[backdrop-filter]:bg-background/60 top-0 w-full shrink-0 font-sans backdrop-blur"><div class="mx-auto flex h-full items-center justify-between border-b p-2 xl:border-none xl:px-5"><div class="flex items-center gap-2.5"><button id="sidebar-toggle" aria-label="Toggle sidebar" class="flex h-10 w-10 cursor-pointer items-center justify-center focus:outline-none xl:hidden"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="text-current"><path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><a class="flex items-center gap-2 xl:hidden" href="/"><svg width="24" height="24" viewBox="0 0 19 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-[32px] w-[29px] min-w-6 shrink-0"><path d="M2.41664 20C1.08113 20 0 18.8812 0 17.4991V2.50091C0 1.11883 1.08113 0 2.41664 0L8.46529 0.00731261C13.8427 0.00731261 18.1954 4.55576 18.1248 10.1353C18.0541 15.6271 13.6307 20 8.32397 20H2.41664Z" fill="#E13F5E"></path><path d="M3.63209 15.6271H3.32118C3.15159 15.6271 3.01733 15.4881 3.01733 15.3126V12.8044C3.01733 12.6289 3.15159 12.49 3.32118 12.49H5.74488C5.91447 12.49 6.04873 12.6289 6.04873 12.8044V13.1262C6.04873 14.5082 4.9676 15.6271 3.63209 15.6271Z" fill="white"></path><path d="M3.32119 8.11701H10.8749C12.2105 8.11701 13.2916 6.99818 13.2916 5.6161V5.31628C13.2916 5.13347 13.1503 4.98721 12.9736 4.98721H5.44105C4.10554 4.98721 3.02441 6.10604 3.02441 7.48813V7.80257C3.02441 7.97807 3.15867 8.11701 3.32119 8.11701Z" fill="white"></path><path d="M3.32118 11.8684H7.24998C8.58549 11.8684 9.66661 10.7496 9.66661 9.36747V9.05303C9.66661 8.87753 9.53236 8.73859 9.36277 8.73859H3.32118C3.15159 8.73859 3.01733 8.87753 3.01733 9.05303V11.5539C3.0244 11.7294 3.15866 11.8684 3.32118 11.8684Z" fill="white"></path></svg><span class="font-ibm-sans text-xs text-[11px] leading-[14.849px] font-bold -tracking-[0.157px] uppercase">Dwarves<br/>Memo</span></a></div><div class="ml-auto flex items-center gap-3.5"><div class="command-palette relative z-50"><button class="hover:border-border hidden w-50 cursor-pointer justify-between rounded-lg border border-transparent bg-transparent px-3 py-1.5 transition-all duration-100 ease-in-out md:flex" aria-label="Open command palette"><div class="flex items-center gap-0.5"><div class="text-muted-foreground flex items-center gap-1 text-sm filter-[opacity(50%)]"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Type=search, Fill=false"><path id="search" d="M19.1383 20.1923L12.8575 13.9113C12.3575 14.3241 11.7825 14.6472 11.1325 14.8805C10.4825 15.1138 9.81008 15.2305 9.11525 15.2305C7.40608 15.2305 5.95958 14.6388 4.77575 13.4553C3.59192 12.2718 3 10.8256 3 9.11675C3 7.40808 3.59175 5.96142 4.77525 4.77675C5.95875 3.59225 7.40492 3 9.11375 3C10.8224 3 12.2691 3.59192 13.4537 4.77575C14.6382 5.95958 15.2305 7.40608 15.2305 9.11525C15.2305 9.82942 15.1107 10.5115 14.871 11.1615C14.6312 11.8115 14.3112 12.3768 13.9113 12.8575L20.192 19.1383L19.1383 20.1923ZM9.11525 13.7308C10.4037 13.7308 11.4951 13.2836 12.3893 12.3892C13.2836 11.4951 13.7308 10.4038 13.7308 9.11525C13.7308 7.82675 13.2836 6.73542 12.3893 5.84125C11.4951 4.94692 10.4037 4.49975 9.11525 4.49975C7.82675 4.49975 6.73542 4.94692 5.84125 5.84125C4.94692 6.73542 4.49975 7.82675 4.49975 9.11525C4.49975 10.4038 4.94692 11.4951 5.84125 12.3892C6.73542 13.2836 7.82675 13.7308 9.11525 13.7308Z" fill="currentColor"></path></g></svg><span class="">Search note</span></div></div><div class="text-muted-foreground flex items-center gap-0.5 text-xs"><kbd class="text-black-secondary dark:bg-border dark:text-foreground rounded-[2px] bg-[#F7F7F7] px-1.5 py-0.5 font-sans shadow-[0px_2px_0px_0px_#D4D3D0] dark:shadow-[0px_2px_0px_0px_#2D2D2D]">⌘</kbd><kbd class="text-black-secondary dark:bg-border dark:text-foreground rounded-[2px] bg-[#F7F7F7] px-1.5 py-0.5 font-sans shadow-[0px_2px_0px_0px_#D4D3D0] dark:shadow-[0px_2px_0px_0px_#2D2D2D]">K</kbd></div></button><button class="text-foreground flex h-10 w-10 items-center justify-center border-none bg-transparent p-0 md:hidden" aria-label="Open search"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Type=search, Fill=false"><path id="search" d="M19.1383 20.1923L12.8575 13.9113C12.3575 14.3241 11.7825 14.6472 11.1325 14.8805C10.4825 15.1138 9.81008 15.2305 9.11525 15.2305C7.40608 15.2305 5.95958 14.6388 4.77575 13.4553C3.59192 12.2718 3 10.8256 3 9.11675C3 7.40808 3.59175 5.96142 4.77525 4.77675C5.95875 3.59225 7.40492 3 9.11375 3C10.8224 3 12.2691 3.59192 13.4537 4.77575C14.6382 5.95958 15.2305 7.40608 15.2305 9.11525C15.2305 9.82942 15.1107 10.5115 14.871 11.1615C14.6312 11.8115 14.3112 12.3768 13.9113 12.8575L20.192 19.1383L19.1383 20.1923ZM9.11525 13.7308C10.4037 13.7308 11.4951 13.2836 12.3893 12.3892C13.2836 11.4951 13.7308 10.4038 13.7308 9.11525C13.7308 7.82675 13.2836 6.73542 12.3893 5.84125C11.4951 4.94692 10.4037 4.49975 9.11525 4.49975C7.82675 4.49975 6.73542 4.94692 5.84125 5.84125C4.94692 6.73542 4.49975 7.82675 4.49975 9.11525C4.49975 10.4038 4.94692 11.4951 5.84125 12.3892C6.73542 13.2836 7.82675 13.7308 9.11525 13.7308Z" fill="currentColor"></path></g></svg></button></div><button class="hidden cursor-pointer items-center justify-center border-0 bg-transparent outline-none hover:opacity-95 active:opacity-100 xl:flex" aria-label="Toggle reading mode" data-reading-mode="false" data-state="closed" data-slot="tooltip-trigger"><svg width="48" height="28" viewBox="0 0 62 34" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-7 w-12 xl:w-14"><g><rect width="62" height="34" rx="17" class="fill-border dark:fill-border"></rect><g class="transition-transform duration-150 ease-in-out translate-x-0"><circle cx="17" cy="17" r="14" class="fill-white"></circle><path d="M17 23.898V18.3265C17 17.9747 17.1398 17.6373 17.3885 17.3885C17.6373 17.1398 17.9747 17 18.3265 17C18.6783 17 19.0158 17.1398 19.2645 17.3885C19.5133 17.6373 19.6531 17.9747 19.6531 18.3265V21.2449H21.7755C22.3384 21.2449 22.8782 21.4685 23.2763 21.8666C23.6744 22.2646 23.898 22.8045 23.898 23.3673V23.898" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2119 12.8561C14.8891 11.4004 13.114 10.4334 11.1736 10.1113C11.0416 10.0926 10.9071 10.1022 10.7791 10.1395C10.6511 10.1768 10.5324 10.2409 10.4311 10.3275C10.3279 10.4158 10.245 10.5253 10.1883 10.6487C10.1315 10.772 10.1021 10.9062 10.1021 11.0419V18.6088C10.1007 18.8411 10.1854 19.0658 10.3399 19.2394C10.4944 19.413 10.7077 19.5232 10.9386 19.5487C12.4542 19.7543 13.8794 20.354 15.0774 21.276" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M16.2124 15.7885V12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path><path d="M21.4852 19.5487C21.7161 19.5232 21.9295 19.413 22.084 19.2394C22.2385 19.0658 22.3232 18.8411 22.3218 18.6088V11.0419C22.3218 10.9062 22.2924 10.772 22.2356 10.6487C22.1788 10.5253 22.096 10.4158 21.9928 10.3275C21.8915 10.2409 21.7728 10.1768 21.6447 10.1395C21.5167 10.1022 21.3823 10.0926 21.2502 10.1113C19.3098 10.4334 17.5347 11.4004 16.2119 12.8561" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-[#333639]"></path></g></g></svg></button><div class="h-9 w-9"></div></div></div></header><div class="main-grid relative w-full flex-1 flex-col"><div class="right-sidebar leading-[140% xl:w-right-sidebar-width hidden font-sans text-sm font-medium xl:flex transition-[transform,opacity,visibility] duration-100 ease-in-out visible w-0 translate-x-0 transform opacity-100 reading:opacity-0 reading:xl:translate-x-[10px] reading:invisible reading:fixed reading:right-[calc((100vw-var(--container-max-width)-var(--nav-sidebar-width))/2-var(--right-sidebar-width)-var(--column-gap))]"><div class="sticky top-[60px] right-0 flex w-full flex-col gap-y-8 pt-4 pb-10 transition-[top] duration-200 ease-in-out"><div class="metadata space-y-6"><div class=""><h3 class="text-black-secondary dark:text-foreground text-2xs mb-3 font-sans font-semibold tracking-[0.8px] uppercase">Properties</h3><ul class="space-y-2 text-sm"><li class="text-secondary-foreground dark:text-secondary-light flex flex-wrap items-center gap-1 text-xs leading-4 -tracking-[0.125px]"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" class="h-4 w-4"><path d="M5.12806 21.3701C4.6229 21.3701 4.19531 21.1951 3.84531 20.8451C3.49531 20.4951 3.32031 20.0676 3.32031 19.5624V6.17788C3.32031 5.67272 3.49531 5.24513 3.84531 4.89513C4.19531 4.54513 4.6229 4.37013 5.12806 4.37013H6.51256V2.25488H8.05106V4.37013H15.6281V2.25488H17.1281V4.37013H18.5126C19.0177 4.37013 19.4453 4.54513 19.7953 4.89513C20.1453 5.24513 20.3203 5.67272 20.3203 6.17788V19.5624C20.3203 20.0676 20.1453 20.4951 19.7953 20.8451C19.4453 21.1951 19.0177 21.3701 18.5126 21.3701H5.12806ZM5.12806 19.8701H18.5126C18.5896 19.8701 18.6601 19.8381 18.7241 19.7739C18.7882 19.7099 18.8203 19.6394 18.8203 19.5624V10.1779H4.82031V19.5624C4.82031 19.6394 4.8524 19.7099 4.91656 19.7739C4.98056 19.8381 5.05106 19.8701 5.12806 19.8701ZM4.82031 8.67788H18.8203V6.17788C18.8203 6.10088 18.7882 6.03038 18.7241 5.96638C18.6601 5.90222 18.5896 5.87013 18.5126 5.87013H5.12806C5.05106 5.87013 4.98056 5.90222 4.91656 5.96638C4.8524 6.03038 4.82031 6.10088 4.82031 6.17788V8.67788Z" fill="currentColor"></path></svg><span>Created:</span><span>Dec 29, 2024</span></li><li class="text-secondary-foreground dark:text-secondary-light flex flex-wrap items-center gap-1 text-xs leading-4 -tracking-[0.125px]"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="16" height="16"><path fill="currentColor" fill-rule="evenodd" d="M18.685 19.097A9.723 9.723 0 0 0 21.75 12c0-5.385-4.365-9.75-9.75-9.75S2.25 6.615 2.25 12a9.723 9.723 0 0 0 3.065 7.097A9.716 9.716 0 0 0 12 21.75a9.716 9.716 0 0 0 6.685-2.653m-12.54-1.285A7.486 7.486 0 0 1 12 15a7.486 7.486 0 0 1 5.855 2.812A8.224 8.224 0 0 1 12 20.25a8.224 8.224 0 0 1-5.855-2.438M15.75 9a3.75 3.75 0 1 1-7.5 0a3.75 3.75 0 0 1 7.5 0" clip-rule="evenodd"></path></svg><span>Author:</span><a class="hover:text-primary hover:underline" href="/contributor/innno_/">innno_</a></li><li class="text-secondary-foreground dark:text-secondary-light flex flex-wrap items-center gap-1 text-xs leading-4 -tracking-[0.125px]"><svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Type=tags, Fill=true"><path id="shoppingmode" d="M11.0909 21.5C10.865 21.5 10.6385 21.4548 10.4114 21.3645C10.1842 21.274 9.97979 21.1384 9.79813 20.9578L3.04238 14.202C2.85921 14.0193 2.72563 13.8176 2.64163 13.5968C2.55763 13.3761 2.51562 13.1516 2.51562 12.9233C2.51562 12.6949 2.55763 12.4672 2.64163 12.24C2.72563 12.013 2.85921 11.8087 3.04238 11.627L11.6116 3.04225C11.7776 2.87658 11.9739 2.74483 12.2004 2.647C12.4267 2.549 12.6606 2.5 12.9021 2.5H19.6829C20.1879 2.5 20.617 2.67792 20.9704 3.03375C21.3235 3.38975 21.5001 3.81758 21.5001 4.31725V11.098C21.5001 11.342 21.455 11.5745 21.3646 11.7955C21.2741 12.0165 21.1436 12.209 20.9731 12.373L12.3789 20.9578C12.1982 21.1384 11.9949 21.274 11.7689 21.3645C11.5429 21.4548 11.3169 21.5 11.0909 21.5ZM17.4714 7.77875C17.8212 7.77875 18.1185 7.65725 18.3634 7.41425C18.6084 7.17125 18.7309 6.87608 18.7309 6.52875C18.7309 6.17892 18.6088 5.88158 18.3646 5.63675C18.1205 5.39175 17.824 5.26925 17.4751 5.26925C17.1265 5.26925 16.8303 5.39133 16.5866 5.6355C16.3431 5.87967 16.2214 6.17617 16.2214 6.525C16.2214 6.87367 16.3429 7.16983 16.5859 7.4135C16.8289 7.657 17.124 7.77875 17.4714 7.77875Z" fill="currentColor"></path></g></svg><span>Tags:</span> <a class="bg-muted hover:bg-muted/80 hover:text-primary dark:bg-border dark:text-foreground dark:hover:text-primary inline-flex items-center rounded-md px-1.5 py-0.5 text-[10px] font-medium text-[#4b4f53]" href="/tags/office-hours/">office-hours</a><a class="bg-muted hover:bg-muted/80 hover:text-primary dark:bg-border dark:text-foreground dark:hover:text-primary inline-flex items-center rounded-md px-1.5 py-0.5 text-[10px] font-medium text-[#4b4f53]" href="/tags/ogif/">OGIF</a><a class="bg-muted hover:bg-muted/80 hover:text-primary dark:bg-border dark:text-foreground dark:hover:text-primary inline-flex items-center rounded-md px-1.5 py-0.5 text-[10px] font-medium text-[#4b4f53]" href="/tags/discord/">discord</a></li></ul></div><div class=""><h3 class="text-black-secondary dark:text-foreground text-2xs mb-3 font-sans font-semibold tracking-[0.8px] uppercase">Location</h3><ul class="space-y-2 text-sm"><li class="text-secondary-foreground dark:text-secondary-light vertical-center inline text-xs leading-4 -tracking-[0.125px]"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="16" height="16" class="mr-1 mb-0.5 inline-block"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M2.25 12.75V12A2.25 2.25 0 0 1 4.5 9.75h15A2.25 2.25 0 0 1 21.75 12v.75m-8.69-6.44l-2.12-2.12a1.5 1.5 0 0 0-1.061-.44H4.5A2.25 2.25 0 0 0 2.25 6v12a2.25 2.25 0 0 0 2.25 2.25h15A2.25 2.25 0 0 0 21.75 18V9a2.25 2.25 0 0 0-2.25-2.25h-5.379a1.5 1.5 0 0 1-1.06-.44"></path></svg><span>Folder:</span> <a class="hover:text-primary inline break-all hover:underline" href="/updates/ogif/">updates/ogif</a></li></ul></div><div class=""><h3 class="text-black-secondary dark:text-foreground text-2xs mb-3 font-sans font-semibold tracking-[0.8px] uppercase">Stats</h3><ul class="space-y-2 text-sm"><li class="text-secondary-foreground dark:text-secondary-light flex items-center justify-between gap-1 text-xs leading-4 -tracking-[0.125px]"><span>Words:</span><span>13,659</span></li><li class="text-secondary-foreground dark:text-secondary-light flex items-center justify-between gap-1 text-xs leading-4 -tracking-[0.125px]"><span>Characters:</span><span>74,667</span></li><li class="text-secondary-foreground dark:text-secondary-light flex items-center justify-between gap-1 text-xs leading-4 -tracking-[0.125px]"><span>Blocks:</span><span>163</span></li><li class="text-secondary-foreground dark:text-secondary-light flex items-center justify-between gap-1 text-xs leading-4 -tracking-[0.125px]"><span>Reading time:</span><span>69m</span></li></ul></div></div></div></div><div class="toc relative z-10 hidden md:block"><div class="toc-indicators peer fixed top-[var(--header-height)] right-0 mt-15 cursor-pointer pr-2 pb-4 pl-5"><div class=""> <ul class="flex h-full list-none flex-col items-end gap-3 pl-0 text-right"><li class="flex h-full list-none flex-col items-end gap-3 pl-0 text-right"><a style="width:15px" class="bg-border flex h-0.5 text-transparent" href="/updates/ogif/37-20241227/#topics-and-highlights">Topics and Highlights</a></li><li class="flex h-full list-none flex-col items-end gap-3 pl-0 text-right"><a style="width:15px" class="bg-border flex h-0.5 text-transparent" href="/updates/ogif/37-20241227/#vietnamese-transcript">Vietnamese transcript</a></li><li class="flex h-full list-none flex-col items-end gap-3 pl-0 text-right"><a style="width:15px" class="bg-border flex h-0.5 text-transparent" href="/updates/ogif/37-20241227/#english-transcript">English transcript</a></li></ul></div></div><div class="toc-modal bg-background fixed top-[var(--header-height)] right-0 m-2 mt-17 max-w-[320px] rounded-xl shadow-[0px_4px_6px_-2px_#10182808,0px_12px_16px_-4px_#10182814] invisible translate-x-[12px] opacity-0 ease transition-all duration-300 peer-hover:visible peer-hover:translate-x-0 peer-hover:opacity-100 hover:visible hover:translate-x-0 hover:opacity-100"><div class="max-h-[min(680px,calc(100vh-var(--header-height)-68px-32px-2rem))] overflow-y-auto p-4"><ul class="flex h-full list-none flex-col items-end pl-0"><li class="w-full leading-6"><a style="margin-left:0px" class="text-foreground flex rounded-lg p-1 text-[13px] leading-4 transition-all duration-150 heading-level-3 hover:bg-background-tertiary-light hover:dark:bg-background-tertiary" href="/updates/ogif/37-20241227/#topics-and-highlights">Topics and Highlights</a></li><li class="w-full leading-6"><a style="margin-left:0px" class="text-foreground flex rounded-lg p-1 text-[13px] leading-4 transition-all duration-150 heading-level-3 hover:bg-background-tertiary-light hover:dark:bg-background-tertiary" href="/updates/ogif/37-20241227/#vietnamese-transcript">Vietnamese transcript</a></li><li class="w-full leading-6"><a style="margin-left:0px" class="text-foreground flex rounded-lg p-1 text-[13px] leading-4 transition-all duration-150 heading-level-3 hover:bg-background-tertiary-light hover:dark:bg-background-tertiary" href="/updates/ogif/37-20241227/#english-transcript">English transcript</a></li></ul></div></div></div><main class="main-content mx-auto max-w-[var(--container-max-width)] min-w-0 flex-1 p-[var(--main-padding-mobile)] font-serif xl:p-[var(--main-padding)]"><img alt="" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" class="yggdrasil-tree no-zoom pointer-events-none object-contain opacity-[0.03] md:w-[20vw] xl:w-[20vw] dark:opacity-100 absolute bottom-8 left-1/2 w-[50vw] max-w-xs -translate-x-1/2 xl:translate-x-[80%]" style="color:transparent" src="/assets/img/footer-bg.svg"/><div class="memo-content pb-8"><div class="content-wrapper"><div class="content-layout xl:m-h-[400px]"><div class="flex flex-col items-start justify-between md:flex-row"><div class="hidden">OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion</div><div class="flex-1"><h1 class="mt-0 mb-5 pb-0 font-serif text-[35px] leading-[42px] font-semibold tracking-tight">OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion</h1></div></div><div class="prose dark:prose-dark prose-headings:font-serif prose-headings:font-semibold prose-headings:tracking-tight max-w-none font-serif prose-a:text-primary prose-a:underline prose-a:decoration-neutral-200 prose-a:hover:text-primary prose-a:hover:decoration-primary prose-a:font-[inherit] prose-table:border prose-img:mt-[var(--element-margin)]"><div class="article-content"><h3 id="topics-and-highlights">Topics and Highlights</h3>
<ul>
<li><strong>Session setup &#x26; check-in</strong>: Kicked off with a casual vibe, confirming no all-hands this week and setting up three talks. Phát skipped his slot, and the team troubleshooted screen sharing for demos.</li>
<li><strong>AI fine-tuning overview</strong>: Explored fine-tuning vs. retraining, using a doctor’s note example to show how fine-tuning embeds knowledge while retraining leans on token-heavy prompts.</li>
<li><strong>Fine-tuning demo</strong>: Showcased a Duty 40 Mini fine-tuning job on Open AI (~4800 tokens), comparing pre- and post-tuning results, with a nod to local vs. hosted model trade-offs.</li>
<li><strong>Data archiving essentials</strong>: Biên broke down archiving vs. backup for apps with 50K-1M daily transactions, focusing on metadata, cloud storage, and recovery to optimize query performance.</li>
<li><strong>Archiving tools &#x26; Q&#x26;A</strong>: Highlighted tools like AWS, Google Cloud, and Timescale, plus hot/warm/cold storage options (Azure, Backblaze), with audience questions on scheduling and platform quirks.</li>
<li><strong>Datalake foundations</strong>: An traced datalakes from 1980s databases to today’s cloud systems, contrasting warehouse ETL (structured) with datalake ELT (raw data) workflows.</li>
<li><strong>Notion’s datalake scaling</strong>: Detailed Notion’s growth to 96 instances and 400+ shards by 2023, shifting from warehouse to datalake with Debezium CDC, Kafka, Hudi, and S3 for analytics.</li>
<li><strong>Interactive wrap-up</strong>: Fielded questions on datalake vs. replication, async processing, and external data handling (e.g., social media), ending with reflections on big data skillset.</li>
</ul>
<h3 id="vietnamese-transcript">Vietnamese transcript</h3>
<p><strong>[00:00]</strong> Hình như tuần sau mới có all-hand. Không thấy ai tạo event gì hết, chắc tuần này cứ bình thường thôi nhá. Anh em kiểm tra xem màn hình sharing có vấn đề gì không. Hay lên luôn nhỉ? Hôm nay chắc có ba bài thôi đâu đó. Phát vừa bảo tuần này cậu không có gì mới, chắc skip hôm nay rồi. Anh em thử share màn hình cá nhân xem sao nào. Xem trước được không?</p>
<p><strong>[10:42]</strong> Theo lịch chắc anh nhỉ, để em lên trước nhá. Fine-tuning này, chủ đề này không mới lắm đâu. Bài này chỉ là 100.5 thôi, không phải 101, nên chỉ giới thiệu sơ sơ, chưa đi sâu được đâu. Tại em cũng mù mờ lắm, nên chắc giới thiệu sơ vậy thôi. Hôm nay em giới thiệu bài fine-tuning. Đây là agenda của bài này nè.</p>
<p><strong>[12:24]</strong> Introduction là nếu mọi người dùng AI, chắc có nghe tới khái niệm fine-tuning rồi. AI có mấy mô hình đa số được fit vào dữ liệu từ một ngày nào đó, với mấy cái data privacy hoặc data của domain riêng. Dữ liệu này không xuất hiện trong knowledge của mô hình nền tảng (foundation model). Để mô hình có được kiến thức đó, người ta thường dùng retraining, đúng không? Nhưng còn một cách khác gọi là fine-tuning. Cuối bài, em sẽ so sánh hai cách này, xem lúc nào nên dùng cái nào, lúc nào không.</p>
<p><strong>[13:08]</strong> Trước mắt, cứ hiểu fine-tuning như cách để mở rộng kiến thức cho mô hình nền tảng vậy. Fine-tuning là gì? Hiểu đơn giản là mọi người retrain lại mô hình, lấy một mô hình nền, đưa vào một dataset gì đó để fine-tune, nghĩa là retrain lại nó. Sau khi fine-tune xong, ta được một mô hình đã điều chỉnh, gọi là fine-tuned model.</p>
<p><strong>[13:44]</strong> Tại sao fine-tuning mang được kiến thức mới? Hiểu đơn giản là trong một AI model, kiến thức được lưu qua mấy cái mạng nơ-ron. Fine-tuning sẽ cập nhật các weight, các thông số của mạng nơ-ron đó, để nó phù hợp với kiến thức mới. Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi.</p>
<p><strong>[14:19]</strong> Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi. Khi fine-tune mô hình trong thực tế, không phải chỉ ném dataset vào rồi retrain là xong. Đúng là ra một mô hình fine-tuned, nhưng không biết cái mô hình sau retrain này có tốt hay không. Em sẽ giới thiệu một workflow mà bên ngoài thường dùng để fine-tune.</p>
<p><strong>[14:59]</strong> Cái flow này, nó gồm nhiều bước như thế này. Mọi người có thể chia thành hai cụm, hai cụm nhá. Cái flow này, em sẽ chia thành hai cụm. Em đi qua cụm một trước. Cụm đầu tiên là cụm ở bên trái, hiểu đơn giản là một base model ban đầu. Sau đó, mọi người có một dataset mới, một cái gì đó mới, mọi người quăng vô, fine-tune nó. Rồi nó ra một model, mọi người sẽ supervise nó, có nghĩa là mọi người retrain nó dưới kiểu là retrain nó.</p>
<p><strong>[15:38]</strong> Sẽ cho nó thêm kiến thức, với input này thì output sẽ ra như này. Nó sẽ ra một cái gọi là supervised fine-tuning. Sau đó, để em đi qua phần tiếp. OK, cái fine-tuning này là retrain on data, nghĩa là sẽ cho một cặp input-output trong dataset để nó học. Nó sẽ học được những kiến thức mới đó. Để bước này hoàn hảo, dataset phải được clean. Nó phải clean, không được lẫn với những cái khác. Nghĩa là nó phải specific cho cái domain mà mình muốn train nó.</p>
<p><strong>[16:31]</strong> Quay lại hình này, sau khi mọi người có một cái model đã được retrain xong, mọi người mang lên production, mọi người dùng, đúng không? Lúc này, bên ngoài sẽ sử dụng một cái system gọi là human feedback. Kiểu như là response của model này có làm bạn hài lòng không, chấm từ 1 tới N sao, kiểu vậy á. Mọi người sẽ collect cái data đó. Nó nằm ở bước này, mọi người sẽ thu thập human feedback từ cái retrained model của mọi người.</p>
<p><strong>[17:06]</strong> Dựa vào cái feedback đó, mọi người gọi bước này hơi tốn tài nguyên chút. Mọi người sẽ phải retrain một cái model riêng. Cái model này dùng để đánh giá xem response này được chấm bao nhiêu điểm. Kế đó, mọi người tới bước thứ ba, bước cuối. Ở bước này, mọi người sẽ dùng thuật toán như reinforcement learning để kết hợp với cái retrained model và cái reward model của mọi người.</p>
<p><strong>[17:46]</strong> Mọi người retrain, mọi người lại fine-tune cái model một lần nữa. Nó sẽ ra cho mọi người một cái gọi là model tối ưu. Cái vòng lặp này cứ tiếp tục, tiếp tục mãi. Mọi người có cái model đã retrain xong, thu thập human feedback, rồi kết hợp ba cái đó để retrain cái model thêm lần nữa. Càng ngày, cái model sẽ càng ok hơn với những gì mà mình muốn. Đó là cái flow mà em thấy bên ngoài, trong production, người ta hay dùng.</p>
<p><strong>[18:24]</strong> Trong quá trình fine-tuning, ,ọi người sẽ thường nghe tới khái niệm gọi là catastrophic forgetting. Nghĩa là sao? Nghĩa là khi mọi người retrain kiến thức mới vào, nó sẽ làm giảm performance với những kiến thức cũ. Tại sao chuyện này xảy ra? Như em đã nói, kiến thức của một model dựa vào mấy cái weight, dựa vào kiến trúc của cái model đó và những tham số của nó. Tham số dynamic trong một model là mấy cái weight. Khi mọi người retrain, mấy cái weight này thay đổi, đúng không?</p>
<p><strong>[19:00]</strong> Khi nó thay đổi, có phải là kiến thức cũ sẽ bị giảm bớt độ chính xác đi không? Nếu trong dataset của mọi người có nhiều dữ liệu bị overfitting, nghĩa là dataset của mọi người quá đúng, quá đúng trong cái dataset đó. Khi một người quăng cái gì mới vào, nó sẽ sai với những cái cũ đi. Người ta gọi đó là overfitting, nghĩa là nó bị fold in quá mức vào những cái training data. Khi gặp data mới, nó sẽ giảm performance.</p>
<p><strong>[19:42]</strong> Nên lúc này, bên ngoài người ta sử dụng một kỹ thuật gọi là parameter-efficient fine-tuning, gọi là PEFT. Nó có nhiều cách, nhiều kỹ thuật trong method này, như LoRA này kia. Nhưng trung quy, đa số bọn họ không phải update hết tất cả các weight trong cái model đó. Bọn họ sẽ chỉ đóng băng những layer nào không cần thiết. Họ sẽ đóng băng mấy cái layer không cần thiết, rồi chỉ update một số lượng nhất định các weight thôi. Để tránh trường hợp kiến thức cũ bị mất đi quá nhiều. Đó là cái cơ bản. Còn sâu hơn về mấy cái algorithm đằng sau, mọi người có thể tự tìm hiểu.</p>
<p><strong>[20:27]</strong> Quay lại câu hỏi lúc ban đầu, ha, nó với retraining khác nhau thế nào, nên dùng cái nào? Có cái bảng đây, mọi người có thể dễ dàng nhận ra. Retraining là dữ liệu phụ thuộc vào database của mọi người. Cứ quăng vào, quăng vào, lúc nào data cũng được update liên tục. Còn fine-tuning là mọi người retrain lại model, nên lúc nào data cũng chỉ ở cái chỗ mà mọi người đã retrain thôi.</p>
<p><strong>[21:16]</strong> Kế tiếp là customize and learning style. Nghĩa là cái retraining, mục đích của nó là cho mình một cái knowledge base để mình lấy mấy cái knowledge base đó ra tham chiếu, sử dụng. Còn fine-tuning thì sao? Nó upgrade cái não của model lên, để nó có sẵn cái knowledge đó luôn. Còn mấy cái ở dưới thì chắc mọi người tự tìm hiểu tiếp ha.</p>
<p><strong>[21:57]</strong> Em có một cái ví dụ như vậy. Ví dụ như là mọi người muốn làm một cái system để giải thích những cái note của bác sĩ, đúng không? Những cái note của bác sĩ, mọi người có thể biết là những cái note của bác sĩ nó có rất là nhiều từ chuyên ngành. Và những từ chuyên ngành đó nó còn viết tắt, viết kiểu luộm thuộm nữa.</p>
<p><strong>[22:44]</strong> Nếu mọi người sử dụng fine-tuning á, mọi người sẽ cho nó học hết tất cả những cái kiến thức luộm thuộm, những cái shorthand, những cái handwriting đó của bác sĩ. Nên khi mọi người input một cái note của bác sĩ vô, nó sẽ trả lời được rất đúng. Còn nếu mọi người dùng retraining á, khi mọi người input một cái note của bác sĩ vô, nó sẽ kiếm được những cái relevant data, mang ra đọc. Nhưng bản chất là cái model nó không hiểu được những từ đó, nên nó cũng sẽ không đưa cho mọi người một câu trả lời chính xác.</p>
<p><strong>[23:16]</strong> Mọi người có thể hiểu như này: fine-tuning là mình nhờ một bác sĩ đọc một cái note của bác sĩ. Còn retraining là mọi người đưa cho một người có kiến thức rất rộng đọc một cái note của bác sĩ. Người đó có thể kiến thức rất rộng, nhưng về mấy cái chuyên ngành, mấy cái chuyên ngành thật sự, thì nó không đủ sâu như của một bác sĩ thực thụ. Nên độ chính xác sẽ không cao.</p>
<p><strong>[23:57]</strong> Thứ hai, mọi người có thể nói, bây giờ với retraining, mình dùng một cái system prompt để list hết mấy cái shorthand của bác sĩ ra trong system prompt, nó sẽ tự hiểu thôi. Nhưng làm vậy, mọi người sẽ bị tốn token, đúng không? Tại vì khi mọi người dùng retraining, mọi người lấy hết cái retraining data ra, quăng một cái knowledge retraining vô, lại cộng thêm đống cái zero-shot, mấy cái description, mấy cái đi kèm theo nó trong một cái prompt á, thì nó rất tốn token.</p>
<p><strong>[24:34]</strong> Và khi ở trong một cái long conversation với một cái model, nó đâu phải chỉ dựa vào câu hỏi của mình đâu. Nó sẽ dựa vào tất cả các cuộc trò chuyện từ trước tới giờ của mình mà nó trả lời cho mình. Lúc này, nó sẽ dẫn tới trường hợp là nó bị limit bởi token. Đó là cái drawback khi sử dụng retraining, là nó sẽ tốn token. Tại vì mọi người cần token để chạy cái system prompt của mọi người nữa. Còn fine-tuning, bản chất là model nó đã có kiến thức rồi, nên không cần phải có system prompt.</p>
<p><strong>[25:14]</strong> Đó là sơ qua về fine-tuning. Chắc có cái demo cho mọi người xem sẽ rõ hơn ha. Bây giờ em sẽ fine-tune một cái model là Duty 40 Mini ha. Em có một cái dataset như này. Ừ, như này thì mỗi thứ nó sẽ có một cái system như retraining, rồi user hỏi cái này thì muốn nó trả lời vậy, đúng không? Em cộng 10 cái, 10 record trong cái dataset này, em sẽ fine-tune nó.</p>
<p><strong>[26:13]</strong> Trước khi fine-tune, em sẽ cho nó chạy qua một đoạn code để em estimate được. Tại vì em dùng Open AI, nên sẽ tốn tiền. Nên mình sẽ tính được estimate là nó sẽ charge mình bao nhiêu. Em dùng xong, khúc cuối nó sẽ kiểu, tầm khoảng 4800, sắp xỉ 4800 token. Cái này chỉ là tham khảo thôi, nhưng em thấy nó cũng đúng. Sau đó, em sẽ upload cái file data này lên Open AI. Nó sẽ cho em cái file ở trên cái Open AI của em.</p>
<p><strong>[27:03]</strong> Rồi em sẽ training nó. Em sẽ tạo một cái fine-tuning job. Lúc này, ở trên Open AI, nó sẽ chạy một cái job này. Mọi người có thể lên đây, mọi người đọc, mọi người quan sát. Nó sẽ không trả kết quả liền, nó sẽ tạo một cái job để pending ra đó, để trên Open AI nó fine-tune cho mình. Trong lúc chờ, mình có thể theo dõi quá trình của nó như thế nào. Sau khi xong đâu rồi, nó sẽ thông báo cho mình. Mình cứ stamp cái câu này, cứ check cái câu này để coi nó đã hoàn thành hay chưa. Mình đọc ở cái chỗ đó.</p>
<p><strong>[27:58]</strong> Sau khi xong, nó sẽ cho mình mấy cái result status. Sau khi fine-tune xong, với cùng một câu hỏi, ví dụ đây là cái câu hỏi em sử dụng, em dùng câu hỏi này. Cái câu hỏi này gần giống với một cái record trong đống dataset của em. Sau khi em chạy, nó sẽ trả lời như vậy. Nhưng trước khi fine-tune, em dùng một cái model bình thường nha, model bình thường thì nó sẽ trả lời kiểu vậy.</p>
<p><strong>[28:49]</strong> Có nghĩa là em fine-tune thì nó đã thành công. Đó là cái cách em sử dụng Open AI để fine-tune một cái model. Demo của em tới đây thôi. Anh em có câu hỏi gì không? Đúng rồi, cái này demo em xài tuning chứ để tự fine-tune bằng local mà xịn xịn thì chắc không đủ đồ. Dạ, đồ ngon nhõ thôi. Thực ra có mấy cái model trước, tô nó trên LoRA các thứ, cũng có thể demo được. Nhưng tô không, bài này easy, bài này kiểu một...</p>
<p><strong>[29:47]</strong> Lẻ tẻ trầm mấy á. Đúng, chắc cũng ổn mà. Nói chung, những đội enterprise hay không muốn tốn thời gian xây dựng GPU thì sẽ dùng cách này. Diagram GPT hồi trước, GPT-4o Mini ra thì fine-tuning đã miễn phí, dùng cái này cũng tiện lợi cho họ. Cái cửa hàng demo cho anh em là sử dụng một cái như kiểu service ấy. Open AI cung cấp service fine-tuning, đưa lên mấy cái model của nó luôn. Mình pick mấy cái model, chắc là pick model mini á. Chắc chi phí nó không cao lắm.</p>
<p><strong>[30:37]</strong> Đấy cũng là một cách. Nhưng vấn đề thực ra là mình vẫn không phải người own cái model đấy. Bản chất là vẫn host ở trên server của họ. Còn có một cách khác là tự build server và tự running. Trường hợp hôm nay đã khác. Anh em xem, hôm qua em có thử một cái model có 3 billion parameter thôi. Nhưng nó chạy hai ba tiếng, nó chưa xong đâu anh. Thực ra bài này, cái version nó đơn giản hơn một cái bây giờ, nhỏ hơn của bài trước. đầu.</p>
<p><strong>[31:26]</strong> Nó là một cái full flow liên quan đến gì ta, reinforcement feedback. Ý là cái em giới thiệu ở bên ngoài production á, là khi người ta tuning á. Người ta không phải chỉ fine-tune xong là dùng liền, người ta phải đánh giá lại coi nó có đúng không. Người ta phải cho nó vô cái cycle để càng cải tiến cái fine-tuned model nữa, kiểu vậy. Đây là một cái flow như vậy. Bản chất nó cũng model thôi, đâu có gì đâu. Quan trọng là mọi người biết được những cái cost để đánh giá cái approach thôi.</p>
<p><strong>[32:05]</strong> Tại ra nó vẫn là bài toán accuracy, đúng không? Mình chọn cách nào để làm cái output nó chính xác hơn. Những cái method như retraining hay fine-tuning, nó sẽ có những nhược điểm khác nhau. Và thực ra kể cả fine-tuning, nó cũng có nhiều method fine-tuning khác nhau. Chắc là cần đi sâu hơn để xác định mấy cái đó. Cái này vẫn hơi general. Chắc vậy, Hoàng. Nếu có điều kiện thì chắc đi sâu hơn tí nữa.</p>
<p><strong>[32:48]</strong> Sâu hơn theo kiểu là có mấy cái method liên quan đến phần retraining các thứ. Sử dụng fine-tuning method á, có một số cái method nó tương đối tiết kiệm về mặt tài nguyên. Tất nhiên, nó sẽ đánh đổi với một số thứ khác, kiểu vậy. Giới thiệu cái đó để anh em xem thử đâu đó. Mọi người hỏi, Đạt hỏi là khi nào cần fine-tuning. Nói là fine-tuning cần khi mà mọi người muốn nó có một cái kiến thức, một cái specific topic nào đó. Mọi người có thể cân nhắc sử dụng fine-tuning.</p>
<p><strong>[33:39]</strong> Nhưng trong tất cả trường hợp, em thấy bên ngoài, đa số mọi người sẽ prefer dùng retraining. Tại vì nó dễ và tốn ít tài nguyên hơn. Nhưng một số trường hợp như lúc này, cái ví dụ em nói về cái note của bác sĩ á, suppose là nên dùng tuning. Rồi tùy cái kiến trúc, tùy cái mình chia system của mình ra nhiều system nhỏ, system nhỏ nó như thế nào nữa, tùy. Có thể có một vài cái use case như kiểu chúng nó muốn host mấy cái model bé bé, model bé chẳng hạn.</p>
<p><strong>[34:18]</strong> Chỉ kiểu dành để làm một cái task cụ thể thôi. Ví dụ như phân tích thời tiết, độ ẩm các thứ để perform cái action nào đấy. Ví dụ như thay đổi cái theme của điện thoại hay để chỉ action nào đấy chẳng hạn. Có thể retrain cái model bé bé để chỉ cần làm chuyện đó thôi, không cần phải cần network các thứ gì cả. Chắc vậy. Từ giờ chắc là Biên ha?</p>
<p><strong>[36:13]</strong> Dụng cái và build cái recovery process cho nó. Chi tiết như nào thì nó sẽ có một vài phần chính. Trước tiên là cái lý do mà mình cần cái kỹ thuật này và so sánh nó với một cái quen thuộc hơn là backup. Sau đó là đi vào việc để mình build và những cái mình cần để ý, những cái gì. Đầu tiên là trên thực tế, thường có những tổ chức, những công ty mà chạy những cái app với lưu lượng dữ liệu cao á. Ví dụ như giao dịch chứng khoán này nọ. Như em ví dụ này là kiểu 50.000 transaction.</p>
<p><strong>[37:12]</strong> Như em ví dụ này là kiểu 50.000 mỗi ngày là ít á, kiểu vọt lên 500.000, triệu transaction mỗi ngày. Sau khoảng thời gian, cái lượng data nó sẽ phồng lên rất rất lớn, ảnh hưởng đến cái việc mà mình query data và ảnh hưởng đến cái trải nghiệm người dùng. Trong những cái data đó, sẽ có những cái data mà dùng rồi thì nó rất ít được access lại nữa. Ví dụ như lịch sử trên 7 năm trước chẳng hạn. Nó sẽ dẫn đến một cái vấn đề, làm sao để mình giải quyết cái đống data đó. Nên mình mới dùng cái kỹ thuật là data archiving.</p>
<p><strong>[38:14]</strong> Nó sẽ có những cái lợi ích để counter lại những chuyện bên trên. Đầu tiên là cái data mà mình sử dụng, mà nó set liên tục á, query ghi đọc liên tục á, thì nó thường tốn chi phí cao. Mình sẽ dùng cái kỹ thuật này, mình sẽ đem data của mình bỏ qua một cái chỗ khác, chi phí rẻ hơn, access ít hơn. Từ đó, nó sẽ làm tăng được cái performance của app của mình trong việc query hay aggregate data các thứ.</p>
<p><strong>[39:07]</strong> Về mặt pháp lý hay reusable, những cái data đó nó sẽ được bảo vệ an toàn, không bị ảnh hưởng bởi những yếu tố bên ngoài. Để sau này khi mình dùng lại, mình có thể lấy ra dùng được. Như mọi người hay nói, mọi người sẽ liên tưởng đến cái data backup, thường dùng trong việc restore data, restore cái system hay app nếu có lỗi xảy ra. Mà hai thằng này, nó sẽ khác nhau ở chỗ là data backup á, nó sẽ dùng cho cái việc hotfix cái system nhiều hơn. Còn cái thằng archiving...</p>
<p><strong>[39:59]</strong> Data archiving thì nó hướng về cái việc lưu trữ data một cách lâu dài. Nó sẽ có cái chi tiết so sánh như này. Để mình đi build một cái architecture, một cái system để archive data, xong rồi dùng nó để recovery lúc mình cần thì sẽ làm như sau. Mọi người thấy, nó sẽ có ba cái note chính. Thứ nhất là mình lưu data lại, mình dùng metadata để interact với những cái data đó, rồi mình bỏ lên một chỗ, ví dụ như những cái cloud-based service, cloud storage service, để mình lưu trữ cái data đó.</p>
<p><strong>[40:51]</strong> Về chi tiết, để lưu trữ cái dữ liệu á, đầu tiên mình phải xác định những cái dữ liệu cần được lưu trữ. Phải phân tích xem dữ liệu nào hay được sử dụng, dữ liệu nào không được sử dụng, ít được truy cập. Sẽ có nhiều công cụ để mình làm những cái đó. Ví dụ như phân tích từ business requirement, hoặc từ các công cụ phân tích, mấy cái công cụ phân tích á. Từ đó, mình mới biết cái data nào là cần, cái nào có thể đem đi archive lại.</p>
<p><strong>[42:05]</strong> Sau đó, mình sẽ gói nó lại, dùng một vài biện pháp như vector hóa nó, encode nó, rồi dùng checksum các thứ để đảm bảo cái data nó sẽ đúng. Sau này, khi mình sử dụng lại, mình truy cập lại một cách nhanh chóng. Tại vì những cái database này, nó gói lại ở một cái storage khác với cái mình hay set, nên mình cần phải lưu lại cái metadata của nó. Ví dụ như lưu theo tháng ha, hoặc lưu theo account, để sau mình query lại thì dễ hơn.</p>
<p><strong>[43:07]</strong> Sau khi archive xong, mình muốn sử dụng lại á, thì vừa đây là cái ví dụ em để recovery. Mình sẽ tận dụng những cái metadata lúc nãy, mình search lại những cái block data mà mình cần, rồi đưa về cái môi trường tính toán lại nó khi cần thiết. Cái này nó có lợi ích là khi mình làm những chuyện này, nó sẽ không tác động đến cái data production của cái ứng dụng đang chạy. Mình có thể làm song song được. Mình muốn làm gì với nó thì làm, không chọc ngoáy vào trong cái production, sẽ đảm bảo an toàn được.</p>
<p><strong>[43:51]</strong> Cho cái trải nghiệm người dùng, như sản phẩm của mình đó. Nói đến đây, có một vài cái practice cho việc sử dụng, xây dựng cái hệ thống này. Nó cũng đơn giản lắm nhỉ. Mình sẽ phải review lại những cái policy mà mình đặt ra để cái hệ thống này chạy, xem data nó có trọn vẹn hay không. Mình sẽ automation những cái step của cái process này. Hiện tại cũng có nhiều tool hỗ trợ mình rồi, ví dụ như AWS, hay Google, đều có những cái như...</p>
<p><strong>[45:14]</strong> Google Cloud chẳng hạn. Mình chỉ cần viết những cái đơn giản để đẩy lên trên đó thôi. Và mình không thể thiếu cái monitoring để xem data này có hoạt động tốt hay không. Xong rồi, có những kỹ thuật khác như checksum này nọ, để đảm bảo data của mình luôn trọn vẹn. Khi mình cần, cũng sẽ có những chiến lược như schedule trước cái data. Tại vì những cái data này nó tồn tại lâu, nó cũng sẽ lớn, cũng sẽ phồng lên trên cái storage, cái cloud storage mà mình dùng để lưu trữ nó.</p>
<p><strong>[46:05]</strong> Nên sẽ có những chiến lược như khi nào cần thì phải schedule trước, bao nhiêu thời gian đó để nó replicate data cho mình chẳng hạn. Kế của em chỉ như vậy thôi. Lý thuyết kiểu để giải quyết cái mục đích cuối cùng là nói mọi người về việc giải quyết những cái data tồn động lâu dài, nhưng không sử dụng đến nhiều trong cái hệ thống mà mình build thôi. Ví dụ như bên ngân hàng chẳng hạn, sẽ có kiểu user trade, trade của user nó lên đến cả trăm triệu record chẳng hạn.</p>
<p><strong>[46:45]</strong> Sau này, nó sẽ lên nữa. Tức là query những cái data gần thôi, nhưng nó cũng rất tốn thời gian, kiểu vậy. Đó là những cái mà em nói hôm nay, hết. Mọi người có hỏi gì không? Khi mà store data, zip data, là mình sẽ zip một cái đoạn fragment trong quá khứ mà nó không sử dụng data đấy cho mục đích hiện tại, đúng không? Dạ, đúng rồi, đúng rồi. Đồng ý, việc em sẽ phải xóa. Khi xong, em phải xóa cái đó, đúng rồi. Nên mình sẽ có những cái load lại để tính toán khi cần.</p>
<p><strong>[47:41]</strong> Nên mình mới có mấy cái kiểu để mình làm nó an toàn. Mình có hỏi kìa. Em chưa biết cái cơm của Thỏ có biết cái này không, so sánh được không? Đứng ra là Timescale, nó có cơ chế move chunk. Ví dụ là mình compression như bình thường thôi. Thêm về cái vụ là mình có hot, warm, và cold storage. Ví dụ mình backup hàng tuần thì để trên hot storage của Azure. Nếu là cũ quá, ví dụ 2, 3, 4, 5 năm, thì để trên cold storage của Azure, hoặc là Backblaze. Nó sẽ có riêng cái dịch vụ cho mình move cái chất data đó.</p>
<p><strong>[48:53]</strong> Đúng cái vị trí object hoặc block storage, mình tương tác với Timescale để đảm bảo lúc mình cần tiết kiệm tiền với data cũ. Có thể tiết kiệm được, vốn có thể query, với trade-off là mình sẽ query hơi chậm với data hơi cũ thôi. Dạ, cái em hiểu là để tùy vào cái platform mình dùng để build ha anh. Ví dụ như bên Microsoft thì cũng sẽ có những cái tùy vào thời gian của database, hoặc tùy vào tuổi thọ của data, hay dung lượng này nọ, thì sẽ có những cái level khác nhau.</p>
<p><strong>[49:40]</strong> Ví dụ nó sẽ có delay, hay bình thường vẫn access, hay delay cho những cái mà không dùng một thời gian lâu nữa. Cái đó là để mình cụ thể trên từng tool thôi. Còn chung chung, nó là anh đang cái này làm gì thì đứng ra là Timescale thì phù hợp cho cái kiểu pattern này, cho về time series. Bên phía Azure thì họ làm cho nó phù hợp với status, hơi giống như Timescale, nhưng nó kiểu giúp mình partition và shard đúng theo kiểu mình mong muốn.</p>
<p><strong>[50:39]</strong> Mỗi một cái nó sẽ có ưu điểm, nhược điểm riêng. Với AWS thì đứng ra là với cái dịch vụ này thì phải coi chừng cái hardware cho lưu cái data này, nó có ổn định không. Ví dụ bên phía Azure cold storage thì nó dùng đĩa, đĩa gì ta, đĩa hơi khá đặc trưng. Phải dùng cái máy laser để in vào trong đó. Nên query rất nhanh, nhưng insert thì cũng hơi chậm, kiểu insert một đống cũng mất vài phút. Vì phải có một cái laser cứng để in ở trên đó, không có virtualization layer.</p>
<p><strong>[51:23]</strong> Mỗi một service và mỗi một cái kiểu tool mình dùng cho compress và lưu trữ sẽ có ưu điểm, nhược điểm riêng, theo cái platform mình subscribe. Dạ, đúng rồi. Cái này không chỉ là mấy cái tool kiểu như AWS hay Google service. Nó là kiểu mình cũng có thể cân nhắc cho cái business của mình nữa. Nên cái này kiểu chung chung thôi. Còn từng platform, nó sẽ dùng những kỹ thuật khác nhau. Mục đích chung cuối cùng là để giải quyết cái vấn đề data nó lớn lên, nhưng ảnh hưởng đến cái việc mình query, mình nó chạy thôi</p>
<p><strong>[52:16]</strong> Nhiều cách giải quyết cho câu chuyện optimize query, đúng không? Khi mà vấn đề là do data quá lớn, thì có một vài cách. Cách của biên là một cách, tức là sẽ có một phần data mình đang không xài đến, thì ta cắt đi ra, lưu đâu đấy. Về sau mà có cần đến past data thì insert lại xài sau. Còn mình để đâu đó tầm bao nhiêu phần trăm data hiện tại, đủ để xài mục đích hiện tại, query đi nó nhanh hơn.</p>
<p><strong>[52:59]</strong> Còn một số cách khác thì xài thằng tooling, có một số kiểu database hay kiểu như Timescale, thì nó sẽ optimize luôn cho chuyện query với lượng data lớn lớn. Em nghĩ là bên dưới thì nó cũng sẽ tự động kiểu nó buff lên đâu đó, nó giữ giúp mình thôi, đúng không anh? Nên mình tỉ mỉ bên dưới, mình dùng là interface thôi. Cái bên dưới thì gần gần như nhau, như các em ta. Cảm ơn biên, vậy thôi. Chắc bài cuối của An, không biết có liên quan không. Không biết còn liên quan một tí gì đến cái cộng đồng viên không.</p>
<p><strong>[54:00]</strong> Chắc có thì chắc cũng nói sơ sơ thôi, cũng không nhiều cái. Cũng gần giống như bài của Biên, nhưng use case cũng gần giống á. Nó mở rộng ra tí thôi. Tí rồi thì bài này là nói sơ về cái datalake với lại cái use case của thằng Notion. Mình nói cái datalake trước. Datalake thì chắc mọi người nghe miết rồi, xưa giờ cũng hơi lâu rồi đó. Mình nhìn lại cái quá trình phát triển của tụi datalake này, coi là mình đang đi tới đâu.</p>
<p><strong>[54:54]</strong> Thật ra từ lúc bắt đầu, hồi tầm 1980 gì đó, là thời đại của thằng database, mấy thằng database warehouse, mấy cái mà mình đang xài hiện tại á. Về table các kiểu, tạo table rồi xử lý data. Sau này, tới cái đợt tầm năm 2000 các kiểu, tụi mấy thằng big tech bắt đầu thu thập data nhiều á. Rồi nó tận dụng mấy data đó, thì mới sinh ra mấy thằng để giải quyết vấn đề lưu trữ data và xử lý data trên dữ liệu lớn. Như là mấy dữ liệu lưu theo dạng file đồ á. Mấy cái này, mấy thuật ngữ như là cái MapReduce này nè.</p>
<p><strong>[55:44]</strong> Hình như trong cái memo của mình có một bài về MapReduce. Nếu mọi người không biết thì có thể search lại, tìm đọc thử xem cái MapReduce hồi xưa nó làm cái gì. Nó là cái tiền thân của tuổi. Sau này nó tích hợp vô thôi, giờ không xài nữa, nhưng chắc là nó tích hợp sẵn hết rồi. Sau cái thời gian phát triển của thằng này, mới bắt đầu 2010, thì mới đẻ ra, trước 2010 tí, đẻ ra khái niệm về datalake, big data, cloud, là cái internal data warehouse á, trên cloud á. Nó cloud thôi.</p>
<p><strong>[56:28]</strong> Sau này, cái đợt bây giờ á, thì nó bắt đầu phát triển hơn nữa, là về cái lake và datamart. Lake chắc bản chất là kết hợp giữa mấy cái của tụi datalake và cái warehouse thôi, để rồi đặt thành cái house. Như là mấy thằng như thằng Datadog, nó đang làm sao không biết, nhưng mình chắc là đang nói về cái này hơi đi sau thời đại tí. Để tập trung vào, chắc mình coi sơ một cái data architecture chung chung trước. Cái này, bữa cái bài của Tom có đăng, cũng có một cái diagram. Nó cũng tinh gọn hơn cái này, tinh gọn hơn tí, là cũng về cái data đi qua mấy cái layer, là processing rồi mới tới thằng gì đó.</p>
<p><strong>[57:20]</strong> Cái này nó sẽ thể hiện rõ hơn tí, là trong một cái datalake, mình sẽ lưu những loại data gì. So với thằng data warehouse, mình chỉ lưu mấy thằng structured data thôi, hoặc là mấy cái như lưu table data clean hết rồi. Còn thằng datalake này thì nó raw data, nó sẽ cả structured, unstructured, semi-structured data luôn. Nó sẽ lưu dạng raw, sau đó nó mới xử lý data, transform data, rồi nó quăng qua cho cái đám bên BI analytics, hoặc là quăng vô cái warehouse khác để chứa cái data đã được process rồi á.</p>
<p><strong>[58:18]</strong> Còn cái layer mà analytics sandbox này, thì nó là một cái layer để cho tụi data scientist, hoặc mấy thằng mà cần dùng cái raw data, process data, mà nó không ảnh hưởng tới cái process chính. Bên đây á, thì nó sẽ làm việc trên cái sandbox này để xử lý data cho tụi mấy thằng đó, mấy thằng cần raw data, nhưng không ảnh hưởng trực tiếp tới cái ruồng chính. Cái giống như hồi nãy Biên có nói á, có làm á đó, là nó sẽ lấy data, rồi nó lưu ở đâu đó để sử dụng sau này, hoặc để process gì đó không biết, nhưng mà nó không muốn ảnh hưởng tới process chính của cái app, thì nó sẽ là cái đống này.</p>
<p><strong>[59:09]</strong> Ở chỗ này, mọi người thấy là mình có khái niệm là cái ETL á, là extract, transform, và load. Bên cái warehouse xưa giờ mình làm á, nó sẽ là extract, transform, và load, nó đi theo thứ tự đó luôn. Nhưng trong cái này, mình sẽ thấy rõ là cái thằng datalake á, nó sẽ là extract và load trước. Rồi sau khi nào cần á, nó bắt đầu process data, là transform. Transform sẽ đứng sau, load sẽ đứng trước. Đó là cái khác biệt giữa hai thằng.</p>
<p><strong>[59:52]</strong> Đây là chỗ so sánh khác biệt giữa thằng data warehouse và datalake thôi. Đó là dữ liệu bên warehouse, nó được clean, structured, organized thành cái table. Còn thằng này thì nó lưu dạng file, raw data các thứ, semi-structured rồi đó, CSV hoặc mấy cái JSON. Cái process nó cũng sẽ khác nhau giữa thằng lake và lake này. Truy vấn thì thằng warehouse sẽ truy vấn bằng SQL, còn kia thì xử lý trực tiếp trên cái dữ liệu luôn. Mấy thằng hỗ trợ xử lý trực tiếp dữ liệu, như thằng Spark đó, thì nó sẽ hỗ trợ mấy cái đó. Nói qua về cái thằng Notion.</p>
<p><strong>[01:00:46]</strong> Datalake thì cái use case của thằng Notion, mọi người biết là Notion mình xài cũng hơi nhiều rồi đó. Hồi xưa, nó cũng đi từ từ thôi. Mấy cái tổ chức, mấy cái block hồi xưa, nó tổ chức thì cũng kiểu data bình thường, giống như mình, là mấy cái app nhỏ nhỏ. Mấy cái block của nó bắt đầu tăng dần. Block của nó được hiểu là mấy cái gì, rồi nó sẽ bao gồm cái title trong trong đó. Nó sẽ gọi là block. Số lượng block của nó tăng lên liên tục theo ngày giờ.</p>
<p><strong>[01:01:35]</strong> Gì đó thì bắt đầu sau này, nó phình ra, nó sẽ bắt đầu sử dụng mấy cái kỹ thuật như là sharding, sharding xưa. Như nhớ có bài của Hải Vũ có xe gì đó, nó scale horizontally. Nó bắt đầu tách ra sharding này nọ, rồi mấy cái instance. Trong giai đoạn từ 2021 đến 2023, nó sẽ có 32 instance. Mỗi instance sẽ có 15 cái shard. Rồi từ 2023 trở đi á, nó bắt đầu chia lại, nó lại tăng lên. Số lượng tăng lên nữa, thì đó là 96 cái instance. Và mỗi cái instance, nó sẽ là 5 cái shard. Nhân lên tầm 400 mấy á, bốn trăm mấy.</p>
<p><strong>[01:02:27]</strong> Để mà xử lý thì lúc này, nó hơi to, đúng không? Khi mà data nó bắt đầu to lên á, nó sẽ có những nhu cầu. Sau này sẽ có những nhu cầu về cái analytics, hoặc là mấy cái về làm bên machine learning á, tập dữ liệu này nọ, mẹo mẹo rồi. Nó sẽ bắt đầu setup một cái data warehouse architecture của nó. Cái này là cái tiền thân trước khi setup cái datalake. Nó sẽ làm data warehouse để xử lý data. Cái luồng cơ bản của nó setup để thu thập data, mấy cái về thay đổi data của mấy cái block trong từng shard.</p>
<p><strong>[01:03:21]</strong> Nó sẽ sử dụng cái file transfer để nó ingest mấy cái data từ mấy shard này nè. Nó đổ về cái gì, rồi nó gộp mấy thằng đó lại thành một cái single database to. Cái này sẽ gặp khó khăn trong việc là nãy mình nói, nó đang có khoảng bốn trăm mấy cái shard, đúng không? Nó sẽ gặp khó khăn trong việc là quản lý bốn trăm mấy connection thằng này. Xong rồi mấy cái khó khăn trong việc scaling. Số lượng data thay đổi trong mỗi cái block của thằng Notion, nó xảy ra thường xuyên và nó rất nặng, sẽ...</p>
<p><strong>[01:04:13]</strong> Gây khó khăn trong việc đọc ghi trong cái table to này. Sau đó, nó mới bắt đầu setup một cái internal datalake của nó. Cái internal datalake này, có note là nó sẽ không thay thế thằng này hoàn toàn, mà nó chỉ sử dụng cái mới thôi. Còn cái này, nó vẫn tận dụng trong một vài tác vụ, kiểu nhẹ hơn, cho mấy cái table thay đổi data không có nặng lắm. Với lại nó cần cái gì. Còn thằng này, nó expect cái luồng này á, là nó sẽ đánh những cái data nó cần để cho những mục đích mà analytics hoặc là machine learning.</p>
<p><strong>[01:05:08]</strong> Data nó có thể chấp nhận cái độ trễ là vài tiếng, vài phút, tiếng gì đó. Nó sẽ sử dụng cái data trong đây. Cái lượng setup thì cũng đơn giản thôi. Nó sẽ sử dụng cái thằng Debezium CDC này nè. Nó là cái capture data change á, để nó watch cái thằng database này, bắn về Kafka. Sau khi nó bắn cái đống event data change về Kafka, thì có một thằng bên đây là Hudi hay gì đó, nó lấy event đó, nó quăng về thằng S3. Rồi bắt đầu từ thằng này, thằng nào muốn sử dụng thì vô đây, nó lấy về, nó setup tiếp, xài data warehouse hoặc xài mấy cái chủ đích về shard gì đó, thì vô đây nó lấy, nó xài.</p>
<p><strong>[01:05:51]</strong> Cái đó là cái thật ra, cái case Notion. Chắc là có thể xài thằng này thử. Vì nó cũng là cái thằng đứng ở ngoài, nó watch vô cái đống đó. Nếu mà xài AWS hay retraining á, sẽ xài cái một là cái thằng Redshift hay gì quên rồi. Nó sẽ watch thằng đó, những thay đổi trên cái database, xong rồi nó sẽ lưu hết về trong một cái bucket hay cái gì đó. Xong rồi từ đó, mình bắt đầu xử lý sau. Cái luồng bên này là có thể sử dụng cái này. Hồi nãy setup một cái demo, nhưng mà có vẻ hơi fail rồi.</p>
<p><strong>[01:06:51]</strong> Tại vì nó chưa có được cái thằng server, nên là nó fail. Để sau đi, rồi chắc chỉ có như đó. Với lại có cái kiểu góc nhìn đó, là cái process này nè. Là cái process mà chắc tụi enterprise, nó sẽ có thể áp dụng. Nó là process kiểu chung chung mà đa số tụi enterprise sau này, em nghĩ là nó có thể. Nhu cầu của nó khi mà cái data lớn lên á, thì cái nhu cầu của nó cũng sẽ đi theo hướng này thôi. Đó là nó cần data, thu thập data để làm cái gì đó, và không ảnh hưởng tới cái luồng chính.</p>
<p><strong>[01:07:52]</strong> Mình thì xưa giờ toàn focus vào cái việc làm việc với mấy cái model AI. Nhưng mà mình nghĩ là sau này, mình cũng cần cái skill set gì đó để mình biết cách xử lý những data như thế này, tụi mà nó data lớn hơn kiểu vậy. Cho xin lỗi cái, anh nào đây? Anh đang nhìn nhận cái process này, thì nó khác gì với chuyện là mình replicate cái database của mình ra một instance khác để phục vụ chuyện retraining ấy anh? Là tại vì ở đây, đứng ra là ý ở đây, thật ra là kiểu em đang sinh giống như kiểu sinh data sang một...</p>
<p><strong>[01:08:54]</strong> Cái shard khác, đúng không? Data warehouse, đúng không? Và sử dụng upload kit process cho những cái tác vụ mà nó không, kiểu mình làm mình làm async được ấy, chứ không cần phải trực tiếp trên nguồn data chính. Câu hỏi là đối với cả mấy cái model dạng như sharding hay sử dụng master-slave ấy, thì sao không theo kiểu cứ duplicate cái database của mình ra thôi? Duplicate data thì nó vẫn chỉ là một cái data warehouse ở dưới dạng table ha. Còn thật ra cái này, nó chỉ là cái process, nghĩa là một process cho database thôi</p>
<p><strong>[01:09:40]</strong></p>
<p>. Nó có thể có những cái event khác. Như là ví dụ, mình sẽ có nhiều cái external data, không hẳn là mình chỉ có một cái battery, database không. Ví dụ mình có mấy cái capture như là từ social media, hoặc là mấy cái tụm lum la nào đó, chả biết. Nhưng mà nó có thể là nhiều loại data khác nhau, gom về, quăng qua thằng này. Thằng Hudi bạn này, nó sẽ là thằng chịu trách nhiệm xử lý cái raw data đó, để nó quăng vào cái thằng S3 này. Nó lưu...</p>
<p><strong>[01:10:23]</strong></p>
<p>Mọi thứ dưới cái đống này. Nó vô luôn, mọi thứ về dạng file gì đó, gom hết vô đây, để bắt đầu sau này, mấy thằng ngoài sao n mới có cái slot để xử lý. Thật ra tụi nó cũng có một câu hỏi là tại sao không dùng mấy thằng database như MySQL hay PostgreSQL á? Nó sẽ có mấy cái... Tại sao phải sử dụng cái thằng capture data change mà không sử dụng mấy thằng đó? Mấy thằng đó, nó có cơ chế để streaming mấy cái event change của nó luôn. Mấy thằng đó, event stream, nó thường sẽ stream trực tiếp từ database này qua database khác.</p>
<p><strong>[01:11:09]</strong></p>
<p>Còn thằng này, nó sẽ là capture cái event đó và đưa đâu cũng được. Vì là nếu mình không có thằng Kafka này ở đây á, thì mình cần một service nào đó, mình cần cái real-time data xử lý liền luôn á, mình không cần phải vô Kafka. Cái thằng CDC này vẫn có thể bypass qua đó được, kiểu kiểu vậy, chứ không hẳn là từ database sang database kiểu như vậy. Ta cũng có nhìn ý là kiểu, thấy là nếu mà theo góc nhìn về operation chẳng hạn ấy. Tất nhiên nếu mà có multiple datasource và sử dụng những cái partition tool các thứ, nó khác nhau ấy.</p>
<p><strong>[01:11:50]</strong></p>
<p>Database khác nhau, thì cái này cũng sẽ cả, thật ra là gom nó lại vào một cái datalake, sao cho một số cái tác vụ, nó cụ thể thôi ấy. Thực ra là một số team, như kiểu team AI hay team về mặt làm report, hay data, thì người ta cũng sẽ chỉ cần work trên cái data warehouse này thôi, kiểu vậy. Hoặc là có extend cho bên nào khác nữa, thì cũng sẽ make sense, phân vùng data riêng cho từng cái team riêng, đúng không? Có họ thêm cái vụ mà cái button, cái button ETL bên database bình thường với cả bên datalake, thì nó sẽ là ELT, đúng không?</p>
<p><strong>[01:12:39]</strong></p>
<p>Đúng rồi, ELT, anh sẽ hiểu là mình extract, nghĩa là mình tìm đúng file, đúng không? Mình load cái file đấy lên đây, và transform nó thành dạng kiểu structured data ha. Ý là nó sẽ transform, nó chỉ là cái action, nó xảy ra ở sau khi mình có raw data rồi. Còn ETL, nghĩa là extract là sẽ lấy data từ cái đám data source á. Xong rồi nó sẽ có cái quá trình log thẳng vào cái raw data, thẳng vào mấy cái gì đó của mình. Nó gọi là cái raw landing, cái layer raw landing. Xong rồi mình mới có cái gọi là transform.</p>
<p><strong>[01:13:34]</strong> Sau đó, sau cái landing sẽ có transform để xử lý data, thì nó sẽ ra sau. Còn cái thằng kia là nó extract xong, rồi transform, nó mới quăng thẳng vào cái warehouse, đó là cái database của mình. Hay anh em có hỏi gì không? Rồi, cảm ơn An, đúng rồi. Anh em nhé, rồi bye anh em, mỗi tuần vui vẻ.</p>
<hr>
<h3 id="english-transcript">English transcript</h3>
<p><strong>[00:00]</strong> It seems like the all-hands meeting is next week. I don’t see anyone creating any events, so this week will probably just be normal, right? Guys, check if there’s any issue with screen sharing. Should we just start? Today, I think we’ll have about three presentations. Phát just said he doesn’t have anything new this week, so he’ll probably skip today. Guys, try sharing your personal screens and see how it goes. Can we preview it first?</p>
<p><strong>[10:42]</strong> According to the schedule, it’s probably you, right, bro? Let me go first then. This fine-tuning topic, it’s not really that new. This presentation is just 100.5, not 101, so it’s only a brief intro, not going deep into it yet. Honestly, I’m pretty clueless about it too, so I’ll just give a quick overview. Today, I’ll present about fine-tuning. Here’s the agenda for this session.</p>
<p><strong>[12:24]</strong> The introduction is, if you guys use AI, you’ve probably heard of the concept of fine-tuning. AI has these models that are mostly fitted to data from some specific day, with stuff like data privacy or data from a particular domain. That data doesn’t show up in the knowledge of the foundation model. To get that knowledge into the model, people usually use retraining, right? But there’s another way called fine-tuning. At the end of this, I’ll compare these two methods, looking at when to use which one and when not to.</p>
<p><strong>[13:08]</strong> For now, just think of fine-tuning as a way to expand the knowledge of a foundation model. What is fine-tuning? Simply put, you retrain the model. You take a foundation model, feed it a dataset to fine-tune it, meaning you retrain it. After fine-tuning, you get an adjusted model, called a fine-tuned model.</p>
<p><strong>[13:44]</strong> Why does fine-tuning bring in new knowledge? Simply put, in an AI model, knowledge is stored through neural networks. Fine-tuning updates the weights, the parameters of that neural network, to fit the new knowledge. When you throw new knowledge in, those weights change, and at that point, the model has updated its knowledge.</p>
<p><strong>[14:19]</strong> When you throw new knowledge in, the weights change, and at that point, the model has updated its knowledge. When fine-tuning a model in practice, it’s not just about throwing a dataset in and retraining it. Sure, you get a fine-tuned model, but you don’t know if that retrained model is any good. I’ll introduce a workflow that people outside commonly use for fine-tuning.</p>
<p><strong>[14:59]</strong> This flow, it’s got several steps like this. You can split it into two clusters, two clusters, alright? This flow, I’ll divide it into two clusters. I’ll go through the first cluster first. The first cluster is the one on the left, simply understood as a base model to start with. Then, you have a new dataset, something new, you throw it in, fine-tune it. Then it produces a model, and you supervise it, meaning you retrain it in a way that’s like retraining it.</p>
<p><strong>[15:38]</strong> It’ll add more knowledge, with this input, the output will come out like this. It results in something called supervised fine-tuning. After that, let me move to the next part. Alright, this fine-tuning is retraining on data, meaning you give it a pair of input-output in the dataset for it to learn. It’ll pick up that new knowledge. For this step to be perfect, the dataset has to be clean. It has to be clean, not mixed with other stuff. Meaning it has to be specific to the domain we want to train it on.</p>
<p><strong>[16:31]</strong></p>
<p>Back to this diagram, after you have a model that’s been retrained, you bring it to production, you use it, right? At this point, people outside use a system called human feedback. It’s like, does the response from this model satisfy you? Rate it from 1 to N stars, something like that. You’ll collect that data. It’s part of this step—you’ll gather human feedback from that retrained model of yours.</p>
<p><strong>[17:06]</strong> Based on that feedback, people call this step a bit resource-intensive. You’ll have to retrain a separate model. That model is used to evaluate how many points this response gets. Next, you move to the third step, the final one. In this step, you’ll use algorithms like reinforcement learning to combine it with the retrained model and your reward model.</p>
<p><strong>[17:46]</strong> You retrain, you fine-tune the model one more time. It’ll give you something called an optimized model. This loop keeps going, going forever. You have a retrained model, collect human feedback, then combine those three things to retrain the model again. The more you do it, the better the model gets at what we want. That’s the flow I’ve seen people use out there in production.</p>
<p><strong>[18:24]</strong> During the fine-tuning process, you’ll often hear about a concept called catastrophic forgetting. What does that mean? It means when you retrain with new knowledge, it reduces performance on the old knowledge. Why does this happen? As I said, a model’s knowledge depends on its weights, its architecture, and its parameters. The dynamic parameters in a model are those weights. When you retrain, those weights change, right?</p>
<p><strong>[19:00]</strong> When they change, doesn’t that mean the old knowledge gets less accurate? If your dataset has a lot of overfitting data, meaning your dataset is too perfect, too perfect within that dataset, then when someone throws something new in, it’ll mess up the old stuff. They call that overfitting, meaning it’s folded in too much to the training data. When it encounters new data, performance drops.</p>
<p><strong>[19:42]</strong> So at this point, people out there use a technique called parameter-efficient fine-tuning, or PEFT. It has lots of methods, techniques within this approach, like LoRA and stuff. But generally, most of them don’t update all the weights in the model. They’ll just freeze the layers that aren’t necessary. They freeze those unneeded layers and only update a certain number of weights. That’s to avoid losing too much of the old knowledge. That’s the basic idea. For deeper stuff about the algorithms behind it, you can look it up yourselves.</p>
<p><strong>[20:27]</strong> Back to the question from the start, how’s it different from retraining, and which should we use? There’s a table here, you can easily see it. Retraining depends on your database. You keep throwing stuff in, throwing stuff in, and the data gets updated constantly. But with fine-tuning, you retrain the model, so the data stays only where you retrained it.</p>
<p><strong>[21:16]</strong> Next is customize and learning style. Meaning, retraining’s purpose is to give us a knowledge base that we can pull from to reference and use. But fine-tuning? It upgrades the model’s brain, so it has that knowledge built-in already. The stuff below that, you guys can probably look into it more yourselves, yeah?</p>
<p><strong>[21:57]</strong> I’ve got an example like this. For instance, say you want to build a system to explain doctors’ notes, right? Doctors’ notes, as you might know, have tons of technical terms. And those technical terms are often abbreviated, written all sloppy too.</p>
<p><strong>[22:44]</strong> If you guys use fine-tuning, you’ll make it learn all that messy knowledge, the shorthand stuff, the handwriting stuff from doctors. So when you input a doctor’s note, it’ll give you a really accurate answer. But if you use retraining, when you input a doctor’s note, it’ll find the relevant data and pull it up to read. But the thing is, the model doesn’t actually understand those terms, so it won’t give you an accurate answer either.</p>
<p><strong>[23:16]</strong> You can think of it like this: fine-tuning is like asking a doctor to read a doctor’s note. Retraining is like giving it to someone with really broad knowledge to read a doctor’s note. That person might know a ton, but when it comes to the specialized stuff, the real technical terms, they don’t have the depth of an actual doctor. So the accuracy won’t be high.</p>
<p><strong>[23:57]</strong> Second, you might say, alright, with retraining, we can use a system prompt to list out all the doctor’s shorthand in the system prompt, and it’ll figure it out on its own. But doing that, you’ll end up using a lot of tokens, right? Because when you use retraining, you pull out all the retraining data, throw in a retraining knowledge base, plus a bunch of zero-shot stuff, descriptions, and whatever else goes with it in a prompt, that takes up a ton of tokens.</p>
<p><strong>[24:34]</strong> And when you’re in a long conversation with a model, it’s not like it only relies on your question. It bases its answers on all the conversations you’ve had with it from the start. At that point, it leads to a situation where it’s limited by tokens. That’s the drawback of using retraining—it eats up tokens. Because you need tokens to run your system prompt too. But with fine-tuning, the thing is, the model already has the knowledge, so you don’t need a system prompt.</p>
<p><strong>[25:14]</strong> That’s a quick rundown on fine-tuning. Probably having a demo for you guys to see would make it clearer, yeah? Now I’ll fine-tune a model called Duty 40 Mini. I’ve got a dataset like this. Yup, like this, each thing has a system like retraining, then the user asks this and wants it to answer like that, right? I’ve got 10 things, 10 records in this dataset, and I’ll fine-tune it.</p>
<p><strong>[26:13]</strong> Before fine-tuning, I’ll run it through a piece of code so I can estimate it. Since I’m using Open AI, it’ll cost money. So we’ll calculate an estimate of how much it’ll charge me. After I run it, at the end it’s like, around 4800, close to 4800 tokens. This is just a reference, but I think it’s pretty accurate. Then I’ll upload this data file to Open AI. It’ll give me the file up on my Open AI account.</p>
<p><strong>[27:03]</strong> Then I’ll train it. I’ll create a fine-tuning job. At this point, on Open AI, it’ll run this job. You guys can go up here, read it, check it out. It won’t give results right away, it’ll create a job and leave it pending there, so Open AI can fine-tune it for us. While waiting, we can track how the process is going. Once it’s done, it’ll notify us. We just keep stamping this sentence, checking this sentence to see if it’s finished or not. We read it from that spot.</p>
<p><strong>[27:58]</strong> Once it’s done, it’ll give us some result statuses. After fine-tuning, with the same question. For example, this is the question I used, I used this question, it’s pretty close to one of the records in my dataset pile. After I run it, it’ll answer like this. But before fine-tuning, I used a normal model, just a regular model, and it answered like that.</p>
<p><strong>[28:49]</strong> Meaning, when I fine-tuned it, it worked. That’s how I used Open AI to fine-tune a model. That’s it for my demo. Any questions, guys? Yeah, for this demo, I used tuning, but to do fine-tuning locally with something fancy, I probably don’t have the gear. Yup, just small-time gear. Actually, with some earlier models, I ran them on LoRA and stuff, and they could’ve been demoed too. But I didn’t, this one’s easy, it’s like a basic one.</p>
<p><strong>[29:47]</strong> A bit scattered and slow, huh? Yeah, it’s probably fine though. Generally, enterprise teams or those who don’t want to spend time building GPUs will use this method. Back with Diagram GPT, when GPT-4o Mini came out, fine-tuning was free, so using this was pretty convenient for them. The demo shop for you guys is using something like a service. Open AI provides a fine-tuning service, putting it right up on their models. We pick some models probably the mini ones, I guess. The cost probably isn’t too high.</p>
<p><strong>[30:37]</strong> That’s one way to do it. But the issue is, we’re still not the ones owning that model. The thing is, it’s still hosted on their server. There’s another way, like building your own server and running it yourself. Today’s case was different. Check it out, guys, yesterday I tried a model with just 3 billion parameters. But it ran for two or three hours and still wasn’t done, bro. Actually, this one, its version is simpler than what we have now, smaller than the previous one from earlier.</p>
<p><strong>[31:26]</strong> It’s a full flow related to what was it reinforcement feedback. The point is, what I introduced about production out there is when people tune stuff. They don’t just fine-tune it and use it right away, they have to evaluate it again to see if it’s right. They put it into a cycle to keep improving the fine-tuned model, something like that. This is one of those flows. At its core, it’s just a model, nothing special. The key is you guys knowing the costs to judge the approach.</p>
<p><strong>[32:05]</strong> Because it’s still about accuracy, right? We pick a method to make the output more accurate. Methods like retraining or fine-tuning they’ve got different downsides. And honestly, even with fine-tuning, there are lots of different fine-tuning methods. We’d probably need to dig deeper to figure those out. This is still kinda general. Probably so, Hoàng. If we’ve got the chance, we should dive a bit deeper.</p>
<p><strong>[32:48]</strong> Deeper in the sense of looking at methods related to retraining and stuff. With fine-tuning methods, some of them are pretty resource-efficient. Of course, there’s a trade-off with some other stuff, like that. I’m introducing this so you guys can check it out somewhere. People asked—Đạt asked when we need fine-tuning. I’d say fine-tuning is needed when you want it to have knowledge on a specific topic. You can consider using fine-tuning then.</p>
<p><strong>[33:39]</strong> But in all cases, from what I’ve seen out there, most people prefer retraining. Because it’s easier and uses fewer resources. But in some cases, like right now, the example I gave about doctors’ notes, I’d suppose tuning is better. Then it depends on the architecture, how we split our system into smaller systems, what those smaller systems are like it varies. There might be some use cases where they want to host small models, tiny ones maybe.</p>
<p><strong>[34:18]</strong> Just for doing a specific task. Like analyzing weather, humidity, stuff like that, to perform some action. For example, changing your phone’s theme or triggering some action or whatever. You could retrain a small model just for that, no need for a network or anything fancy. Probably like that. From now on, it’s Biên’s turn, yeah?</p>
<p><strong>[36:13]</strong> Using it and building a recovery process for it. How it works in detail, it’s got a few main parts. First is the reason we need this technique and comparing it to something more familiar like backup. Then it’s about how we build it and the things we need to watch out for, what stuff. To start, in reality, there are often organizations, companies running apps with high data traffic. Like stock trading stuff, for example. My example here is something like 50,000 transactions.</p>
<p><strong>[37:12]</strong> My example is like 50,000 a day is low it could shoot up to 500,000, a million transactions a day. After a while, that data volume swells up huge, affecting how we query data and impacting the user experience. In that data, there’s stuff that, once used, barely gets accessed again. Like history from over 7 years ago, for instance. That leads to a problem how do we deal with that pile of data? So we use a technique called data archiving.</p>
<p><strong>[38:14]</strong> It’s got benefits to counter those issues up there. First off, the data we use, the stuff that’s constantly being set, queried, read, and written all the time, it usually costs a lot. With this technique, we take our data and move it somewhere else, somewhere cheaper with less access. That way, it boosts our app’s performance when querying or aggregating data and such.</p>
<p><strong>[39:07]</strong> In terms of legal stuff or reusability, that data will be kept safe, not affected by external factors. So later, when we need to use it again, we can pull it out and use it. As people often say, you’ll think of data backup, which is usually used to restore data, restore the system, or the app if something goes wrong. But these two things are different in that data backup is more for hotfixing the system. As for archiving data archiving focuses on storing data long-term.</p>
<p><strong>[39:59]</strong> It has a detailed comparison like this. To build an architecture, a system to archive data, and then use it for recovery when we need it, here’s how it works. You guys see, it has three main notes. First, we store the data, we use metadata to interact with that data, then we put it somewhere, like cloud-based services or cloud storage services, to keep that data stored.</p>
<p><strong>[40:51]</strong> In detail, to store the data, first we have to figure out which data needs to be stored. We need to analyze which data gets used a lot, which doesn’t get used, or gets accessed rarely. There are lots of tools to help us do that. For example, analyzing from business requirements or using analytics tools, those analytics tools. From there, we figure out which data is necessary, which can be archived.</p>
<p><strong>[42:05]</strong> After that, we’ll package it up, using a few methods like vectorizing it, encoding it, then using checksums and stuff to make sure the data stays correct. Later, when we use it again, we can access it quickly. Because these databases are packaged in a storage different from what we usually set, we need to save its metadata. For instance, store it by month, yeah, or by account, so it’s easier to query later.</p>
<p><strong>[43:07]</strong> After archiving, when we want to use it again, just now I gave an example for recovery. We’ll use that metadata from earlier, search for the data blocks we need, then bring it back to the computing environment when necessary. The benefit here is that when we do this stuff, it doesn’t mess with the production data of the running app. We can do it in parallel. Whatever we want to do with it, we do, without poking around in production, so it keeps things safe.</p>
<p><strong>[43:51]</strong> For the user experience, like our product. Speaking of this, there are a few practices for using and building this system. It’s pretty simple, right? We’ll have to review the policies we set up for this system to run, check if the data stays intact. We’ll automate the steps of this process. Nowadays, there are plenty of tools supporting us already, like AWS or Google, they’ve got stuff like...</p>
<p><strong>[45:14]</strong> Google Cloud, for example. We just need to write some simple stuff to push it up there. And we can’t skip monitoring to see if this data is working well or not. Then, there are other techniques like checksums and such, to ensure our data always stays intact. When we need it, there’ll also be strategies like scheduling the data beforehand. Because this data sticks around for a long time, it’ll grow big, it’ll swell up in the storage, the cloud storage we use to keep it.</p>
<p><strong>[46:05]</strong> So there’ll be strategies like, when we need it, we have to schedule in advance, how much time it’ll take to replicate the data for us, for instance. That’s my plan, that’s it. The theory is kind of to address the ultimate goal of explaining to you guys about handling data that sticks around long-term but isn’t used much in the system we build. Like in banking, for example, there’s stuff like user trades, user trades hitting hundreds of millions of records or something.</p>
<p><strong>[46:45]</strong> Later on, it’ll grow even more. Meaning querying just the recent data, but it still takes a ton of time, something like that. That’s what I talked about today, done. Any questions, guys? When we store data, zip data, it’s like we zip up a fragment from the past that doesn’t use that data for current purposes, right? Yup, exactly, exactly. Agreed, I’ll have to delete it. Once it’s done, I’ve got to delete that, yeah. So we’ll have ways to reload it for calculations when needed.</p>
<p><strong>[47:41]</strong> That’s why we’ve got these methods to keep it safe. I’ve got a question over here. I don’t know if Thỏ’s crew knows about this, can we compare it? Standing out is Timescale, it’s got a chunk-moving mechanism. For example, we compress it like normal. Plus, there’s this thing about having hot, warm, and cold storage. Like, if we back up weekly, it goes on Azure’s hot storage. If it’s too old, say 2, 3, 4, 5 years, then it’s on Azure’s cold storage or Backblaze. It’s got a separate service for us to move that data stuff.</p>
<p><strong>[48:53]</strong> Right to the object or block storage spot, we interact with Timescale to make sure we save money with old data when we need to. It can save costs, it can still query, with the trade-off being that querying is a bit slow with older data. Yup, what I get is it depends on the platform we use to build, right, bro? For example, with Microsoft, it’ll depend on the database’s timing or the data’s lifespan or capacity and stuff, so there’ll be different levels.</p>
<p><strong>[49:40]</strong> For example, there’ll be delays, or normal access still, or delays for stuff that hasn’t been used in a long time. That’s for us to specify on each tool. But generally, it’s like, bro, what’s this doing? Standing out is Timescale, it fits this kind of pattern, for time series stuff. On Azure’s side, they make it fit the status, kinda like Timescale, but it helps us partition and shard the way we want.</p>
<p><strong>[50:39]</strong> Each one has its own pros and cons. With AWS, standing out is that with this service, you’ve got to watch the hardware storing this data, whether it’s stable or not. For example, Azure’s cold storage uses disks, what kind of disks, some pretty unique ones. They’ve got to use a laser machine to burn it in there. So querying is super fast, but inserting is kinda slow, like inserting a bunch takes a few minutes. Because it needs a hard laser to burn it on there, no virtualization layer.</p>
<p><strong>[51:23]</strong> Each service and each type of tool we use for compressing and storing has its own pros and cons, depending on the platform we subscribe to. Yup, exactly. This isn’t just about tools like AWS or Google services. It’s like we can also weigh it for our business too. So this is kinda general. Each platform uses different techniques. The ultimate common goal is to tackle the issue of data growing big but affecting how we query, how it runs.</p>
<p><strong>[52:16]</strong> Lots of ways to solve the query optimization problem, right? When the issue is that the data’s too big, there are a few approaches. Biên’s way is one approach, meaning there’s a chunk of data we’re not using, so we cut it out, store it somewhere. Later, if we need past data, we insert it back to use it. For now, we keep some percentage of current data, enough for current purposes, so querying is faster.</p>
<p><strong>[52:59]</strong> Other ways use tooling, some database types, like Timescale, optimize querying for huge data right off the bat. I think underneath, it kinda auto-buffs it somewhere, holds it for us, right, bro? So we fuss over the details underneath, we just use the interface. Underneath, it’s pretty much the same, like us kids. Thanks, Biên, that’s it. Probably An’s last piece, not sure if it’s related. Not sure if it ties a bit to the community stuff.</p>
<p><strong>[54:00]</strong> If there is, it’s probably just a quick rundown, not a lot. Pretty similar to Biên’s piece, but the use case is kinda close too. It expands a bit more. Alright, so this piece is a quick talk about datalakes and Notion’s use case. Let’s talk datalakes first. Datalakes, you guys have probably heard about them tons, been around for a while now. Let’s look back at how these datalakes evolved, see where we’re at.</p>
<p><strong>[54:54]</strong> Actually, from the start, around 1980 or something, it was the era of databases, database warehouses, the stuff we’re using now. Table stuff, creating tables and processing data. Later, around the 2000s or so, the big tech folks started collecting tons of data. They used that data, so new stuff popped up to handle storing and processing data on big datasets. Like data stored as files and such. These things, terms like MapReduce, for instance.</p>
<p><strong>[55:44]</strong> I think in my memo, there’s an article on MapReduce. If you guys don’t know, you can search it up, check it out to see what MapReduce did back then. It was the ancestor of this era. Later, it just got integrated in, not used standalone anymore, but it’s probably all built-in now. After that development phase, around 2010, it started giving birth, a bit before 2010, to concepts like datalakes, big data, cloud, internal data warehouses on the cloud. It’s just cloud stuff.</p>
<p><strong>[56:28]</strong> Now, these days, it’s evolving further, into lakes and datamarts. Lakes are probably just a mix of datalake stuff and warehouses, then turned into a house. Like Datadog or whatever they’re doing, I don’t know, but we’re probably talking about this a bit behind the times. To focus in, let’s take a quick look at a general data architecture first. This one, Tom’s piece the other day posted it, had a diagram too. It’s a bit more streamlined than this, a bit more concise, about data going through layers, processing then to some other thing.</p>
<p><strong>[57:20]</strong> This one shows it a bit clearer, about what kinds of data we store in a datalake. Compared to a data warehouse, we only store structured data, or stuff like table data that’s all cleaned up. But this datalake, it’s raw data, it’ll handle structured, unstructured, semi-structured data all together. It stores it raw, then it processes the data, transforms it, and tosses it over to the BI analytics crew or into another warehouse to hold the processed data.</p>
<p><strong>[58:18]</strong> Then there’s this analytics sandbox layer, which is a layer for data scientists or folks who need to use raw data, process data, without messing with the main process. Over here, they’ll work on this sandbox to handle data for those guys, the ones who need raw data but don’t directly affect the main flow. It’s like what Biên said earlier, doing that stuff, taking data and storing it somewhere for later use or to process something, I don’t know, but it doesn’t want to mess with the app’s main process, so it’s this pile.</p>
<p><strong>[59:09]</strong> Here, you guys see we’ve got this concept called ETL, extract, transform, and load. With warehouses, what we’ve done so far is extract, transform, and load, it follows that order straight up. But in this one, you’ll clearly see the datalake does extract and load first. Then when it’s needed, it starts processing the data, that’s transform. Transform comes after, load comes before. That’s the difference between the two.</p>
<p><strong>[59:52]</strong> This is just the spot comparing the differences between data warehouses and datalakes. With warehouses, the data is cleaned, structured, organized into tables. But this one stores it as files, raw data and stuff, semi-structured already, CSV or JSON files. The processing is different between this lake and that lake too. Querying, the warehouse uses SQL, while over there, it processes directly on the data itself. Tools that support direct data processing, like Spark, handle that stuff. Moving on to Notion.</p>
<p><strong>[01:00:46]</strong> For datalakes, the use case of Notion, you guys know we’ve been using Notion quite a bit already. Back in the day, it started slow. The organizations, the blocks from before, they were organized like normal data, just like us, small apps. Its blocks started growing gradually. Blocks are understood as what, and they’ll include the title in there. They call it a block. The number of blocks keeps increasing constantly by the day and hour.</p>
<p><strong>[01:01:35]</strong> Something like that, then later it started swelling up, and it began using techniques like sharding, old-school sharding. Like I remember Hải Vũ’s article mentioning something about it, scaling horizontally. It started splitting into sharding and stuff, then instances. From 2021 to 2023, it had 32 instances. Each instance had 15 shards. Then from 2023 onward, it started splitting again, increasing even more. The number went up again, so that’s 96 instances. And each instance had 5 shards. Multiply that, it’s around 400-something, four hundred and some.</p>
<p><strong>[01:02:27]</strong> To handle that, at this point it’s pretty big, right? When the data starts getting big, it’ll have some needs. Later on, it’ll have needs for analytics or stuff related to machine learning, datasets and tricks and all that. It started setting up its own data warehouse architecture. This was the precursor before setting up the datalake. It did a data warehouse to process data. The basic flow it set up was to collect data, stuff about data changes in the blocks across each shard.</p>
<p><strong>[01:03:21]</strong> It used file transfers to ingest the data from these shards here. It dumped it into something, then merged those things into one big single database. This ran into issues because, like I said earlier, it’s got about four hundred-something shards, right? It struggled with managing four hundred-something connections for this thing. Plus the scaling challenges. The amount of data changing in each block of Notion happens often and is super heavy, so it made reading and writing in this big table tough.</p>
<p><strong>[01:04:13]</strong> After that, it started setting up its own internal datalake. This internal datalake has a note that it won’t completely replace this one, it just uses the new stuff. The old one, it still uses for some tasks, lighter ones, for tables where data changes aren’t too heavy. And it needs something. But this one, it expects this flow to tag the data it needs for purposes like analytics or machine learning.</p>
<p><strong>[01:05:08]</strong> The data can handle a delay of a few hours, a few minutes, something like that. It’ll use the data in here. The setup amount is pretty simple. It uses this thing, Debezium CDC, you know. It’s the capture data change thing, to watch this database and shoot it over to Kafka. After it shoots that pile of event data changes to Kafka, there’s a thing over here, Hudi or something, that grabs those events and tosses them to S3. Then from this point, whoever wants to use it goes in here, grabs it, sets it up further, uses it for data warehouses or some shard-related purposes, they take it from here and use it.</p>
<p><strong>[01:05:51]</strong> That’s actually the Notion case. We could probably try using this thing. Because it’s also one of those that stands outside, watching that pile. If we use AWS or retraining, it’d use something like Redshift or whatever, I forget. It’d watch that, the changes on the database, then save it all into a bucket or something. From there, we start processing afterward. This flow here could use that. Earlier, I set up a demo, but it kinda flopped.</p>
<p><strong>[01:06:51]</strong> Because it didn’t have the server yet, so it failed. Let’s leave it for later, probably just that much for now. Plus there’s this perspective, this process here. It’s a process that enterprise folks could probably apply. It’s a kinda general process that most enterprises later on, I think, might use. Their needs, when the data grows big, will probably head in this direction anyway. It’s that they need data, collect data to do something, without messing with the main flow.</p>
<p><strong>[01:07:52]</strong> For us, so far we’ve always focused on working with AI models. But I think later on, we’ll also need some skill set to know how to handle data like this, stuff where the data’s bigger, you know. Sorry, which bro is this? You’re looking at this process, how’s it different from us replicating our database to another instance for retraining purposes, bro? Because here, standing out, the point is, it’s like I’m kinda generating data to another different shard, right?</p>
<p><strong>[01:08:54]</strong> And using an upload kit process for tasks that don’t, like, we can do async, you know, instead of needing to work directly on the main data source. The question is, for all those models like sharding or using master-slave setups, why not just duplicate our database? Duplicating data, it’s still just a data warehouse in table form, yeah. But actually this, it’s just a process, meaning a process for the database.</p>
<p><strong>[01:09:40]</strong> It could have other events. Like, for example, we’ll have lots of external data, not just one battery, a database, you know. Say we’ve got captures from social media or some random messy stuff, who knows. But it could be lots of different data types, gathered up, tossed to this thing. This Hudi buddy here, it’s the one responsible for processing that raw data, to throw it into this S3 thing. It stores everything under this pile.</p>
<p><strong>[01:10:23]</strong> It goes right in, everything in some file format, all dumped in here, so later on, the outside folks have a slot to process it. Actually, they had a question too, why not use databases like MySQL or PostgreSQL? They’ve got their own... Why use this capture data change thing instead of those? Those ones, they’ve got mechanisms to stream their event changes already. With them, event streams usually stream straight from one database to another.</p>
<p><strong>[01:11:09]</strong> But this one, it captures that event and sends it wherever. Because if we don’t have this Kafka here, we’d need some service, we’d need real-time data processed right away, without going through Kafka. This CDC thing can still bypass to there, kinda like that, not exactly from database to database like that. We also noticed something, like, it feels like, from an operations angle, for instance. Of course, if there are multiple datasources and we use partition tools and stuff, they’re different.</p>
<p><strong>[01:11:50]</strong> Different databases, so this will also, actually, bundle it into a datalake, so some tasks are specific, you know. Actually, some teams, like the AI team or the reporting team or data folks, they’d probably just need to work on this data warehouse, like that. Or if it extends to other sides too, it’d make sense, splitting data zones for each team separately, right? They added this thing about the button, the ETL button in regular databases versus datalakes, it’d be ELT, right?</p>
<p><strong>[01:12:39]</strong> Yup, ELT, you’d get it as extract, meaning we find the right file, right? We load that file up here and transform it into something like structured data, yeah. The idea is it transforms, it’s just an action, it happens after we’ve got the raw data. But ETL means extract is pulling data from the data source pile. Then it’s got a process to log straight into the raw data, straight into some stuff of ours. They call it raw landing, the raw landing layer. Then we’ve got what’s called transform.</p>
<p><strong>[01:13:34]</strong> After that, after the landing, there’s transform to process the data, so it comes out later. But the other one extracts, then transforms, then tosses it straight into the warehouse, that’s our database. Any questions, guys? Alright, thanks An, yup. See you, guys, bye, have a fun week</p></div></div></div><section class="bg-background-secondary dark:bg-secondary-background my-8 flex flex-col items-start justify-between rounded-lg p-6 md:flex-row md:items-center md:justify-between"><div><h6 class="m-0 mb-1.5 text-sm leading-5 font-medium">Subscribe to Dwarves Memo</h6><p class="text-muted-foreground mt-0 text-sm leading-5">Receive the latest updates directly to your inbox.</p></div><form class="font-ibm-sans mt-5 flex w-full md:w-fit xl:mt-0"><input type="email" class="border-border dark:border-border dark:bg-background focus:border-primary min-w-[160px] flex-1 rounded-l-lg border bg-white px-4 py-3 text-[13px] leading-4 transition-colors duration-200 ease-in-out outline-none placeholder:opacity-70" placeholder="Email Address" required="" value=""/><button type="submit" class="bg-primary hover:bg-primary/90 dark:hover:bg-primary/80 text-primary-foreground inline-flex w-[100px] cursor-pointer items-center justify-center rounded-r-lg border-none px-4 py-3 text-sm text-[13px] leading-4 font-medium transition-all duration-200 ease-in-out disabled:cursor-not-allowed disabled:opacity-70 "><span>Subscribe</span><span class="border-primary-foreground ml-2 h-2 w-2 flex-shrink-0 animate-spin rounded-full border-2 border-t-transparent hidden"></span></button></form></section><div class="relative"></div></div></div></main><div class="toc-space"></div></div></div><footer class="border-t-border bg-background fixed right-0 bottom-0 left-0 z-40 flex h-8 items-stretch overflow-hidden border-t px-3 py-0 text-[0.875rem] leading-[140%] font-normal tracking-[-0.0125rem]"><div class="socials flex items-center gap-x-[10px] pr-3"><a href="https://github.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Plus/Github"><path id="Vector" d="M9 1.5C8.01509 1.5 7.03982 1.69399 6.12987 2.0709C5.21993 2.44781 4.39314 3.00026 3.6967 3.6967C2.29018 5.10322 1.5 7.01088 1.5 9C1.5 12.315 3.6525 15.1275 6.63 16.125C7.005 16.185 7.125 15.9525 7.125 15.75V14.4825C5.0475 14.9325 4.605 13.4775 4.605 13.4775C4.26 12.6075 3.7725 12.375 3.7725 12.375C3.09 11.91 3.825 11.925 3.825 11.925C4.575 11.9775 4.9725 12.6975 4.9725 12.6975C5.625 13.8375 6.7275 13.5 7.155 13.32C7.2225 12.8325 7.4175 12.5025 7.6275 12.315C5.9625 12.1275 4.215 11.4825 4.215 8.625C4.215 7.7925 4.5 7.125 4.9875 6.5925C4.9125 6.405 4.65 5.625 5.0625 4.6125C5.0625 4.6125 5.6925 4.41 7.125 5.3775C7.7175 5.2125 8.3625 5.13 9 5.13C9.6375 5.13 10.2825 5.2125 10.875 5.3775C12.3075 4.41 12.9375 4.6125 12.9375 4.6125C13.35 5.625 13.0875 6.405 13.0125 6.5925C13.5 7.125 13.785 7.7925 13.785 8.625C13.785 11.49 12.03 12.12 10.3575 12.3075C10.6275 12.54 10.875 12.9975 10.875 13.695V15.75C10.875 15.9525 10.995 16.1925 11.3775 16.125C14.355 15.12 16.5 12.315 16.5 9C16.5 8.01509 16.306 7.03982 15.9291 6.12987C15.5522 5.21993 14.9997 4.39314 14.3033 3.6967C13.6069 3.00026 12.7801 2.44781 11.8701 2.0709C10.9602 1.69399 9.98491 1.5 9 1.5Z" fill="#9B9B9B"></path></g></svg></a><a href="https://discord.gg/dfoundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="Discord"><path id="Union" d="M14.1919 3.95003C13.2419 3.50716 12.2133 3.18572 11.1418 3C11.1324 2.9997 11.1231 3.00146 11.1144 3.00517C11.1058 3.00887 11.0981 3.01442 11.0918 3.02143C10.9632 3.25715 10.8132 3.5643 10.7132 3.80002C9.57677 3.62859 8.42102 3.62859 7.28456 3.80002C7.18456 3.55716 7.03455 3.25715 6.89884 3.02143C6.89169 3.00715 6.87026 3 6.84883 3C5.77738 3.18572 4.75592 3.50716 3.79875 3.95003C3.79161 3.95003 3.78447 3.95717 3.77732 3.96431C1.83441 6.87154 1.29868 9.70019 1.56298 12.5003C1.56298 12.5145 1.57012 12.5288 1.58441 12.536C2.87015 13.4789 4.1059 14.0503 5.32737 14.4289C5.34879 14.436 5.37022 14.4289 5.37737 14.4146C5.66309 14.0217 5.92024 13.6074 6.14167 13.1717C6.15596 13.1431 6.14167 13.1146 6.1131 13.1074C5.70595 12.9503 5.32022 12.7646 4.94164 12.5503C4.91307 12.536 4.91307 12.4931 4.9345 12.4717C5.01307 12.4145 5.09164 12.3503 5.17022 12.2931C5.1845 12.2788 5.20593 12.2788 5.22022 12.286C7.67743 13.4074 10.3275 13.4074 12.7561 12.286C12.7704 12.2788 12.7919 12.2788 12.8061 12.2931C12.8847 12.3574 12.9633 12.4145 13.0419 12.4788C13.0704 12.5003 13.0704 12.5431 13.0347 12.5574C12.6633 12.7788 12.2704 12.9574 11.8633 13.1146C11.8347 13.1217 11.8275 13.1574 11.8347 13.1789C12.0633 13.6146 12.3204 14.0289 12.599 14.4217C12.6204 14.4289 12.6419 14.436 12.6633 14.4289C13.8919 14.0503 15.1276 13.4789 16.4134 12.536C16.4277 12.5288 16.4348 12.5145 16.4348 12.5003C16.7491 9.26446 15.9134 6.45724 14.2205 3.96431C14.2133 3.95717 14.2062 3.95003 14.1919 3.95003ZM6.51311 10.7931C5.77738 10.7931 5.16307 10.1145 5.16307 9.27875C5.16307 8.44301 5.76309 7.76442 6.51311 7.76442C7.27028 7.76442 7.87029 8.45015 7.86315 9.27875C7.86315 10.1145 7.26313 10.7931 6.51311 10.7931ZM11.4918 10.7931C10.7561 10.7931 10.1418 10.1145 10.1418 9.27875C10.1418 8.44301 10.7418 7.76442 11.4918 7.76442C12.249 7.76442 12.849 8.45015 12.8419 9.27875C12.8419 10.1145 12.249 10.7931 11.4918 10.7931Z" fill="#9B9B9B"></path></g></svg></a><a href="https://www.facebook.com/dwarvesf" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55 0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95"></path></svg></a><a href="https://dwarves.foundation/" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path fill="#9b9b9b" d="M17.9 17.39c-.26-.8-1.01-1.39-1.9-1.39h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41a7.984 7.984 0 0 1 2.9 12.8M11 19.93c-3.95-.49-7-3.85-7-7.93c0-.62.08-1.22.21-1.79L9 15v1a2 2 0 0 0 2 2m1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10a10 10 0 0 0 10-10A10 10 0 0 0 12 2"></path></svg></a><a href="mailto:team@dwarves.foundation" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 36 36"><path fill="#9b9b9b" d="M32.33 6a2 2 0 0 0-.41 0h-28a2 2 0 0 0-.53.08l14.45 14.39Z" class="clr-i-solid clr-i-solid-path-1"></path><path fill="#9b9b9b" d="m33.81 7.39l-14.56 14.5a2 2 0 0 1-2.82 0L2 7.5a2 2 0 0 0-.07.5v20a2 2 0 0 0 2 2h28a2 2 0 0 0 2-2V8a2 2 0 0 0-.12-.61M5.3 28H3.91v-1.43l7.27-7.21l1.41 1.41Zm26.61 0h-1.4l-7.29-7.23l1.41-1.41l7.27 7.21Z" class="clr-i-solid clr-i-solid-path-2"></path><path fill="none" d="M0 0h36v36H0z"></path></svg></a><a href="https://memo.d.foundation/rss.xml" target="_blank" rel="noreferrer" class="aspect-square cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" stroke="#9b9b9b" fill="transparent" stroke-width="3.6" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></div><div class="authors !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Dwarves Foundation</span></div><div class="filename !hidden items-center border-r border-r-[var(--border-color-light)] px-3 text-[#9b9b9b] md:flex dark:border-r-[var(--border-color)]"><span class="text-[var(--secondary-font-color-light-2)]">Memo</span></div><div class="last-updated hidden items-center px-3 text-[#9b9b9b]"><span class="text-[var(--secondary-font-color-light-2)]">© 2025</span></div></footer></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"directoryTree":{"/pinned":{"label":"Pinned Notes","children":{"/culture/readme":{"label":"Notes on our culture","children":{}},"/culture/ogif-intro":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/":{"label":"Home","children":{"/earn":{"label":"Earn","children":{"/earn/000-productivity":{"label":"Productivity","children":{}},"/earn/001-quality":{"label":"Software Quality","children":{}},"/earn/002-open-source":{"label":"Open Source","children":{}},"/earn/003-liquidity":{"label":"Liquidity","children":{}},"/earn/readme":{"label":"👾 Open bounties","children":{}}}},"/playground":{"label":"Playground","children":{"/playground/rfc":{"label":"RFC","children":{"/playground/rfc/readme":{"label":"RFCs","children":{}},"/playground/rfc/000-template":{"label":"000 RFC template","children":{}}}},"/playground/topics":{"label":"Topics","children":{"/playground/topics/blockchain":{"label":"Blockchain","children":{"/playground/topics/blockchain/build-custom-ai-agent-with-elizaos":{"label":"Build custom AI Agent with ElizaOS","children":{}},"/playground/topics/blockchain/web3-development-with-foundry":{"label":"Web3 Development with Foundry","children":{}},"/playground/topics/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin":{"label":"Implement a Token Swap from the Base chain to Bitcoin for cross-chain transactions","children":{}},"/playground/topics/blockchain/ton_core_concept":{"label":"Ton's base concepts","children":{}},"/playground/topics/blockchain/ton_blockchain_of_blockchains":{"label":"Ton: Blockchain of blockchains","children":{}},"/playground/topics/blockchain/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana":{"label":"Introduce to Solana Token 2022 - new standard to create a token in solana","children":{}},"/playground/topics/blockchain/solana-core-concept":{"label":"Solana core concepts","children":{}},"/playground/topics/blockchain/metaplex-nft-compression":{"label":"Metaplex NFT Compression","children":{}},"/playground/topics/blockchain/plonky2":{"label":"Plonky2","children":{}},"/playground/topics/blockchain/polygon-zkevm-architecture":{"label":"Polygon zkEVM architecture","children":{}},"/playground/topics/blockchain/starknet-architecture":{"label":"StarkNet architecture","children":{}},"/playground/topics/blockchain/zk-snarks":{"label":"zk-SNARKs","children":{}},"/playground/topics/blockchain/layer-2":{"label":"Layer 2: Scaling Solutions for Ethereum","children":{}},"/playground/topics/blockchain/solana-account":{"label":"Solana Account","children":{}},"/playground/topics/blockchain/foundational-topics":{"label":"Foundational Topics","children":{"/playground/topics/blockchain/foundational-topics/zero-knowledge-proofs":{"label":"Zero-knowledge Proofs","children":{}},"/playground/topics/blockchain/foundational-topics/blocks":{"label":"Blocks","children":{}},"/playground/topics/blockchain/foundational-topics/distributed-systems":{"label":"Distributed systems","children":{}},"/playground/topics/blockchain/foundational-topics/pos":{"label":"PoS","children":{}},"/playground/topics/blockchain/foundational-topics/smart-contract":{"label":"Smart Contract","children":{}},"/playground/topics/blockchain/foundational-topics/topics":{"label":"Topics","children":{}}}},"/playground/topics/blockchain/multisign-wallet":{"label":"Multisign wallet","children":{}},"/playground/topics/blockchain/anchor-framework":{"label":"Anchor framework","children":{}},"/playground/topics/blockchain/blockchain-bridge":{"label":"Blockchain Bridge","children":{}},"/playground/topics/blockchain/nft-fractionalization":{"label":"NFT Fractionalization","children":{}},"/playground/topics/blockchain/how-tokens-work-on-solana":{"label":"How Tokens Work on Solana","children":{}},"/playground/topics/blockchain/liquidity-pool":{"label":"Liquidity pool","children":{}}}},"/playground/topics/frontend":{"label":"Frontend","children":{"/playground/topics/frontend/report":{"label":"Report","children":{"/playground/topics/frontend/report/frontend-report-march-2025":{"label":"March 2025","children":{}},"/playground/topics/frontend/report/frontend-report-february-2025":{"label":"February 2025","children":{}},"/playground/topics/frontend/report/frontend-report-january-2025":{"label":"January 2025","children":{}},"/playground/topics/frontend/report/frontend-report-second-half-of-november-2024":{"label":"Nov 2024 (Second Half)","children":{}},"/playground/topics/frontend/report/frontend-report-first-half-of-november-2024":{"label":"Nov 2024 (First Half)","children":{}},"/playground/topics/frontend/report/frontend-report-october-2024":{"label":"October 2024","children":{}},"/playground/topics/frontend/report/frontend-report-september-2024":{"label":"September 2024","children":{}},"/playground/topics/frontend/report/frontend-report-august-2024":{"label":"August 2024","children":{}},"/playground/topics/frontend/report/frontend-report-july-2024":{"label":"July 2024","children":{}}}},"/playground/topics/frontend/react":{"label":"React","children":{"/playground/topics/frontend/react/code-splitting":{"label":"Code splitting","children":{}},"/playground/topics/frontend/react/component-composition-patterns":{"label":"Component composition patterns","children":{}},"/playground/topics/frontend/react/design-system-integration":{"label":"Design system integration","children":{}},"/playground/topics/frontend/react/hook-architecture":{"label":"Hook architecture","children":{}},"/playground/topics/frontend/react/rendering-strategies":{"label":"Rendering strategies","children":{}},"/playground/topics/frontend/react/state-management-strategy":{"label":"State management strategy","children":{}},"/playground/topics/frontend/react/testing-strategies":{"label":"Testing strategies","children":{}}}},"/playground/topics/frontend/websockets":{"label":"WebSockets","children":{}},"/playground/topics/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree":{"label":"From Markup to Pixels - A look inside the DOM, CSSOM, and Render Tree","children":{}},"/playground/topics/frontend/window-and-iframe-communication":{"label":"Window and iframe communication","children":{}},"/playground/topics/frontend/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"Applying Mock Service Worker (MSW) for Seamless Web Development","children":{}},"/playground/topics/frontend/render-optimization-in-data-fetching-libraries":{"label":"Render optimization in data-fetching libraries","children":{}},"/playground/topics/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql":{"label":"A Fragment Colocation Pattern with React \u0026 Apollo GraphQL","children":{}},"/playground/topics/frontend/scroll-driven-animations":{"label":"Scroll-driven animations","children":{}},"/playground/topics/frontend/react-server-component":{"label":"React Server Components, NextJs Route and Data Fetching","children":{}},"/playground/topics/frontend/url-formats-for-sharing-via-social-networks":{"label":"URL formats for sharing via social networks","children":{}},"/playground/topics/frontend/shadow-dom":{"label":"Shadow DOM","children":{}},"/playground/topics/frontend/retain-scroll-position-in-infinite-scroll":{"label":"Retain scroll position in infinite scroll","children":{}},"/playground/topics/frontend/continuous-translation":{"label":"Continuous Translation","children":{}},"/playground/topics/frontend/what-is-pnpm-compare-to-npmyarn":{"label":"What is PNPM Compare To NPM/Yarn","children":{}},"/playground/topics/frontend/why-micro-frontend":{"label":"Why Micro Frontend","children":{}},"/playground/topics/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution":{"label":"Why We Chose Our Tech Stack Accelerating Development With A Robust Frontend Solution","children":{}},"/playground/topics/frontend/tackling-server-state-complexity-in-frontend-development":{"label":"Tackling Server State complexity in Frontend Development","children":{}},"/playground/topics/frontend/variable-fonts":{"label":"Variable Fonts","children":{}},"/playground/topics/frontend/when-should-we-use-usereducer-instead-of-usestate":{"label":"When should we use useReducer instead of useState?","children":{}},"/playground/topics/frontend/preserving-and-resetting-state-in-react":{"label":"Preserving and Resetting state in React","children":{}},"/playground/topics/frontend/mixpanel":{"label":"Mixpanel","children":{}},"/playground/topics/frontend/validation-with-zod":{"label":"Validation with Zod","children":{}},"/playground/topics/frontend/parse-dont-validate-in-typescript":{"label":"Parse, don't validate in TypeScript","children":{}},"/playground/topics/frontend/webassembly":{"label":"Webassembly","children":{}},"/playground/topics/frontend/singleton-design-pattern-in-javascript":{"label":"Singleton Design Pattern in Javascript","children":{}},"/playground/topics/frontend/an-introduction-to-atomic-css":{"label":"An Introduction to Atomic CSS","children":{}},"/playground/topics/frontend/intro-to-indexeddb":{"label":"Intro to IndexedDB","children":{}},"/playground/topics/frontend/the-fundamental-of-web-performance":{"label":"The fundamental of web performance","children":{}},"/playground/topics/frontend/wai-aria":{"label":"WAI-ARIA","children":{}},"/playground/topics/frontend/build-polymorphic-react-components-with-typescript":{"label":"Build polymorphic React components with Typescript","children":{}},"/playground/topics/frontend/threejs":{"label":"Threejs","children":{"/playground/topics/frontend/threejs/cameras-in-threejs":{"label":"Cameras in ThreeJS","children":{}}}},"/playground/topics/frontend/prevent-layout-thrashing":{"label":"Prevent Layout Thrashing","children":{}},"/playground/topics/frontend/pure-css-parallax":{"label":"Pure CSS Parallax","children":{}},"/playground/topics/frontend/css-container-queries":{"label":"CSS Container Queries","children":{}},"/playground/topics/frontend/hsl-color":{"label":"HSL Color","children":{}},"/playground/topics/frontend/mitigate-blocking-the-main-thread":{"label":"Mitigate blocking the main thread","children":{}},"/playground/topics/frontend/css-in-js":{"label":"CSS in JS","children":{}},"/playground/topics/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second":{"label":"Dark mode flickers a white background for a fraction of a second","children":{}},"/playground/topics/frontend/why-dom-manipulation-is-slow":{"label":"Why DOM manipulation is slow?","children":{}},"/playground/topics/frontend/why-virtual-dom-is-fast":{"label":"Why Virtual DOM is fast?","children":{}},"/playground/topics/frontend/vitejs-native-modules":{"label":"ViteJS native modules","children":{}},"/playground/topics/frontend/javascript-modules":{"label":"JavaScript modules","children":{}},"/playground/topics/frontend/atomic-design-pattern":{"label":"Atomic Design Pattern","children":{}},"/playground/topics/frontend/focus-trap":{"label":"Focus trap","children":{}},"/playground/topics/frontend/html-inert":{"label":"HTML inert","children":{}},"/playground/topics/frontend/useeffect-double-calls-in-react-18":{"label":"useEffect double calls in React 18","children":{}},"/playground/topics/frontend/react-18":{"label":"React 18","children":{}},"/playground/topics/frontend/remix-versus-nextjs":{"label":"Remix Versus Nextjs","children":{}},"/playground/topics/frontend/zaplib-post-mortem":{"label":"Zaplib post-mortem","children":{}},"/playground/topics/frontend/parallelism-in-javascript":{"label":"Parallelism in JavaScript","children":{}},"/playground/topics/frontend/mpa-spa-and-partial-hydration":{"label":"MPA, SPA and Partial Hydration","children":{}},"/playground/topics/frontend/micro-frontends-microservices-for-frontend-development":{"label":"Micro Frontends Microservices For Frontend Development","children":{}},"/playground/topics/frontend/using-correct-html-element-to-increase-website-accessibility":{"label":"Using Correct Html Element To Increase Website Accessibility","children":{}},"/playground/topics/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss":{"label":"Remove Unused CSS Styles From Bootstrap Using Purgecss","children":{}}}},"/playground/topics/ai":{"label":"AI","children":{"/playground/topics/ai/securing-your-remote-mcp-servers":{"label":"Securing your remote MCP servers","children":{}},"/playground/topics/ai/tool-level-security-for-remote-mcp-servers":{"label":"Tool-Level Security for Remote MCP Servers","children":{}},"/playground/topics/ai/model-context-protocol":{"label":"Intro to Model Context Protocol","children":{}},"/playground/topics/ai/building-llm-system":{"label":"Building LLM System","children":{"/playground/topics/ai/building-llm-system/quantization-in-llm":{"label":"Quantization for large language models","children":{}},"/playground/topics/ai/building-llm-system/graphrag":{"label":"GraphRAG - Building a knowledge graph for RAG system","children":{}},"/playground/topics/ai/building-llm-system/guardrails-in-llm":{"label":"Guardrails in LLM","children":{}},"/playground/topics/ai/building-llm-system/react-in-llm":{"label":"ReAct(Reason + Act) in LLM","children":{}},"/playground/topics/ai/building-llm-system/rewoo-in-llm":{"label":"ReWOO: Reasoning without observation - A deeper look","children":{}},"/playground/topics/ai/building-llm-system/model-selection":{"label":"Model selection","children":{}},"/playground/topics/ai/building-llm-system/logs-pillar":{"label":"Logging","children":{}},"/playground/topics/ai/building-llm-system/metric-pillar":{"label":"Metrics","children":{}},"/playground/topics/ai/building-llm-system/observability-in-ai-platforms":{"label":"Observability in AI platforms","children":{}},"/playground/topics/ai/building-llm-system/trace-pillar":{"label":"Tracing","children":{}},"/playground/topics/ai/building-llm-system/intent-classification-by-llm":{"label":"Intent classification by LLM","children":{}},"/playground/topics/ai/building-llm-system/llm-as-a-judge":{"label":"LLM as a judge","children":{}},"/playground/topics/ai/building-llm-system/use-cases-for-llm-applications":{"label":"Use cases for LLM applications","children":{}},"/playground/topics/ai/building-llm-system/the-rise-of-ai-applications-with-llm":{"label":"The rise of AI applications with LLM","children":{}},"/playground/topics/ai/building-llm-system/evaluation-guideline-for-llm-application":{"label":"Evaluation guidelines for LLM applications","children":{}},"/playground/topics/ai/building-llm-system/prevent-prompt-injection":{"label":"Prevent prompt injection","children":{}},"/playground/topics/ai/building-llm-system/building-llm-system":{"label":"§ Building LLM system","children":{}},"/playground/topics/ai/building-llm-system/multi-agent-collaboration-for-task-completion":{"label":"Multi-agent collaboration for task completion","children":{}},"/playground/topics/ai/building-llm-system/multimodal-in-rag":{"label":"Multimodal in RAG","children":{}}}},"/playground/topics/ai/digest":{"label":"Digest","children":{"/playground/topics/ai/digest/ai-digest-02":{"label":"AI digest #2 New command Aider, OpenHands, Qwen2.5 Coder 32B, Predicted Output","children":{}},"/playground/topics/ai/digest/ai-digest-01":{"label":"AI digest #1 Aider reasoning, OpenAI Realtime API, Cline - pre Claude-dev ","children":{}}}},"/playground/topics/ai/copilots":{"label":"Copilots","children":{"/playground/topics/ai/copilots/projects-operations":{"label":"Project Operations Copilots","children":{}},"/playground/topics/ai/copilots/team-copilots":{"label":"Team Copilots","children":{}}}},"/playground/topics/ai/text-to-mongodb":{"label":"Natural Language to Database Queries: Text-to-MongoDB","children":{}},"/playground/topics/ai/use-cases":{"label":"Use Cases","children":{"/playground/topics/ai/use-cases/salesforce":{"label":"Salesforce use cases","children":{}},"/playground/topics/ai/use-cases/yelp":{"label":"Yelp use cases","children":{}}}},"/playground/topics/ai/evaluate-chatbot-agent-by-simulated-user":{"label":"Evaluate Chatbot Agent by User Simulation","children":{}},"/playground/topics/ai/journey-of-thought-prompting":{"label":"Journey of Thought Prompting: Harnessing AI to Craft Better Prompts","children":{}},"/playground/topics/ai/llm-tracing-in-ai-system":{"label":"LLM tracing in AI system","children":{}},"/playground/topics/ai/caching-with-rag-system":{"label":"Evaluating caching in RAG systems","children":{}},"/playground/topics/ai/generative-ui":{"label":"What is Generative UI?","children":{}},"/playground/topics/ai/re-ranking-in-rag":{"label":"Re-ranking in RAG","children":{}},"/playground/topics/ai/function-calling":{"label":"Function calling in AI agents","children":{}},"/playground/topics/ai/building-llm-powered-tools-with-dify":{"label":"Streamlining Internal Tool Development with Managed LLMOps: A Dify Case Study","children":{}},"/playground/topics/ai/thumbs-up-and-thumbs-down-pattern":{"label":"Thumbs up and Thumbs down pattern","children":{}},"/playground/topics/ai/supervisor-ai-agents":{"label":"Building Agent Supervisors to Generate Insights","children":{}},"/playground/topics/ai/raptor-llm-retrieval":{"label":"RAPTOR: Tree-based Retrieval for Language Models","children":{}},"/playground/topics/ai/proximal-policy-optimization":{"label":"Proximal Policy Optimization","children":{}},"/playground/topics/ai/a-grand-unified-theory-of-the-ai-hype-cycle":{"label":"A Grand Unified Theory of the AI Hype Cycle","children":{}},"/playground/topics/ai/developing-rapidly-with-generative-ai":{"label":"Developing rapidly with Generative AI","children":{}},"/playground/topics/ai/rlhf-with-open-assistant":{"label":"RLHF with Open Assistant","children":{}},"/playground/topics/ai/story-map-for-llms":{"label":"Story map for LLMs","children":{}},"/playground/topics/ai/adversarial-prompting":{"label":"Adversarial Prompting in Prompt Engineering","children":{}},"/playground/topics/ai/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"Chunking strategies to overcome context limitation in LLM","children":{}},"/playground/topics/ai/llms-accuracy-self-refinement":{"label":"LLM's Accuracy - Self Refinement","children":{}},"/playground/topics/ai/llm-query-caching":{"label":"Query Caching for Large Language Models","children":{}},"/playground/topics/ai/reinforcement-learning":{"label":"Introduction to Reinforcement Learning and Its Application with LLMs","children":{}},"/playground/topics/ai/foundation-model":{"label":"Foundation Models: The Latest Advancement in AI","children":{}},"/playground/topics/ai/select-vector-database-for-llm":{"label":"Select Vector Database for LLM","children":{}},"/playground/topics/ai/build-your-chatbot-with-open-source-large-language-models":{"label":"Build your chatbot with open source Large Language Models","children":{}},"/playground/topics/ai/workaround-with-openais-token-limit-with-langchain":{"label":"Workaround with OpenAI's token limit with Langchain","children":{}},"/playground/topics/ai/working-with-langchain-document-loaders":{"label":"Working with langchain document loaders","children":{}}}},"/playground/topics/golang":{"label":"Golang","children":{"/playground/topics/golang/weekly":{"label":"Weekly","children":{"/playground/topics/golang/weekly/dec-13":{"label":"#24 Go 1.24 testing/synctest experiment for time and concurrency testing","children":{}},"/playground/topics/golang/weekly/dec-06":{"label":"#23 Draft Release Notes for Go 1.24 and weak pointers in Go","children":{}},"/playground/topics/golang/weekly/nov-29":{"label":"#22 GoMLX: ML in Go without Python","children":{}},"/playground/topics/golang/weekly/nov-22":{"label":"#21 Go sync.Once is Simple","children":{}},"/playground/topics/golang/weekly/nov-15":{"label":"#20 Go Turns 15","children":{}},"/playground/topics/golang/weekly/nov-08":{"label":"#19 Writing secure Go code","children":{}},"/playground/topics/golang/weekly/nov-01":{"label":"#18 Fuzz Testing Go HTTP Services","children":{}},"/playground/topics/golang/weekly/oct-25":{"label":"#17 Leveraging benchstat Projects in Go benchmark and Go Plan9 memo on 450% speeding up calculations","children":{}},"/playground/topics/golang/weekly/oct-18":{"label":"#16 Understand sync.Map","children":{}},"/playground/topics/golang/weekly/oct-11":{"label":"#15 Go embed and Reflect","children":{}},"/playground/topics/golang/weekly/oct-04":{"label":"#14 Compile-time eval \u0026 SQLite with wazero","children":{}},"/playground/topics/golang/weekly/sep-27":{"label":"#13 Compiler Quests and Vector Vexations","children":{}},"/playground/topics/golang/weekly/sep-20":{"label":"#12 CLI Tools for K8s, REST, and Terminals","children":{}},"/playground/topics/golang/weekly/sep-13":{"label":"#11 Actors, Frameworks, and the Future of Go","children":{}},"/playground/topics/golang/weekly/sep-06":{"label":"#10 Script, Telemetry","children":{}},"/playground/topics/golang/weekly/aug-30":{"label":"#9 TinyGo, SQLite vector search, and Permify","children":{}},"/playground/topics/golang/weekly/aug-23":{"label":"#8 GoNB, kubetrim, and GopherCon UK 2024","children":{}},"/playground/topics/golang/weekly/aug-16":{"label":"#7 Go 1.23, Websockets, and Structs","children":{}},"/playground/topics/golang/weekly/aug-09":{"label":"#6 Cogent Core, Russ Cox stepping down","children":{}},"/playground/topics/golang/weekly/aug-02":{"label":"#5 Go 1.23 features, Memory, Minecraft, and More","children":{}},"/playground/topics/golang/weekly/jul-26":{"label":"#4 Ethical Hacking, HTTP Requests, Mac App Development","children":{}},"/playground/topics/golang/weekly/jul-12":{"label":"#3 Generic Collections, Generics Constraints, AI Bot","children":{}},"/playground/topics/golang/weekly/jul-05":{"label":"#2 Go 1.23 Iterators","children":{}},"/playground/topics/golang/weekly/june-27":{"label":"#1 eBPF and PGO Optimization Techniques","children":{}}}},"/playground/topics/golang/extension-interface-pattern":{"label":"Go extension interface pattern","children":{}},"/playground/topics/golang/go-import":{"label":"Go import design: using git repo path","children":{}},"/playground/topics/golang/go-package":{"label":"Package first design","children":{}},"/playground/topics/golang/go-generics-type-safety":{"label":"How does Go achieve type safety when it enables generics?","children":{}},"/playground/topics/golang/go-for-enterprise":{"label":"Go For Enterprise","children":{"/playground/topics/golang/go-for-enterprise/who-using-golang-in-enterprise":{"label":"Who is using Go in enterprise?","children":{}},"/playground/topics/golang/go-for-enterprise/enterprise-standard-language":{"label":"Go as an Enterprise Standard Language","children":{}},"/playground/topics/golang/go-for-enterprise/how-to-use-go-in-enterprise":{"label":"How to use Go in the Enterprise","children":{}},"/playground/topics/golang/go-for-enterprise/when-to-use-golang-in-enterprise":{"label":"When to use Go in the Enterprise","children":{}},"/playground/topics/golang/go-for-enterprise/why-enterprise-chose-java":{"label":"Why Enterprise Chose Java","children":{}},"/playground/topics/golang/go-for-enterprise/why-go":{"label":"Why Go?","children":{}}}},"/playground/topics/golang/compute-union-2-finite-automata":{"label":"Efficient Union of Finite Automata in Golang: A Practical Approach","children":{}},"/playground/topics/golang/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{}},"/playground/topics/golang/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq":{"label":"Message Queues And Streaming Platforms Eg Kafka Nats Rabbitmq","children":{}},"/playground/topics/golang/unit-testing-best-practices-in-golang":{"label":"Unit Testing Best Practices In Golang","children":{}},"/playground/topics/golang/profiling-in-go":{"label":"Profiling in Go","children":{}},"/playground/topics/golang/go-in-software-engineering":{"label":"Go In Software Engineering","children":{}},"/playground/topics/golang/bunk-license-check":{"label":"Bunk license check","children":{}},"/playground/topics/golang/go-concurrency":{"label":"Go Concurrency","children":{}},"/playground/topics/golang/slice-and-array-in-golang":{"label":"Slice And Array In Golang","children":{}},"/playground/topics/golang/use-go-selenium-to-crawl-data":{"label":"Use Go Selenium To Crawl Data","children":{}},"/playground/topics/golang/connecting-vim-with-golang":{"label":"Connecting Vim With Golang","children":{}}}},"/playground/topics/devbox":{"label":"Devbox","children":{"/playground/topics/devbox/devbox":{"label":"§ Devbox","children":{}},"/playground/topics/devbox/story":{"label":"Story","children":{"/playground/topics/devbox/story/devbox-production-success-story":{"label":"Devbox in Production: Our Success Story","children":{}},"/playground/topics/devbox/story/devbox-local-development-env":{"label":"Using Devbox to setup local development environment","children":{}},"/playground/topics/devbox/story/devbox-nix-and-our-devbox-adoption":{"label":"The overview into Nix \u0026 how we use Devbox @ Dwarves","children":{}},"/playground/topics/devbox/story/devbox-docker-adoption-and-challenges":{"label":"Our Docker adoption and its challenges","children":{}},"/playground/topics/devbox/story/devbox-a-world-before-docker":{"label":"The world before Docker","children":{}}}},"/playground/topics/devbox/guide":{"label":"Guide","children":{"/playground/topics/devbox/guide/containerless":{"label":"Ditch the Containers: Go Containerless with Devbox","children":{}},"/playground/topics/devbox/guide/devboxjson":{"label":"Devbox.json: Your Project's DNA","children":{}},"/playground/topics/devbox/guide/run-your-own-shell":{"label":"Devbox Shell: Your Dev Environment, Your Rules","children":{}}}},"/playground/topics/devbox/introduction":{"label":"Introduction","children":{"/playground/topics/devbox/introduction/the-reason-for-being":{"label":"The reason for being","children":{}},"/playground/topics/devbox/introduction/why-devbox-but-not-nix":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}}}},"/playground/topics/devbox/research":{"label":"Research","children":{"/playground/topics/devbox/research/content-addressable-storage-in-docker":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}},"/playground/topics/devbox/research/fixed-output-derivation":{"label":"Fixed-output Derivation in Nix","children":{}},"/playground/topics/devbox/research/nix-is-faster-than-docker-build":{"label":"Nix is Faster Than Docker Build","children":{}},"/playground/topics/devbox/research/pinning-nixpkgs":{"label":"Pinning nixpkgs in Nix","children":{}},"/playground/topics/devbox/research/shadow-copies":{"label":"Shadow Copies in Docker Builds","children":{}},"/playground/topics/devbox/research/unstable-package-installation":{"label":"Unstable Package Installation in Docker","children":{}}}}}}}},"/playground/use-cases":{"label":"Use Cases","children":{"/playground/use-cases/ai-ruby-travel-assistant-chatbot":{"label":"AI-powered Ruby travel assistant","children":{}}}},"/playground/notes":{"label":"Notes","children":{"/playground/notes/why-hollywood-and-gaming-struggle-with-ai":{"label":"Why Hollywood and gaming struggle with AI","children":{}},"/playground/notes/using-foundry-for-evm-smart-contract-developement":{"label":"Using Foundry for EVM smart contract development","children":{}},"/playground/notes/visitor-design-pattern":{"label":"Visitor design pattern, the concept, problem solution and use cases","children":{}},"/playground/notes/vietnam-tech-ecosystem-report":{"label":"Vietnam Tech Ecosystem 2024 Report","children":{}},"/playground/notes/understanding-saving-investing-and-speculating-key-differences-and-strategies":{"label":"Understanding Saving, Investing, and Speculating: Key Differences and Strategies","children":{}},"/playground/notes/writing-content-for-multimedia-guidelines":{"label":"Writing Content for Multimedia Guidelines","children":{}},"/playground/notes/working-on-a-project-interview-assessment-at-dwarves":{"label":"Working On A Project Interview Assessment At Dwarves","children":{}},"/playground/notes/writing":{"label":"Writing","children":{"/playground/notes/writing/state-explain-link":{"label":"State, Explain, Link - An all-purpose writing technique","children":{}}}},"/playground/notes/understanding-an-application-design":{"label":"Understanding An Application Design","children":{}},"/playground/notes/what-i-learned-on-design-thinking-and-software-development":{"label":"What I Learned On Design Thinking And Software Development","children":{}},"/playground/notes/xpc-services-on-macos-app-using-swift":{"label":"Xpc Services On Macos App Using Swift","children":{}},"/playground/notes/well-crafted-software":{"label":"Well Crafted Software","children":{}},"/playground/notes/what-is-kubernetes":{"label":"What Is Kubernetes","children":{}},"/playground/notes/ux-model":{"label":"UX Model","children":{}},"/playground/notes/evolutionary-database-design":{"label":"Evolutionary Database Design: Managing Change and Scaling with the System","children":{}},"/playground/notes/giving-a-talk-checklist":{"label":"Giving a talk","children":{}},"/playground/notes/database-design-circular":{"label":"Database design Circular","children":{}},"/playground/notes/a-lens-to-modern-data-engineering":{"label":"A Lens to Modern Data Engineering","children":{}},"/playground/notes/automata":{"label":"Automata","children":{}},"/playground/notes/error-handling-patterns":{"label":"Error handling patterns","children":{}},"/playground/notes/founder-liquidity":{"label":"Founder Liquidity","children":{}},"/playground/notes/security":{"label":"Security","children":{"/playground/notes/security/a-holistic-guide-to-security":{"label":"A Holistic Guide to Security","children":{}},"/playground/notes/security/how-i-came-up-with-our-security-standard":{"label":"How I came up with our Security Standard","children":{}}}},"/playground/notes/record-reward-sharing-culture":{"label":"Record and reward sharing at Dwarves","children":{}},"/playground/notes/designing-for-forgiveness":{"label":"Designing for Forgiveness: Creating Error-Tolerant Interfaces","children":{}},"/playground/notes/design-file-sharing-system-part-2-permission-and-password":{"label":"Design file-sharing system - Part 2: Permission \u0026 Password","children":{}},"/playground/notes/designing-a-model-with-dynamic-properties":{"label":"Designing a model with dynamic properties","children":{}},"/playground/notes/hybrid-search":{"label":"Evaluating search engine in RAG systems","children":{}},"/playground/notes/design-file-sharing-system-part-1-directory-structure":{"label":"Design file-sharing system - Part 1: Directory Structure","children":{}},"/playground/notes/subscription-pricing-models":{"label":"Subscription Pricing Models","children":{}},"/playground/notes/creating-a-fully-local-search-engine-on-memo":{"label":"Building a Local Search Engine for Our Memo Website","children":{}},"/playground/notes/erlang-fsm":{"label":"Erlang Finite State Machine","children":{}},"/playground/notes/observer-pattern":{"label":"Introduce the Observer pattern and its use cases","children":{}},"/playground/notes/strategy-design-pattern":{"label":"Strategy design pattern, the concept, use cases and difference with the state design pattern","children":{}},"/playground/notes/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing":{"label":"How we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","children":{}},"/playground/notes/feedback-mechanism":{"label":"Design feedback mechanism for LLM applications","children":{}},"/playground/notes/local-first-software":{"label":"Local-first Software","children":{}},"/playground/notes/error-handling-in-rust":{"label":"Error handling on Rust","children":{}},"/playground/notes/rust-trait":{"label":"Rust Trait","children":{}},"/playground/notes/engineering":{"label":"Engineering","children":{"/playground/notes/engineering/backend":{"label":"Backend","children":{"/playground/notes/engineering/backend/bloom-filter":{"label":"Bloom Filter","children":{}},"/playground/notes/engineering/backend/introduction-to-crdt":{"label":"Introduction to CRDT","children":{}},"/playground/notes/engineering/backend/sql-sargable-queries-and-their-impact-on-database-performance":{"label":"SQL Saragable Queries and Their Impact on Database Performance","children":{}},"/playground/notes/engineering/backend/the-removal-of-apache-kafkas-dependency-on-zookeeper":{"label":"The removal of Apache Kafka's dependency on Zookeeper","children":{}},"/playground/notes/engineering/backend/sql-and-how-it-relates-to-disk-reads-and-writes":{"label":"SQL and how it relates to Disk Reads and Writes","children":{}}}},"/playground/notes/engineering/data":{"label":"Data","children":{"/playground/notes/engineering/data/data-pipeline-design-framework":{"label":"Data Pipeline Design Framework","children":{}},"/playground/notes/engineering/data/quick-learning-vector-database":{"label":"Quick Learning Vector Database","children":{}},"/playground/notes/engineering/data/mapreduce":{"label":"MapReduce","children":{}}}},"/playground/notes/engineering/google-data-fusion":{"label":"Google Data Fusion","children":{}},"/playground/notes/engineering/google-dataproc":{"label":"Google Dataproc","children":{}},"/playground/notes/engineering/introducing-htmx-navigating-the-advantages-and-concerns":{"label":"Introducing HTMX - Navigating the Advantages and Concerns","children":{}},"/playground/notes/engineering/typesafe-client-server":{"label":"Typesafe Client Server","children":{}},"/playground/notes/engineering/golang-for-high-performance-video-streaming":{"label":"Leveraging Golang and WebRTC for High-Performance Video Streaming","children":{}},"/playground/notes/engineering/url-redirect-vs-rewrite":{"label":"URL Redirect vs. Rewrite; What’s the difference?","children":{}}}},"/playground/notes/template-method-design-pattern":{"label":"A Tour of Template method pattern with Golang","children":{}},"/playground/notes/command-pattern":{"label":"Command Pattern","children":{}},"/playground/notes/radix-sort":{"label":"Radix Sort","children":{}},"/playground/notes/state-pattern":{"label":"State Pattern","children":{}},"/playground/notes/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy":{"label":"Explaining Gradient Descent in Machine Learning with a simple analogy","children":{}},"/playground/notes/organize-team-know-how-with-zettelkasten-method":{"label":"Organize team know-how with Zettelkasten Method","children":{}},"/playground/notes/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana":{"label":"Dynamic Liquidity Market Maker - a new form of concentrated liquidity AMM on Solana","children":{}},"/playground/notes/how-to-talk-to-chatgpt-effectively":{"label":"How to talk to ChatGPT effectively","children":{}},"/playground/notes/memo-knowledge-base-meeting":{"label":"Memo Knowledge Base Meeting","children":{}},"/playground/notes/peep-nft":{"label":"Claim your Peeps NFT","children":{}},"/playground/notes/recording-flow":{"label":"How We Set Up a Recording Workflow for Dwarves Office Hours","children":{}},"/playground/notes/memo-publication-workflow":{"label":"Memo Publication Workflow","children":{}},"/playground/notes/history-of-structured-output-for-llms":{"label":"History of Structured Outputs for LLMs","children":{}},"/playground/notes/builder-design-pattern":{"label":"Introduce the Builder pattern and its use cases","children":{}},"/playground/notes/how-to-make-a-moc":{"label":"How to make a MOC","children":{}},"/playground/notes/prototype-design-pattern":{"label":"Going Through use cases of the prototype design pattern and it place among the creational patterns","children":{}},"/playground/notes/singleton-design-pattern":{"label":"A tour of Singleton design pattern with Golang","children":{}},"/playground/notes/echelon-x-singapore-2024-where-innovations-meet-inspiration":{"label":"Echelon X Singapore 2024: Where Innovations Meet Inspiration","children":{}},"/playground/notes/c4-modelling":{"label":"Breaking Down Complexity: The Role of Abstractions and UML in C4 Modelling","children":{}},"/playground/notes/dollar-cost-averaging":{"label":"Dollar Cost Averaging (DCA)","children":{}},"/playground/notes/how-i-create-content-for-multiple-platforms-at-dwarves":{"label":"How I Create Content for Multiple Platforms at Dwarves","children":{}},"/playground/notes/how-to-earn-reward-from-staking-dfg":{"label":"How to earn reward from staking DFG","children":{}},"/playground/notes/how-to-transfer-dfg-from-eth-to-base-for-staking":{"label":"How to bridge $DFG from Ethereum Mainnet to Base Network for staking","children":{}},"/playground/notes/design-less-present-more-with-deckset":{"label":"Design less, present more with Deckset","children":{}},"/playground/notes/level-up-your-markdown-memos":{"label":"Level Up Your Markdown Memos: Avoiding Common Pitfalls","children":{}},"/playground/notes/tech-canvas":{"label":"Tech Canvas","children":{}},"/playground/notes/how-to-recap-a-publication":{"label":"Recapping A publication","children":{}},"/playground/notes/lifecycle-of-a-publication":{"label":"Life cycle of a publication","children":{}},"/playground/notes/how-to-set-up-environment-for-editing-memo":{"label":"How to set up environment to edit memo","children":{}},"/playground/notes/_how-to-setup-crypto-wallet-to-withdraw-icy":{"label":"How to set up crypto wallet to withdraw ICY","children":{}},"/playground/notes/_how-to-withdraw-icy":{"label":"How to withdraw ICY","children":{}},"/playground/notes/how-to-take-better-screenshots-on-mac":{"label":"How To Take Better Screenshots On Mac","children":{}},"/playground/notes/how-to-push-content-on-note-d":{"label":"How to push content on memo.d.foundation","children":{}},"/playground/notes/labs-weekly-catchup-5":{"label":"Labs Weekly Catchup #5","children":{}},"/playground/notes/labs-weekly-catchup-4":{"label":"Labs Weekly Catchup #4","children":{}},"/playground/notes/labs-weekly-catchup-3":{"label":"Labs Weekly Catchup #3","children":{}},"/playground/notes/labs-weekly-catchup-2":{"label":"Labs Weekly Catchup #2","children":{}},"/playground/notes/labs-weekly-catchup-1":{"label":"Labs Weekly Catchup #1","children":{}},"/playground/notes/duckdb-demo-and-showcase":{"label":"DuckDB demo and showcase","children":{}},"/playground/notes/icy-in-2024":{"label":"$icy in 2024","children":{}},"/playground/notes/salary-advance":{"label":"$icy Salary Advance","children":{}},"/playground/notes/icy-dfg":{"label":"💠 df protocol, $icy and $dfg","children":{}},"/playground/notes/how-rd-contributes-to-performance-review":{"label":"How R\u0026D contributes to Performance Review","children":{}},"/playground/notes/knowledge-journey":{"label":"Knowledge Journey","children":{}},"/playground/notes/labs-roadmap-nov-23-update":{"label":"Labs Roadmap (Nov 23 update)","children":{}},"/playground/notes/reward-model-nomination":{"label":"Reward Model \u0026 Nomination","children":{}},"/playground/notes/our-view-on-fullstack-engineering":{"label":"Our View On Fullstack Engineering","children":{}},"/playground/notes/adoption-of-pnpm":{"label":"Adoption Of Pnpm","children":{}},"/playground/notes/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt":{"label":"How We Created An AI Powered Interview System Using Openais Chatgpt","children":{}},"/playground/notes/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{}},"/playground/notes/exploring-machine-learning-approaches-for-fine-tuning-llama-models":{"label":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","children":{}},"/playground/notes/managing-dataflow-and-sql-database-with-concurrency-control":{"label":"Managing Dataflow And Sql Database With Concurrency Control","children":{}},"/playground/notes/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue":{"label":"Choosing The Right Javascript Framework A Deep Dive Into React Vs Angular Vs Vue","children":{}},"/playground/notes/design-system-for-layer-2-using-zk-rollup":{"label":"Design System For Layer 2 Using Zk Rollup","children":{}},"/playground/notes/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation":{"label":"Lessons Learned From Being A Part Of Corporate Micro Frontend Implementation","children":{}},"/playground/notes/cost-of-react-native":{"label":"Cost Of React Native","children":{}},"/playground/notes/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{}},"/playground/notes/database-designs-for-multilingual-apps":{"label":"Database Designs For Multilingual Apps","children":{}},"/playground/notes/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{}},"/playground/notes/how-blue-green-deployment-helped-mochi":{"label":"How Blue Green Deployment Helped Mochi","children":{}},"/playground/notes/i18n-frontend-guideline":{"label":"I18n Frontend Guideline","children":{}},"/playground/notes/radio-talk-61-monorepo":{"label":"Radio Talk 61 Monorepo","children":{}},"/playground/notes/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan Turbo Monorepo","children":{}},"/playground/notes/radio-talk-60-blue-green-deployment":{"label":"Radio Talk 60 Blue Green Deployment","children":{}},"/playground/notes/202302281019-case-study-write-heavy-scalable-and-reliable-inventory-platform":{"label":"Case study: Write-heavy scalable and reliable inventory platform","children":{}},"/playground/notes/growth-is-our-universal-language":{"label":"Growth Is Our Universal Language","children":{}},"/playground/notes/202301191192-multi-column-index-in-db":{"label":"Multi-column index in DB","children":{}},"/playground/notes/202301091379-invoking-component-functions-in-react":{"label":"Invoking component functions in React","children":{}},"/playground/notes/the-key-of-security-mechanisms-in-tackling-cyber-threats":{"label":"The Key Of Security Mechanisms In Tackling Cyber Threats","children":{}},"/playground/notes/202212131609-how-to-deal-with-technical-debt-in-scrum":{"label":"How to deal with technical debt in Scrum","children":{}},"/playground/notes/responsibility":{"label":"Responsibility","children":{}},"/playground/notes/configure-the-company-email":{"label":"Configure The Company Email","children":{}},"/playground/notes/tech-event-in-the-latest-transforming-healthcare-with-technology":{"label":"Tech Event In The Latest Transforming Healthcare With Technology","children":{}},"/playground/notes/202211141287-go-json-parsing":{"label":"Go JSON parser: number \u003c-\u003e interface","children":{}},"/playground/notes/202211141513-materialized-view-pattern":{"label":"Materialized View Pattern","children":{}},"/playground/notes/202211081111-error-messaging":{"label":"Error Messaging","children":{}},"/playground/notes/202210172128-sign-in-form-best-practices":{"label":"Sign-in Form Best Practices","children":{}},"/playground/notes/202210162154-the-best-of-css-tldr":{"label":"The Best of CSS TLDR","children":{}},"/playground/notes/202210150019-migration-planning":{"label":"Migration Planning","children":{}},"/playground/notes/202210131000-behavior-driven-development":{"label":"Behavior Driven Development","children":{}},"/playground/notes/202210131516-react-fiber":{"label":"React Fiber","children":{}},"/playground/notes/202210122014-forward-proxy":{"label":"Forward Proxy","children":{}},"/playground/notes/data-analyst-in-retail-trading":{"label":"Data Analyst In Retail Trading","children":{}},"/playground/notes/passing-the-probation-get-3-upvotes":{"label":"Passing The Probation Get 3 Upvotes","children":{}},"/playground/notes/react-native-new-architecture":{"label":"React Native New Architecture","children":{}},"/playground/notes/dwarves-radio-talk-17-conduct-a-1-1-session":{"label":"Dwarves Radio Talk 17 Conduct A 1 1 Session","children":{}},"/playground/notes/dwarves-radio-talk-16-run-an-effective-performance-review":{"label":"Dwarves Radio Talk 16 Run An Effective Performance Review","children":{}},"/playground/notes/sql-practices-orm-vs-plain-sql":{"label":"Sql Practices Orm Vs Plain Sql","children":{}},"/playground/notes/six-things-i-extracted-from-design-thinking":{"label":"Six Things I Extracted From Design Thinking","children":{}},"/playground/notes/gitflow-pull-request":{"label":"Gitflow Pull Request","children":{}},"/playground/notes/git-commit-message-convention":{"label":"Git Commit Message Convention","children":{}},"/playground/notes/are-we-really-engineers":{"label":"Are We Really Engineers","children":{}},"/playground/notes/how-we-setup-cicd":{"label":"How We Setup Cicd","children":{}},"/playground/notes/getting-started-with-webflow":{"label":"Getting Started With Webflow","children":{}},"/playground/notes/ui-design-best-practices-dwarves":{"label":"UI Design Best Practices Dwarves","children":{}},"/playground/notes/the-correct-way-to-build-kpi":{"label":"The Correct Way To Build Kpi","children":{}},"/playground/notes/domain-insight-research-framework":{"label":"Domain Insight Research Framework","children":{}},"/playground/notes/asking-as-a-junior":{"label":"Asking As A Junior","children":{}},"/playground/notes/infinite-image-gallery-with-r3f-an-approach":{"label":"Infinite Image Gallery With R3f An Approach","children":{}},"/playground/notes/market":{"label":"Market","children":{"/playground/notes/market/an-overview-of-micro-investment-in-real-estate":{"label":"An Overview Of Micro Investment In Real Estate","children":{}}}},"/playground/notes/grid-and-layout":{"label":"Grid And Layout","children":{}},"/playground/notes/startups-vs-junior-designers":{"label":"Startups Vs Junior Designers","children":{}},"/playground/notes/gestalt-principles-in-ui-design":{"label":"Gestalt Principles In UI Design","children":{}},"/playground/notes/aarrr-framework-in-a-nutshell":{"label":"AARRR Framework In A Nutshell","children":{}},"/playground/notes/a-quick-intro-to-webassembly":{"label":"A Quick Intro To Webassembly","children":{}},"/playground/notes/sdk-event-sourcing":{"label":"Sdk Event Sourcing","children":{}},"/playground/notes/software-development-life-cycle-101":{"label":"Software Development Life Cycle 101","children":{}},"/playground/notes/introduce-to-dwarves-memo":{"label":"Introduce To Dwarves Memo","children":{}},"/playground/notes/daemons-and-services-programming-guide":{"label":"Daemons And Services Programming Guide","children":{}},"/playground/notes/remote-moderated-usability-testing":{"label":"Remote Moderated Usability Testing","children":{}},"/playground/notes/an-alternative-to-tm":{"label":"An Alternative To Tm","children":{}},"/playground/notes/how-a-design-system-work":{"label":"How A Design System Work","children":{}},"/playground/notes/software-modeling":{"label":"Software Modeling","children":{}},"/playground/notes/reusability-in-software-development":{"label":"Reusability In Software Development","children":{}},"/playground/notes/blockchain-for-designers":{"label":"Blockchain For Designers","children":{}},"/playground/notes/design-better-mobile-application":{"label":"Design Better Mobile Application","children":{}},"/playground/notes/introduction-to-software-craftsmanship":{"label":"Introduction To Software Craftsmanship","children":{}},"/playground/notes/domain-glossary":{"label":"Domain Glossary","children":{}},"/playground/notes/architecture-decision-record":{"label":"Architecture Decision Record","children":{}},"/playground/notes/build-an-assistant-on-the-terminal":{"label":"Build An Assistant On The Terminal","children":{}},"/playground/notes/create-circular-text-using-swiftui":{"label":"Create Circular Text Using Swiftui","children":{}},"/playground/notes/draw-watch-face-using-swiftui":{"label":"Draw Watch Face Using Swiftui","children":{}},"/playground/notes/applied-security-basis":{"label":"Applied Security Basis","children":{}},"/playground/notes/swiftui":{"label":"Swiftui","children":{}},"/playground/notes/bunk-license-check":{"label":"Bunk License Check","children":{}},"/playground/notes/objective":{"label":"Objective","children":{}},"/playground/notes/project-management":{"label":"Project Management","children":{}},"/playground/notes/kubernetes-helm-101":{"label":"Kubernetes Helm 101","children":{}},"/playground/notes/traits-to-assess-during-an-interview":{"label":"Traits To Assess During An Interview","children":{}},"/playground/notes/recursively-export-file-pattern-in-javascript-es6-application":{"label":"Recursively Export File Pattern In Javascript Es6 Application","children":{}},"/playground/notes/playaround-with-clojure":{"label":"Playaround With Clojure","children":{}},"/playground/notes/playaround-with-rust":{"label":"Playaround With Rust","children":{}},"/playground/notes/overview-on-broker-pattern-in-distributed-system":{"label":"Overview On Broker Pattern In Distributed System","children":{}},"/playground/notes/fundamental-end-to-end-frontend-testing-with-cypress":{"label":"Fundamental End To End Frontend Testing With Cypress","children":{}},"/playground/notes/uidynamicanimator":{"label":"Uidynamicanimator","children":{}},"/playground/notes/reproduce-apple-find-me-bottom-menu-view":{"label":"Reproduce Apple Find Me Bottom Menu View","children":{}},"/playground/notes/build-a-passcode-view-with-swift":{"label":"Build A Passcode View With Swift","children":{}},"/playground/notes/istio":{"label":"Istio","children":{}},"/playground/notes/different-ways-to-test-react-application":{"label":"Different Ways To Test React Application","children":{}},"/playground/notes/federated-byzantine":{"label":"Federated Byzantine","children":{}},"/playground/notes/fabric-hyperledger-architecture-explanation":{"label":"Fabric Hyperledger Architecture Explanation","children":{}},"/playground/notes/setup-react-project-with-webpack-and-babel":{"label":"Setup React Project With Webpack And Babel","children":{}},"/playground/notes/split-and-reuse-code-in-react-application":{"label":"Split And Reuse Code In React Application","children":{}},"/playground/notes/hoc-renderprops-and-hook-in-reactjs":{"label":"Hoc Renderprops And Hook In Reactjs","children":{}},"/playground/notes/resource-assignment":{"label":"Resource Assignment","children":{}},"/playground/notes/the-principle-of-spacing-in-ui-design-part-2":{"label":"The Principle Of Spacing In UI Design Part 2","children":{}},"/playground/notes/finite-state-machine":{"label":"Finite State Machine","children":{}},"/playground/notes/card-sorting-and-a-glimpse-at-experimental-sorting-session":{"label":"Card Sorting And A Glimpse At Experimental Sorting Session","children":{}},"/playground/notes/about-devops":{"label":"About Devops","children":{}},"/playground/notes/our-daily-standup-format":{"label":"Our Daily Standup Format","children":{}},"/playground/notes/good-design-understanding":{"label":"Good Design Understanding","children":{}},"/playground/notes/competency-mapping":{"label":"Competency Mapping","children":{}},"/playground/notes/design-resourcestools":{"label":"Design Resourcestools","children":{}},"/playground/notes/design-tips-tricks":{"label":"Design Tips Tricks","children":{}},"/playground/notes/design-system":{"label":"Design System","children":{}},"/playground/notes/design-workflow":{"label":"Design Workflow","children":{}},"/playground/notes/three-levels-of-design":{"label":"Three Levels Of Design","children":{}},"/playground/notes/ui-design-fundamental":{"label":"UI Design Fundamental","children":{}},"/playground/notes/the-principle-of-spacing-in-ui-design-part-1":{"label":"The Principle Of Spacing In UI Design Part 1","children":{}},"/playground/notes/be-careful-with-your-code-splitting-setup":{"label":"Be Careful With Your Code Splitting Setup","children":{}},"/playground/notes/qc-onboarding":{"label":"Qc Onboarding","children":{}},"/playground/notes/dcos-series-part-5-gitlab":{"label":"Dcos Series Part 5 Gitlab","children":{}},"/playground/notes/dcos-series-part-4-deploy-simple-application-with-backend-database":{"label":"Dcos Series Part 4 Deploy Simple Application With Backend Database","children":{}},"/playground/notes/dcos-series-part-3-service-discovery-and-load-balancing":{"label":"Dcos Series Part 3 Service Discovery And Load Balancing","children":{}},"/playground/notes/dcos-series-part-2-deploy-simple-applications":{"label":"Dcos Series Part 2 Deploy Simple Applications","children":{}},"/playground/notes/dcos-series-part-1-quick-look-installation":{"label":"Dcos Series Part 1 Quick Look Installation","children":{}},"/playground/notes/skill-of-software-engineer":{"label":"Skill Of Software Engineer","children":{}},"/playground/notes/docker-registry":{"label":"Docker Registry","children":{}},"/playground/notes/agile-using-clickup-as-agile-management-tool":{"label":"Agile Using Clickup As Agile Management Tool","children":{}},"/playground/notes/agile-how-to-create-clickup-tickets":{"label":"Agile How To Create Clickup Tickets","children":{}},"/playground/notes/considering-factors-for-performance-evaluating":{"label":"Considering Factors For Performance Evaluating","children":{}},"/playground/notes/how-we-contribute-to-homebrew":{"label":"How We Contribute To Homebrew","children":{}},"/playground/notes/the-10x-engineer":{"label":"The 10x Engineer","children":{}},"/playground/notes/definition-of-done":{"label":"Definition Of Done","children":{}},"/playground/notes/estimation-in-agile":{"label":"Estimation In Agile","children":{}},"/playground/notes/sprint-lifecycle":{"label":"Sprint Lifecycle","children":{}},"/playground/notes/docker-microcontainers":{"label":"Docker Microcontainers","children":{}},"/playground/notes/remote-prepare-and-get-going":{"label":"Remote Prepare And Get Going","children":{}}}},"/playground/onboarding":{"label":"Labs - New Member Onboarding","children":{}},"/playground/schedule":{"label":"Labs x Consulting Workflow","children":{}},"/playground/intro":{"label":"Labs - Who we are","children":{}}}},"/consulting":{"label":"Consulting","children":{"/consulting/case-study":{"label":"Case Study","children":{"/consulting/case-study/screenz-ai":{"label":"Screenz.ai","children":{}},"/consulting/case-study/kafi":{"label":"Kafi","children":{}},"/consulting/case-study/droppii":{"label":"Droppii","children":{}},"/consulting/case-study/konvoy":{"label":"Konvoy","children":{}},"/consulting/case-study/cimb":{"label":"CIMB","children":{}},"/consulting/case-study/swift":{"label":"Swift","children":{}},"/consulting/case-study/startupvn":{"label":"StartupVN","children":{}},"/consulting/case-study/open-fabric":{"label":"Open Fabric","children":{}},"/consulting/case-study/icrosschain":{"label":"iCrosschain","children":{}},"/consulting/case-study/hedge-foundation":{"label":"Hedge Foundation","children":{}},"/consulting/case-study/searchio":{"label":"Search.io","children":{}},"/consulting/case-study/tokenomy":{"label":"Tokenomy","children":{}},"/consulting/case-study/basehq":{"label":"BaseHQ","children":{}},"/consulting/case-study/momos":{"label":"Momos","children":{}},"/consulting/case-study/attrace":{"label":"Attrace","children":{}},"/consulting/case-study/setel":{"label":"Setel","children":{}},"/consulting/case-study/joinpara":{"label":"JoinPara","children":{}},"/consulting/case-study/relay":{"label":"Relay","children":{}},"/consulting/case-study/naru":{"label":"Naru","children":{}},"/consulting/case-study/mudah":{"label":"Mudah","children":{}},"/consulting/case-study/reapit":{"label":"Reapit","children":{}},"/consulting/case-study/aharooms":{"label":"Aharooms","children":{}},"/consulting/case-study/begroup":{"label":"beGroup","children":{}},"/consulting/case-study/airwatt":{"label":"AirWatt","children":{}},"/consulting/case-study/voconic":{"label":"Voconic","children":{}},"/consulting/case-study/sol":{"label":"Sol","children":{}},"/consulting/case-study/dental-marketplace":{"label":"Dental Marketplace","children":{}},"/consulting/case-study/bhd":{"label":"BHD Cinema","children":{}}}},"/consulting/bill-by-hours":{"label":"Pricing model: Bill by hours","children":{}},"/consulting/partners-network":{"label":"Partners Network","children":{}},"/consulting/readme":{"label":"💼 Consulting team","children":{}},"/consulting/collaboration-guideline":{"label":"Collaboration Guideline","children":{}},"/consulting/fbsc":{"label":"FBSC","children":{}},"/consulting/how-to-work-with-clients":{"label":"How to work with clients","children":{}},"/consulting/service-feedbacks":{"label":"Service feedbacks","children":{}},"/consulting/setting-the-budget":{"label":"Setting The Budget","children":{}},"/consulting/fixed-budget-scope-controlled":{"label":"Fixed Budget Scope Controlled","children":{}},"/consulting/the-adjacent-possible":{"label":"The Adjacent Possible","children":{}}}},"/handbook":{"label":"Handbook","children":{"/handbook/navigate-changes":{"label":"Navigate changes","children":{}},"/handbook/community":{"label":"Community","children":{"/handbook/community/icy-worth":{"label":"How much is your ICY worth","children":{}},"/handbook/community/icy-swap":{"label":"How to swap ICY to BTC","children":{}},"/handbook/community/icy":{"label":"ICY","children":{}},"/handbook/community/discord":{"label":"Discord","children":{}},"/handbook/community/earn":{"label":"Earn","children":{}},"/handbook/community/readme":{"label":"Radar","children":{}},"/handbook/community/radar":{"label":"Radar","children":{}},"/handbook/community/sharing":{"label":"Sharing knowledge","children":{}},"/handbook/community/showcase":{"label":"Showcase","children":{}},"/handbook/community/memo":{"label":"Memo","children":{}}}},"/handbook/guides":{"label":"Guides","children":{"/handbook/guides/check-in-at-office":{"label":"Office check-in process for earning ICY","children":{}},"/handbook/guides/leave-request":{"label":"Leave request","children":{}},"/handbook/guides/nda":{"label":"NDA \u0026 Agreements","children":{}},"/handbook/guides/configure-company-email":{"label":"Configure your company email","children":{}},"/handbook/guides/one-on-one-meeting":{"label":"1-on-1 meetings","children":{}},"/handbook/guides/continuing-education-allowance":{"label":"Continuing education allowance","children":{}},"/handbook/guides/reimbursement":{"label":"Reimbursement","children":{}},"/handbook/guides/email-communication-and-use":{"label":"Email use","children":{}},"/handbook/guides/password-sharing":{"label":"Password Sharing","children":{}},"/handbook/guides/asset-request":{"label":"Request an asset","children":{}},"/handbook/guides/effective-meeting":{"label":"Effective meetings","children":{}},"/handbook/guides/conduct-a-meeting":{"label":"How to conduct a meeting","children":{}}}},"/handbook/making-a-career":{"label":"Making a career","children":{}},"/handbook/as-a-community":{"label":"As a community","children":{}},"/handbook/knowledge-base":{"label":"Knowledge base","children":{}},"/handbook/stock-option-plan":{"label":"Stock option plan","children":{}},"/handbook/readme":{"label":"📔 Handbook","children":{}},"/handbook/compliance":{"label":"Compliance","children":{}},"/handbook/mma":{"label":"MMA","children":{}},"/handbook/hybrid-working":{"label":"Hybrid Working","children":{}},"/handbook/routine":{"label":"Work routine","children":{}},"/handbook/ventures":{"label":"Ventures arm","children":{}},"/handbook/purpose":{"label":"Purpose","children":{}},"/handbook/benefits-and-perks":{"label":"Benefits \u0026 perks","children":{}},"/handbook/dwarves-foundation-is-you":{"label":"You are Dwarves Foundation","children":{}},"/handbook/getting-started":{"label":"💎 Getting started","children":{}},"/handbook/how-we-hire":{"label":"How we hire","children":{}},"/handbook/how-we-spend-money":{"label":"How we spend money","children":{}},"/handbook/misc":{"label":"Misc","children":{"/handbook/misc/marketing-assets":{"label":"Marketing assets","children":{}}}},"/handbook/moonlighting":{"label":"Moonlighting","children":{}},"/handbook/places-to-work":{"label":"Places to work","children":{}},"/handbook/security-rules":{"label":"Security rules","children":{}},"/handbook/tools-and-systems":{"label":"Tools and systems","children":{}},"/handbook/what-we-stand-for":{"label":"What we stand for","children":{}},"/handbook/what-we-value":{"label":"What we value","children":{}},"/handbook/where-we-work":{"label":"Where we work","children":{}},"/handbook/who-does-what":{"label":"Who does what","children":{}},"/handbook/faq":{"label":"FAQs","children":{}},"/handbook/how-we-work":{"label":"How we work","children":{}}}},"/radar":{"label":"Radar","children":{"/radar/index":{"label":"Index","children":{"/radar/index/readme":{"label":"Tech Radar","children":{}},"/radar/index/apache-spark":{"label":"Apache Spark","children":{}},"/radar/index/ant-design":{"label":"Ant Design","children":{}},"/radar/index/apache-kafka":{"label":"Apache Kafka","children":{}},"/radar/index/argocd":{"label":"Argocd","children":{}},"/radar/index/astro":{"label":"Astro","children":{}},"/radar/index/backstage":{"label":"Backstage","children":{}},"/radar/index/blue-green-deployment":{"label":"Blue Green Deployment","children":{}},"/radar/index/browserstack":{"label":"Browserstack","children":{}},"/radar/index/carbon":{"label":"Carbon","children":{}},"/radar/index/chatgpt-assistance":{"label":"Chatgpt Assistance","children":{}},"/radar/index/chromatic":{"label":"Chromatic","children":{}},"/radar/index/clickhouse":{"label":"Clickhouse","children":{}},"/radar/index/cloudflare-workers":{"label":"Cloudflare Workers","children":{}},"/radar/index/codecept":{"label":"Codecept","children":{}},"/radar/index/commitlint":{"label":"Commitlint","children":{}},"/radar/index/copilot":{"label":"Copilot","children":{}},"/radar/index/cucumber":{"label":"Cucumber","children":{}},"/radar/index/cypress":{"label":"Cypress","children":{}},"/radar/index/dapr":{"label":"Dapr","children":{}},"/radar/index/deno":{"label":"Deno","children":{}},"/radar/index/detox":{"label":"Detox","children":{}},"/radar/index/devcontainers":{"label":"Devcontainers","children":{}},"/radar/index/devpod":{"label":"Devpod","children":{}},"/radar/index/dora-metrics":{"label":"Dora Metrics","children":{}},"/radar/index/duckdb":{"label":"Duckdb","children":{}},"/radar/index/earthly":{"label":"Earthly","children":{}},"/radar/index/elixir-umbrella-project":{"label":"Elixir Umbrella Project","children":{}},"/radar/index/elixir":{"label":"Elixir","children":{}},"/radar/index/erlang":{"label":"Erlang","children":{}},"/radar/index/error-logging-convention":{"label":"Error Logging Convention","children":{}},"/radar/index/eslint":{"label":"Eslint","children":{}},"/radar/index/event-sourcing":{"label":"Event Sourcing","children":{}},"/radar/index/excalidraw":{"label":"Excalidraw","children":{}},"/radar/index/expo":{"label":"Expo","children":{}},"/radar/index/figma":{"label":"Figma","children":{}},"/radar/index/formal-verification":{"label":"Formal Verification","children":{}},"/radar/index/fullstack-tracing":{"label":"Fullstack Tracing","children":{}},"/radar/index/gestalt-principle":{"label":"Gestalt Principle","children":{}},"/radar/index/github-actions":{"label":"Github Actions","children":{}},"/radar/index/golang":{"label":"Golang","children":{}},"/radar/index/grafana":{"label":"Grafana","children":{}},"/radar/index/graylog":{"label":"Graylog","children":{}},"/radar/index/headless-ui":{"label":"Headless UI","children":{}},"/radar/index/hoppscotch":{"label":"Hoppscotch","children":{}},"/radar/index/ipfs":{"label":"Ipfs","children":{}},"/radar/index/jotai":{"label":"Jotai","children":{}},"/radar/index/k6":{"label":"K6","children":{}},"/radar/index/k9s":{"label":"K9s","children":{}},"/radar/index/kaniko":{"label":"Kaniko","children":{}},"/radar/index/kotlin":{"label":"Kotlin","children":{}},"/radar/index/kubeseal-sops":{"label":"Kubeseal Sops","children":{}},"/radar/index/ladle":{"label":"Ladle","children":{}},"/radar/index/langchain":{"label":"Langchain","children":{}},"/radar/index/large-language-model-llm":{"label":"Large Language Model LLM","children":{}},"/radar/index/loki":{"label":"Loki","children":{}},"/radar/index/makefile":{"label":"Makefile","children":{}},"/radar/index/micro-frontend":{"label":"Micro Frontend","children":{}},"/radar/index/monorepo":{"label":"Monorepo","children":{}},"/radar/index/msw":{"label":"Msw","children":{}},"/radar/index/n6n":{"label":"N6n","children":{}},"/radar/index/nestjs":{"label":"Nestjs","children":{}},"/radar/index/netlify":{"label":"Netlify","children":{}},"/radar/index/newrelic":{"label":"Newrelic","children":{}},"/radar/index/nextjs":{"label":"Nextjs","children":{}},"/radar/index/nodejs":{"label":"Nodejs","children":{}},"/radar/index/nostrum":{"label":"Nostrum","children":{}},"/radar/index/nx":{"label":"Nx","children":{}},"/radar/index/orval":{"label":"Orval","children":{}},"/radar/index/page-object-model":{"label":"Page Object Model","children":{}},"/radar/index/partytown":{"label":"Partytown","children":{}},"/radar/index/phaser":{"label":"Phaser","children":{}},"/radar/index/phoenix":{"label":"Phoenix","children":{}},"/radar/index/playwright":{"label":"Playwright","children":{}},"/radar/index/pnpm":{"label":"Pnpm","children":{}},"/radar/index/progressive-delivery":{"label":"Progressive Delivery","children":{}},"/radar/index/prometheus":{"label":"Prometheus","children":{}},"/radar/index/prompt-engineering":{"label":"Prompt Engineering","children":{}},"/radar/index/qwik":{"label":"Qwik","children":{}},"/radar/index/radix-ui":{"label":"Radix UI","children":{}},"/radar/index/react-hook-form":{"label":"React Hook Form","children":{}},"/radar/index/react-llm":{"label":"React LLM","children":{}},"/radar/index/react-native":{"label":"React Native","children":{}},"/radar/index/react-query":{"label":"React Query","children":{}},"/radar/index/react-server-component":{"label":"React Server Component","children":{}},"/radar/index/react-testing-library":{"label":"React Testing Library","children":{}},"/radar/index/react":{"label":"React","children":{}},"/radar/index/reinforcement-learning-from-human-feedback":{"label":"Reinforcement Learning From Human Feedback","children":{}},"/radar/index/remix":{"label":"Remix","children":{}},"/radar/index/replayio":{"label":"Replayio","children":{}},"/radar/index/reverse-engineering":{"label":"Reverse Engineering","children":{}},"/radar/index/rust":{"label":"Rust","children":{}},"/radar/index/selenium":{"label":"Selenium","children":{}},"/radar/index/semantic-release-auto-release":{"label":"Semantic Release Auto Release","children":{}},"/radar/index/sentry":{"label":"Sentry","children":{}},"/radar/index/serverlessq":{"label":"Serverlessq","children":{}},"/radar/index/solidity":{"label":"Solidity","children":{}},"/radar/index/solidjs":{"label":"Solidjs","children":{}},"/radar/index/stern":{"label":"Stern","children":{}},"/radar/index/svelte":{"label":"Svelte","children":{}},"/radar/index/swagger":{"label":"Swagger","children":{}},"/radar/index/swift-ui":{"label":"Swift UI","children":{}},"/radar/index/swift":{"label":"Swift","children":{}},"/radar/index/swr":{"label":"Swr","children":{}},"/radar/index/tailwindcss":{"label":"Tailwindcss","children":{}},"/radar/index/tauri":{"label":"Tauri","children":{}},"/radar/index/team-topologies":{"label":"Team Topologies","children":{}},"/radar/index/timescaledb":{"label":"Timescaledb","children":{}},"/radar/index/tla":{"label":"Tla","children":{}},"/radar/index/trunk-based-development":{"label":"Trunk Based Development","children":{}},"/radar/index/turborepo":{"label":"Turborepo","children":{}},"/radar/index/type-safe-client-server":{"label":"Type Safe Client Server","children":{}},"/radar/index/typescript":{"label":"Typescript","children":{}},"/radar/index/ui-documentation":{"label":"UI Documentation","children":{}},"/radar/index/uno-css":{"label":"Uno Css","children":{}},"/radar/index/upptime":{"label":"Upptime","children":{}},"/radar/index/v-model":{"label":"V Model","children":{}},"/radar/index/vector-database":{"label":"Vector Database","children":{}},"/radar/index/vercel":{"label":"Vercel","children":{}},"/radar/index/vitejs":{"label":"Vitejs","children":{}},"/radar/index/volta":{"label":"Volta","children":{}},"/radar/index/wasm":{"label":"Wasm","children":{}},"/radar/index/webdriverio":{"label":"Webdriverio","children":{}},"/radar/index/webflow":{"label":"Webflow","children":{}},"/radar/index/yup":{"label":"Yup","children":{}},"/radar/index/zod":{"label":"Zod","children":{}},"/radar/index/zustand":{"label":"Zustand","children":{}}}},"/radar/readme":{"label":"Tech radar index","children":{}}}},"/updates":{"label":"Updates","children":{"/updates/forward":{"label":"Forward","children":{"/updates/forward/vol-01":{"label":"Vol 01","children":{"/updates/forward/vol-01/istio":{"label":"New Member","children":{}}}},"/updates/forward/market-commentary":{"label":"Market Commentary","children":{"/updates/forward/market-commentary/event-takeaways-2nd":{"label":"2nd Talks and Takeaways","children":{}},"/updates/forward/market-commentary/event-takeaways-1st":{"label":"1st Talks and Takeaways","children":{}},"/updates/forward/market-commentary/2025-28th-feb":{"label":"#9: Bybit Loses $1.5B in Hack, Claude 3.7 Sonnet Drops, and OpenArt Designs Characters","children":{}},"/updates/forward/market-commentary/2025-21th-feb":{"label":"#8: R1 1776 Goes Open-Source, Cardex Gets Hacked, and Grok-3 Debuts","children":{}},"/updates/forward/market-commentary/2025-14th-feb":{"label":"#7: 10x AI Cost Reduction, Lyft’s 2026 Robotaxi Milestone, and Solana ETF Buzz","children":{}},"/updates/forward/market-commentary/2025-7th-feb":{"label":"#6 Trending Products, DeepSeek Wave, and Ethereum Predictions","children":{}},"/updates/forward/market-commentary/2025-17th-jan":{"label":"#5 VC Trends, Blockchain Breakthroughs, and AI Innovations","children":{}},"/updates/forward/market-commentary/2025-10th-jan":{"label":"#4 AI Supercomputers, Mini AI PCs, SEA VC","children":{}},"/updates/forward/market-commentary/2025-3rd-jan":{"label":"#3 AI at CES, Wall Street Boom, Blockchain Trends","children":{}},"/updates/forward/market-commentary/2024-27th-dec":{"label":"#2 AI Talent Wars, OpenAI’s New Models, Hyperliquid","children":{}},"/updates/forward/market-commentary/2024-13th-dec":{"label":"#1 Gemini 2.0, OpenAI’s Sora,  a16z’s Predictions","children":{}}}},"/updates/forward/2025-02":{"label":"20242025","children":{}},"/updates/forward/product-design":{"label":"Product Design","children":{"/updates/forward/product-design/product-design-commentary-20241122":{"label":"Product Design Commentary #7: Hyper-personalization - How AI improves user experience personalization","children":{}},"/updates/forward/product-design/product-design-commentary-20241115":{"label":"Product Design Commentary #6: AI in Design - Cool ideas and how to make them happen","children":{}},"/updates/forward/product-design/product-design-commentary-20241101":{"label":"Product Design Commentary #5: Figma to SwiftUI (functional code) with Claude AI","children":{}},"/updates/forward/product-design/product-design-commentary-20241018":{"label":"Product Design Commentary #4: Generative AI UX design patterns","children":{}},"/updates/forward/product-design/product-design-commentary-20241011":{"label":"Product Design Commentary #3: The art of prompting in AI-human interaction","children":{}},"/updates/forward/product-design/product-design-commentary-20241004":{"label":"Product Design Commentary #2: Unpacking the sparkles icon and AI onboarding challenges","children":{}},"/updates/forward/product-design/product-design-commentary-20240927":{"label":"Product Design Commentary #1: New technologies changing UX/UI and product design","children":{}}}},"/updates/forward/market-report":{"label":"Market Report","children":{"/updates/forward/market-report/2024-october":{"label":"October 2024","children":{}},"/updates/forward/market-report/2024-september":{"label":"September 2024","children":{}},"/updates/forward/market-report/2024-august":{"label":"August 2024","children":{}},"/updates/forward/market-report/2024-july":{"label":"July 2024","children":{}},"/updates/forward/market-report/2024-may":{"label":"May 2024","children":{}},"/updates/forward/market-report/2024-april":{"label":"April 2024","children":{}},"/updates/forward/market-report/2024-march":{"label":"March 2024","children":{}},"/updates/forward/market-report/2024-february":{"label":"February 2024","children":{}},"/updates/forward/market-report/2024-january":{"label":"January 2024","children":{}},"/updates/forward/market-report/2023-december":{"label":"December 2023","children":{}}}},"/updates/forward/2024-09":{"label":"September 2024","children":{}},"/updates/forward/2023-11":{"label":"November 2023","children":{}},"/updates/forward/2023-10":{"label":"October 2023","children":{}},"/updates/forward/2023-08":{"label":"August 2023","children":{}},"/updates/forward/2023-06":{"label":"June 2023","children":{}},"/updates/forward/2023-05":{"label":"May 2023","children":{}},"/updates/forward/2023-03":{"label":"March 2023","children":{}},"/updates/forward/2023-12":{"label":"December 2023","children":{}},"/updates/forward/2022":{"label":"2022","children":{}},"/updates/forward/volume-03":{"label":"Tech Radar Volume 03","children":{}},"/updates/forward/volume-02":{"label":"Tech Radar Volume 02","children":{}},"/updates/forward/volume-01":{"label":"Tech Radar Volume 01","children":{}},"/updates/forward/readme":{"label":"Forward Engineering","children":{}}}},"/updates/build-log":{"label":"Build Log","children":{"/updates/build-log/service_monitoring_with_upptime":{"label":"Secure and transparent uptime monitoring with Upptime and GitHub secrets","children":{}},"/updates/build-log/create-slides-with-overleaf":{"label":"Create slides with Overleaf and ChatGPT","children":{}},"/updates/build-log/optimize-init-load-time-for-trading-platform":{"label":"Optimizing initial load time for a Trading Platform","children":{}},"/updates/build-log/ai-interview-platform-mvp":{"label":"Building MVP for AI-driven interview platform","children":{}},"/updates/build-log/optimizing-ui-for-effective-investment-experience":{"label":"Hedge Foundation - Optimizing UI for effective investment experience","children":{}},"/updates/build-log/implement-binance-future-pnl-analysis-page":{"label":"Implement Binance Futures PNL analysis page by Phoenix LiveView","children":{}},"/updates/build-log/migrate-normal-table-to-timescale-table":{"label":"Migrate regular tables into TimescaleDB hypertables to improve query performance","children":{}},"/updates/build-log/bitcoin-alt-performance-tracking":{"label":"Tracking Bitcoin-Altcoin Performance Indicators in BTC Hedging Strategy","children":{}},"/updates/build-log/database-hardening-for-trading-platform":{"label":"Database hardening for a trading platform","children":{}},"/updates/build-log/data-archive-and-recovery":{"label":"Building a data archive and recovery strategy for high-volume trading system","children":{}},"/updates/build-log/persist-history-using-data-snapshot-pattern":{"label":"Implementing data snapshot pattern to persist historical data","children":{}},"/updates/build-log/ai-ruby-travel-assistant-chatbot":{"label":"AI-powered Ruby travel assistant","children":{}},"/updates/build-log/building-chatbot-agent-for-project-management-tool":{"label":"Building chatbot agent to streamline project management","children":{}},"/updates/build-log/building-data-pipeline-ogif-transcriber":{"label":"Building data pipeline for OGIF transcriber","children":{}},"/updates/build-log/centralized-monitoring-setup-for-trading-platform":{"label":"Setup centralized monitoring system for Hedge Foundation trading platform","children":{}},"/updates/build-log/binance-transfer-matching":{"label":"Building better Binance transfer tracking","children":{}},"/updates/build-log/crypto-market-outperform-chart-rendering":{"label":"Visualizing crypto market performance: BTC-Alt dynamic indicators in Golang","children":{}},"/updates/build-log/enhancing-cryptocurrency-transfer-logger":{"label":"Transfer mapping: enhancing loggers for better transparency","children":{}},"/updates/build-log/reconstructing_trading_pnl_data_pipeline_approach":{"label":"Reconstructing historical trading PnL: a data pipeline approach","children":{}},"/updates/build-log/ai-powered-monthly-project-reports":{"label":"Project reports system: a case study","children":{}}}},"/updates/ogif":{"label":"OGIF","children":{"/updates/ogif/41-20250314":{"label":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","children":{}},"/updates/ogif/39-20250207":{"label":"#39 Frontend report, DB Scaling, AI Workflow","children":{}},"/updates/ogif/38-20250117":{"label":"#38 Erlang automata, AI Trends, Year-End Awards","children":{}},"/updates/ogif/37-20241227":{"label":"#37 AI Fine-tuning, Data archiving, Datalakes","children":{}},"/updates/ogif/28-20241018":{"label":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","children":{}},"/updates/ogif/27-20241011":{"label":"#27 Go weekly, Frontend, AI UX, Finite Automata","children":{}},"/updates/ogif/26-20241004":{"label":"#26 Design insights, Go tools, Trading app, Chatbots, Essays","children":{}},"/updates/ogif/25-20240927":{"label":"#25 Team updates, Hybrid work, AI insights, Go weekly","children":{}},"/updates/ogif/24-20240920":{"label":"#24 Go weekly, AI workflows, Team AI demo, Figma-UI with Claude","children":{}},"/updates/ogif/23-20240913":{"label":"#23 Go weekly, FE report, Hybrid work, AI agents","children":{}},"/updates/ogif/22-20240906":{"label":"#22 Hybrid work, Tech report, Go weekly, AI demo","children":{}},"/updates/ogif/21-20240830":{"label":"#21 Community engagement, Go weekly, Journey of thought for prompt engineering","children":{}},"/updates/ogif/20-20240823":{"label":"#20 Go weekly, Dynamic objects, Devbox, LLM tracing, Cursor AI","children":{}},"/updates/ogif/19-20240821":{"label":"#19 Go weekly, UI design, File sharing, Dify AI","children":{}},"/updates/ogif/18-20240809":{"label":"#18 Go weekly, RAG, UI, FE updates","children":{}},"/updates/ogif/17-20240802":{"label":"#17 Community Call July, C4 Model, Interview Life in the US","children":{}},"/updates/ogif/16-20240726":{"label":"#16 Go weekly, Dune query, AI voice clone, RAG re-ranking","children":{}},"/updates/ogif/15-20240719":{"label":"#15 AI Supervisors, Local-first Software, Code Completion, Bot Commands","children":{}},"/updates/ogif/14-20240712":{"label":"#14 Generic Collections, Pricing Models, and OGIF Summarizer","children":{}},"/updates/ogif/13-20240705":{"label":"#13 Go Weekly updates, Radix Sort, Human Feedback Mechanism, and effective ChatGPT usage","children":{}},"/updates/ogif/12-20240628":{"label":"#12 June updates, Go Performance, eBPF, PGO, Multimodal RAG","children":{}},"/updates/ogif/11-20240621":{"label":"#11 Design patterns: template method \u0026 visitor, Radix sort, and weekly tech commentary","children":{}},"/updates/ogif/10-20240614":{"label":"#10 Behavioral Patterns and Map Content Organization","children":{}},"/updates/ogif/9-20240607":{"label":"#9 What's next for June and Behavior Design Patterns","children":{}},"/updates/ogif/7-20240517":{"label":"#7 Echelon EXPO, Programming patterns, and Moonlighting","children":{}},"/updates/ogif/6-20240510":{"label":"#6 Factory Pattern, Erlang State Machines, and Trading Process","children":{}},"/updates/ogif/5-20240503":{"label":"#5 Singapore Market Report, C4 Modelling, Memo's Nested Sidebar","children":{}},"/updates/ogif/4-20240426":{"label":"#4 DCA, Devbox","children":{}},"/updates/ogif/3-20240419":{"label":"#3 Generative AI, Tokenomics, and Finance Talks","children":{}},"/updates/ogif/2-20240412":{"label":"#2 Devbox as the new Docker, Security Standards, and Understanding Liquidity","children":{}},"/updates/ogif/1-20240405":{"label":"#1 Markdown Presentations, Research Pipeline, Screenshots How-to","children":{}},"/updates/ogif/readme":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/updates/changelog":{"label":"Changelog","children":{"/updates/changelog/2024-10-25-knowledge-base":{"label":"Build your knowledge base","children":{}},"/updates/changelog/2024-09-13-dwarve-updates-ai-llm":{"label":"The Stage of AI and LLM at Dwarves","children":{}},"/updates/changelog/readme":{"label":"Dwarves Updates","children":{}},"/updates/changelog/2023-09-12-growth-stages":{"label":"The Stage of Growth at Dwarves","children":{}},"/updates/changelog/2022-08-26-the-next-leading-chairs":{"label":"The Next Leading Chairs","children":{}},"/updates/changelog/2022-06-26-blockchain-and-data":{"label":"The future is blockchain and data","children":{}},"/updates/changelog/2022-03-31-hiring-stages":{"label":"The stages of hiring at Dwarves","children":{}},"/updates/changelog/2021-12-30-2021-in-review":{"label":"It's a wrap: 2021 in Review","children":{}},"/updates/changelog/2021-12-01-engineering-org-structure":{"label":"Engineering Organizational Structure","children":{}},"/updates/changelog/2021-10-31-path-to-growth":{"label":"The Path To Growth at Dwarves","children":{}},"/updates/changelog/2021-09-29-engineer-performance-review":{"label":"Engineer Performance Review","children":{}},"/updates/changelog/2021-08-23-project-compliance":{"label":"Project Compliance","children":{}},"/updates/changelog/2021-07-11-dalat-office":{"label":"Da Lat Office","children":{}},"/updates/changelog/2021-06-10-dwarves-updates":{"label":"Dwarves Updates","children":{}}}},"/updates/wala":{"label":"WALA","children":{"/updates/wala/001-43-factory":{"label":"43 Factory","children":{}},"/updates/wala/002-dzs-media":{"label":"DZS Media","children":{}},"/updates/wala/003-sp-group":{"label":"SP Group","children":{}},"/updates/wala/readme":{"label":"WALA","children":{}}}}}},"/careers":{"label":"Careers","children":{"/careers/archived":{"label":"Archived","children":{"/careers/archived/full-stack-engineer":{"label":"Full-Stack Engineer","children":{}},"/careers/archived/executive-assistant":{"label":"Executive Assistant","children":{}},"/careers/archived/technical-recruiter":{"label":"Technical Recruiter","children":{}},"/careers/archived/backend-engineer-go-elixir-rust":{"label":"Backend Engineer, Go/Elixir/Rust","children":{}},"/careers/archived/react-native-developer":{"label":"React Native Developer","children":{}},"/careers/archived/android-developer":{"label":"Mobile Engineer, Android","children":{}},"/careers/archived/community-executive":{"label":"Community Executive","children":{}},"/careers/archived/data-engineering":{"label":"Energy - Data Engineering","children":{}},"/careers/archived/devops":{"label":"DevOps Engineer - FinTech","children":{}},"/careers/archived/frontend-developer-junior":{"label":"Junior Frontend Developer","children":{}},"/careers/archived/frontend":{"label":"Frontend","children":{}},"/careers/archived/ios-developer":{"label":"iOS Developer - EnergyTech","children":{}},"/careers/archived/macos-developer":{"label":"Software Engineer, macOS","children":{}},"/careers/archived/product-designer-new-grad":{"label":"Product Designer, New Grad","children":{}},"/careers/archived/product-designer":{"label":"Product Designer","children":{}},"/careers/archived/qc-automation":{"label":"QC Engineer, Automation - Logistics","children":{}},"/careers/archived/qc-manual":{"label":"Fintech - QC Engineer, Manual","children":{}},"/careers/archived/reactjs-web-engineer":{"label":"Web Engineer, React.js","children":{}},"/careers/archived/visual-designer":{"label":"Visual Designer","children":{}},"/careers/archived/android":{"label":"Android","children":{}},"/careers/archived/golang":{"label":"Golang","children":{}},"/careers/archived/intern":{"label":"Intern","children":{}},"/careers/archived/ios":{"label":"iOS Developer","children":{}},"/careers/archived/qa":{"label":"QA Engineer","children":{}}}},"/careers/open-positions":{"label":"Open Positions","children":{"/careers/open-positions/business-manager":{"label":"Business Development Manager","children":{}},"/careers/open-positions/growth-lead":{"label":"Growth Lead","children":{}}}},"/careers/life":{"label":"Life","children":{"/careers/life/2024-09-26-29-dat-nguyen":{"label":"Dat Nguyen","children":{}},"/careers/life/2024-02-19-28-duyen-tran":{"label":"Duyen Tran","children":{}},"/careers/life/2024-01-22-27-tri-tran":{"label":"Tri Tran","children":{}},"/careers/life/2024-01-03-25-khoi-nguyen":{"label":"Khoi Nguyen","children":{}},"/careers/life/2023-12-13-24-tai-pham":{"label":"Tai Pham","children":{}},"/careers/life/2023-12-12-23-hieu-nghia":{"label":"Hieu Nghia","children":{}},"/careers/life/2023-11-27-22-cat-nguyen":{"label":"Cat Nguyen","children":{}},"/careers/life/2023-11-20-21-minh-cloud":{"label":"Minh Cloud","children":{}},"/careers/life/2023-11-13-20-hoai-khang":{"label":"Hoai Khang","children":{}},"/careers/life/2023-11-03-19-vi-tran":{"label":"Vi Tran","children":{}},"/careers/life/2023-10-30-18-tuan-tran":{"label":"Tuan Tran","children":{}},"/careers/life/2023-10-16-16-kim-ngan":{"label":"Kim Ngan","children":{}},"/careers/life/2023-10-13-17-hoang-nguyen":{"label":"Hoang Nguyen","children":{}},"/careers/life/2023-10-09-15-khoi-ngo":{"label":"Khoi Ngo","children":{}},"/careers/life/2023-10-02-14-dat-pham":{"label":"Dat Pham","children":{}},"/careers/life/2023-09-29-13-bien-vo":{"label":"Bien Vo","children":{}},"/careers/life/2023-09-18-12-toan-ho":{"label":"Toan Ho","children":{}},"/careers/life/2023-09-05-11-dinh-nam":{"label":"Dinh Nam","children":{}},"/careers/life/2023-08-17-10-cuong-mai":{"label":"Cuong Mai","children":{}},"/careers/life/2023-08-07-9-hoang-anh":{"label":"Hoang Anh","children":{}},"/careers/life/2023-06-30-7-khac-vy":{"label":"Khac Vy","children":{}},"/careers/life/group":{"label":"Group","children":{"/careers/life/group/2023-06-01-software-design-group":{"label":"Software Design Group","children":{}}}},"/careers/life/2022-09-21-7-my-anh":{"label":"My Anh","children":{}},"/careers/life/2022-08-11-6-hieu-vu":{"label":"Hieu Vu","children":{}},"/careers/life/2022-08-04-6-duy-nguyen":{"label":"Duy Nguyen","children":{}},"/careers/life/2022-08-03-5-nam-nguyen":{"label":"Nam Nguyen","children":{}},"/careers/life/2022-07-22-4-an-tran":{"label":"An Tran","children":{}},"/careers/life/2022-03-17-3-tom-nguyen":{"label":"Tom Nguyen","children":{}},"/careers/life/2022-02-25-2-anh-tran":{"label":"Anh Tran","children":{}},"/careers/life/2022-02-14-1-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/life/2021-03-31-0-tuan-dao":{"label":"Tuan Dao","children":{}},"/careers/life/2021-03-11-0-phat-nguyen":{"label":"Phat Nguyen","children":{}},"/careers/life/2020-05-08-0-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/life/2020-04-10-0-huy-nguyen":{"label":"Huy Nguyen","children":{}}}},"/careers/culture":{"label":"Culture","children":{}},"/careers/manifesto":{"label":"Manifesto","children":{}},"/careers/internship":{"label":"Internship","children":{"/careers/internship/2019":{"label":"2019","children":{"/careers/internship/2019/2019":{"label":"Spring Internship 2019","children":{}}}}}},"/careers/apprentice":{"label":"Apprentice","children":{"/careers/apprentice/2022":{"label":"2022","children":{"/careers/apprentice/2022/batch-of-2022":{"label":"Batch of 2022","children":{}},"/careers/apprentice/2022/2022-meet-ngoc-thanh-pham":{"label":"Thanh Pham","children":{}},"/careers/apprentice/2022/2022-meet-tuan-dao":{"label":"Tuan Dao","children":{}}}},"/careers/apprentice/apprentice":{"label":"Apprentice program","children":{}}}},"/careers/readme":{"label":"👋 Join the Dwarves","children":{}}}},"/opensource":{"label":"Opensource","children":{"/opensource/readme":{"label":"☀️ Open source","children":{}}}},"/culture":{"label":"Culture","children":{"/culture/culture-test":{"label":"The culture test","children":{}},"/culture/readme":{"label":"Notes on our culture","children":{}},"/culture/ogif-intro":{"label":"OGIF - Oh God It's Friday","children":{}},"/culture/red-flags":{"label":"Red flags","children":{}},"/culture/focus-on-delivery":{"label":"Focus on delivery","children":{}},"/culture/the-inner-circle":{"label":"The inner circle","children":{}},"/culture/making-decision":{"label":"Making decision as a team member","children":{}},"/culture/beyond-the-title":{"label":"Beyond the title","children":{}},"/culture/go-the-extra-mile":{"label":"Go the extra mile","children":{}},"/culture/runs-by-ideas":{"label":"The Dwarves runs by ideas","children":{}},"/culture/a-tips-of-hiring-dont":{"label":"A tips of hiring - Do \u0026 Don't","children":{}},"/culture/culture-handbook":{"label":"The Dwarves culture handbook","children":{}},"/culture/people-matter":{"label":"How people matter should work","children":{}},"/culture/delegation-and-believe-it-will-work":{"label":"Delegation and believe it will work","children":{}},"/culture/constructive-feedback":{"label":"Constructive feedback","children":{}},"/culture/transparency":{"label":"Transparency","children":{}},"/culture/account-management-strategy":{"label":"Account management strategies","children":{}},"/culture/avoid-burn-out":{"label":"Avoid burn out","children":{}},"/culture/high-performing-team":{"label":"Building a solid high performing team","children":{}},"/culture/delegate-work-not-responsibility":{"label":"Delegate work, not responsibility","children":{}},"/culture/blocking-distraction":{"label":"Blocking distraction","children":{}}}},"/playbook":{"label":"Playbook","children":{"/playbook/operations":{"label":"Operations","children":{"/playbook/operations/checklists":{"label":"Checklists","children":{"/playbook/operations/checklists/leave-and-request-checklist":{"label":"Leave Request","children":{}},"/playbook/operations/checklists/offboarding-checklist":{"label":"Offboarding","children":{}},"/playbook/operations/checklists/artifact-checklist":{"label":"Back up Artifact","children":{}},"/playbook/operations/checklists/project-archive":{"label":"Project Archive","children":{}},"/playbook/operations/checklists/project-case-study":{"label":"Project Case Study","children":{}},"/playbook/operations/checklists/project-communication":{"label":"Project Communication","children":{}},"/playbook/operations/checklists/project-handover":{"label":"Project Handover","children":{}},"/playbook/operations/checklists/project-initialization":{"label":"Project Initialization","children":{}},"/playbook/operations/checklists/assets-checklist":{"label":"Assets","children":{}},"/playbook/operations/checklists/billing-checklist":{"label":"Billing","children":{}},"/playbook/operations/checklists/candidate-checklist":{"label":"Candidate","children":{}},"/playbook/operations/checklists/consulting-contract-checklist":{"label":"Consulting Contract","children":{}},"/playbook/operations/checklists/hiring-checklist":{"label":"Hiring","children":{}},"/playbook/operations/checklists/onboarding-checklist":{"label":"Onboarding","children":{}},"/playbook/operations/checklists/unemployment-social-health-insurance":{"label":"Unemployment, Social, Health Insurance","children":{}},"/playbook/operations/checklists/vietnam-invoice-checklist":{"label":"Vietnam Invoice","children":{}}}},"/playbook/operations/how-to-conduct-delivery-reports":{"label":"How to conduct delivery reports","children":{}},"/playbook/operations/how-we-do-effective-planning-and-reporting":{"label":"How we do effective planning and reporting","children":{}},"/playbook/operations/project-schedule-delivery-guidelines":{"label":"Project Delivery Schedule and Guidelines","children":{}},"/playbook/operations/mbti-type-intj":{"label":"MBTI Type INTJ","children":{}},"/playbook/operations/mbti-type-istp":{"label":"MBTI Type ISTP","children":{}},"/playbook/operations/mbti-type-estj":{"label":"MBTI Type ESTJ","children":{}},"/playbook/operations/mbti-type-istj":{"label":"MBTI Type ISTJ","children":{}},"/playbook/operations/applying-myersbriggs-type-indicator-in-hr":{"label":"Applying Myersbriggs Type Indicator In Hiring","children":{}},"/playbook/operations/the-four-preferences":{"label":"The Four Preferences","children":{}},"/playbook/operations/adjust-the-way-we-work-in-basecamp-style":{"label":"Adjust The Way We Work In Basecamp Style","children":{}},"/playbook/operations/bric-a-brac":{"label":"Bric A Brac","children":{}},"/playbook/operations/writing-management-objectives-in-smart":{"label":"Writing Management Objectives In Smart","children":{}},"/playbook/operations/hiring-for-operations-team":{"label":"Hiring For Operations Team","children":{}},"/playbook/operations/annual-bonus-for-sales":{"label":"Annual bonus for sales","children":{}},"/playbook/operations/collaboration-guidelines":{"label":"Collaboration Guidelines","children":{}},"/playbook/operations/compliance-check-process":{"label":"Compliance Check Process","children":{}},"/playbook/operations/email-template":{"label":"Email Template","children":{"/playbook/operations/email-template/assignment-invitation-2":{"label":"Assignment Inviation (Skip pre-assessment)","children":{}},"/playbook/operations/email-template/assignment-invitation":{"label":"Assignment Inviation","children":{}},"/playbook/operations/email-template/confirm-resume-date":{"label":"Confirm Employee's Resume Date Day","children":{}},"/playbook/operations/email-template/farewell":{"label":"Farewell Letter","children":{}},"/playbook/operations/email-template/follow-up-onboarding-items":{"label":"Follow-up Onboarding Items","children":{}},"/playbook/operations/email-template/hung-king-commemoration-day":{"label":"Hung King Commemoration Day","children":{}},"/playbook/operations/email-template/information-about-resource-change":{"label":"Inform about resource change","children":{}},"/playbook/operations/email-template/international-labour-day":{"label":"International Labour Day","children":{}},"/playbook/operations/email-template/interview-invitation":{"label":"Interview Invitation","children":{}},"/playbook/operations/email-template/milestone-sign-off":{"label":"Milestone sign-off","children":{}},"/playbook/operations/email-template/national-day":{"label":"National Day","children":{}},"/playbook/operations/email-template/new-year-day":{"label":"New Year Day","children":{}},"/playbook/operations/email-template/offer-letter":{"label":"Offer Letter","children":{}},"/playbook/operations/email-template/referral-bonus-confirmation-note":{"label":"Referral Bonus Confirmation Note","children":{}},"/playbook/operations/email-template/rejection-email":{"label":"Rejection","children":{}},"/playbook/operations/email-template/salary-increment":{"label":"Salary Increment Announcement","children":{}},"/playbook/operations/email-template/tet-holiday":{"label":"Tet Holiday","children":{}},"/playbook/operations/email-template/thank-you-letter":{"label":"Thank you letter","children":{}},"/playbook/operations/email-template/welcome-onboard":{"label":"Welcome Onboard","children":{}},"/playbook/operations/email-template/welcome-to-dwarves-update":{"label":"Welcome to Dwarves Updates","children":{}}}},"/playbook/operations/naming-convention":{"label":"Naming convention","children":{}},"/playbook/operations/setup-email-template":{"label":"Setup email template in Gmail","children":{}},"/playbook/operations/types-of-employees":{"label":"Types Of Employees","children":{}},"/playbook/operations/hiring-approach":{"label":"Hiring Approach","children":{}},"/playbook/operations/the-okr":{"label":"The OKR","children":{}},"/playbook/operations/our-metrics-for-performance-review":{"label":"Our Metrics For Performance Review","children":{}},"/playbook/operations/make-remote-working-works":{"label":"Make Remote Working Works","children":{}},"/playbook/operations/our-policy-for-remote-working":{"label":"Our Policy For Remote Working","children":{}}}},"/playbook/engineering":{"label":"Engineering","children":{"/playbook/engineering/estimation-guidelines":{"label":"Estimation Guidelines","children":{}},"/playbook/engineering/presentation":{"label":"monitoring","children":{}},"/playbook/engineering/repo-icon":{"label":"release","children":{}}}},"/playbook/design":{"label":"Design","children":{"/playbook/design/design-system":{"label":"Design System","children":{}},"/playbook/design/ia-design":{"label":"Information Architecture","children":{}},"/playbook/design/ix-design":{"label":"IX","children":{}},"/playbook/design/aarrr":{"label":"AARRR","children":{}},"/playbook/design/design-sprint":{"label":"Design Sprint","children":{}},"/playbook/design/lean-canvas":{"label":"Lean Canvas","children":{}},"/playbook/design/prototype":{"label":"Low-fidelity prototype: UI Design","children":{}},"/playbook/design/ui-design":{"label":"UI","children":{}},"/playbook/design/ux-design":{"label":"UX","children":{}},"/playbook/design/wireframe":{"label":"wireframe","children":{}}}},"/playbook/readme":{"label":"Playbook","children":{}}}},"/fund":{"label":"Fund","children":{"/fund/ventures-fund-1":{"label":"Dwarves Ventures Fund 1","children":{}},"/fund/ventures-fund-0":{"label":"Dwarves Ventures Fund 0","children":{}}}}}},"/tags":{"label":"Popular Tags","children":{"/tags/earn":{"label":"#earn","children":{},"count":8},"/tags/productivity":{"label":"#productivity","children":{},"count":13},"/tags/quality":{"label":"#quality","children":{},"count":3},"/tags/open-source":{"label":"#open-source","children":{},"count":3},"/tags/liquidity":{"label":"#liquidity","children":{},"count":2},"/tags/rfc":{"label":"#RFC","children":{},"count":2},"/tags/icy":{"label":"#icy","children":{},"count":13},"/tags/blockchain":{"label":"#blockchain","children":{},"count":47},"/tags/ai":{"label":"#AI","children":{},"count":59},"/tags/evm":{"label":"#evm","children":{},"count":5},"/tags/web3":{"label":"#web3","children":{},"count":4},"/tags/foundry":{"label":"#foundry","children":{},"count":2},"/tags/hiring":{"label":"#hiring","children":{},"count":21},"/tags/case-study":{"label":"#case-study","children":{},"count":29},"/tags/handbook":{"label":"#handbook","children":{},"count":47},"/tags/business":{"label":"#business","children":{},"count":5},"/tags/growth":{"label":"#growth","children":{},"count":2},"/tags/frontend":{"label":"#frontend","children":{},"count":68},"/tags/market-report":{"label":"#market-report","children":{},"count":32},"/tags/security":{"label":"#security","children":{},"count":10},"/tags/mcp":{"label":"#MCP","children":{},"count":3},"/tags/btc":{"label":"#btc","children":{},"count":1},"/tags/swap":{"label":"#swap","children":{},"count":2},"/tags/golang":{"label":"#golang","children":{},"count":50},"/tags/go-weekly":{"label":"#go-weekly","children":{},"count":24},"/tags/llm":{"label":"#LLM","children":{},"count":76},"/tags/protocol":{"label":"#protocol","children":{},"count":2},"/tags/agents":{"label":"#agents","children":{},"count":5},"/tags/tooling":{"label":"#tooling","children":{},"count":9},"/tags/aider":{"label":"#aider","children":{},"count":2},"/tags/qwen2.5":{"label":"#qwen2.5","children":{},"count":1},"/tags/openhand":{"label":"#openhand","children":{},"count":1},"/tags/predicted output":{"label":"#predicted output","children":{},"count":1},"/tags/project-management":{"label":"#project-management","children":{},"count":1},"/tags/copilots":{"label":"#copilots","children":{},"count":2},"/tags/team-management":{"label":"#team-management","children":{},"count":1},"/tags/mongodb":{"label":"#mongodb","children":{},"count":1},"/tags/database":{"label":"#database","children":{},"count":8},"/tags/rag":{"label":"#RAG","children":{},"count":5},"/tags/salesforce":{"label":"#salesforce","children":{},"count":1},"/tags/use cases":{"label":"#use cases","children":{},"count":2},"/tags/react":{"label":"#react","children":{},"count":15},"/tags/performance":{"label":"#performance","children":{},"count":17},"/tags/design-system":{"label":"#design-system","children":{},"count":1},"/tags/storybook":{"label":"#storybook","children":{},"count":1},"/tags/hook":{"label":"#hook","children":{},"count":1},"/tags/testing":{"label":"#testing","children":{},"count":4},"/tags/cline":{"label":"#cline","children":{},"count":1},"/tags/realtime api":{"label":"#realtime api","children":{},"count":1},"/tags/interface":{"label":"#interface","children":{},"count":1},"/tags/import":{"label":"#import","children":{},"count":1},"/tags/package":{"label":"#package","children":{},"count":1},"/tags/yelp":{"label":"#yelp","children":{},"count":1},"/tags/generics":{"label":"#generics","children":{},"count":2},"/tags/entertainment":{"label":"#entertainment","children":{},"count":1},"/tags/observability":{"label":"#observability","children":{},"count":5},"/tags/log":{"label":"#log","children":{},"count":1},"/tags/pillar":{"label":"#pillar","children":{},"count":3},"/tags/metric":{"label":"#metric","children":{},"count":1},"/tags/tracing":{"label":"#tracing","children":{},"count":1},"/tags/intent-classification":{"label":"#intent-classification","children":{},"count":1},"/tags/prompting":{"label":"#prompting","children":{},"count":1},"/tags/evaluation":{"label":"#evaluation","children":{},"count":3},"/tags/enterprise":{"label":"#enterprise","children":{},"count":10},"/tags/language":{"label":"#language","children":{},"count":5},"/tags/ai-agents":{"label":"#ai-agents","children":{},"count":2},"/tags/ai-evaluation":{"label":"#ai-evaluation","children":{},"count":1},"/tags/moc":{"label":"#moc","children":{},"count":3},"/tags/prompt-engineering":{"label":"#prompt-engineering","children":{},"count":4},"/tags/ai-integration":{"label":"#ai-integration","children":{},"count":1},"/tags/networking":{"label":"#networking","children":{},"count":7},"/tags/finite-automata":{"label":"#finite-automata","children":{},"count":1},"/tags/pattern-matching":{"label":"#pattern-matching","children":{},"count":1},"/tags/state-machines":{"label":"#state-machines","children":{},"count":1},"/tags/java":{"label":"#java","children":{},"count":1},"/tags/programming":{"label":"#programming","children":{},"count":1},"/tags/caching":{"label":"#caching","children":{},"count":1},"/tags/devbox":{"label":"#devbox","children":{},"count":17},"/tags/nix":{"label":"#nix","children":{},"count":9},"/tags/generative-ui":{"label":"#generative-ui","children":{},"count":1},"/tags/docker":{"label":"#docker","children":{},"count":11},"/tags/function-calling":{"label":"#function-calling","children":{},"count":1},"/tags/ton":{"label":"#ton","children":{},"count":2},"/tags/design-pattern":{"label":"#design-pattern","children":{},"count":9},"/tags/gang-of-four":{"label":"#gang-of-four","children":{},"count":9},"/tags/behavior-pattern":{"label":"#behavior-pattern","children":{},"count":2},"/tags/visitor-design-pattern":{"label":"#visitor-design-pattern","children":{},"count":1},"/tags/ai-powered":{"label":"#ai-powered","children":{},"count":1},"/tags/feedback":{"label":"#feedback","children":{},"count":2},"/tags/pattern":{"label":"#pattern","children":{},"count":1},"/tags/supervisor-architecture":{"label":"#supervisor-architecture","children":{},"count":1},"/tags/document-processing":{"label":"#document-processing","children":{},"count":1},"/tags/information-retrieval":{"label":"#information-retrieval","children":{},"count":1},"/tags/iterators":{"label":"#iterators","children":{},"count":1},"/tags/reinforcement-learning":{"label":"#reinforcement-learning","children":{},"count":3},"/tags/vector-database":{"label":"#vector-database","children":{},"count":4},"/tags/kernel-programing":{"label":"#kernel-programing","children":{},"count":1},"/tags/solana":{"label":"#solana","children":{},"count":7},"/tags/defi":{"label":"#DeFi","children":{},"count":2},"/tags/anchor":{"label":"#anchor","children":{},"count":2},"/tags/machine-learning":{"label":"#machine-learning","children":{},"count":2},"/tags/containerization":{"label":"#containerization","children":{},"count":4},"/tags/virtualization":{"label":"#virtualization","children":{},"count":4},"/tags/investment":{"label":"#investment","children":{},"count":1},"/tags/personal-finance":{"label":"#personal-finance","children":{},"count":1},"/tags/content":{"label":"#content","children":{},"count":6},"/tags/instructions":{"label":"#instructions","children":{},"count":10},"/tags/guideline":{"label":"#guideline","children":{},"count":14},"/tags/websocket":{"label":"#websocket","children":{},"count":1},"/tags/protocols":{"label":"#protocols","children":{},"count":1},"/tags/dwarves":{"label":"#dwarves","children":{},"count":19},"/tags/radar":{"label":"#radar","children":{},"count":10},"/tags/labs":{"label":"#labs","children":{},"count":25},"/tags/process":{"label":"#process","children":{},"count":10},"/tags/rendering":{"label":"#rendering","children":{},"count":1},"/tags/dom":{"label":"#dom","children":{},"count":3},"/tags/cssom":{"label":"#cssom","children":{},"count":1},"/tags/render-tree":{"label":"#render-tree","children":{},"count":1},"/tags/iframe":{"label":"#iframe","children":{},"count":1},"/tags/postmessage":{"label":"#postmessage","children":{},"count":1},"/tags/mock-service-worker":{"label":"#mock-service-worker","children":{},"count":1},"/tags/api-mocking":{"label":"#api-mocking","children":{},"count":1},"/tags/web-development-tool":{"label":"#web-development-tool","children":{},"count":1},"/tags/engineering":{"label":"#engineering","children":{},"count":55},"/tags/bounty":{"label":"#bounty","children":{},"count":3},"/tags/community":{"label":"#community","children":{},"count":15},"/tags/data-fetching":{"label":"#data-fetching","children":{},"count":1},"/tags/frontend,":{"label":"#frontend,","children":{},"count":1},"/tags/graphql":{"label":"#graphql","children":{},"count":1},"/tags/reactjs":{"label":"#reactjs","children":{},"count":2},"/tags/scroll-driven-animations":{"label":"#scroll-driven-animations","children":{},"count":1},"/tags/animations":{"label":"#animations","children":{},"count":1},"/tags/intersection-observer":{"label":"#intersection-observer","children":{},"count":1},"/tags/nextjs":{"label":"#nextjs","children":{},"count":1},"/tags/server-component":{"label":"#server-component","children":{},"count":1},"/tags/caching-data":{"label":"#caching-data","children":{},"count":1},"/tags/social-networks":{"label":"#social-networks","children":{},"count":1},"/tags/tool":{"label":"#tool","children":{},"count":3},"/tags/practice":{"label":"#practice","children":{},"count":5},"/tags/foundation-model":{"label":"#foundation-model","children":{},"count":1},"/tags/fine-tuning":{"label":"#fine-tuning","children":{},"count":1},"/tags/vector database":{"label":"#vector database","children":{},"count":1},"/tags/shadow-dom":{"label":"#shadow-dom","children":{},"count":1},"/tags/web-api":{"label":"#web-api","children":{},"count":1},"/tags/backend":{"label":"#backend","children":{},"count":5},"/tags/swr-infinite":{"label":"#swr-infinite","children":{},"count":1},"/tags/web-design":{"label":"#web-design","children":{},"count":1},"/tags/tuning-llm":{"label":"#tuning-llm","children":{},"count":2},"/tags/langchain":{"label":"#langchain","children":{},"count":1},"/tags/web":{"label":"#web","children":{},"count":9},"/tags/translation":{"label":"#translation","children":{},"count":1},"/tags/profiling":{"label":"#profiling","children":{},"count":1},"/tags/micro-frontend":{"label":"#micro-frontend","children":{},"count":3},"/tags/architecture":{"label":"#architecture","children":{},"count":4},"/tags/nft":{"label":"#nft","children":{},"count":3},"/tags/state-mangement":{"label":"#state-mangement","children":{},"count":1},"/tags/global-state-management":{"label":"#global-state-management","children":{},"count":1},"/tags/css":{"label":"#css","children":{},"count":5},"/tags/fonts":{"label":"#fonts","children":{},"count":1},"/tags/variable-fonts":{"label":"#variable-fonts","children":{},"count":1},"/tags/state-management":{"label":"#state-management","children":{},"count":2},"/tags/component":{"label":"#component","children":{},"count":1},"/tags/proof-of-knowledge":{"label":"#proof-of-knowledge","children":{},"count":1},"/tags/fronten":{"label":"#fronten","children":{},"count":1},"/tags/hooks":{"label":"#hooks","children":{},"count":2},"/tags/typescript":{"label":"#typescript","children":{},"count":4},"/tags/analytics-tools":{"label":"#analytics-tools","children":{},"count":1},"/tags/analytics-platform":{"label":"#analytics-platform","children":{},"count":1},"/tags/parsing":{"label":"#parsing","children":{},"count":1},"/tags/validation":{"label":"#validation","children":{},"count":1},"/tags/webassembly":{"label":"#webassembly","children":{},"count":1},"/tags/sandbox":{"label":"#sandbox","children":{},"count":1},"/tags/zk-rollup":{"label":"#zk-rollup","children":{},"count":2},"/tags/polygon":{"label":"#polygon","children":{},"count":1},"/tags/starknet":{"label":"#starknet","children":{},"count":1},"/tags/ethereum":{"label":"#ethereum","children":{},"count":2},"/tags/zero-knowledge":{"label":"#zero-knowledge","children":{},"count":1},"/tags/network":{"label":"#network","children":{},"count":2},"/tags/atomic-css":{"label":"#atomic-css","children":{},"count":1},"/tags/client-side":{"label":"#client-side","children":{},"count":1},"/tags/storage":{"label":"#storage","children":{},"count":1},"/tags/frontend/performance":{"label":"#frontend/performance","children":{},"count":2},"/tags/wai-aria":{"label":"#wai-aria","children":{},"count":1},"/tags/accessibility":{"label":"#accessibility","children":{},"count":4},"/tags/polymorphic-component":{"label":"#polymorphic-component","children":{},"count":1},"/tags/threejs":{"label":"#threejs","children":{},"count":1},"/tags/web-performance":{"label":"#web-performance","children":{},"count":2},"/tags/html":{"label":"#html","children":{},"count":4},"/tags/animation":{"label":"#animation","children":{},"count":1},"/tags/zk-proof":{"label":"#zk-proof","children":{},"count":1},"/tags/guides":{"label":"#guides","children":{},"count":1},"/tags/responsive-design":{"label":"#responsive-design","children":{},"count":1},"/tags/hsl":{"label":"#hsl","children":{},"count":1},"/tags/javascript":{"label":"#javascript","children":{},"count":4},"/tags/css-in-js":{"label":"#css-in-js","children":{},"count":1},"/tags/tip":{"label":"#tip","children":{},"count":1},"/tags/dark-mode":{"label":"#dark-mode","children":{},"count":1},"/tags/multisign-wallet":{"label":"#multisign-wallet","children":{},"count":1},"/tags/virtual-dom":{"label":"#virtual-dom","children":{},"count":1},"/tags/native-modules":{"label":"#native-modules","children":{},"count":1},"/tags/vitejs":{"label":"#vitejs","children":{},"count":1},"/tags/esm":{"label":"#esm","children":{},"count":1},"/tags/tutorial":{"label":"#tutorial","children":{},"count":7},"/tags/modules":{"label":"#modules","children":{},"count":1},"/tags/blockchain-bridge":{"label":"#blockchain-bridge","children":{},"count":1},"/tags/foundational-topics":{"label":"#foundational-topics","children":{},"count":5},"/tags/distributed-systems":{"label":"#distributed-systems","children":{},"count":1},"/tags/pos":{"label":"#pos","children":{},"count":1},"/tags/smart-contract":{"label":"#smart-contract","children":{},"count":1},"/tags/atomic-design":{"label":"#atomic-design","children":{},"count":1},"/tags/a11y":{"label":"#a11y","children":{},"count":1},"/tags/useeffect":{"label":"#useeffect","children":{},"count":1},"/tags/token":{"label":"#token","children":{},"count":2},"/tags/report":{"label":"#report","children":{},"count":8},"/tags/radio":{"label":"#radio","children":{},"count":3},"/tags/wasm":{"label":"#wasm","children":{},"count":2},"/tags/rust":{"label":"#rust","children":{},"count":10},"/tags/concurrency":{"label":"#concurrency","children":{},"count":2},"/tags/parallelism":{"label":"#parallelism","children":{},"count":1},"/tags/engineering/frontend":{"label":"#engineering/frontend","children":{},"count":1},"/tags/writing":{"label":"#writing","children":{},"count":1},"/tags/english":{"label":"#english","children":{},"count":1},"/tags/design":{"label":"#design","children":{},"count":31},"/tags/design-thinking":{"label":"#design-thinking","children":{},"count":2},"/tags/macos":{"label":"#macos","children":{},"count":3},"/tags/swift":{"label":"#swift","children":{},"count":7},"/tags/operations":{"label":"#operations","children":{},"count":60},"/tags/project":{"label":"#project","children":{},"count":13},"/tags/data-structures":{"label":"#data-structures","children":{},"count":2},"/tags/software":{"label":"#software","children":{},"count":5},"/tags/vim":{"label":"#vim","children":{},"count":1},"/tags/monitoring":{"label":"#monitoring","children":{},"count":2},"/tags/upptime":{"label":"#upptime","children":{},"count":1},"/tags/consulting":{"label":"#consulting","children":{},"count":22},"/tags/tech-report":{"label":"#tech-report","children":{},"count":11},"/tags/overleaf":{"label":"#overleaf","children":{},"count":1},"/tags/slide":{"label":"#slide","children":{},"count":1},"/tags/office-hours":{"label":"#office-hours","children":{},"count":31},"/tags/ogif":{"label":"#OGIF","children":{},"count":32},"/tags/discord":{"label":"#discord","children":{},"count":32},"/tags/fintech":{"label":"#fintech","children":{},"count":16},"/tags/software-development":{"label":"#software-development","children":{},"count":1},"/tags/database-management":{"label":"#database-management","children":{},"count":1},"/tags/ux-ui":{"label":"#ux-ui","children":{},"count":13},"/tags/forward-engineering":{"label":"#forward-engineering","children":{},"count":14},"/tags/career":{"label":"#career","children":{},"count":39},"/tags/fullstack":{"label":"#fullstack","children":{},"count":2},"/tags/real-time":{"label":"#real-time","children":{},"count":1},"/tags/phoenix-live-view":{"label":"#phoenix-live-view","children":{},"count":1},"/tags/data":{"label":"#data","children":{},"count":14},"/tags/timescaledb":{"label":"#timescaledb","children":{},"count":1},"/tags/finance":{"label":"#finance","children":{},"count":1},"/tags/product-design":{"label":"#product-design","children":{},"count":7},"/tags/checklist":{"label":"#checklist","children":{},"count":17},"/tags/presentation":{"label":"#presentation","children":{},"count":1},"/tags/sql":{"label":"#sql","children":{},"count":4},"/tags/data-modeling":{"label":"#data-modeling","children":{},"count":1},"/tags/data-engineering":{"label":"#data-engineering","children":{},"count":5},"/tags/system-design":{"label":"#system-design","children":{},"count":2},"/tags/etl":{"label":"#etl","children":{},"count":3},"/tags/knowledge":{"label":"#knowledge","children":{},"count":2},"/tags/updates":{"label":"#updates","children":{},"count":14},"/tags/automata":{"label":"#automata","children":{},"count":1},"/tags/mobile":{"label":"#mobile","children":{},"count":3},"/tags/wala":{"label":"#WALA","children":{},"count":4},"/tags/fnb":{"label":"#fnb","children":{},"count":2},"/tags/film":{"label":"#film","children":{},"count":1},"/tags/error":{"label":"#error","children":{},"count":1},"/tags/startup":{"label":"#startup","children":{},"count":9},"/tags/shares":{"label":"#shares","children":{},"count":1},"/tags/founder":{"label":"#founder","children":{},"count":1},"/tags/culture":{"label":"#culture","children":{},"count":12},"/tags/test":{"label":"#test","children":{},"count":1},"/tags/life-at-dwarves, ai-developer, hybrid-work":{"label":"#life-at-dwarves, ai-developer, hybrid-work","children":{},"count":1},"/tags/hybrid-working":{"label":"#hybrid-working","children":{},"count":1},"/tags/guide":{"label":"#guide","children":{},"count":11},"/tags/team":{"label":"#team","children":{},"count":13},"/tags/newsletter":{"label":"#newsletter","children":{},"count":11},"/tags/reward":{"label":"#reward","children":{},"count":1},"/tags/ux":{"label":"#UX","children":{},"count":2},"/tags/directory-structure":{"label":"#directory-structure","children":{},"count":2},"/tags/file-management":{"label":"#file-management","children":{},"count":2},"/tags/file-system":{"label":"#file-system","children":{},"count":2},"/tags/permissions":{"label":"#permissions","children":{},"count":1},"/tags/database-modelling":{"label":"#database-modelling","children":{},"count":1},"/tags/nda":{"label":"#NDA","children":{},"count":1},"/tags/compliance":{"label":"#compliance","children":{},"count":2},"/tags/people":{"label":"#people","children":{},"count":27},"/tags/search":{"label":"#search","children":{},"count":1},"/tags/delivery":{"label":"#delivery","children":{},"count":3},"/tags/reporting":{"label":"#reporting","children":{},"count":1},"/tags/engagement":{"label":"#engagement","children":{},"count":2},"/tags/subscription":{"label":"#subscription","children":{},"count":1},"/tags/pricing":{"label":"#pricing","children":{},"count":1},"/tags/product":{"label":"#product","children":{},"count":1},"/tags/search-engine":{"label":"#search-engine","children":{},"count":1},"/tags/duckdb":{"label":"#duckdb","children":{},"count":3},"/tags/transformers.js":{"label":"#transformers.js","children":{},"count":1},"/tags/hybrid-search":{"label":"#hybrid-search","children":{},"count":1},"/tags/erlang":{"label":"#erlang","children":{},"count":1},"/tags/elixir":{"label":"#elixir","children":{},"count":5},"/tags/fsm":{"label":"#fsm","children":{},"count":1},"/tags/observer-pattern":{"label":"#observer-pattern","children":{},"count":1},"/tags/strategy-design-pattern":{"label":"#strategy-design-pattern","children":{},"count":1},"/tags/guidelines":{"label":"#guidelines","children":{},"count":3},"/tags/mechanism":{"label":"#mechanism","children":{},"count":1},"/tags/local-first":{"label":"#local-first","children":{},"count":1},"/tags/crdt":{"label":"#crdt","children":{},"count":2},"/tags/data-synchronization":{"label":"#data-synchronization","children":{},"count":1},"/tags/data-ownership":{"label":"#data-ownership","children":{},"count":1},"/tags/real-time-collaboration":{"label":"#real-time-collaboration","children":{},"count":1},"/tags/error-handling":{"label":"#error-handling","children":{},"count":1},"/tags/trait":{"label":"#trait","children":{},"count":1},"/tags/data-structure":{"label":"#data-structure","children":{},"count":1},"/tags/bloom-filter":{"label":"#bloom-filter","children":{},"count":1},"/tags/big-o":{"label":"#big-o","children":{},"count":1},"/tags/behavioral-pattern":{"label":"#behavioral-pattern","children":{},"count":1},"/tags/behavior-patterns":{"label":"#behavior-patterns","children":{},"count":2},"/tags/algorithms":{"label":"#algorithms","children":{},"count":1},"/tags/sorting":{"label":"#sorting","children":{},"count":1},"/tags/zettelkasten":{"label":"#zettelkasten","children":{},"count":1},"/tags/amm":{"label":"#amm","children":{},"count":1},"/tags/prompt":{"label":"#prompt","children":{},"count":1},"/tags/chatgpt":{"label":"#chatgpt","children":{},"count":1},"/tags/memo":{"label":"#memo","children":{},"count":3},"/tags/ops":{"label":"#ops","children":{},"count":2},"/tags/workflow":{"label":"#workflow","children":{},"count":4},"/tags/recording":{"label":"#recording","children":{},"count":1},"/tags/history":{"label":"#history","children":{},"count":1},"/tags/creational-design-pattern":{"label":"#creational-design-pattern","children":{},"count":1},"/tags/software-design":{"label":"#software-design","children":{},"count":2},"/tags/software-architecture":{"label":"#software-architecture","children":{},"count":3},"/tags/graphical-notation":{"label":"#graphical-notation","children":{},"count":2},"/tags/techecosystem":{"label":"#techecosystem","children":{},"count":1},"/tags/summit":{"label":"#summit","children":{},"count":1},"/tags/energy":{"label":"#energy","children":{},"count":1},"/tags/crypto":{"label":"#crypto","children":{},"count":1},"/tags/overview":{"label":"#overview","children":{},"count":1},"/tags/dfg":{"label":"#dfg","children":{},"count":6},"/tags/standardization":{"label":"#standardization","children":{},"count":1},"/tags/work-adoption":{"label":"#work-adoption","children":{},"count":1},"/tags/code of conduct":{"label":"#code of conduct","children":{},"count":1},"/tags/research":{"label":"#research","children":{},"count":3},"/tags/field-notes":{"label":"#field-notes","children":{},"count":1},"/tags/innovation":{"label":"#innovation","children":{},"count":2},"/tags/communications":{"label":"#communications","children":{},"count":3},"/tags/brain":{"label":"#brain","children":{},"count":1},"/tags/knowledge-base":{"label":"#knowledge-base","children":{},"count":1},"/tags/engineering/data":{"label":"#engineering/data","children":{},"count":5},"/tags/data-pipeline":{"label":"#data-pipeline","children":{},"count":1},"/tags/payment":{"label":"#payment","children":{},"count":2},"/tags/partners":{"label":"#partners","children":{},"count":1},"/tags/cybersecurity":{"label":"#cybersecurity","children":{},"count":2},"/tags/serverless":{"label":"#serverless","children":{},"count":1},"/tags/life-at-dwarves, community-contributor, techie-project":{"label":"#life-at-dwarves, community-contributor, techie-project","children":{},"count":1},"/tags/htmx":{"label":"#htmx","children":{},"count":2},"/tags/brainery":{"label":"#brainery","children":{},"count":2},"/tags/devops":{"label":"#devops","children":{},"count":6},"/tags/google-cloud":{"label":"#google-cloud","children":{},"count":1},"/tags/google-data-studio":{"label":"#google-data-studio","children":{},"count":1},"/tags/google-data-fusion":{"label":"#google-data-fusion","children":{},"count":1},"/tags/reliability":{"label":"#reliability","children":{},"count":2},"/tags/cdap":{"label":"#cdap","children":{},"count":1},"/tags/google-dataproc":{"label":"#google-dataproc","children":{},"count":1},"/tags/hadoop":{"label":"#hadoop","children":{},"count":2},"/tags/streaming":{"label":"#streaming","children":{},"count":1},"/tags/life-at-dwarves, alumni, career-growth":{"label":"#life-at-dwarves, alumni, career-growth","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, continuous-learning":{"label":"#life-at-dwarves, backend-engineer, continuous-learning","children":{},"count":1},"/tags/ecommerce":{"label":"#ecommerce","children":{},"count":2},"/tags/dropshipping":{"label":"#dropshipping","children":{},"count":1},"/tags/work":{"label":"#work","children":{},"count":14},"/tags/internal":{"label":"#internal","children":{},"count":11},"/tags/discussion":{"label":"#discussion","children":{},"count":6},"/tags/event":{"label":"#event","children":{},"count":6},"/tags/catchup":{"label":"#catchup","children":{},"count":5},"/tags/home":{"label":"#home","children":{},"count":1},"/tags/tauri":{"label":"#tauri","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, community-building":{"label":"#life-at-dwarves, backend-engineer, community-building","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, personal-development":{"label":"#life-at-dwarves, backend-engineer, personal-development","children":{},"count":1},"/tags/engineer":{"label":"#engineer","children":{},"count":1},"/tags/estimation":{"label":"#estimation","children":{},"count":1},"/tags/code-generation":{"label":"#code-generation","children":{},"count":1},"/tags/typesafe":{"label":"#typesafe","children":{},"count":1},"/tags/internship":{"label":"#internship","children":{},"count":2},"/tags/life-at-dwarves, backend-engineer, teamwork":{"label":"#life-at-dwarves, backend-engineer, teamwork","children":{},"count":1},"/tags/workshop":{"label":"#workshop","children":{},"count":1},"/tags/demo":{"label":"#demo","children":{},"count":1},"/tags/webrtc":{"label":"#webrtc","children":{},"count":1},"/tags/video-streaming":{"label":"#video-streaming","children":{},"count":1},"/tags/performance-review":{"label":"#performance-review","children":{},"count":1},"/tags/assessment":{"label":"#assessment","children":{},"count":1},"/tags/tech-radar":{"label":"#tech-radar","children":{},"count":1},"/tags/evaluating-tech":{"label":"#evaluating-tech","children":{},"count":1},"/tags/life-at-dwarves, product-executive, personal-growth":{"label":"#life-at-dwarves, product-executive, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-member":{"label":"#life-at-dwarves, frontend-engineer, community-member","children":{},"count":1},"/tags/distributed-system":{"label":"#distributed-system","children":{},"count":1},"/tags/data-types":{"label":"#data-types","children":{},"count":1},"/tags/life-at-dwarves, communication-specialist, remote-work":{"label":"#life-at-dwarves, communication-specialist, remote-work","children":{},"count":1},"/tags/life-at-dwarves, qa-engineer, mentorship":{"label":"#life-at-dwarves, qa-engineer, mentorship","children":{},"count":1},"/tags/life-at-dwarves, qa-engineer, quality-standards":{"label":"#life-at-dwarves, qa-engineer, quality-standards","children":{},"count":1},"/tags/guidline":{"label":"#guidline","children":{},"count":1},"/tags/budgeting":{"label":"#budgeting","children":{},"count":3},"/tags/client":{"label":"#client","children":{},"count":1},"/tags/framework":{"label":"#framework","children":{},"count":6},"/tags/learning":{"label":"#learning","children":{},"count":3},"/tags/system design":{"label":"#system design","children":{},"count":1},"/tags/playbook":{"label":"#playbook","children":{},"count":3},"/tags/life-at-dwarves, backend-engineer, learning-culture":{"label":"#life-at-dwarves, backend-engineer, learning-culture","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, mentorship":{"label":"#life-at-dwarves, backend-engineer, mentorship","children":{},"count":3},"/tags/life-at-dwarves":{"label":"#life-at-dwarves","children":{},"count":2},"/tags/backend-engineer":{"label":"#backend-engineer","children":{},"count":1},"/tags/remote-work":{"label":"#remote-work","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-building":{"label":"#life-at-dwarves, frontend-engineer, community-building","children":{},"count":1},"/tags/australia":{"label":"#australia","children":{},"count":1},"/tags/sargable-queries":{"label":"#sargable-queries","children":{},"count":1},"/tags/zookeeper":{"label":"#zookeeper","children":{},"count":1},"/tags/kafka":{"label":"#kafka","children":{},"count":1},"/tags/sequential-reads":{"label":"#sequential-reads","children":{},"count":1},"/tags/sequential-writes":{"label":"#sequential-writes","children":{},"count":1},"/tags/random-reads":{"label":"#random-reads","children":{},"count":1},"/tags/random-writes":{"label":"#random-writes","children":{},"count":1},"/tags/url-redirect":{"label":"#url-redirect","children":{},"count":1},"/tags/url-rewrite":{"label":"#url-rewrite","children":{},"count":1},"/tags/http":{"label":"#http","children":{},"count":1},"/tags/seo":{"label":"#SEO","children":{},"count":1},"/tags/life-at-dwarves, frontend-engineer, community-learning":{"label":"#life-at-dwarves, frontend-engineer, community-learning","children":{},"count":1},"/tags/life-at-dwarves, engineer, work-culture":{"label":"#life-at-dwarves, engineer, work-culture","children":{},"count":1},"/tags/dx":{"label":"#dx","children":{},"count":1},"/tags/life-at-dwarves, software-engineer, mentorship":{"label":"#life-at-dwarves, software-engineer, mentorship","children":{},"count":1},"/tags/machine learning":{"label":"#machine learning","children":{},"count":1},"/tags/r\u0026d":{"label":"#r\u0026d","children":{},"count":1},"/tags/technique":{"label":"#technique","children":{},"count":9},"/tags/vietnam":{"label":"#vietnam","children":{},"count":1},"/tags/write-heavy":{"label":"#write-heavy","children":{},"count":1},"/tags/inventory-platform":{"label":"#inventory-platform","children":{},"count":1},"/tags/scalability":{"label":"#scalability","children":{},"count":1},"/tags/doordash":{"label":"#doordash","children":{},"count":1},"/tags/low-latency":{"label":"#low-latency","children":{},"count":1},"/tags/sharing":{"label":"#sharing","children":{},"count":1},"/tags/management":{"label":"#management","children":{},"count":12},"/tags/teamwork":{"label":"#teamwork","children":{},"count":1},"/tags/multi-column-index":{"label":"#multi-column-index","children":{},"count":1},"/tags/index":{"label":"#index","children":{},"count":1},"/tags/composite-index":{"label":"#composite-index","children":{},"count":1},"/tags/components":{"label":"#components","children":{},"count":1},"/tags/scrum":{"label":"#scrum","children":{},"count":2},"/tags/technicaldebt":{"label":"#technicaldebt","children":{},"count":1},"/tags/projectmanagement":{"label":"#projectmanagement","children":{},"count":1},"/tags/email":{"label":"#email","children":{},"count":22},"/tags/decoder":{"label":"#decoder","children":{},"count":1},"/tags/json":{"label":"#json","children":{},"count":1},"/tags/materialized-view":{"label":"#materialized-view","children":{},"count":1},"/tags/data-warehouse":{"label":"#data-warehouse","children":{},"count":1},"/tags/mapreduce":{"label":"#mapreduce","children":{},"count":1},"/tags/distributed":{"label":"#distributed","children":{},"count":3},"/tags/form":{"label":"#form","children":{},"count":1},"/tags/uilibraries":{"label":"#uilibraries","children":{},"count":1},"/tags/migrations":{"label":"#migrations","children":{},"count":1},"/tags/agile":{"label":"#agile","children":{},"count":6},"/tags/behavior-driven-development":{"label":"#behavior-driven-development","children":{},"count":1},"/tags/ubiquitous-language":{"label":"#ubiquitous-language","children":{},"count":1},"/tags/forward-proxy":{"label":"#forward-proxy","children":{},"count":1},"/tags/apprenticeship":{"label":"#apprenticeship","children":{},"count":3},"/tags/life-at-dwarves, apprenticeship, backend-engineer":{"label":"#life-at-dwarves, apprenticeship, backend-engineer","children":{},"count":1},"/tags/remote":{"label":"#remote","children":{},"count":7},"/tags/showcase":{"label":"#showcase","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, golang":{"label":"#life-at-dwarves, backend-engineer, golang","children":{},"count":1},"/tags/life-at-dwarves, operations, techie-story":{"label":"#life-at-dwarves, operations, techie-story","children":{},"count":1},"/tags/life-at-dwarves, devops-engineer, personal-growth":{"label":"#life-at-dwarves, devops-engineer, personal-growth","children":{},"count":1},"/tags/life-at-dwarves, senior-engineer, mentorship":{"label":"#life-at-dwarves, senior-engineer, mentorship","children":{},"count":1},"/tags/quant":{"label":"#quant","children":{},"count":1},"/tags/life-at-dwarves, data-engineer, remote-work":{"label":"#life-at-dwarves, data-engineer, remote-work","children":{},"count":1},"/tags/apprentice":{"label":"#apprentice","children":{},"count":1},"/tags/life-at-dwarves, ui-designer, design-communication":{"label":"#life-at-dwarves, ui-designer, design-communication","children":{},"count":1},"/tags/life-at-dwarves, engineering-manager, mentorship":{"label":"#life-at-dwarves, engineering-manager, mentorship","children":{},"count":1},"/tags/meeting":{"label":"#meeting","children":{},"count":3},"/tags/us":{"label":"#us","children":{},"count":4},"/tags/funding":{"label":"#funding","children":{},"count":2},"/tags/ventures":{"label":"#ventures","children":{},"count":3},"/tags/mbti":{"label":"#mbti","children":{},"count":6},"/tags/intj":{"label":"#intj","children":{},"count":1},"/tags/istp":{"label":"#istp","children":{},"count":1},"/tags/estj":{"label":"#estj","children":{},"count":1},"/tags/istj":{"label":"#istj","children":{},"count":1},"/tags/personalities":{"label":"#personalities","children":{},"count":1},"/tags/early-stage":{"label":"#early-stage","children":{},"count":3},"/tags/healthcare":{"label":"#healthcare","children":{},"count":1},"/tags/browser-extension":{"label":"#browser-extension","children":{},"count":2},"/tags/git":{"label":"#git","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, growth-mindset":{"label":"#life-at-dwarves, software-engineer, growth-mindset","children":{},"count":1},"/tags/life-at-dwarves, backend-engineer, career-change":{"label":"#life-at-dwarves, backend-engineer, career-change","children":{},"count":1},"/tags/marketplace":{"label":"#marketplace","children":{},"count":2},"/tags/real-estate":{"label":"#real-estate","children":{},"count":1},"/tags/nocode":{"label":"#nocode","children":{},"count":1},"/tags/hospitality":{"label":"#hospitality","children":{},"count":1},"/tags/ride-hailing":{"label":"#ride-hailing","children":{},"count":1},"/tags/iot":{"label":"#iot","children":{},"count":1},"/tags/tips":{"label":"#tips","children":{},"count":6},"/tags/partnership":{"label":"#partnership","children":{},"count":1},"/tags/travel":{"label":"#travel","children":{},"count":1},"/tags/purpose":{"label":"#purpose","children":{},"count":2},"/tags/delegate":{"label":"#delegate","children":{},"count":2},"/tags/transparency":{"label":"#transparency","children":{},"count":1},"/tags/event-sourcing":{"label":"#event-sourcing","children":{},"count":1},"/tags/sdlc":{"label":"#sdlc","children":{},"count":1},"/tags/pm":{"label":"#pm","children":{},"count":2},"/tags/life-at-dwarves, frontend-engineer, design-engineering":{"label":"#life-at-dwarves, frontend-engineer, design-engineering","children":{},"count":1},"/tags/modeling":{"label":"#modeling","children":{},"count":2},"/tags/life-at-dwarves, software-engineer, engineering-values":{"label":"#life-at-dwarves, software-engineer, engineering-values","children":{},"count":1},"/tags/burn-out":{"label":"#burn-out","children":{},"count":1},"/tags/goal":{"label":"#goal","children":{},"count":2},"/tags/license":{"label":"#license","children":{},"count":1},"/tags/operation":{"label":"#operation","children":{},"count":4},"/tags/template":{"label":"#template","children":{},"count":20},"/tags/k8s":{"label":"#k8s","children":{},"count":1},"/tags/js":{"label":"#js","children":{},"count":2},"/tags/clojure":{"label":"#clojure","children":{},"count":1},"/tags/react.js":{"label":"#react.js","children":{},"count":2},"/tags/employee":{"label":"#employee","children":{},"count":1},"/tags/onboarding":{"label":"#onboarding","children":{},"count":1},"/tags/assets":{"label":"#assets","children":{},"count":1},"/tags/marketing":{"label":"#marketing","children":{},"count":1},"/tags/human-resource":{"label":"#human-resource","children":{},"count":1},"/tags/dcos":{"label":"#dcos","children":{},"count":5},"/tags/okr":{"label":"#okr","children":{},"count":1},"/tags/oss":{"label":"#oss","children":{},"count":1},"/tags/policy":{"label":"#policy","children":{},"count":1}}}},"content":"\u003ch3 id=\"topics-and-highlights\"\u003eTopics and Highlights\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSession setup \u0026#x26; check-in\u003c/strong\u003e: Kicked off with a casual vibe, confirming no all-hands this week and setting up three talks. Phát skipped his slot, and the team troubleshooted screen sharing for demos.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAI fine-tuning overview\u003c/strong\u003e: Explored fine-tuning vs. retraining, using a doctor’s note example to show how fine-tuning embeds knowledge while retraining leans on token-heavy prompts.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFine-tuning demo\u003c/strong\u003e: Showcased a Duty 40 Mini fine-tuning job on Open AI (~4800 tokens), comparing pre- and post-tuning results, with a nod to local vs. hosted model trade-offs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData archiving essentials\u003c/strong\u003e: Biên broke down archiving vs. backup for apps with 50K-1M daily transactions, focusing on metadata, cloud storage, and recovery to optimize query performance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchiving tools \u0026#x26; Q\u0026#x26;A\u003c/strong\u003e: Highlighted tools like AWS, Google Cloud, and Timescale, plus hot/warm/cold storage options (Azure, Backblaze), with audience questions on scheduling and platform quirks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDatalake foundations\u003c/strong\u003e: An traced datalakes from 1980s databases to today’s cloud systems, contrasting warehouse ETL (structured) with datalake ELT (raw data) workflows.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNotion’s datalake scaling\u003c/strong\u003e: Detailed Notion’s growth to 96 instances and 400+ shards by 2023, shifting from warehouse to datalake with Debezium CDC, Kafka, Hudi, and S3 for analytics.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInteractive wrap-up\u003c/strong\u003e: Fielded questions on datalake vs. replication, async processing, and external data handling (e.g., social media), ending with reflections on big data skillset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"vietnamese-transcript\"\u003eVietnamese transcript\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e[00:00]\u003c/strong\u003e Hình như tuần sau mới có all-hand. Không thấy ai tạo event gì hết, chắc tuần này cứ bình thường thôi nhá. Anh em kiểm tra xem màn hình sharing có vấn đề gì không. Hay lên luôn nhỉ? Hôm nay chắc có ba bài thôi đâu đó. Phát vừa bảo tuần này cậu không có gì mới, chắc skip hôm nay rồi. Anh em thử share màn hình cá nhân xem sao nào. Xem trước được không?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[10:42]\u003c/strong\u003e Theo lịch chắc anh nhỉ, để em lên trước nhá. Fine-tuning này, chủ đề này không mới lắm đâu. Bài này chỉ là 100.5 thôi, không phải 101, nên chỉ giới thiệu sơ sơ, chưa đi sâu được đâu. Tại em cũng mù mờ lắm, nên chắc giới thiệu sơ vậy thôi. Hôm nay em giới thiệu bài fine-tuning. Đây là agenda của bài này nè.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[12:24]\u003c/strong\u003e Introduction là nếu mọi người dùng AI, chắc có nghe tới khái niệm fine-tuning rồi. AI có mấy mô hình đa số được fit vào dữ liệu từ một ngày nào đó, với mấy cái data privacy hoặc data của domain riêng. Dữ liệu này không xuất hiện trong knowledge của mô hình nền tảng (foundation model). Để mô hình có được kiến thức đó, người ta thường dùng retraining, đúng không? Nhưng còn một cách khác gọi là fine-tuning. Cuối bài, em sẽ so sánh hai cách này, xem lúc nào nên dùng cái nào, lúc nào không.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[13:08]\u003c/strong\u003e Trước mắt, cứ hiểu fine-tuning như cách để mở rộng kiến thức cho mô hình nền tảng vậy. Fine-tuning là gì? Hiểu đơn giản là mọi người retrain lại mô hình, lấy một mô hình nền, đưa vào một dataset gì đó để fine-tune, nghĩa là retrain lại nó. Sau khi fine-tune xong, ta được một mô hình đã điều chỉnh, gọi là fine-tuned model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[13:44]\u003c/strong\u003e Tại sao fine-tuning mang được kiến thức mới? Hiểu đơn giản là trong một AI model, kiến thức được lưu qua mấy cái mạng nơ-ron. Fine-tuning sẽ cập nhật các weight, các thông số của mạng nơ-ron đó, để nó phù hợp với kiến thức mới. Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[14:19]\u003c/strong\u003e Khi ném kiến thức mới vào, mấy cái weight thay đổi, lúc này mô hình đã được cập nhật kiến thức rồi. Khi fine-tune mô hình trong thực tế, không phải chỉ ném dataset vào rồi retrain là xong. Đúng là ra một mô hình fine-tuned, nhưng không biết cái mô hình sau retrain này có tốt hay không. Em sẽ giới thiệu một workflow mà bên ngoài thường dùng để fine-tune.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[14:59]\u003c/strong\u003e Cái flow này, nó gồm nhiều bước như thế này. Mọi người có thể chia thành hai cụm, hai cụm nhá. Cái flow này, em sẽ chia thành hai cụm. Em đi qua cụm một trước. Cụm đầu tiên là cụm ở bên trái, hiểu đơn giản là một base model ban đầu. Sau đó, mọi người có một dataset mới, một cái gì đó mới, mọi người quăng vô, fine-tune nó. Rồi nó ra một model, mọi người sẽ supervise nó, có nghĩa là mọi người retrain nó dưới kiểu là retrain nó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[15:38]\u003c/strong\u003e Sẽ cho nó thêm kiến thức, với input này thì output sẽ ra như này. Nó sẽ ra một cái gọi là supervised fine-tuning. Sau đó, để em đi qua phần tiếp. OK, cái fine-tuning này là retrain on data, nghĩa là sẽ cho một cặp input-output trong dataset để nó học. Nó sẽ học được những kiến thức mới đó. Để bước này hoàn hảo, dataset phải được clean. Nó phải clean, không được lẫn với những cái khác. Nghĩa là nó phải specific cho cái domain mà mình muốn train nó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[16:31]\u003c/strong\u003e Quay lại hình này, sau khi mọi người có một cái model đã được retrain xong, mọi người mang lên production, mọi người dùng, đúng không? Lúc này, bên ngoài sẽ sử dụng một cái system gọi là human feedback. Kiểu như là response của model này có làm bạn hài lòng không, chấm từ 1 tới N sao, kiểu vậy á. Mọi người sẽ collect cái data đó. Nó nằm ở bước này, mọi người sẽ thu thập human feedback từ cái retrained model của mọi người.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[17:06]\u003c/strong\u003e Dựa vào cái feedback đó, mọi người gọi bước này hơi tốn tài nguyên chút. Mọi người sẽ phải retrain một cái model riêng. Cái model này dùng để đánh giá xem response này được chấm bao nhiêu điểm. Kế đó, mọi người tới bước thứ ba, bước cuối. Ở bước này, mọi người sẽ dùng thuật toán như reinforcement learning để kết hợp với cái retrained model và cái reward model của mọi người.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[17:46]\u003c/strong\u003e Mọi người retrain, mọi người lại fine-tune cái model một lần nữa. Nó sẽ ra cho mọi người một cái gọi là model tối ưu. Cái vòng lặp này cứ tiếp tục, tiếp tục mãi. Mọi người có cái model đã retrain xong, thu thập human feedback, rồi kết hợp ba cái đó để retrain cái model thêm lần nữa. Càng ngày, cái model sẽ càng ok hơn với những gì mà mình muốn. Đó là cái flow mà em thấy bên ngoài, trong production, người ta hay dùng.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[18:24]\u003c/strong\u003e Trong quá trình fine-tuning, ,ọi người sẽ thường nghe tới khái niệm gọi là catastrophic forgetting. Nghĩa là sao? Nghĩa là khi mọi người retrain kiến thức mới vào, nó sẽ làm giảm performance với những kiến thức cũ. Tại sao chuyện này xảy ra? Như em đã nói, kiến thức của một model dựa vào mấy cái weight, dựa vào kiến trúc của cái model đó và những tham số của nó. Tham số dynamic trong một model là mấy cái weight. Khi mọi người retrain, mấy cái weight này thay đổi, đúng không?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[19:00]\u003c/strong\u003e Khi nó thay đổi, có phải là kiến thức cũ sẽ bị giảm bớt độ chính xác đi không? Nếu trong dataset của mọi người có nhiều dữ liệu bị overfitting, nghĩa là dataset của mọi người quá đúng, quá đúng trong cái dataset đó. Khi một người quăng cái gì mới vào, nó sẽ sai với những cái cũ đi. Người ta gọi đó là overfitting, nghĩa là nó bị fold in quá mức vào những cái training data. Khi gặp data mới, nó sẽ giảm performance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[19:42]\u003c/strong\u003e Nên lúc này, bên ngoài người ta sử dụng một kỹ thuật gọi là parameter-efficient fine-tuning, gọi là PEFT. Nó có nhiều cách, nhiều kỹ thuật trong method này, như LoRA này kia. Nhưng trung quy, đa số bọn họ không phải update hết tất cả các weight trong cái model đó. Bọn họ sẽ chỉ đóng băng những layer nào không cần thiết. Họ sẽ đóng băng mấy cái layer không cần thiết, rồi chỉ update một số lượng nhất định các weight thôi. Để tránh trường hợp kiến thức cũ bị mất đi quá nhiều. Đó là cái cơ bản. Còn sâu hơn về mấy cái algorithm đằng sau, mọi người có thể tự tìm hiểu.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[20:27]\u003c/strong\u003e Quay lại câu hỏi lúc ban đầu, ha, nó với retraining khác nhau thế nào, nên dùng cái nào? Có cái bảng đây, mọi người có thể dễ dàng nhận ra. Retraining là dữ liệu phụ thuộc vào database của mọi người. Cứ quăng vào, quăng vào, lúc nào data cũng được update liên tục. Còn fine-tuning là mọi người retrain lại model, nên lúc nào data cũng chỉ ở cái chỗ mà mọi người đã retrain thôi.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[21:16]\u003c/strong\u003e Kế tiếp là customize and learning style. Nghĩa là cái retraining, mục đích của nó là cho mình một cái knowledge base để mình lấy mấy cái knowledge base đó ra tham chiếu, sử dụng. Còn fine-tuning thì sao? Nó upgrade cái não của model lên, để nó có sẵn cái knowledge đó luôn. Còn mấy cái ở dưới thì chắc mọi người tự tìm hiểu tiếp ha.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[21:57]\u003c/strong\u003e Em có một cái ví dụ như vậy. Ví dụ như là mọi người muốn làm một cái system để giải thích những cái note của bác sĩ, đúng không? Những cái note của bác sĩ, mọi người có thể biết là những cái note của bác sĩ nó có rất là nhiều từ chuyên ngành. Và những từ chuyên ngành đó nó còn viết tắt, viết kiểu luộm thuộm nữa.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[22:44]\u003c/strong\u003e Nếu mọi người sử dụng fine-tuning á, mọi người sẽ cho nó học hết tất cả những cái kiến thức luộm thuộm, những cái shorthand, những cái handwriting đó của bác sĩ. Nên khi mọi người input một cái note của bác sĩ vô, nó sẽ trả lời được rất đúng. Còn nếu mọi người dùng retraining á, khi mọi người input một cái note của bác sĩ vô, nó sẽ kiếm được những cái relevant data, mang ra đọc. Nhưng bản chất là cái model nó không hiểu được những từ đó, nên nó cũng sẽ không đưa cho mọi người một câu trả lời chính xác.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[23:16]\u003c/strong\u003e Mọi người có thể hiểu như này: fine-tuning là mình nhờ một bác sĩ đọc một cái note của bác sĩ. Còn retraining là mọi người đưa cho một người có kiến thức rất rộng đọc một cái note của bác sĩ. Người đó có thể kiến thức rất rộng, nhưng về mấy cái chuyên ngành, mấy cái chuyên ngành thật sự, thì nó không đủ sâu như của một bác sĩ thực thụ. Nên độ chính xác sẽ không cao.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[23:57]\u003c/strong\u003e Thứ hai, mọi người có thể nói, bây giờ với retraining, mình dùng một cái system prompt để list hết mấy cái shorthand của bác sĩ ra trong system prompt, nó sẽ tự hiểu thôi. Nhưng làm vậy, mọi người sẽ bị tốn token, đúng không? Tại vì khi mọi người dùng retraining, mọi người lấy hết cái retraining data ra, quăng một cái knowledge retraining vô, lại cộng thêm đống cái zero-shot, mấy cái description, mấy cái đi kèm theo nó trong một cái prompt á, thì nó rất tốn token.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[24:34]\u003c/strong\u003e Và khi ở trong một cái long conversation với một cái model, nó đâu phải chỉ dựa vào câu hỏi của mình đâu. Nó sẽ dựa vào tất cả các cuộc trò chuyện từ trước tới giờ của mình mà nó trả lời cho mình. Lúc này, nó sẽ dẫn tới trường hợp là nó bị limit bởi token. Đó là cái drawback khi sử dụng retraining, là nó sẽ tốn token. Tại vì mọi người cần token để chạy cái system prompt của mọi người nữa. Còn fine-tuning, bản chất là model nó đã có kiến thức rồi, nên không cần phải có system prompt.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[25:14]\u003c/strong\u003e Đó là sơ qua về fine-tuning. Chắc có cái demo cho mọi người xem sẽ rõ hơn ha. Bây giờ em sẽ fine-tune một cái model là Duty 40 Mini ha. Em có một cái dataset như này. Ừ, như này thì mỗi thứ nó sẽ có một cái system như retraining, rồi user hỏi cái này thì muốn nó trả lời vậy, đúng không? Em cộng 10 cái, 10 record trong cái dataset này, em sẽ fine-tune nó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[26:13]\u003c/strong\u003e Trước khi fine-tune, em sẽ cho nó chạy qua một đoạn code để em estimate được. Tại vì em dùng Open AI, nên sẽ tốn tiền. Nên mình sẽ tính được estimate là nó sẽ charge mình bao nhiêu. Em dùng xong, khúc cuối nó sẽ kiểu, tầm khoảng 4800, sắp xỉ 4800 token. Cái này chỉ là tham khảo thôi, nhưng em thấy nó cũng đúng. Sau đó, em sẽ upload cái file data này lên Open AI. Nó sẽ cho em cái file ở trên cái Open AI của em.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[27:03]\u003c/strong\u003e Rồi em sẽ training nó. Em sẽ tạo một cái fine-tuning job. Lúc này, ở trên Open AI, nó sẽ chạy một cái job này. Mọi người có thể lên đây, mọi người đọc, mọi người quan sát. Nó sẽ không trả kết quả liền, nó sẽ tạo một cái job để pending ra đó, để trên Open AI nó fine-tune cho mình. Trong lúc chờ, mình có thể theo dõi quá trình của nó như thế nào. Sau khi xong đâu rồi, nó sẽ thông báo cho mình. Mình cứ stamp cái câu này, cứ check cái câu này để coi nó đã hoàn thành hay chưa. Mình đọc ở cái chỗ đó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[27:58]\u003c/strong\u003e Sau khi xong, nó sẽ cho mình mấy cái result status. Sau khi fine-tune xong, với cùng một câu hỏi, ví dụ đây là cái câu hỏi em sử dụng, em dùng câu hỏi này. Cái câu hỏi này gần giống với một cái record trong đống dataset của em. Sau khi em chạy, nó sẽ trả lời như vậy. Nhưng trước khi fine-tune, em dùng một cái model bình thường nha, model bình thường thì nó sẽ trả lời kiểu vậy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[28:49]\u003c/strong\u003e Có nghĩa là em fine-tune thì nó đã thành công. Đó là cái cách em sử dụng Open AI để fine-tune một cái model. Demo của em tới đây thôi. Anh em có câu hỏi gì không? Đúng rồi, cái này demo em xài tuning chứ để tự fine-tune bằng local mà xịn xịn thì chắc không đủ đồ. Dạ, đồ ngon nhõ thôi. Thực ra có mấy cái model trước, tô nó trên LoRA các thứ, cũng có thể demo được. Nhưng tô không, bài này easy, bài này kiểu một...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[29:47]\u003c/strong\u003e Lẻ tẻ trầm mấy á. Đúng, chắc cũng ổn mà. Nói chung, những đội enterprise hay không muốn tốn thời gian xây dựng GPU thì sẽ dùng cách này. Diagram GPT hồi trước, GPT-4o Mini ra thì fine-tuning đã miễn phí, dùng cái này cũng tiện lợi cho họ. Cái cửa hàng demo cho anh em là sử dụng một cái như kiểu service ấy. Open AI cung cấp service fine-tuning, đưa lên mấy cái model của nó luôn. Mình pick mấy cái model, chắc là pick model mini á. Chắc chi phí nó không cao lắm.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[30:37]\u003c/strong\u003e Đấy cũng là một cách. Nhưng vấn đề thực ra là mình vẫn không phải người own cái model đấy. Bản chất là vẫn host ở trên server của họ. Còn có một cách khác là tự build server và tự running. Trường hợp hôm nay đã khác. Anh em xem, hôm qua em có thử một cái model có 3 billion parameter thôi. Nhưng nó chạy hai ba tiếng, nó chưa xong đâu anh. Thực ra bài này, cái version nó đơn giản hơn một cái bây giờ, nhỏ hơn của bài trước. đầu.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[31:26]\u003c/strong\u003e Nó là một cái full flow liên quan đến gì ta, reinforcement feedback. Ý là cái em giới thiệu ở bên ngoài production á, là khi người ta tuning á. Người ta không phải chỉ fine-tune xong là dùng liền, người ta phải đánh giá lại coi nó có đúng không. Người ta phải cho nó vô cái cycle để càng cải tiến cái fine-tuned model nữa, kiểu vậy. Đây là một cái flow như vậy. Bản chất nó cũng model thôi, đâu có gì đâu. Quan trọng là mọi người biết được những cái cost để đánh giá cái approach thôi.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[32:05]\u003c/strong\u003e Tại ra nó vẫn là bài toán accuracy, đúng không? Mình chọn cách nào để làm cái output nó chính xác hơn. Những cái method như retraining hay fine-tuning, nó sẽ có những nhược điểm khác nhau. Và thực ra kể cả fine-tuning, nó cũng có nhiều method fine-tuning khác nhau. Chắc là cần đi sâu hơn để xác định mấy cái đó. Cái này vẫn hơi general. Chắc vậy, Hoàng. Nếu có điều kiện thì chắc đi sâu hơn tí nữa.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[32:48]\u003c/strong\u003e Sâu hơn theo kiểu là có mấy cái method liên quan đến phần retraining các thứ. Sử dụng fine-tuning method á, có một số cái method nó tương đối tiết kiệm về mặt tài nguyên. Tất nhiên, nó sẽ đánh đổi với một số thứ khác, kiểu vậy. Giới thiệu cái đó để anh em xem thử đâu đó. Mọi người hỏi, Đạt hỏi là khi nào cần fine-tuning. Nói là fine-tuning cần khi mà mọi người muốn nó có một cái kiến thức, một cái specific topic nào đó. Mọi người có thể cân nhắc sử dụng fine-tuning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[33:39]\u003c/strong\u003e Nhưng trong tất cả trường hợp, em thấy bên ngoài, đa số mọi người sẽ prefer dùng retraining. Tại vì nó dễ và tốn ít tài nguyên hơn. Nhưng một số trường hợp như lúc này, cái ví dụ em nói về cái note của bác sĩ á, suppose là nên dùng tuning. Rồi tùy cái kiến trúc, tùy cái mình chia system của mình ra nhiều system nhỏ, system nhỏ nó như thế nào nữa, tùy. Có thể có một vài cái use case như kiểu chúng nó muốn host mấy cái model bé bé, model bé chẳng hạn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[34:18]\u003c/strong\u003e Chỉ kiểu dành để làm một cái task cụ thể thôi. Ví dụ như phân tích thời tiết, độ ẩm các thứ để perform cái action nào đấy. Ví dụ như thay đổi cái theme của điện thoại hay để chỉ action nào đấy chẳng hạn. Có thể retrain cái model bé bé để chỉ cần làm chuyện đó thôi, không cần phải cần network các thứ gì cả. Chắc vậy. Từ giờ chắc là Biên ha?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[36:13]\u003c/strong\u003e Dụng cái và build cái recovery process cho nó. Chi tiết như nào thì nó sẽ có một vài phần chính. Trước tiên là cái lý do mà mình cần cái kỹ thuật này và so sánh nó với một cái quen thuộc hơn là backup. Sau đó là đi vào việc để mình build và những cái mình cần để ý, những cái gì. Đầu tiên là trên thực tế, thường có những tổ chức, những công ty mà chạy những cái app với lưu lượng dữ liệu cao á. Ví dụ như giao dịch chứng khoán này nọ. Như em ví dụ này là kiểu 50.000 transaction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[37:12]\u003c/strong\u003e Như em ví dụ này là kiểu 50.000 mỗi ngày là ít á, kiểu vọt lên 500.000, triệu transaction mỗi ngày. Sau khoảng thời gian, cái lượng data nó sẽ phồng lên rất rất lớn, ảnh hưởng đến cái việc mà mình query data và ảnh hưởng đến cái trải nghiệm người dùng. Trong những cái data đó, sẽ có những cái data mà dùng rồi thì nó rất ít được access lại nữa. Ví dụ như lịch sử trên 7 năm trước chẳng hạn. Nó sẽ dẫn đến một cái vấn đề, làm sao để mình giải quyết cái đống data đó. Nên mình mới dùng cái kỹ thuật là data archiving.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[38:14]\u003c/strong\u003e Nó sẽ có những cái lợi ích để counter lại những chuyện bên trên. Đầu tiên là cái data mà mình sử dụng, mà nó set liên tục á, query ghi đọc liên tục á, thì nó thường tốn chi phí cao. Mình sẽ dùng cái kỹ thuật này, mình sẽ đem data của mình bỏ qua một cái chỗ khác, chi phí rẻ hơn, access ít hơn. Từ đó, nó sẽ làm tăng được cái performance của app của mình trong việc query hay aggregate data các thứ.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[39:07]\u003c/strong\u003e Về mặt pháp lý hay reusable, những cái data đó nó sẽ được bảo vệ an toàn, không bị ảnh hưởng bởi những yếu tố bên ngoài. Để sau này khi mình dùng lại, mình có thể lấy ra dùng được. Như mọi người hay nói, mọi người sẽ liên tưởng đến cái data backup, thường dùng trong việc restore data, restore cái system hay app nếu có lỗi xảy ra. Mà hai thằng này, nó sẽ khác nhau ở chỗ là data backup á, nó sẽ dùng cho cái việc hotfix cái system nhiều hơn. Còn cái thằng archiving...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[39:59]\u003c/strong\u003e Data archiving thì nó hướng về cái việc lưu trữ data một cách lâu dài. Nó sẽ có cái chi tiết so sánh như này. Để mình đi build một cái architecture, một cái system để archive data, xong rồi dùng nó để recovery lúc mình cần thì sẽ làm như sau. Mọi người thấy, nó sẽ có ba cái note chính. Thứ nhất là mình lưu data lại, mình dùng metadata để interact với những cái data đó, rồi mình bỏ lên một chỗ, ví dụ như những cái cloud-based service, cloud storage service, để mình lưu trữ cái data đó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[40:51]\u003c/strong\u003e Về chi tiết, để lưu trữ cái dữ liệu á, đầu tiên mình phải xác định những cái dữ liệu cần được lưu trữ. Phải phân tích xem dữ liệu nào hay được sử dụng, dữ liệu nào không được sử dụng, ít được truy cập. Sẽ có nhiều công cụ để mình làm những cái đó. Ví dụ như phân tích từ business requirement, hoặc từ các công cụ phân tích, mấy cái công cụ phân tích á. Từ đó, mình mới biết cái data nào là cần, cái nào có thể đem đi archive lại.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[42:05]\u003c/strong\u003e Sau đó, mình sẽ gói nó lại, dùng một vài biện pháp như vector hóa nó, encode nó, rồi dùng checksum các thứ để đảm bảo cái data nó sẽ đúng. Sau này, khi mình sử dụng lại, mình truy cập lại một cách nhanh chóng. Tại vì những cái database này, nó gói lại ở một cái storage khác với cái mình hay set, nên mình cần phải lưu lại cái metadata của nó. Ví dụ như lưu theo tháng ha, hoặc lưu theo account, để sau mình query lại thì dễ hơn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[43:07]\u003c/strong\u003e Sau khi archive xong, mình muốn sử dụng lại á, thì vừa đây là cái ví dụ em để recovery. Mình sẽ tận dụng những cái metadata lúc nãy, mình search lại những cái block data mà mình cần, rồi đưa về cái môi trường tính toán lại nó khi cần thiết. Cái này nó có lợi ích là khi mình làm những chuyện này, nó sẽ không tác động đến cái data production của cái ứng dụng đang chạy. Mình có thể làm song song được. Mình muốn làm gì với nó thì làm, không chọc ngoáy vào trong cái production, sẽ đảm bảo an toàn được.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[43:51]\u003c/strong\u003e Cho cái trải nghiệm người dùng, như sản phẩm của mình đó. Nói đến đây, có một vài cái practice cho việc sử dụng, xây dựng cái hệ thống này. Nó cũng đơn giản lắm nhỉ. Mình sẽ phải review lại những cái policy mà mình đặt ra để cái hệ thống này chạy, xem data nó có trọn vẹn hay không. Mình sẽ automation những cái step của cái process này. Hiện tại cũng có nhiều tool hỗ trợ mình rồi, ví dụ như AWS, hay Google, đều có những cái như...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[45:14]\u003c/strong\u003e Google Cloud chẳng hạn. Mình chỉ cần viết những cái đơn giản để đẩy lên trên đó thôi. Và mình không thể thiếu cái monitoring để xem data này có hoạt động tốt hay không. Xong rồi, có những kỹ thuật khác như checksum này nọ, để đảm bảo data của mình luôn trọn vẹn. Khi mình cần, cũng sẽ có những chiến lược như schedule trước cái data. Tại vì những cái data này nó tồn tại lâu, nó cũng sẽ lớn, cũng sẽ phồng lên trên cái storage, cái cloud storage mà mình dùng để lưu trữ nó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[46:05]\u003c/strong\u003e Nên sẽ có những chiến lược như khi nào cần thì phải schedule trước, bao nhiêu thời gian đó để nó replicate data cho mình chẳng hạn. Kế của em chỉ như vậy thôi. Lý thuyết kiểu để giải quyết cái mục đích cuối cùng là nói mọi người về việc giải quyết những cái data tồn động lâu dài, nhưng không sử dụng đến nhiều trong cái hệ thống mà mình build thôi. Ví dụ như bên ngân hàng chẳng hạn, sẽ có kiểu user trade, trade của user nó lên đến cả trăm triệu record chẳng hạn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[46:45]\u003c/strong\u003e Sau này, nó sẽ lên nữa. Tức là query những cái data gần thôi, nhưng nó cũng rất tốn thời gian, kiểu vậy. Đó là những cái mà em nói hôm nay, hết. Mọi người có hỏi gì không? Khi mà store data, zip data, là mình sẽ zip một cái đoạn fragment trong quá khứ mà nó không sử dụng data đấy cho mục đích hiện tại, đúng không? Dạ, đúng rồi, đúng rồi. Đồng ý, việc em sẽ phải xóa. Khi xong, em phải xóa cái đó, đúng rồi. Nên mình sẽ có những cái load lại để tính toán khi cần.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[47:41]\u003c/strong\u003e Nên mình mới có mấy cái kiểu để mình làm nó an toàn. Mình có hỏi kìa. Em chưa biết cái cơm của Thỏ có biết cái này không, so sánh được không? Đứng ra là Timescale, nó có cơ chế move chunk. Ví dụ là mình compression như bình thường thôi. Thêm về cái vụ là mình có hot, warm, và cold storage. Ví dụ mình backup hàng tuần thì để trên hot storage của Azure. Nếu là cũ quá, ví dụ 2, 3, 4, 5 năm, thì để trên cold storage của Azure, hoặc là Backblaze. Nó sẽ có riêng cái dịch vụ cho mình move cái chất data đó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[48:53]\u003c/strong\u003e Đúng cái vị trí object hoặc block storage, mình tương tác với Timescale để đảm bảo lúc mình cần tiết kiệm tiền với data cũ. Có thể tiết kiệm được, vốn có thể query, với trade-off là mình sẽ query hơi chậm với data hơi cũ thôi. Dạ, cái em hiểu là để tùy vào cái platform mình dùng để build ha anh. Ví dụ như bên Microsoft thì cũng sẽ có những cái tùy vào thời gian của database, hoặc tùy vào tuổi thọ của data, hay dung lượng này nọ, thì sẽ có những cái level khác nhau.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[49:40]\u003c/strong\u003e Ví dụ nó sẽ có delay, hay bình thường vẫn access, hay delay cho những cái mà không dùng một thời gian lâu nữa. Cái đó là để mình cụ thể trên từng tool thôi. Còn chung chung, nó là anh đang cái này làm gì thì đứng ra là Timescale thì phù hợp cho cái kiểu pattern này, cho về time series. Bên phía Azure thì họ làm cho nó phù hợp với status, hơi giống như Timescale, nhưng nó kiểu giúp mình partition và shard đúng theo kiểu mình mong muốn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[50:39]\u003c/strong\u003e Mỗi một cái nó sẽ có ưu điểm, nhược điểm riêng. Với AWS thì đứng ra là với cái dịch vụ này thì phải coi chừng cái hardware cho lưu cái data này, nó có ổn định không. Ví dụ bên phía Azure cold storage thì nó dùng đĩa, đĩa gì ta, đĩa hơi khá đặc trưng. Phải dùng cái máy laser để in vào trong đó. Nên query rất nhanh, nhưng insert thì cũng hơi chậm, kiểu insert một đống cũng mất vài phút. Vì phải có một cái laser cứng để in ở trên đó, không có virtualization layer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[51:23]\u003c/strong\u003e Mỗi một service và mỗi một cái kiểu tool mình dùng cho compress và lưu trữ sẽ có ưu điểm, nhược điểm riêng, theo cái platform mình subscribe. Dạ, đúng rồi. Cái này không chỉ là mấy cái tool kiểu như AWS hay Google service. Nó là kiểu mình cũng có thể cân nhắc cho cái business của mình nữa. Nên cái này kiểu chung chung thôi. Còn từng platform, nó sẽ dùng những kỹ thuật khác nhau. Mục đích chung cuối cùng là để giải quyết cái vấn đề data nó lớn lên, nhưng ảnh hưởng đến cái việc mình query, mình nó chạy thôi\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[52:16]\u003c/strong\u003e Nhiều cách giải quyết cho câu chuyện optimize query, đúng không? Khi mà vấn đề là do data quá lớn, thì có một vài cách. Cách của biên là một cách, tức là sẽ có một phần data mình đang không xài đến, thì ta cắt đi ra, lưu đâu đấy. Về sau mà có cần đến past data thì insert lại xài sau. Còn mình để đâu đó tầm bao nhiêu phần trăm data hiện tại, đủ để xài mục đích hiện tại, query đi nó nhanh hơn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[52:59]\u003c/strong\u003e Còn một số cách khác thì xài thằng tooling, có một số kiểu database hay kiểu như Timescale, thì nó sẽ optimize luôn cho chuyện query với lượng data lớn lớn. Em nghĩ là bên dưới thì nó cũng sẽ tự động kiểu nó buff lên đâu đó, nó giữ giúp mình thôi, đúng không anh? Nên mình tỉ mỉ bên dưới, mình dùng là interface thôi. Cái bên dưới thì gần gần như nhau, như các em ta. Cảm ơn biên, vậy thôi. Chắc bài cuối của An, không biết có liên quan không. Không biết còn liên quan một tí gì đến cái cộng đồng viên không.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[54:00]\u003c/strong\u003e Chắc có thì chắc cũng nói sơ sơ thôi, cũng không nhiều cái. Cũng gần giống như bài của Biên, nhưng use case cũng gần giống á. Nó mở rộng ra tí thôi. Tí rồi thì bài này là nói sơ về cái datalake với lại cái use case của thằng Notion. Mình nói cái datalake trước. Datalake thì chắc mọi người nghe miết rồi, xưa giờ cũng hơi lâu rồi đó. Mình nhìn lại cái quá trình phát triển của tụi datalake này, coi là mình đang đi tới đâu.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[54:54]\u003c/strong\u003e Thật ra từ lúc bắt đầu, hồi tầm 1980 gì đó, là thời đại của thằng database, mấy thằng database warehouse, mấy cái mà mình đang xài hiện tại á. Về table các kiểu, tạo table rồi xử lý data. Sau này, tới cái đợt tầm năm 2000 các kiểu, tụi mấy thằng big tech bắt đầu thu thập data nhiều á. Rồi nó tận dụng mấy data đó, thì mới sinh ra mấy thằng để giải quyết vấn đề lưu trữ data và xử lý data trên dữ liệu lớn. Như là mấy dữ liệu lưu theo dạng file đồ á. Mấy cái này, mấy thuật ngữ như là cái MapReduce này nè.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[55:44]\u003c/strong\u003e Hình như trong cái memo của mình có một bài về MapReduce. Nếu mọi người không biết thì có thể search lại, tìm đọc thử xem cái MapReduce hồi xưa nó làm cái gì. Nó là cái tiền thân của tuổi. Sau này nó tích hợp vô thôi, giờ không xài nữa, nhưng chắc là nó tích hợp sẵn hết rồi. Sau cái thời gian phát triển của thằng này, mới bắt đầu 2010, thì mới đẻ ra, trước 2010 tí, đẻ ra khái niệm về datalake, big data, cloud, là cái internal data warehouse á, trên cloud á. Nó cloud thôi.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[56:28]\u003c/strong\u003e Sau này, cái đợt bây giờ á, thì nó bắt đầu phát triển hơn nữa, là về cái lake và datamart. Lake chắc bản chất là kết hợp giữa mấy cái của tụi datalake và cái warehouse thôi, để rồi đặt thành cái house. Như là mấy thằng như thằng Datadog, nó đang làm sao không biết, nhưng mình chắc là đang nói về cái này hơi đi sau thời đại tí. Để tập trung vào, chắc mình coi sơ một cái data architecture chung chung trước. Cái này, bữa cái bài của Tom có đăng, cũng có một cái diagram. Nó cũng tinh gọn hơn cái này, tinh gọn hơn tí, là cũng về cái data đi qua mấy cái layer, là processing rồi mới tới thằng gì đó.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[57:20]\u003c/strong\u003e Cái này nó sẽ thể hiện rõ hơn tí, là trong một cái datalake, mình sẽ lưu những loại data gì. So với thằng data warehouse, mình chỉ lưu mấy thằng structured data thôi, hoặc là mấy cái như lưu table data clean hết rồi. Còn thằng datalake này thì nó raw data, nó sẽ cả structured, unstructured, semi-structured data luôn. Nó sẽ lưu dạng raw, sau đó nó mới xử lý data, transform data, rồi nó quăng qua cho cái đám bên BI analytics, hoặc là quăng vô cái warehouse khác để chứa cái data đã được process rồi á.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[58:18]\u003c/strong\u003e Còn cái layer mà analytics sandbox này, thì nó là một cái layer để cho tụi data scientist, hoặc mấy thằng mà cần dùng cái raw data, process data, mà nó không ảnh hưởng tới cái process chính. Bên đây á, thì nó sẽ làm việc trên cái sandbox này để xử lý data cho tụi mấy thằng đó, mấy thằng cần raw data, nhưng không ảnh hưởng trực tiếp tới cái ruồng chính. Cái giống như hồi nãy Biên có nói á, có làm á đó, là nó sẽ lấy data, rồi nó lưu ở đâu đó để sử dụng sau này, hoặc để process gì đó không biết, nhưng mà nó không muốn ảnh hưởng tới process chính của cái app, thì nó sẽ là cái đống này.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[59:09]\u003c/strong\u003e Ở chỗ này, mọi người thấy là mình có khái niệm là cái ETL á, là extract, transform, và load. Bên cái warehouse xưa giờ mình làm á, nó sẽ là extract, transform, và load, nó đi theo thứ tự đó luôn. Nhưng trong cái này, mình sẽ thấy rõ là cái thằng datalake á, nó sẽ là extract và load trước. Rồi sau khi nào cần á, nó bắt đầu process data, là transform. Transform sẽ đứng sau, load sẽ đứng trước. Đó là cái khác biệt giữa hai thằng.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[59:52]\u003c/strong\u003e Đây là chỗ so sánh khác biệt giữa thằng data warehouse và datalake thôi. Đó là dữ liệu bên warehouse, nó được clean, structured, organized thành cái table. Còn thằng này thì nó lưu dạng file, raw data các thứ, semi-structured rồi đó, CSV hoặc mấy cái JSON. Cái process nó cũng sẽ khác nhau giữa thằng lake và lake này. Truy vấn thì thằng warehouse sẽ truy vấn bằng SQL, còn kia thì xử lý trực tiếp trên cái dữ liệu luôn. Mấy thằng hỗ trợ xử lý trực tiếp dữ liệu, như thằng Spark đó, thì nó sẽ hỗ trợ mấy cái đó. Nói qua về cái thằng Notion.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:00:46]\u003c/strong\u003e Datalake thì cái use case của thằng Notion, mọi người biết là Notion mình xài cũng hơi nhiều rồi đó. Hồi xưa, nó cũng đi từ từ thôi. Mấy cái tổ chức, mấy cái block hồi xưa, nó tổ chức thì cũng kiểu data bình thường, giống như mình, là mấy cái app nhỏ nhỏ. Mấy cái block của nó bắt đầu tăng dần. Block của nó được hiểu là mấy cái gì, rồi nó sẽ bao gồm cái title trong trong đó. Nó sẽ gọi là block. Số lượng block của nó tăng lên liên tục theo ngày giờ.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:01:35]\u003c/strong\u003e Gì đó thì bắt đầu sau này, nó phình ra, nó sẽ bắt đầu sử dụng mấy cái kỹ thuật như là sharding, sharding xưa. Như nhớ có bài của Hải Vũ có xe gì đó, nó scale horizontally. Nó bắt đầu tách ra sharding này nọ, rồi mấy cái instance. Trong giai đoạn từ 2021 đến 2023, nó sẽ có 32 instance. Mỗi instance sẽ có 15 cái shard. Rồi từ 2023 trở đi á, nó bắt đầu chia lại, nó lại tăng lên. Số lượng tăng lên nữa, thì đó là 96 cái instance. Và mỗi cái instance, nó sẽ là 5 cái shard. Nhân lên tầm 400 mấy á, bốn trăm mấy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:02:27]\u003c/strong\u003e Để mà xử lý thì lúc này, nó hơi to, đúng không? Khi mà data nó bắt đầu to lên á, nó sẽ có những nhu cầu. Sau này sẽ có những nhu cầu về cái analytics, hoặc là mấy cái về làm bên machine learning á, tập dữ liệu này nọ, mẹo mẹo rồi. Nó sẽ bắt đầu setup một cái data warehouse architecture của nó. Cái này là cái tiền thân trước khi setup cái datalake. Nó sẽ làm data warehouse để xử lý data. Cái luồng cơ bản của nó setup để thu thập data, mấy cái về thay đổi data của mấy cái block trong từng shard.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:03:21]\u003c/strong\u003e Nó sẽ sử dụng cái file transfer để nó ingest mấy cái data từ mấy shard này nè. Nó đổ về cái gì, rồi nó gộp mấy thằng đó lại thành một cái single database to. Cái này sẽ gặp khó khăn trong việc là nãy mình nói, nó đang có khoảng bốn trăm mấy cái shard, đúng không? Nó sẽ gặp khó khăn trong việc là quản lý bốn trăm mấy connection thằng này. Xong rồi mấy cái khó khăn trong việc scaling. Số lượng data thay đổi trong mỗi cái block của thằng Notion, nó xảy ra thường xuyên và nó rất nặng, sẽ...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:04:13]\u003c/strong\u003e Gây khó khăn trong việc đọc ghi trong cái table to này. Sau đó, nó mới bắt đầu setup một cái internal datalake của nó. Cái internal datalake này, có note là nó sẽ không thay thế thằng này hoàn toàn, mà nó chỉ sử dụng cái mới thôi. Còn cái này, nó vẫn tận dụng trong một vài tác vụ, kiểu nhẹ hơn, cho mấy cái table thay đổi data không có nặng lắm. Với lại nó cần cái gì. Còn thằng này, nó expect cái luồng này á, là nó sẽ đánh những cái data nó cần để cho những mục đích mà analytics hoặc là machine learning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:05:08]\u003c/strong\u003e Data nó có thể chấp nhận cái độ trễ là vài tiếng, vài phút, tiếng gì đó. Nó sẽ sử dụng cái data trong đây. Cái lượng setup thì cũng đơn giản thôi. Nó sẽ sử dụng cái thằng Debezium CDC này nè. Nó là cái capture data change á, để nó watch cái thằng database này, bắn về Kafka. Sau khi nó bắn cái đống event data change về Kafka, thì có một thằng bên đây là Hudi hay gì đó, nó lấy event đó, nó quăng về thằng S3. Rồi bắt đầu từ thằng này, thằng nào muốn sử dụng thì vô đây, nó lấy về, nó setup tiếp, xài data warehouse hoặc xài mấy cái chủ đích về shard gì đó, thì vô đây nó lấy, nó xài.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:05:51]\u003c/strong\u003e Cái đó là cái thật ra, cái case Notion. Chắc là có thể xài thằng này thử. Vì nó cũng là cái thằng đứng ở ngoài, nó watch vô cái đống đó. Nếu mà xài AWS hay retraining á, sẽ xài cái một là cái thằng Redshift hay gì quên rồi. Nó sẽ watch thằng đó, những thay đổi trên cái database, xong rồi nó sẽ lưu hết về trong một cái bucket hay cái gì đó. Xong rồi từ đó, mình bắt đầu xử lý sau. Cái luồng bên này là có thể sử dụng cái này. Hồi nãy setup một cái demo, nhưng mà có vẻ hơi fail rồi.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:06:51]\u003c/strong\u003e Tại vì nó chưa có được cái thằng server, nên là nó fail. Để sau đi, rồi chắc chỉ có như đó. Với lại có cái kiểu góc nhìn đó, là cái process này nè. Là cái process mà chắc tụi enterprise, nó sẽ có thể áp dụng. Nó là process kiểu chung chung mà đa số tụi enterprise sau này, em nghĩ là nó có thể. Nhu cầu của nó khi mà cái data lớn lên á, thì cái nhu cầu của nó cũng sẽ đi theo hướng này thôi. Đó là nó cần data, thu thập data để làm cái gì đó, và không ảnh hưởng tới cái luồng chính.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:07:52]\u003c/strong\u003e Mình thì xưa giờ toàn focus vào cái việc làm việc với mấy cái model AI. Nhưng mà mình nghĩ là sau này, mình cũng cần cái skill set gì đó để mình biết cách xử lý những data như thế này, tụi mà nó data lớn hơn kiểu vậy. Cho xin lỗi cái, anh nào đây? Anh đang nhìn nhận cái process này, thì nó khác gì với chuyện là mình replicate cái database của mình ra một instance khác để phục vụ chuyện retraining ấy anh? Là tại vì ở đây, đứng ra là ý ở đây, thật ra là kiểu em đang sinh giống như kiểu sinh data sang một...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:08:54]\u003c/strong\u003e Cái shard khác, đúng không? Data warehouse, đúng không? Và sử dụng upload kit process cho những cái tác vụ mà nó không, kiểu mình làm mình làm async được ấy, chứ không cần phải trực tiếp trên nguồn data chính. Câu hỏi là đối với cả mấy cái model dạng như sharding hay sử dụng master-slave ấy, thì sao không theo kiểu cứ duplicate cái database của mình ra thôi? Duplicate data thì nó vẫn chỉ là một cái data warehouse ở dưới dạng table ha. Còn thật ra cái này, nó chỉ là cái process, nghĩa là một process cho database thôi\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:09:40]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e. Nó có thể có những cái event khác. Như là ví dụ, mình sẽ có nhiều cái external data, không hẳn là mình chỉ có một cái battery, database không. Ví dụ mình có mấy cái capture như là từ social media, hoặc là mấy cái tụm lum la nào đó, chả biết. Nhưng mà nó có thể là nhiều loại data khác nhau, gom về, quăng qua thằng này. Thằng Hudi bạn này, nó sẽ là thằng chịu trách nhiệm xử lý cái raw data đó, để nó quăng vào cái thằng S3 này. Nó lưu...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:10:23]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMọi thứ dưới cái đống này. Nó vô luôn, mọi thứ về dạng file gì đó, gom hết vô đây, để bắt đầu sau này, mấy thằng ngoài sao n mới có cái slot để xử lý. Thật ra tụi nó cũng có một câu hỏi là tại sao không dùng mấy thằng database như MySQL hay PostgreSQL á? Nó sẽ có mấy cái... Tại sao phải sử dụng cái thằng capture data change mà không sử dụng mấy thằng đó? Mấy thằng đó, nó có cơ chế để streaming mấy cái event change của nó luôn. Mấy thằng đó, event stream, nó thường sẽ stream trực tiếp từ database này qua database khác.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:11:09]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCòn thằng này, nó sẽ là capture cái event đó và đưa đâu cũng được. Vì là nếu mình không có thằng Kafka này ở đây á, thì mình cần một service nào đó, mình cần cái real-time data xử lý liền luôn á, mình không cần phải vô Kafka. Cái thằng CDC này vẫn có thể bypass qua đó được, kiểu kiểu vậy, chứ không hẳn là từ database sang database kiểu như vậy. Ta cũng có nhìn ý là kiểu, thấy là nếu mà theo góc nhìn về operation chẳng hạn ấy. Tất nhiên nếu mà có multiple datasource và sử dụng những cái partition tool các thứ, nó khác nhau ấy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:11:50]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDatabase khác nhau, thì cái này cũng sẽ cả, thật ra là gom nó lại vào một cái datalake, sao cho một số cái tác vụ, nó cụ thể thôi ấy. Thực ra là một số team, như kiểu team AI hay team về mặt làm report, hay data, thì người ta cũng sẽ chỉ cần work trên cái data warehouse này thôi, kiểu vậy. Hoặc là có extend cho bên nào khác nữa, thì cũng sẽ make sense, phân vùng data riêng cho từng cái team riêng, đúng không? Có họ thêm cái vụ mà cái button, cái button ETL bên database bình thường với cả bên datalake, thì nó sẽ là ELT, đúng không?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:12:39]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eĐúng rồi, ELT, anh sẽ hiểu là mình extract, nghĩa là mình tìm đúng file, đúng không? Mình load cái file đấy lên đây, và transform nó thành dạng kiểu structured data ha. Ý là nó sẽ transform, nó chỉ là cái action, nó xảy ra ở sau khi mình có raw data rồi. Còn ETL, nghĩa là extract là sẽ lấy data từ cái đám data source á. Xong rồi nó sẽ có cái quá trình log thẳng vào cái raw data, thẳng vào mấy cái gì đó của mình. Nó gọi là cái raw landing, cái layer raw landing. Xong rồi mình mới có cái gọi là transform.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:13:34]\u003c/strong\u003e Sau đó, sau cái landing sẽ có transform để xử lý data, thì nó sẽ ra sau. Còn cái thằng kia là nó extract xong, rồi transform, nó mới quăng thẳng vào cái warehouse, đó là cái database của mình. Hay anh em có hỏi gì không? Rồi, cảm ơn An, đúng rồi. Anh em nhé, rồi bye anh em, mỗi tuần vui vẻ.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"english-transcript\"\u003eEnglish transcript\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e[00:00]\u003c/strong\u003e It seems like the all-hands meeting is next week. I don’t see anyone creating any events, so this week will probably just be normal, right? Guys, check if there’s any issue with screen sharing. Should we just start? Today, I think we’ll have about three presentations. Phát just said he doesn’t have anything new this week, so he’ll probably skip today. Guys, try sharing your personal screens and see how it goes. Can we preview it first?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[10:42]\u003c/strong\u003e According to the schedule, it’s probably you, right, bro? Let me go first then. This fine-tuning topic, it’s not really that new. This presentation is just 100.5, not 101, so it’s only a brief intro, not going deep into it yet. Honestly, I’m pretty clueless about it too, so I’ll just give a quick overview. Today, I’ll present about fine-tuning. Here’s the agenda for this session.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[12:24]\u003c/strong\u003e The introduction is, if you guys use AI, you’ve probably heard of the concept of fine-tuning. AI has these models that are mostly fitted to data from some specific day, with stuff like data privacy or data from a particular domain. That data doesn’t show up in the knowledge of the foundation model. To get that knowledge into the model, people usually use retraining, right? But there’s another way called fine-tuning. At the end of this, I’ll compare these two methods, looking at when to use which one and when not to.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[13:08]\u003c/strong\u003e For now, just think of fine-tuning as a way to expand the knowledge of a foundation model. What is fine-tuning? Simply put, you retrain the model. You take a foundation model, feed it a dataset to fine-tune it, meaning you retrain it. After fine-tuning, you get an adjusted model, called a fine-tuned model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[13:44]\u003c/strong\u003e Why does fine-tuning bring in new knowledge? Simply put, in an AI model, knowledge is stored through neural networks. Fine-tuning updates the weights, the parameters of that neural network, to fit the new knowledge. When you throw new knowledge in, those weights change, and at that point, the model has updated its knowledge.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[14:19]\u003c/strong\u003e When you throw new knowledge in, the weights change, and at that point, the model has updated its knowledge. When fine-tuning a model in practice, it’s not just about throwing a dataset in and retraining it. Sure, you get a fine-tuned model, but you don’t know if that retrained model is any good. I’ll introduce a workflow that people outside commonly use for fine-tuning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[14:59]\u003c/strong\u003e This flow, it’s got several steps like this. You can split it into two clusters, two clusters, alright? This flow, I’ll divide it into two clusters. I’ll go through the first cluster first. The first cluster is the one on the left, simply understood as a base model to start with. Then, you have a new dataset, something new, you throw it in, fine-tune it. Then it produces a model, and you supervise it, meaning you retrain it in a way that’s like retraining it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[15:38]\u003c/strong\u003e It’ll add more knowledge, with this input, the output will come out like this. It results in something called supervised fine-tuning. After that, let me move to the next part. Alright, this fine-tuning is retraining on data, meaning you give it a pair of input-output in the dataset for it to learn. It’ll pick up that new knowledge. For this step to be perfect, the dataset has to be clean. It has to be clean, not mixed with other stuff. Meaning it has to be specific to the domain we want to train it on.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[16:31]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBack to this diagram, after you have a model that’s been retrained, you bring it to production, you use it, right? At this point, people outside use a system called human feedback. It’s like, does the response from this model satisfy you? Rate it from 1 to N stars, something like that. You’ll collect that data. It’s part of this step—you’ll gather human feedback from that retrained model of yours.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[17:06]\u003c/strong\u003e Based on that feedback, people call this step a bit resource-intensive. You’ll have to retrain a separate model. That model is used to evaluate how many points this response gets. Next, you move to the third step, the final one. In this step, you’ll use algorithms like reinforcement learning to combine it with the retrained model and your reward model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[17:46]\u003c/strong\u003e You retrain, you fine-tune the model one more time. It’ll give you something called an optimized model. This loop keeps going, going forever. You have a retrained model, collect human feedback, then combine those three things to retrain the model again. The more you do it, the better the model gets at what we want. That’s the flow I’ve seen people use out there in production.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[18:24]\u003c/strong\u003e During the fine-tuning process, you’ll often hear about a concept called catastrophic forgetting. What does that mean? It means when you retrain with new knowledge, it reduces performance on the old knowledge. Why does this happen? As I said, a model’s knowledge depends on its weights, its architecture, and its parameters. The dynamic parameters in a model are those weights. When you retrain, those weights change, right?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[19:00]\u003c/strong\u003e When they change, doesn’t that mean the old knowledge gets less accurate? If your dataset has a lot of overfitting data, meaning your dataset is too perfect, too perfect within that dataset, then when someone throws something new in, it’ll mess up the old stuff. They call that overfitting, meaning it’s folded in too much to the training data. When it encounters new data, performance drops.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[19:42]\u003c/strong\u003e So at this point, people out there use a technique called parameter-efficient fine-tuning, or PEFT. It has lots of methods, techniques within this approach, like LoRA and stuff. But generally, most of them don’t update all the weights in the model. They’ll just freeze the layers that aren’t necessary. They freeze those unneeded layers and only update a certain number of weights. That’s to avoid losing too much of the old knowledge. That’s the basic idea. For deeper stuff about the algorithms behind it, you can look it up yourselves.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[20:27]\u003c/strong\u003e Back to the question from the start, how’s it different from retraining, and which should we use? There’s a table here, you can easily see it. Retraining depends on your database. You keep throwing stuff in, throwing stuff in, and the data gets updated constantly. But with fine-tuning, you retrain the model, so the data stays only where you retrained it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[21:16]\u003c/strong\u003e Next is customize and learning style. Meaning, retraining’s purpose is to give us a knowledge base that we can pull from to reference and use. But fine-tuning? It upgrades the model’s brain, so it has that knowledge built-in already. The stuff below that, you guys can probably look into it more yourselves, yeah?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[21:57]\u003c/strong\u003e I’ve got an example like this. For instance, say you want to build a system to explain doctors’ notes, right? Doctors’ notes, as you might know, have tons of technical terms. And those technical terms are often abbreviated, written all sloppy too.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[22:44]\u003c/strong\u003e If you guys use fine-tuning, you’ll make it learn all that messy knowledge, the shorthand stuff, the handwriting stuff from doctors. So when you input a doctor’s note, it’ll give you a really accurate answer. But if you use retraining, when you input a doctor’s note, it’ll find the relevant data and pull it up to read. But the thing is, the model doesn’t actually understand those terms, so it won’t give you an accurate answer either.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[23:16]\u003c/strong\u003e You can think of it like this: fine-tuning is like asking a doctor to read a doctor’s note. Retraining is like giving it to someone with really broad knowledge to read a doctor’s note. That person might know a ton, but when it comes to the specialized stuff, the real technical terms, they don’t have the depth of an actual doctor. So the accuracy won’t be high.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[23:57]\u003c/strong\u003e Second, you might say, alright, with retraining, we can use a system prompt to list out all the doctor’s shorthand in the system prompt, and it’ll figure it out on its own. But doing that, you’ll end up using a lot of tokens, right? Because when you use retraining, you pull out all the retraining data, throw in a retraining knowledge base, plus a bunch of zero-shot stuff, descriptions, and whatever else goes with it in a prompt, that takes up a ton of tokens.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[24:34]\u003c/strong\u003e And when you’re in a long conversation with a model, it’s not like it only relies on your question. It bases its answers on all the conversations you’ve had with it from the start. At that point, it leads to a situation where it’s limited by tokens. That’s the drawback of using retraining—it eats up tokens. Because you need tokens to run your system prompt too. But with fine-tuning, the thing is, the model already has the knowledge, so you don’t need a system prompt.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[25:14]\u003c/strong\u003e That’s a quick rundown on fine-tuning. Probably having a demo for you guys to see would make it clearer, yeah? Now I’ll fine-tune a model called Duty 40 Mini. I’ve got a dataset like this. Yup, like this, each thing has a system like retraining, then the user asks this and wants it to answer like that, right? I’ve got 10 things, 10 records in this dataset, and I’ll fine-tune it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[26:13]\u003c/strong\u003e Before fine-tuning, I’ll run it through a piece of code so I can estimate it. Since I’m using Open AI, it’ll cost money. So we’ll calculate an estimate of how much it’ll charge me. After I run it, at the end it’s like, around 4800, close to 4800 tokens. This is just a reference, but I think it’s pretty accurate. Then I’ll upload this data file to Open AI. It’ll give me the file up on my Open AI account.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[27:03]\u003c/strong\u003e Then I’ll train it. I’ll create a fine-tuning job. At this point, on Open AI, it’ll run this job. You guys can go up here, read it, check it out. It won’t give results right away, it’ll create a job and leave it pending there, so Open AI can fine-tune it for us. While waiting, we can track how the process is going. Once it’s done, it’ll notify us. We just keep stamping this sentence, checking this sentence to see if it’s finished or not. We read it from that spot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[27:58]\u003c/strong\u003e Once it’s done, it’ll give us some result statuses. After fine-tuning, with the same question. For example, this is the question I used, I used this question, it’s pretty close to one of the records in my dataset pile. After I run it, it’ll answer like this. But before fine-tuning, I used a normal model, just a regular model, and it answered like that.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[28:49]\u003c/strong\u003e Meaning, when I fine-tuned it, it worked. That’s how I used Open AI to fine-tune a model. That’s it for my demo. Any questions, guys? Yeah, for this demo, I used tuning, but to do fine-tuning locally with something fancy, I probably don’t have the gear. Yup, just small-time gear. Actually, with some earlier models, I ran them on LoRA and stuff, and they could’ve been demoed too. But I didn’t, this one’s easy, it’s like a basic one.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[29:47]\u003c/strong\u003e A bit scattered and slow, huh? Yeah, it’s probably fine though. Generally, enterprise teams or those who don’t want to spend time building GPUs will use this method. Back with Diagram GPT, when GPT-4o Mini came out, fine-tuning was free, so using this was pretty convenient for them. The demo shop for you guys is using something like a service. Open AI provides a fine-tuning service, putting it right up on their models. We pick some models probably the mini ones, I guess. The cost probably isn’t too high.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[30:37]\u003c/strong\u003e That’s one way to do it. But the issue is, we’re still not the ones owning that model. The thing is, it’s still hosted on their server. There’s another way, like building your own server and running it yourself. Today’s case was different. Check it out, guys, yesterday I tried a model with just 3 billion parameters. But it ran for two or three hours and still wasn’t done, bro. Actually, this one, its version is simpler than what we have now, smaller than the previous one from earlier.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[31:26]\u003c/strong\u003e It’s a full flow related to what was it reinforcement feedback. The point is, what I introduced about production out there is when people tune stuff. They don’t just fine-tune it and use it right away, they have to evaluate it again to see if it’s right. They put it into a cycle to keep improving the fine-tuned model, something like that. This is one of those flows. At its core, it’s just a model, nothing special. The key is you guys knowing the costs to judge the approach.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[32:05]\u003c/strong\u003e Because it’s still about accuracy, right? We pick a method to make the output more accurate. Methods like retraining or fine-tuning they’ve got different downsides. And honestly, even with fine-tuning, there are lots of different fine-tuning methods. We’d probably need to dig deeper to figure those out. This is still kinda general. Probably so, Hoàng. If we’ve got the chance, we should dive a bit deeper.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[32:48]\u003c/strong\u003e Deeper in the sense of looking at methods related to retraining and stuff. With fine-tuning methods, some of them are pretty resource-efficient. Of course, there’s a trade-off with some other stuff, like that. I’m introducing this so you guys can check it out somewhere. People asked—Đạt asked when we need fine-tuning. I’d say fine-tuning is needed when you want it to have knowledge on a specific topic. You can consider using fine-tuning then.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[33:39]\u003c/strong\u003e But in all cases, from what I’ve seen out there, most people prefer retraining. Because it’s easier and uses fewer resources. But in some cases, like right now, the example I gave about doctors’ notes, I’d suppose tuning is better. Then it depends on the architecture, how we split our system into smaller systems, what those smaller systems are like it varies. There might be some use cases where they want to host small models, tiny ones maybe.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[34:18]\u003c/strong\u003e Just for doing a specific task. Like analyzing weather, humidity, stuff like that, to perform some action. For example, changing your phone’s theme or triggering some action or whatever. You could retrain a small model just for that, no need for a network or anything fancy. Probably like that. From now on, it’s Biên’s turn, yeah?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[36:13]\u003c/strong\u003e Using it and building a recovery process for it. How it works in detail, it’s got a few main parts. First is the reason we need this technique and comparing it to something more familiar like backup. Then it’s about how we build it and the things we need to watch out for, what stuff. To start, in reality, there are often organizations, companies running apps with high data traffic. Like stock trading stuff, for example. My example here is something like 50,000 transactions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[37:12]\u003c/strong\u003e My example is like 50,000 a day is low it could shoot up to 500,000, a million transactions a day. After a while, that data volume swells up huge, affecting how we query data and impacting the user experience. In that data, there’s stuff that, once used, barely gets accessed again. Like history from over 7 years ago, for instance. That leads to a problem how do we deal with that pile of data? So we use a technique called data archiving.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[38:14]\u003c/strong\u003e It’s got benefits to counter those issues up there. First off, the data we use, the stuff that’s constantly being set, queried, read, and written all the time, it usually costs a lot. With this technique, we take our data and move it somewhere else, somewhere cheaper with less access. That way, it boosts our app’s performance when querying or aggregating data and such.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[39:07]\u003c/strong\u003e In terms of legal stuff or reusability, that data will be kept safe, not affected by external factors. So later, when we need to use it again, we can pull it out and use it. As people often say, you’ll think of data backup, which is usually used to restore data, restore the system, or the app if something goes wrong. But these two things are different in that data backup is more for hotfixing the system. As for archiving data archiving focuses on storing data long-term.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[39:59]\u003c/strong\u003e It has a detailed comparison like this. To build an architecture, a system to archive data, and then use it for recovery when we need it, here’s how it works. You guys see, it has three main notes. First, we store the data, we use metadata to interact with that data, then we put it somewhere, like cloud-based services or cloud storage services, to keep that data stored.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[40:51]\u003c/strong\u003e In detail, to store the data, first we have to figure out which data needs to be stored. We need to analyze which data gets used a lot, which doesn’t get used, or gets accessed rarely. There are lots of tools to help us do that. For example, analyzing from business requirements or using analytics tools, those analytics tools. From there, we figure out which data is necessary, which can be archived.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[42:05]\u003c/strong\u003e After that, we’ll package it up, using a few methods like vectorizing it, encoding it, then using checksums and stuff to make sure the data stays correct. Later, when we use it again, we can access it quickly. Because these databases are packaged in a storage different from what we usually set, we need to save its metadata. For instance, store it by month, yeah, or by account, so it’s easier to query later.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[43:07]\u003c/strong\u003e After archiving, when we want to use it again, just now I gave an example for recovery. We’ll use that metadata from earlier, search for the data blocks we need, then bring it back to the computing environment when necessary. The benefit here is that when we do this stuff, it doesn’t mess with the production data of the running app. We can do it in parallel. Whatever we want to do with it, we do, without poking around in production, so it keeps things safe.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[43:51]\u003c/strong\u003e For the user experience, like our product. Speaking of this, there are a few practices for using and building this system. It’s pretty simple, right? We’ll have to review the policies we set up for this system to run, check if the data stays intact. We’ll automate the steps of this process. Nowadays, there are plenty of tools supporting us already, like AWS or Google, they’ve got stuff like...\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[45:14]\u003c/strong\u003e Google Cloud, for example. We just need to write some simple stuff to push it up there. And we can’t skip monitoring to see if this data is working well or not. Then, there are other techniques like checksums and such, to ensure our data always stays intact. When we need it, there’ll also be strategies like scheduling the data beforehand. Because this data sticks around for a long time, it’ll grow big, it’ll swell up in the storage, the cloud storage we use to keep it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[46:05]\u003c/strong\u003e So there’ll be strategies like, when we need it, we have to schedule in advance, how much time it’ll take to replicate the data for us, for instance. That’s my plan, that’s it. The theory is kind of to address the ultimate goal of explaining to you guys about handling data that sticks around long-term but isn’t used much in the system we build. Like in banking, for example, there’s stuff like user trades, user trades hitting hundreds of millions of records or something.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[46:45]\u003c/strong\u003e Later on, it’ll grow even more. Meaning querying just the recent data, but it still takes a ton of time, something like that. That’s what I talked about today, done. Any questions, guys? When we store data, zip data, it’s like we zip up a fragment from the past that doesn’t use that data for current purposes, right? Yup, exactly, exactly. Agreed, I’ll have to delete it. Once it’s done, I’ve got to delete that, yeah. So we’ll have ways to reload it for calculations when needed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[47:41]\u003c/strong\u003e That’s why we’ve got these methods to keep it safe. I’ve got a question over here. I don’t know if Thỏ’s crew knows about this, can we compare it? Standing out is Timescale, it’s got a chunk-moving mechanism. For example, we compress it like normal. Plus, there’s this thing about having hot, warm, and cold storage. Like, if we back up weekly, it goes on Azure’s hot storage. If it’s too old, say 2, 3, 4, 5 years, then it’s on Azure’s cold storage or Backblaze. It’s got a separate service for us to move that data stuff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[48:53]\u003c/strong\u003e Right to the object or block storage spot, we interact with Timescale to make sure we save money with old data when we need to. It can save costs, it can still query, with the trade-off being that querying is a bit slow with older data. Yup, what I get is it depends on the platform we use to build, right, bro? For example, with Microsoft, it’ll depend on the database’s timing or the data’s lifespan or capacity and stuff, so there’ll be different levels.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[49:40]\u003c/strong\u003e For example, there’ll be delays, or normal access still, or delays for stuff that hasn’t been used in a long time. That’s for us to specify on each tool. But generally, it’s like, bro, what’s this doing? Standing out is Timescale, it fits this kind of pattern, for time series stuff. On Azure’s side, they make it fit the status, kinda like Timescale, but it helps us partition and shard the way we want.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[50:39]\u003c/strong\u003e Each one has its own pros and cons. With AWS, standing out is that with this service, you’ve got to watch the hardware storing this data, whether it’s stable or not. For example, Azure’s cold storage uses disks, what kind of disks, some pretty unique ones. They’ve got to use a laser machine to burn it in there. So querying is super fast, but inserting is kinda slow, like inserting a bunch takes a few minutes. Because it needs a hard laser to burn it on there, no virtualization layer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[51:23]\u003c/strong\u003e Each service and each type of tool we use for compressing and storing has its own pros and cons, depending on the platform we subscribe to. Yup, exactly. This isn’t just about tools like AWS or Google services. It’s like we can also weigh it for our business too. So this is kinda general. Each platform uses different techniques. The ultimate common goal is to tackle the issue of data growing big but affecting how we query, how it runs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[52:16]\u003c/strong\u003e Lots of ways to solve the query optimization problem, right? When the issue is that the data’s too big, there are a few approaches. Biên’s way is one approach, meaning there’s a chunk of data we’re not using, so we cut it out, store it somewhere. Later, if we need past data, we insert it back to use it. For now, we keep some percentage of current data, enough for current purposes, so querying is faster.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[52:59]\u003c/strong\u003e Other ways use tooling, some database types, like Timescale, optimize querying for huge data right off the bat. I think underneath, it kinda auto-buffs it somewhere, holds it for us, right, bro? So we fuss over the details underneath, we just use the interface. Underneath, it’s pretty much the same, like us kids. Thanks, Biên, that’s it. Probably An’s last piece, not sure if it’s related. Not sure if it ties a bit to the community stuff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[54:00]\u003c/strong\u003e If there is, it’s probably just a quick rundown, not a lot. Pretty similar to Biên’s piece, but the use case is kinda close too. It expands a bit more. Alright, so this piece is a quick talk about datalakes and Notion’s use case. Let’s talk datalakes first. Datalakes, you guys have probably heard about them tons, been around for a while now. Let’s look back at how these datalakes evolved, see where we’re at.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[54:54]\u003c/strong\u003e Actually, from the start, around 1980 or something, it was the era of databases, database warehouses, the stuff we’re using now. Table stuff, creating tables and processing data. Later, around the 2000s or so, the big tech folks started collecting tons of data. They used that data, so new stuff popped up to handle storing and processing data on big datasets. Like data stored as files and such. These things, terms like MapReduce, for instance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[55:44]\u003c/strong\u003e I think in my memo, there’s an article on MapReduce. If you guys don’t know, you can search it up, check it out to see what MapReduce did back then. It was the ancestor of this era. Later, it just got integrated in, not used standalone anymore, but it’s probably all built-in now. After that development phase, around 2010, it started giving birth, a bit before 2010, to concepts like datalakes, big data, cloud, internal data warehouses on the cloud. It’s just cloud stuff.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[56:28]\u003c/strong\u003e Now, these days, it’s evolving further, into lakes and datamarts. Lakes are probably just a mix of datalake stuff and warehouses, then turned into a house. Like Datadog or whatever they’re doing, I don’t know, but we’re probably talking about this a bit behind the times. To focus in, let’s take a quick look at a general data architecture first. This one, Tom’s piece the other day posted it, had a diagram too. It’s a bit more streamlined than this, a bit more concise, about data going through layers, processing then to some other thing.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[57:20]\u003c/strong\u003e This one shows it a bit clearer, about what kinds of data we store in a datalake. Compared to a data warehouse, we only store structured data, or stuff like table data that’s all cleaned up. But this datalake, it’s raw data, it’ll handle structured, unstructured, semi-structured data all together. It stores it raw, then it processes the data, transforms it, and tosses it over to the BI analytics crew or into another warehouse to hold the processed data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[58:18]\u003c/strong\u003e Then there’s this analytics sandbox layer, which is a layer for data scientists or folks who need to use raw data, process data, without messing with the main process. Over here, they’ll work on this sandbox to handle data for those guys, the ones who need raw data but don’t directly affect the main flow. It’s like what Biên said earlier, doing that stuff, taking data and storing it somewhere for later use or to process something, I don’t know, but it doesn’t want to mess with the app’s main process, so it’s this pile.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[59:09]\u003c/strong\u003e Here, you guys see we’ve got this concept called ETL, extract, transform, and load. With warehouses, what we’ve done so far is extract, transform, and load, it follows that order straight up. But in this one, you’ll clearly see the datalake does extract and load first. Then when it’s needed, it starts processing the data, that’s transform. Transform comes after, load comes before. That’s the difference between the two.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[59:52]\u003c/strong\u003e This is just the spot comparing the differences between data warehouses and datalakes. With warehouses, the data is cleaned, structured, organized into tables. But this one stores it as files, raw data and stuff, semi-structured already, CSV or JSON files. The processing is different between this lake and that lake too. Querying, the warehouse uses SQL, while over there, it processes directly on the data itself. Tools that support direct data processing, like Spark, handle that stuff. Moving on to Notion.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:00:46]\u003c/strong\u003e For datalakes, the use case of Notion, you guys know we’ve been using Notion quite a bit already. Back in the day, it started slow. The organizations, the blocks from before, they were organized like normal data, just like us, small apps. Its blocks started growing gradually. Blocks are understood as what, and they’ll include the title in there. They call it a block. The number of blocks keeps increasing constantly by the day and hour.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:01:35]\u003c/strong\u003e Something like that, then later it started swelling up, and it began using techniques like sharding, old-school sharding. Like I remember Hải Vũ’s article mentioning something about it, scaling horizontally. It started splitting into sharding and stuff, then instances. From 2021 to 2023, it had 32 instances. Each instance had 15 shards. Then from 2023 onward, it started splitting again, increasing even more. The number went up again, so that’s 96 instances. And each instance had 5 shards. Multiply that, it’s around 400-something, four hundred and some.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:02:27]\u003c/strong\u003e To handle that, at this point it’s pretty big, right? When the data starts getting big, it’ll have some needs. Later on, it’ll have needs for analytics or stuff related to machine learning, datasets and tricks and all that. It started setting up its own data warehouse architecture. This was the precursor before setting up the datalake. It did a data warehouse to process data. The basic flow it set up was to collect data, stuff about data changes in the blocks across each shard.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:03:21]\u003c/strong\u003e It used file transfers to ingest the data from these shards here. It dumped it into something, then merged those things into one big single database. This ran into issues because, like I said earlier, it’s got about four hundred-something shards, right? It struggled with managing four hundred-something connections for this thing. Plus the scaling challenges. The amount of data changing in each block of Notion happens often and is super heavy, so it made reading and writing in this big table tough.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:04:13]\u003c/strong\u003e After that, it started setting up its own internal datalake. This internal datalake has a note that it won’t completely replace this one, it just uses the new stuff. The old one, it still uses for some tasks, lighter ones, for tables where data changes aren’t too heavy. And it needs something. But this one, it expects this flow to tag the data it needs for purposes like analytics or machine learning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:05:08]\u003c/strong\u003e The data can handle a delay of a few hours, a few minutes, something like that. It’ll use the data in here. The setup amount is pretty simple. It uses this thing, Debezium CDC, you know. It’s the capture data change thing, to watch this database and shoot it over to Kafka. After it shoots that pile of event data changes to Kafka, there’s a thing over here, Hudi or something, that grabs those events and tosses them to S3. Then from this point, whoever wants to use it goes in here, grabs it, sets it up further, uses it for data warehouses or some shard-related purposes, they take it from here and use it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:05:51]\u003c/strong\u003e That’s actually the Notion case. We could probably try using this thing. Because it’s also one of those that stands outside, watching that pile. If we use AWS or retraining, it’d use something like Redshift or whatever, I forget. It’d watch that, the changes on the database, then save it all into a bucket or something. From there, we start processing afterward. This flow here could use that. Earlier, I set up a demo, but it kinda flopped.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:06:51]\u003c/strong\u003e Because it didn’t have the server yet, so it failed. Let’s leave it for later, probably just that much for now. Plus there’s this perspective, this process here. It’s a process that enterprise folks could probably apply. It’s a kinda general process that most enterprises later on, I think, might use. Their needs, when the data grows big, will probably head in this direction anyway. It’s that they need data, collect data to do something, without messing with the main flow.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:07:52]\u003c/strong\u003e For us, so far we’ve always focused on working with AI models. But I think later on, we’ll also need some skill set to know how to handle data like this, stuff where the data’s bigger, you know. Sorry, which bro is this? You’re looking at this process, how’s it different from us replicating our database to another instance for retraining purposes, bro? Because here, standing out, the point is, it’s like I’m kinda generating data to another different shard, right?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:08:54]\u003c/strong\u003e And using an upload kit process for tasks that don’t, like, we can do async, you know, instead of needing to work directly on the main data source. The question is, for all those models like sharding or using master-slave setups, why not just duplicate our database? Duplicating data, it’s still just a data warehouse in table form, yeah. But actually this, it’s just a process, meaning a process for the database.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:09:40]\u003c/strong\u003e It could have other events. Like, for example, we’ll have lots of external data, not just one battery, a database, you know. Say we’ve got captures from social media or some random messy stuff, who knows. But it could be lots of different data types, gathered up, tossed to this thing. This Hudi buddy here, it’s the one responsible for processing that raw data, to throw it into this S3 thing. It stores everything under this pile.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:10:23]\u003c/strong\u003e It goes right in, everything in some file format, all dumped in here, so later on, the outside folks have a slot to process it. Actually, they had a question too, why not use databases like MySQL or PostgreSQL? They’ve got their own... Why use this capture data change thing instead of those? Those ones, they’ve got mechanisms to stream their event changes already. With them, event streams usually stream straight from one database to another.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:11:09]\u003c/strong\u003e But this one, it captures that event and sends it wherever. Because if we don’t have this Kafka here, we’d need some service, we’d need real-time data processed right away, without going through Kafka. This CDC thing can still bypass to there, kinda like that, not exactly from database to database like that. We also noticed something, like, it feels like, from an operations angle, for instance. Of course, if there are multiple datasources and we use partition tools and stuff, they’re different.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:11:50]\u003c/strong\u003e Different databases, so this will also, actually, bundle it into a datalake, so some tasks are specific, you know. Actually, some teams, like the AI team or the reporting team or data folks, they’d probably just need to work on this data warehouse, like that. Or if it extends to other sides too, it’d make sense, splitting data zones for each team separately, right? They added this thing about the button, the ETL button in regular databases versus datalakes, it’d be ELT, right?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:12:39]\u003c/strong\u003e Yup, ELT, you’d get it as extract, meaning we find the right file, right? We load that file up here and transform it into something like structured data, yeah. The idea is it transforms, it’s just an action, it happens after we’ve got the raw data. But ETL means extract is pulling data from the data source pile. Then it’s got a process to log straight into the raw data, straight into some stuff of ours. They call it raw landing, the raw landing layer. Then we’ve got what’s called transform.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[01:13:34]\u003c/strong\u003e After that, after the landing, there’s transform to process the data, so it comes out later. But the other one extracts, then transforms, then tosses it straight into the warehouse, that’s our database. Any questions, guys? Alright, thanks An, yup. See you, guys, bye, have a fun week\u003c/p\u003e","frontmatter":{"tags":["office-hours","ogif","discord"],"title":"OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion","short_title":"#37 AI Fine-tuning, Data archiving, Datalakes","date":"2024-12-29T00:00:00.000Z","description":"In OGIF 37, our team dives into AI fine-tuning with an Open AI demo, data archiving for high-traffic apps, and datalake scaling via Notion’s use case. Join us for a session packed with practical insights and collaborative Q\u0026A to boost our technical skills.","authors":["innno_"]},"slug":["updates","ogif","37-20241227"],"backlinks":[],"tocItems":[{"id":"topics-and-highlights","value":"Topics and Highlights","depth":3,"children":[]},{"id":"vietnamese-transcript","value":"Vietnamese transcript","depth":3,"children":[]},{"id":"english-transcript","value":"English transcript","depth":3,"children":[]}],"metadata":{"created":"Sun Dec 29 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","updated":null,"author":"innno_","coAuthors":[],"tags":["office-hours","ogif","discord"],"folder":"updates/ogif","wordCount":13659,"readingTime":"69m","characterCount":74667,"blocksCount":163,"tokenId":"","permaStorageId":"","title":"OGIF Office Hours #37 - AI Fine-Tuning, Data Archiving, and Datalake Scaling with Notion","authorRole":"","image":"","firstImage":null},"isListPage":false},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["updates","ogif","37-20241227"]},"buildId":"vgOJ-EFVJTcloGB4AsrPT","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>