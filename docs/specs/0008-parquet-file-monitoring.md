# Parquet File Monitoring and Discord Notifications

## Overview

This specification outlines monitoring requirements for the DuckDB parquet files in the `@/db` directory, focusing on data quality, processing health, and operational metrics that should trigger Discord notifications.

## Background

The memo.d.foundation project uses DuckDB with parquet files as the core data storage layer:

- **`vault.parquet`** (26MB): Main content database containing markdown metadata, embeddings, and frontmatter
- **`processing_metadata.parquet`** (487B): Processing timestamps and metadata tracking

These files are generated by the Elixir-based obsidian-compiler from markdown content in the vault submodule and serve as the primary data source for:

- Web application content rendering
- Search index generation
- Menu and navigation generation
- Redirect mapping
- NFT minting workflows
- Arweave permanent storage

## Monitoring Categories

### 1. File Health Metrics

#### File Existence and Accessibility

- **Metric**: File presence check
- **Threshold**: File missing or inaccessible for > 5 minutes
- **Discord Alert**: ðŸš¨ **CRITICAL** - Parquet file missing
- **Message**: `vault.parquet or processing_metadata.parquet is missing or inaccessible`

#### File Size Monitoring

- **Metric**: File size changes
- **Thresholds**:
  - `vault.parquet` < 1MB (unusually small)
  - `vault.parquet` > 100MB (unusually large)
  - Size decrease > 50% from previous measurement
- **Discord Alert**: âš ï¸ **WARNING** - Unusual file size
- **Message**: `vault.parquet size: {current_size} (previous: {previous_size})`

#### File Age Monitoring

- **Metric**: Last modified timestamp
- **Threshold**: No updates for > 24 hours during active development
- **Discord Alert**: âš ï¸ **WARNING** - Stale parquet file
- **Message**: `vault.parquet last updated: {timestamp} ({hours_ago} hours ago)`

### 2. Data Quality Metrics

#### Record Count Monitoring

- **Metric**: Total row count in vault table
- **Thresholds**:
  - Row count = 0 (empty database)
  - Row count decrease > 10% without corresponding deletions
  - Row count increase > 50% in single update (potential duplication)
- **Discord Alert**: âš ï¸ **WARNING** - Unusual record count
- **Message**: `Vault records: {current_count} (previous: {previous_count}, change: {percentage}%)`

#### Schema Validation

- **Metric**: Column presence and types
- **Threshold**: Missing required columns or type mismatches
- **Discord Alert**: ðŸš¨ **CRITICAL** - Schema corruption
- **Message**: `Schema validation failed: {missing_columns} or {type_errors}`

#### Data Integrity Checks

- **Metric**: Critical field validation
- **Thresholds**:
  - > 5% of records missing `file_path`
  - > 10% of records missing `title`
  - > 20% of records with null `md_content`
- **Discord Alert**: âš ï¸ **WARNING** - Data quality issues
- **Message**: `Data integrity check failed: {percentage}% records missing {field}`

### 3. Processing Pipeline Health

#### Processing Metadata Tracking

- **Metric**: `last_processed_at` timestamp in processing_metadata table
- **Threshold**: Processing timestamp older than current file modification times
- **Discord Alert**: âš ï¸ **WARNING** - Processing lag detected
- **Message**: `Processing lag: last_processed_at {timestamp} vs file mtime {file_mtime}`

#### Embedding Generation Status

- **Metric**: Records with missing embeddings
- **Thresholds**:
  - > 5% of eligible records missing OpenAI embeddings
  - > 5% of eligible records missing custom embeddings
- **Discord Alert**: âš ï¸ **WARNING** - Embedding generation issues
- **Message**: `Missing embeddings: {openai_missing} OpenAI, {custom_missing} custom out of {total} records`

#### AI Processing Failures

- **Metric**: Records marked for AI processing but failed
- **Threshold**: > 3% failure rate in recent processing batch
- **Discord Alert**: âš ï¸ **WARNING** - AI processing failures
- **Message**: `AI processing failures: {failed_count}/{total_count} ({percentage}%)`

### 4. Business Logic Monitoring

#### Content Publication Status

- **Metric**: Draft vs published content ratio
- **Threshold**: > 80% content marked as draft (potential publishing issue)
- **Discord Alert**: â„¹ï¸ **INFO** - High draft content ratio
- **Message**: `Content status: {draft_count} drafts, {published_count} published ({draft_percentage}% drafts)`

#### NFT Minting Queue

- **Metric**: Records marked for minting but not processed
- **Threshold**: > 10 items pending minting for > 48 hours
- **Discord Alert**: âš ï¸ **WARNING** - NFT minting backlog
- **Message**: `NFT minting backlog: {pending_count} items pending for > 48h`

#### Arweave Storage Queue

- **Metric**: Records marked for permanent storage but not deployed
- **Threshold**: > 5 items pending Arweave deployment for > 24 hours
- **Discord Alert**: âš ï¸ **WARNING** - Arweave deployment backlog
- **Message**: `Arweave backlog: {pending_count} items pending for > 24h`

### 5. Performance Metrics

#### Query Performance

- **Metric**: Time to execute standard queries
- **Threshold**: Query time > 5 seconds for standard operations
- **Discord Alert**: âš ï¸ **WARNING** - Query performance degradation
- **Message**: `Slow query detected: {query_type} took {duration}s (threshold: 5s)`

#### File I/O Performance

- **Metric**: Time to read/scan parquet file
- **Threshold**: Read time > 10 seconds for full table scan
- **Discord Alert**: âš ï¸ **WARNING** - File I/O performance issues
- **Message**: `Slow file I/O: parquet scan took {duration}s (threshold: 10s)`

## Implementation Strategy

### Monitoring Script Structure

```typescript
// monitoring/parquet-monitor.ts
interface ParquetMetrics {
  fileHealth: FileHealthMetrics;
  dataQuality: DataQualityMetrics;
  processingHealth: ProcessingHealthMetrics;
  businessLogic: BusinessLogicMetrics;
  performance: PerformanceMetrics;
}

interface AlertConfig {
  level: 'CRITICAL' | 'WARNING' | 'INFO';
  emoji: string;
  threshold: any;
  message: (data: any) => string;
}
```

### Monitoring Frequency

- **File health checks**: Every 5 minutes
- **Data quality checks**: Every 15 minutes
- **Processing health**: Every 10 minutes
- **Business logic**: Every 30 minutes
- **Performance metrics**: Every 10 minutes

### Discord Integration

```typescript
interface DiscordNotification {
  level: AlertLevel;
  title: string;
  description: string;
  timestamp: string;
  fields: Array<{
    name: string;
    value: string;
    inline?: boolean;
  }>;
  color: number; // Green: 0x00ff00, Yellow: 0xffff00, Red: 0xff0000
}
```

### Alert Aggregation

- Group similar alerts within 10-minute windows
- Suppress duplicate alerts for 1 hour
- Escalate CRITICAL alerts if not acknowledged within 30 minutes

## Deployment Options

### Option 1: GitHub Actions Scheduled Workflow

- **Pros**: Integrated with existing CI/CD, no additional infrastructure
- **Cons**: Limited to 5-minute minimum intervals, potential rate limiting
- **Implementation**: New workflow file `.github/workflows/parquet-monitoring.yml`

### Option 2: Standalone Node.js Service

- **Pros**: Flexible scheduling, can run continuously, better performance monitoring
- **Cons**: Requires deployment infrastructure, additional maintenance
- **Implementation**: Docker container or Railway service

### Option 3: Integration with Existing Build Pipeline

- **Pros**: Leverages existing infrastructure, runs during content updates
- **Cons**: Only monitors during builds, misses runtime issues
- **Implementation**: Add monitoring to existing workflows

## Recommended Implementation

**Phase 1: Critical Monitoring (GitHub Actions)**

- File existence and basic health checks
- Record count monitoring
- Processing lag detection
- Deploy as scheduled workflow running every 10 minutes

**Phase 2: Enhanced Monitoring (Standalone Service)**

- Full data quality validation
- Performance monitoring
- Business logic checks
- Deploy as Railway service with continuous monitoring

**Phase 3: Alerting Optimization**

- Smart alert aggregation
- Dashboard integration
- Historical trend analysis
- Custom alert rules per environment

## Success Metrics

- **Detection Time**: Alert within 5 minutes of issue occurrence
- **False Positive Rate**: < 5% of alerts are false positives
- **Coverage**: 95% of data pipeline issues detected before user impact
- **Response Time**: Critical issues acknowledged within 15 minutes

## Configuration

### Environment Variables

```bash
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...
PARQUET_MONITOR_INTERVAL=600000  # 10 minutes in ms
ALERT_SUPPRESSION_WINDOW=3600000 # 1 hour in ms
PERFORMANCE_THRESHOLD_SECONDS=5
DATA_QUALITY_THRESHOLD_PERCENT=5
```

### Alert Thresholds (Configurable)

```json
{
  "fileSize": {
    "vault_min_mb": 1,
    "vault_max_mb": 100,
    "decrease_threshold_percent": 50
  },
  "dataQuality": {
    "missing_file_path_percent": 5,
    "missing_title_percent": 10,
    "missing_content_percent": 20
  },
  "performance": {
    "query_timeout_seconds": 5,
    "file_scan_timeout_seconds": 10
  }
}
```

This monitoring system will provide comprehensive visibility into the health and performance of the parquet files that are critical to the memo.d.foundation platform's operation.
