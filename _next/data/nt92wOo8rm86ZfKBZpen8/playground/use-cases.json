{"pageProps":{"directoryTree":{"/pinned":{"label":"Pinned Notes","children":{"/playbook/operations/ogif":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/":{"label":"Home","children":{"/consulting":{"label":"Consulting","children":{"/consulting/case-study":{"label":"Case Study","children":{"/consulting/case-study/screenz-ai":{"label":"Screenz.ai","children":{}},"/consulting/case-study/kafi":{"label":"Kafi","children":{}},"/consulting/case-study/droppii":{"label":"Droppii","children":{}},"/consulting/case-study/konvoy":{"label":"Konvoy","children":{}},"/consulting/case-study/cimb":{"label":"CIMB","children":{}},"/consulting/case-study/swift":{"label":"Swift","children":{}},"/consulting/case-study/startupvn":{"label":"StartupVN","children":{}},"/consulting/case-study/open-fabric":{"label":"Open Fabric","children":{}},"/consulting/case-study/icrosschain":{"label":"iCrosschain","children":{}},"/consulting/case-study/hedge-foundation":{"label":"Hedge Foundation","children":{}},"/consulting/case-study/searchio":{"label":"Search.io","children":{}},"/consulting/case-study/tokenomy":{"label":"Tokenomy","children":{}},"/consulting/case-study/basehq":{"label":"BaseHQ","children":{}},"/consulting/case-study/momos":{"label":"Momos","children":{}},"/consulting/case-study/attrace":{"label":"Attrace","children":{}},"/consulting/case-study/setel":{"label":"Setel","children":{}},"/consulting/case-study/joinpara":{"label":"JoinPara","children":{}},"/consulting/case-study/relay":{"label":"Relay","children":{}},"/consulting/case-study/naru":{"label":"Naru","children":{}},"/consulting/case-study/mudah":{"label":"Mudah","children":{}},"/consulting/case-study/reapit":{"label":"Reapit","children":{}},"/consulting/case-study/aharooms":{"label":"Aharooms","children":{}},"/consulting/case-study/begroup":{"label":"beGroup","children":{}},"/consulting/case-study/airwatt":{"label":"AirWatt","children":{}},"/consulting/case-study/voconic":{"label":"Voconic","children":{}},"/consulting/case-study/sol":{"label":"Sol","children":{}},"/consulting/case-study/dental-marketplace":{"label":"Dental Marketplace","children":{}},"/consulting/case-study/bhd":{"label":"BHD Cinema","children":{}}}},"/consulting/market-report":{"label":"Market Report","children":{"/consulting/market-report/event-takeaways-2nd":{"label":"2nd Talks and Takeaways","children":{}},"/consulting/market-report/event-takeaways-1st":{"label":"1st Talks and Takeaways","children":{}},"/consulting/market-report/2025-28th-feb":{"label":"#9: Bybit Loses $1.5B in Hack, Claude 3.7 Sonnet Drops, and OpenArt Designs Characters","children":{}},"/consulting/market-report/2025-21th-feb":{"label":"#8: R1 1776 Goes Open-Source, Cardex Gets Hacked, and Grok-3 Debuts","children":{}},"/consulting/market-report/2025-14th-feb":{"label":"#7: 10x AI Cost Reduction, Lyftâ€™s 2026 Robotaxi Milestone, and Solana ETF Buzz","children":{}},"/consulting/market-report/2025-7th-feb":{"label":"#6 Trending Products, DeepSeek Wave, and Ethereum Predictions","children":{}},"/consulting/market-report/2025-17th-jan":{"label":"#5 VC Trends, Blockchain Breakthroughs, and AI Innovations","children":{}},"/consulting/market-report/2025-10th-jan":{"label":"#4 AI Supercomputers, Mini AI PCs, SEA VC","children":{}},"/consulting/market-report/2025-3rd-jan":{"label":"#3 AI at CES, Wall Street Boom, Blockchain Trends","children":{}},"/consulting/market-report/2024-27th-dec":{"label":"#2 AI Talent Wars, OpenAIâ€™s New Models, Hyperliquid","children":{}},"/consulting/market-report/2024-13th-dec":{"label":"#1 Gemini 2.0, OpenAIâ€™s Sora,  a16zâ€™s Predictions","children":{}}}},"/consulting/wala":{"label":"Wala","children":{"/consulting/wala/43-factory":{"label":"43 Factory","children":{}},"/consulting/wala/dzs-media":{"label":"DZS Media","children":{}},"/consulting/wala/sp-group":{"label":"SP Group","children":{}}}},"/consulting/partners-network":{"label":"Partners Network","children":{}},"/consulting/readme":{"label":"Consulting Team","children":{}}}},"/handbook":{"label":"Handbook","children":{"/handbook/navigate-changes":{"label":"Navigate changes","children":{}},"/handbook/community":{"label":"Community","children":{"/handbook/community/icy-worth":{"label":"How much is your ICY worth","children":{}},"/handbook/community/icy-swap":{"label":"How to swap ICY to BTC","children":{}},"/handbook/community/icy":{"label":"ICY","children":{}},"/handbook/community/discord":{"label":"Discord","children":{}},"/handbook/community/earn":{"label":"Earn","children":{}},"/handbook/community/radar":{"label":"Radar","children":{}},"/handbook/community/sharing":{"label":"Sharing knowledge","children":{}},"/handbook/community/showcase":{"label":"Showcase","children":{}},"/handbook/community/memo":{"label":"Memo","children":{}}}},"/handbook/guides":{"label":"Guides","children":{"/handbook/guides/check-in-at-office":{"label":"Office check-in process for earning ICY","children":{}},"/handbook/guides/leave-request":{"label":"Leave request","children":{}},"/handbook/guides/configure-the-company-email":{"label":"Configure your company email","children":{}},"/handbook/guides/one-on-one-meeting":{"label":"1-on-1 meetings","children":{}},"/handbook/guides/continuing-education-allowance":{"label":"Continuing education allowance","children":{}},"/handbook/guides/reimbursement":{"label":"Reimbursement","children":{}},"/handbook/guides/email-communication-and-use":{"label":"Email Communication and Use","children":{}},"/handbook/guides/password-sharing":{"label":"Password Sharing","children":{}},"/handbook/guides/asset-request":{"label":"Assets","children":{}},"/handbook/guides/effective-meeting":{"label":"Effective meetings","children":{}},"/handbook/guides/conduct-a-meeting":{"label":"How to conduct a meeting","children":{}}}},"/handbook/making-a-career":{"label":"Making a career","children":{}},"/handbook/as-a-community":{"label":"As a community","children":{}},"/handbook/knowledge-base":{"label":"Knowledge base","children":{}},"/handbook/stock-option-plan":{"label":"Stock option plan","children":{}},"/handbook/compliance":{"label":"Compliance","children":{}},"/handbook/mma":{"label":"MMA","children":{}},"/handbook/hybrid-working":{"label":"Hybrid Working","children":{}},"/handbook/routine":{"label":"Work routine","children":{}},"/handbook/ventures":{"label":"Ventures arm","children":{}},"/handbook/purpose":{"label":"Purpose","children":{}},"/handbook/benefits-and-perks":{"label":"Benefits & perks","children":{}},"/handbook/dwarves-foundation-is-you":{"label":"You are Dwarves Foundation","children":{}},"/handbook/getting-started":{"label":"ðŸ’Ž Getting started","children":{}},"/handbook/how-we-hire":{"label":"How we hire","children":{}},"/handbook/how-we-spend-money":{"label":"How we spend money","children":{}},"/handbook/misc":{"label":"Misc","children":{"/handbook/misc/marketing-assets":{"label":"Marketing assets","children":{}}}},"/handbook/moonlighting":{"label":"Moonlighting","children":{}},"/handbook/places-to-work":{"label":"Places to work","children":{}},"/handbook/security-rules":{"label":"Security rules","children":{}},"/handbook/tools-and-systems":{"label":"Tools and systems","children":{}},"/handbook/what-we-stand-for":{"label":"What we stand for","children":{}},"/handbook/what-we-value":{"label":"What we value","children":{}},"/handbook/where-we-work":{"label":"Where we work","children":{}},"/handbook/who-does-what":{"label":"Who does what","children":{}},"/handbook/faq":{"label":"FAQ","children":{}},"/handbook/how-we-work":{"label":"How we work","children":{}}}},"/playground":{"label":"Playground","children":{"/playground/01_literature":{"label":"01_literature","children":{"/playground/01_literature/evolutionary-database-design":{"label":"Evolutionary Database Design: Managing Change and Scaling with the System","children":{}},"/playground/01_literature/design":{"label":"Design","children":{"/playground/01_literature/design/product-design-commentary-20241122":{"label":"Product Design Commentary #7: Hyper-personalization - How AI improves user experience personalization","children":{}},"/playground/01_literature/design/product-design-commentary-20241115":{"label":"Product Design Commentary #6: AI in Design - Cool ideas and how to make them happen","children":{}},"/playground/01_literature/design/product-design-commentary-20241101":{"label":"Product Design Commentary #5: Figma to SwiftUI (functional code) with Claude AI","children":{}},"/playground/01_literature/design/product-design-commentary-20241018":{"label":"Product Design Commentary #4: Generative AI UX design patterns","children":{}},"/playground/01_literature/design/product-design-commentary-20241011":{"label":"Product Design Commentary #3: The art of prompting in AI-human interaction","children":{}},"/playground/01_literature/design/product-design-commentary-20241004":{"label":"Product Design Commentary #2: Unpacking the sparkles icon and AI onboarding challenges","children":{}},"/playground/01_literature/design/product-design-commentary-20240927":{"label":"Product Design Commentary #1: New technologies changing UX/UI and product design","children":{}}}},"/playground/01_literature/giving-a-talk-checklist":{"label":"Giving a talk","children":{}},"/playground/01_literature/database-design-circular":{"label":"Database design Circular","children":{}},"/playground/01_literature/a-lens-to-modern-data-engineering":{"label":"A Lens to Modern Data Engineering","children":{}},"/playground/01_literature/security":{"label":"Security","children":{"/playground/01_literature/security/a-holistic-guide-to-security":{"label":"A Holistic Guide to Security","children":{}},"/playground/01_literature/security/how-i-came-up-with-our-security-standard":{"label":"How I came up with our Security Standard","children":{}}}},"/playground/01_literature/record-reward-sharing-culture":{"label":"Record and reward sharing at Dwarves","children":{}},"/playground/01_literature/designing-for-forgiveness":{"label":"Designing for Forgiveness: Creating Error-Tolerant Interfaces","children":{}},"/playground/01_literature/design-file-sharing-system-part-2-permission-and-password":{"label":"Design file-sharing system - Part 2: Permission & Password","children":{}},"/playground/01_literature/designing-a-model-with-dynamic-properties":{"label":"Designing a model with dynamic properties","children":{}},"/playground/01_literature/hybrid-search":{"label":"Evaluating search engine in RAG systems","children":{}},"/playground/01_literature/design-file-sharing-system-part-1-directory-structure":{"label":"Design file-sharing system - Part 1: Directory Structure","children":{}},"/playground/01_literature/using-foundry-for-evm-smart-contract-developement":{"label":"Using Foundry for EVM smart contract development","children":{}},"/playground/01_literature/creating-a-fully-local-search-engine-on-memo":{"label":"Building a Local Search Engine for Our Memo Website","children":{}},"/playground/01_literature/observer-pattern":{"label":"Introduce the Observer pattern and its use cases","children":{}},"/playground/01_literature/visitor-design-pattern":{"label":"Visitor design pattern, the concept, problem solution and use cases","children":{}},"/playground/01_literature/strategy-design-pattern":{"label":"Strategy design pattern, the concept, use cases and difference with the state design pattern","children":{}},"/playground/01_literature/vietnam-tech-ecosystem-report":{"label":"Vietnam Tech Ecosystem 2024 Report","children":{}},"/playground/01_literature/how-we-crafted-the-ogif-summarizer-bot-to-streamline-weekly-knowledge-sharing":{"label":"How we crafted the OGIF summarizer bot to streamline weekly knowledge-sharing","children":{}},"/playground/01_literature/feedback-mechanism":{"label":"Design feedback mechanism for LLM applications","children":{}},"/playground/01_literature/local-first-software":{"label":"Local-first Software","children":{}},"/playground/01_literature/error-handling-in-rust":{"label":"Error handling on Rust","children":{}},"/playground/01_literature/engineering":{"label":"Engineering","children":{"/playground/01_literature/engineering/backend":{"label":"Backend","children":{"/playground/01_literature/engineering/backend/bloom-filter":{"label":"Bloom Filter","children":{}},"/playground/01_literature/engineering/backend/introduction-to-crdt":{"label":"Introduction to CRDT","children":{}},"/playground/01_literature/engineering/backend/sql-sargable-queries-and-their-impact-on-database-performance":{"label":"SQL Saragable Queries and Their Impact on Database Performance","children":{}},"/playground/01_literature/engineering/backend/the-removal-of-apache-kafkas-dependency-on-zookeeper":{"label":"The removal of Apache Kafka's dependency on Zookeeper","children":{}},"/playground/01_literature/engineering/backend/sql-and-how-it-relates-to-disk-reads-and-writes":{"label":"SQL and how it relates to Disk Reads and Writes","children":{}}}},"/playground/01_literature/engineering/data":{"label":"Data","children":{"/playground/01_literature/engineering/data/data-pipeline-design-framework":{"label":"Data Pipeline Design Framework","children":{}},"/playground/01_literature/engineering/data/quick-learning-vector-database":{"label":"Quick Learning Vector Database","children":{}},"/playground/01_literature/engineering/data/mapreduce":{"label":"MapReduce","children":{}}}},"/playground/01_literature/engineering/google-data-fusion":{"label":"Google Data Fusion","children":{}},"/playground/01_literature/engineering/google-dataproc":{"label":"Google Dataproc","children":{}},"/playground/01_literature/engineering/introducing-htmx-navigating-the-advantages-and-concerns":{"label":"Introducing HTMX - Navigating the Advantages and Concerns","children":{}},"/playground/01_literature/engineering/typesafe-client-server":{"label":"Typesafe Client Server","children":{}},"/playground/01_literature/engineering/url-redirect-vs-rewrite":{"label":"URL Redirect vs. Rewrite; Whatâ€™s the difference?","children":{}}}},"/playground/01_literature/template-method-design-pattern":{"label":"A Tour of Template method pattern with Golang","children":{}},"/playground/01_literature/command-pattern":{"label":"Command Pattern","children":{}},"/playground/01_literature/radix-sort":{"label":"Radix Sort","children":{}},"/playground/01_literature/state-pattern":{"label":"State Pattern","children":{}},"/playground/01_literature/dynamic-liquidity-market-a-new-form-of-concentrated-liquidity-amm-on-solana":{"label":"Dynamic Liquidity Market Maker - a new form of concentrated liquidity AMM on Solana","children":{}},"/playground/01_literature/memo-knowledge-base-meeting":{"label":"Memo Knowledge Base Meeting","children":{}},"/playground/01_literature/peep-nft":{"label":"Claim your Peeps NFT","children":{}},"/playground/01_literature/recording-flow":{"label":"How We Set Up a Recording Workflow for Dwarves Office Hours","children":{}},"/playground/01_literature/memo-publication-workflow":{"label":"Memo Publication Workflow","children":{}},"/playground/01_literature/history-of-structured-output-for-llms":{"label":"History of Structured Outputs for LLMs","children":{}},"/playground/01_literature/builder-design-pattern":{"label":"Introduce the Builder pattern and its use cases","children":{}},"/playground/01_literature/how-to-make-a-moc":{"label":"How to make a MOC","children":{}},"/playground/01_literature/prototype-design-pattern":{"label":"Going Through use cases of the prototype design pattern and it place among the creational patterns","children":{}},"/playground/01_literature/singleton-design-pattern":{"label":"A tour of Singleton design pattern with Golang","children":{}},"/playground/01_literature/echelon-x-singapore-2024-where-innovations-meet-inspiration":{"label":"Echelon X Singapore 2024: Where Innovations Meet Inspiration","children":{}},"/playground/01_literature/c4-modelling":{"label":"Breaking Down Complexity: The Role of Abstractions and UML in C4 Modelling","children":{}},"/playground/01_literature/dollar-cost-averaging":{"label":"Dollar Cost Averaging (DCA)","children":{}},"/playground/01_literature/how-i-create-content-for-multiple-platforms-at-dwarves":{"label":"How I Create Content for Multiple Platforms at Dwarves","children":{}},"/playground/01_literature/understanding-saving-investing-and-speculating-key-differences-and-strategies":{"label":"Understanding Saving, Investing, and Speculating: Key Differences and Strategies","children":{}},"/playground/01_literature/writing-content-for-multimedia-guidelines":{"label":"Writing Content for Multimedia Guidelines","children":{}},"/playground/01_literature/how-to-earn-reward-from-staking-dfg":{"label":"How to earn reward from staking DFG","children":{}},"/playground/01_literature/how-to-transfer-dfg-from-eth-to-base-for-staking":{"label":"How to bridge $DFG from Ethereum Mainnet to Base Network for staking","children":{}},"/playground/01_literature/design-less-present-more-with-deckset":{"label":"Design less, present more with Deckset","children":{}},"/playground/01_literature/level-up-your-markdown-memos":{"label":"Level Up Your Markdown Memos: Avoiding Common Pitfalls","children":{}},"/playground/01_literature/tech-canvas":{"label":"Tech Canvas","children":{}},"/playground/01_literature/how-to-recap-a-publication":{"label":"Recapping A publication","children":{}},"/playground/01_literature/lifecycle-of-a-publication":{"label":"Life cycle of a publication","children":{}},"/playground/01_literature/how-to-set-up-environment-for-editing-memo":{"label":"How to set up environment to edit memo","children":{}},"/playground/01_literature/how-to-take-better-screenshots-on-mac":{"label":"How To Take Better Screenshots On Mac","children":{}},"/playground/01_literature/how-to-push-content-on-note-d":{"label":"How to push content on memo.d.foundation","children":{}},"/playground/01_literature/labs-weekly-catchup-5":{"label":"Labs Weekly Catchup #5","children":{}},"/playground/01_literature/labs-weekly-catchup-4":{"label":"Labs Weekly Catchup #4","children":{}},"/playground/01_literature/labs-weekly-catchup-3":{"label":"Labs Weekly Catchup #3","children":{}},"/playground/01_literature/labs-weekly-catchup-2":{"label":"Labs Weekly Catchup #2","children":{}},"/playground/01_literature/labs-weekly-catchup-1":{"label":"Labs Weekly Catchup #1","children":{}},"/playground/01_literature/labs-who-we-are":{"label":"Labs - Who we are","children":{}},"/playground/01_literature/readme":{"label":"Dwarves Memo","children":{}},"/playground/01_literature/duckdb-demo-and-showcase":{"label":"DuckDB demo and showcase","children":{}},"/playground/01_literature/salary-advance":{"label":"$icy Salary Advance","children":{}},"/playground/01_literature/how-rd-contributes-to-performance-review":{"label":"How R&D contributes to Performance Review","children":{}},"/playground/01_literature/knowledge-journey":{"label":"Knowledge Journey","children":{}},"/playground/01_literature/labs-new-member-onboarding":{"label":"Labs - New Member Onboarding","children":{}},"/playground/01_literature/labs-roadmap-nov-23-update":{"label":"Labs Roadmap (Nov 23 update)","children":{}},"/playground/01_literature/labs-topic-proposal-progress-tracking":{"label":"Labs - Topic proposal & progress tracking","children":{}},"/playground/01_literature/labs-x-consulting-workflow":{"label":"Labs x Consulting Workflow","children":{}},"/playground/01_literature/reward-model-nomination":{"label":"Reward Model & Nomination","children":{}},"/playground/01_literature/our-view-on-fullstack-engineering":{"label":"Our View On Fullstack Engineering","children":{}},"/playground/01_literature/adoption-of-pnpm":{"label":"Adoption Of Pnpm","children":{}},"/playground/01_literature/working-on-a-project-interview-assessment-at-dwarves":{"label":"Working On A Project Interview Assessment At Dwarves","children":{}},"/playground/01_literature/how-we-created-an-ai-powered-interview-system-using-openais-chatgpt":{"label":"How We Created An Ai Powered Interview System Using Openais Chatgpt","children":{}},"/playground/01_literature/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{}},"/playground/01_literature/exploring-machine-learning-approaches-for-fine-tuning-llama-models":{"label":"Exploring Machine Learning Approaches For Fine Tuning Llama Models","children":{}},"/playground/01_literature/managing-dataflow-and-sql-database-with-concurrency-control":{"label":"Managing Dataflow And Sql Database With Concurrency Control","children":{}},"/playground/01_literature/choosing-the-right-javascript-framework-a-deep-dive-into-react-vs-angular-vs-vue":{"label":"Choosing The Right Javascript Framework A Deep Dive Into React Vs Angular Vs Vue","children":{}},"/playground/01_literature/design-system-for-layer-2-using-zk-rollup":{"label":"Design System For Layer 2 Using Zk Rollup","children":{}},"/playground/01_literature/lessons-learned-from-being-a-part-of-corporate-micro-frontend-implementation":{"label":"Lessons Learned From Being A Part Of Corporate Micro Frontend Implementation","children":{}},"/playground/01_literature/cost-of-react-native":{"label":"Cost Of React Native","children":{}},"/playground/01_literature/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{}},"/playground/01_literature/database-designs-for-multilingual-apps":{"label":"Database Designs For Multilingual Apps","children":{}},"/playground/01_literature/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{}},"/playground/01_literature/how-blue-green-deployment-helped-mochi":{"label":"How Blue Green Deployment Helped Mochi","children":{}},"/playground/01_literature/i18n-frontend-guideline":{"label":"I18n Frontend Guideline","children":{}},"/playground/01_literature/radio-talk-61-monorepo":{"label":"Radio Talk 61 Monorepo","children":{}},"/playground/01_literature/from-multi-repo-to-monorepo-a-case-study-with-nghenhan-turbo-monorepo":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan Turbo Monorepo","children":{}},"/playground/01_literature/radio-talk-60-blue-green-deployment":{"label":"Radio Talk 60 Blue Green Deployment","children":{}},"/playground/01_literature/growth-is-our-universal-language":{"label":"Growth Is Our Universal Language","children":{}},"/playground/01_literature/the-key-of-security-mechanisms-in-tackling-cyber-threats":{"label":"The Key Of Security Mechanisms In Tackling Cyber Threats","children":{}},"/playground/01_literature/responsibility":{"label":"Responsibility","children":{}},"/playground/01_literature/configure-the-company-email":{"label":"Configure The Company Email","children":{}},"/playground/01_literature/tech-event-in-the-latest-transforming-healthcare-with-technology":{"label":"Tech Event In The Latest Transforming Healthcare With Technology","children":{}},"/playground/01_literature/from-data-to-backend-an-apprentice-sharing":{"label":"From Data To Backend An Apprentice Sharing","children":{}},"/playground/01_literature/data-analyst-in-retail-trading":{"label":"Data Analyst In Retail Trading","children":{}},"/playground/01_literature/passing-the-probation-get-3-upvotes":{"label":"Passing The Probation Get 3 Upvotes","children":{}},"/playground/01_literature/react-native-new-architecture":{"label":"React Native New Architecture","children":{}},"/playground/01_literature/writing":{"label":"Writing","children":{"/playground/01_literature/writing/state-explain-link":{"label":"State, Explain, Link - An all-purpose writing technique","children":{}}}},"/playground/01_literature/dwarves-radio-talk-17-conduct-a-1-1-session":{"label":"Dwarves Radio Talk 17 Conduct A 1 1 Session","children":{}},"/playground/01_literature/dwarves-radio-talk-16-run-an-effective-performance-review":{"label":"Dwarves Radio Talk 16 Run An Effective Performance Review","children":{}},"/playground/01_literature/understanding-an-application-design":{"label":"Understanding An Application Design","children":{}},"/playground/01_literature/sql-practices-orm-vs-plain-sql":{"label":"Sql Practices Orm Vs Plain Sql","children":{}},"/playground/01_literature/what-i-learned-on-design-thinking-and-software-development":{"label":"What I Learned On Design Thinking And Software Development","children":{}},"/playground/01_literature/six-things-i-extracted-from-design-thinking":{"label":"Six Things I Extracted From Design Thinking","children":{}},"/playground/01_literature/gitflow-pull-request":{"label":"Gitflow Pull Request","children":{}},"/playground/01_literature/git-commit-message-convention":{"label":"Git Commit Message Convention","children":{}},"/playground/01_literature/are-we-really-engineers":{"label":"Are We Really Engineers","children":{}},"/playground/01_literature/how-we-setup-cicd":{"label":"How We Setup Cicd","children":{}},"/playground/01_literature/getting-started-with-webflow":{"label":"Getting Started With Webflow","children":{}},"/playground/01_literature/ui-design-best-practices-dwarves":{"label":"Ui Design Best Practices Dwarves","children":{}},"/playground/01_literature/xpc-services-on-macos-app-using-swift":{"label":"Xpc Services On Macos App Using Swift","children":{}},"/playground/01_literature/the-correct-way-to-build-kpi":{"label":"The Correct Way To Build Kpi","children":{}},"/playground/01_literature/domain-insight-research-framework":{"label":"Domain Insight Research Framework","children":{}},"/playground/01_literature/asking-as-a-junior":{"label":"Asking As A Junior","children":{}},"/playground/01_literature/infinite-image-gallery-with-r3f-an-approach":{"label":"Infinite Image Gallery With R3f An Approach","children":{}},"/playground/01_literature/market":{"label":"Market","children":{"/playground/01_literature/market/an-overview-of-micro-investment-in-real-estate":{"label":"An Overview Of Micro Investment In Real Estate","children":{}}}},"/playground/01_literature/grid-and-layout":{"label":"Grid And Layout","children":{}},"/playground/01_literature/startups-vs-junior-designers":{"label":"Startups Vs Junior Designers","children":{}},"/playground/01_literature/gestalt-principles-in-ui-design":{"label":"Gestalt Principles In Ui Design","children":{}},"/playground/01_literature/aarrr-framework-in-a-nutshell":{"label":"Aarrr Framework In A Nutshell","children":{}},"/playground/01_literature/a-quick-intro-to-webassembly":{"label":"A Quick Intro To Webassembly","children":{}},"/playground/01_literature/sdk-event-sourcing":{"label":"Sdk Event Sourcing","children":{}},"/playground/01_literature/software-development-life-cycle-101":{"label":"Software Development Life Cycle 101","children":{}},"/playground/01_literature/introduce-to-dwarves-memo":{"label":"Introduce To Dwarves Memo","children":{}},"/playground/01_literature/daemons-and-services-programming-guide":{"label":"Daemons And Services Programming Guide","children":{}},"/playground/01_literature/remote-moderated-usability-testing":{"label":"Remote Moderated Usability Testing","children":{}},"/playground/01_literature/an-alternative-to-tm":{"label":"An Alternative To Tm","children":{}},"/playground/01_literature/how-a-design-system-work":{"label":"How A Design System Work","children":{}},"/playground/01_literature/software-modeling":{"label":"Software Modeling","children":{}},"/playground/01_literature/reusability-in-software-development":{"label":"Reusability In Software Development","children":{}},"/playground/01_literature/blockchain-for-designers":{"label":"Blockchain For Designers","children":{}},"/playground/01_literature/design-better-mobile-application":{"label":"Design Better Mobile Application","children":{}},"/playground/01_literature/introduction-to-software-craftsmanship":{"label":"Introduction To Software Craftsmanship","children":{}},"/playground/01_literature/domain-glossary":{"label":"Domain Glossary","children":{}},"/playground/01_literature/architecture-decision-record":{"label":"Architecture Decision Record","children":{}},"/playground/01_literature/build-an-assistant-on-the-terminal":{"label":"Build An Assistant On The Terminal","children":{}},"/playground/01_literature/create-circular-text-using-swiftui":{"label":"Create Circular Text Using Swiftui","children":{}},"/playground/01_literature/draw-watch-face-using-swiftui":{"label":"Draw Watch Face Using Swiftui","children":{}},"/playground/01_literature/applied-security-basis":{"label":"Applied Security Basis","children":{}},"/playground/01_literature/swiftui":{"label":"Swiftui","children":{}},"/playground/01_literature/bunk-license-check":{"label":"Bunk License Check","children":{}},"/playground/01_literature/well-crafted-software":{"label":"Well Crafted Software","children":{}},"/playground/01_literature/objective":{"label":"Objective","children":{}},"/playground/01_literature/project-management":{"label":"Project Management","children":{}},"/playground/01_literature/kubernetes-helm-101":{"label":"Kubernetes Helm 101","children":{}},"/playground/01_literature/what-is-kubernetes":{"label":"What Is Kubernetes","children":{}},"/playground/01_literature/traits-to-assess-during-an-interview":{"label":"Traits To Assess During An Interview","children":{}},"/playground/01_literature/recursively-export-file-pattern-in-javascript-es6-application":{"label":"Recursively Export File Pattern In Javascript Es6 Application","children":{}},"/playground/01_literature/playaround-with-clojure":{"label":"Playaround With Clojure","children":{}},"/playground/01_literature/playaround-with-rust":{"label":"Playaround With Rust","children":{}},"/playground/01_literature/overview-on-broker-pattern-in-distributed-system":{"label":"Overview On Broker Pattern In Distributed System","children":{}},"/playground/01_literature/fundamental-end-to-end-frontend-testing-with-cypress":{"label":"Fundamental End To End Frontend Testing With Cypress","children":{}},"/playground/01_literature/uidynamicanimator":{"label":"Uidynamicanimator","children":{}},"/playground/01_literature/reproduce-apple-find-me-bottom-menu-view":{"label":"Reproduce Apple Find Me Bottom Menu View","children":{}},"/playground/01_literature/build-a-passcode-view-with-swift":{"label":"Build A Passcode View With Swift","children":{}},"/playground/01_literature/istio":{"label":"Istio","children":{}},"/playground/01_literature/different-ways-to-test-react-application":{"label":"Different Ways To Test React Application","children":{}},"/playground/01_literature/federated-byzantine":{"label":"Federated Byzantine","children":{}},"/playground/01_literature/fabric-hyperledger-architecture-explanation":{"label":"Fabric Hyperledger Architecture Explanation","children":{}},"/playground/01_literature/setup-react-project-with-webpack-and-babel":{"label":"Setup React Project With Webpack And Babel","children":{}},"/playground/01_literature/split-and-reuse-code-in-react-application":{"label":"Split And Reuse Code In React Application","children":{}},"/playground/01_literature/hoc-renderprops-and-hook-in-reactjs":{"label":"Hoc Renderprops And Hook In Reactjs","children":{}},"/playground/01_literature/resource-assignment":{"label":"Resource Assignment","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-2":{"label":"The Principle Of Spacing In Ui Design Part 2","children":{}},"/playground/01_literature/finite-state-machine":{"label":"Finite State Machine","children":{}},"/playground/01_literature/card-sorting-and-a-glimpse-at-experimental-sorting-session":{"label":"Card Sorting And A Glimpse At Experimental Sorting Session","children":{}},"/playground/01_literature/about-devops":{"label":"About Devops","children":{}},"/playground/01_literature/our-daily-standup-format":{"label":"Our Daily Standup Format","children":{}},"/playground/01_literature/good-design-understanding":{"label":"Good Design Understanding","children":{}},"/playground/01_literature/competency-mapping":{"label":"Competency Mapping","children":{}},"/playground/01_literature/design-resourcestools":{"label":"Design Resourcestools","children":{}},"/playground/01_literature/design-tips-tricks":{"label":"Design Tips Tricks","children":{}},"/playground/01_literature/design-system":{"label":"Design System","children":{}},"/playground/01_literature/design-workflow":{"label":"Design Workflow","children":{}},"/playground/01_literature/three-levels-of-design":{"label":"Three Levels Of Design","children":{}},"/playground/01_literature/ui-design-fundamental":{"label":"Ui Design Fundamental","children":{}},"/playground/01_literature/ux-model":{"label":"Ux Model","children":{}},"/playground/01_literature/the-principle-of-spacing-in-ui-design-part-1":{"label":"The Principle Of Spacing In Ui Design Part 1","children":{}},"/playground/01_literature/be-careful-with-your-code-splitting-setup":{"label":"Be Careful With Your Code Splitting Setup","children":{}},"/playground/01_literature/qc-onboarding":{"label":"Qc Onboarding","children":{}},"/playground/01_literature/dcos-series-part-5-gitlab":{"label":"Dcos Series Part 5 Gitlab","children":{}},"/playground/01_literature/dcos-series-part-4-deploy-simple-application-with-backend-database":{"label":"Dcos Series Part 4 Deploy Simple Application With Backend Database","children":{}},"/playground/01_literature/dcos-series-part-3-service-discovery-and-load-balancing":{"label":"Dcos Series Part 3 Service Discovery And Load Balancing","children":{}},"/playground/01_literature/dcos-series-part-2-deploy-simple-applications":{"label":"Dcos Series Part 2 Deploy Simple Applications","children":{}},"/playground/01_literature/dcos-series-part-1-quick-look-installation":{"label":"Dcos Series Part 1 Quick Look Installation","children":{}},"/playground/01_literature/skill-of-software-engineer":{"label":"Skill Of Software Engineer","children":{}},"/playground/01_literature/docker-registry":{"label":"Docker Registry","children":{}},"/playground/01_literature/agile-using-clickup-as-agile-management-tool":{"label":"Agile Using Clickup As Agile Management Tool","children":{}},"/playground/01_literature/agile-how-to-create-clickup-tickets":{"label":"Agile How To Create Clickup Tickets","children":{}},"/playground/01_literature/considering-factors-for-performance-evaluating":{"label":"Considering Factors For Performance Evaluating","children":{}},"/playground/01_literature/how-we-contribute-to-homebrew":{"label":"How We Contribute To Homebrew","children":{}},"/playground/01_literature/the-10x-engineer":{"label":"The 10x Engineer","children":{}},"/playground/01_literature/definition-of-done":{"label":"Definition Of Done","children":{}},"/playground/01_literature/estimation-in-agile":{"label":"Estimation In Agile","children":{}},"/playground/01_literature/sprint-lifecycle":{"label":"Sprint Lifecycle","children":{}},"/playground/01_literature/remote-prepare-and-get-going":{"label":"Remote Prepare And Get Going","children":{}},"/playground/01_literature/docker-microcontainers":{"label":"Docker Microcontainers","children":{}}}},"/playground/00_fleeting":{"label":"00_fleeting","children":{"/playground/00_fleeting/automata":{"label":"Automata","children":{}},"/playground/00_fleeting/error-handling-patterns":{"label":"Error Handling Patterns","children":{}},"/playground/00_fleeting/founder-liquidity":{"label":"Founder Liquidity","children":{}},"/playground/00_fleeting/why-hollywood-and-gaming-struggle-with-ai":{"label":"Why Hollywood and gaming struggle with AI","children":{}},"/playground/00_fleeting/subscription-pricing-models":{"label":"Subscription Pricing Models","children":{}},"/playground/00_fleeting/erlang-fsm":{"label":"Erlang Finite State Machine","children":{}},"/playground/00_fleeting/rust-trait":{"label":"Rust Trait","children":{}},"/playground/00_fleeting/explaining-gradient-descent-in-machine-learning-with-a-simple-analogy":{"label":"Explaining Gradient Descent in Machine Learning with a simple analogy","children":{}},"/playground/00_fleeting/organize-team-know-how-with-zettelkasten-method":{"label":"Organize team know-how with Zettelkasten Method","children":{}},"/playground/00_fleeting/how-to-talk-to-chatgpt-effectively":{"label":"How to talk to ChatGPT effectively","children":{}},"/playground/00_fleeting/202302281019-case-study-write-heavy-scalable-and-reliable-inventory-platform":{"label":"Case study: Write-heavy scalable and reliable inventory platform","children":{}},"/playground/00_fleeting/202301191192-multi-column-index-in-db":{"label":"Multi-column index in DB","children":{}},"/playground/00_fleeting/202301091379-invoking-component-functions-in-react":{"label":"Invoking component functions in React","children":{}},"/playground/00_fleeting/202212131609-how-to-deal-with-technical-debt-in-scrum":{"label":"How to deal with technical debt in Scrum","children":{}},"/playground/00_fleeting/202211141287-go-json-parsing":{"label":"Go JSON parser: number <-> interface","children":{}},"/playground/00_fleeting/202211141513-materialized-view-pattern":{"label":"Materialized View Pattern","children":{}},"/playground/00_fleeting/202211081111-error-messaging":{"label":"Error Messaging","children":{}},"/playground/00_fleeting/202210172128-sign-in-form-best-practices":{"label":"Sign-in Form Best Practices","children":{}},"/playground/00_fleeting/202210162154-the-best-of-css-tldr":{"label":"The Best of CSS TLDR","children":{}},"/playground/00_fleeting/202210150019-migration-planning":{"label":"Migration Planning","children":{}},"/playground/00_fleeting/202210131000-behavior-driven-development":{"label":"Behavior Driven Development","children":{}},"/playground/00_fleeting/202210131516-react-fiber":{"label":"React Fiber","children":{}},"/playground/00_fleeting/202210122014-forward-proxy":{"label":"Forward Proxy","children":{}}}},"/playground/_radar":{"label":"_radar","children":{"/playground/_radar/readme":{"label":"Tech Radar","children":{}},"/playground/_radar/apache-spark":{"label":"Apache Spark","children":{}},"/playground/_radar/ant-design":{"label":"Ant Design","children":{}},"/playground/_radar/apache-kafka":{"label":"Apache Kafka","children":{}},"/playground/_radar/argocd":{"label":"Argocd","children":{}},"/playground/_radar/astro":{"label":"Astro","children":{}},"/playground/_radar/backstage":{"label":"Backstage","children":{}},"/playground/_radar/blue-green-deployment":{"label":"Blue Green Deployment","children":{}},"/playground/_radar/browserstack":{"label":"Browserstack","children":{}},"/playground/_radar/carbon":{"label":"Carbon","children":{}},"/playground/_radar/chatgpt-assistance":{"label":"Chatgpt Assistance","children":{}},"/playground/_radar/chromatic":{"label":"Chromatic","children":{}},"/playground/_radar/clickhouse":{"label":"Clickhouse","children":{}},"/playground/_radar/cloudflare-workers":{"label":"Cloudflare Workers","children":{}},"/playground/_radar/codecept":{"label":"Codecept","children":{}},"/playground/_radar/commitlint":{"label":"Commitlint","children":{}},"/playground/_radar/copilot":{"label":"Copilot","children":{}},"/playground/_radar/cucumber":{"label":"Cucumber","children":{}},"/playground/_radar/cypress":{"label":"Cypress","children":{}},"/playground/_radar/dapr":{"label":"Dapr","children":{}},"/playground/_radar/deno":{"label":"Deno","children":{}},"/playground/_radar/detox":{"label":"Detox","children":{}},"/playground/_radar/devcontainers":{"label":"Devcontainers","children":{}},"/playground/_radar/devpod":{"label":"Devpod","children":{}},"/playground/_radar/dora-metrics":{"label":"Dora Metrics","children":{}},"/playground/_radar/duckdb":{"label":"Duckdb","children":{}},"/playground/_radar/earthly":{"label":"Earthly","children":{}},"/playground/_radar/elixir-umbrella-project":{"label":"Elixir Umbrella Project","children":{}},"/playground/_radar/elixir":{"label":"Elixir","children":{}},"/playground/_radar/erlang":{"label":"Erlang","children":{}},"/playground/_radar/error-logging-convention":{"label":"Error Logging Convention","children":{}},"/playground/_radar/eslint":{"label":"Eslint","children":{}},"/playground/_radar/event-sourcing":{"label":"Event Sourcing","children":{}},"/playground/_radar/excalidraw":{"label":"Excalidraw","children":{}},"/playground/_radar/expo":{"label":"Expo","children":{}},"/playground/_radar/figma":{"label":"Figma","children":{}},"/playground/_radar/formal-verification":{"label":"Formal Verification","children":{}},"/playground/_radar/fullstack-tracing":{"label":"Fullstack Tracing","children":{}},"/playground/_radar/gestalt-principle":{"label":"Gestalt Principle","children":{}},"/playground/_radar/github-actions":{"label":"Github Actions","children":{}},"/playground/_radar/golang":{"label":"Golang","children":{}},"/playground/_radar/grafana":{"label":"Grafana","children":{}},"/playground/_radar/graylog":{"label":"Graylog","children":{}},"/playground/_radar/headless-ui":{"label":"Headless Ui","children":{}},"/playground/_radar/hoppscotch":{"label":"Hoppscotch","children":{}},"/playground/_radar/ipfs":{"label":"Ipfs","children":{}},"/playground/_radar/jotai":{"label":"Jotai","children":{}},"/playground/_radar/k6":{"label":"K6","children":{}},"/playground/_radar/k9s":{"label":"K9s","children":{}},"/playground/_radar/kaniko":{"label":"Kaniko","children":{}},"/playground/_radar/kotlin":{"label":"Kotlin","children":{}},"/playground/_radar/kubeseal-sops":{"label":"Kubeseal Sops","children":{}},"/playground/_radar/ladle":{"label":"Ladle","children":{}},"/playground/_radar/langchain":{"label":"Langchain","children":{}},"/playground/_radar/large-language-model-llm":{"label":"Large Language Model Llm","children":{}},"/playground/_radar/loki":{"label":"Loki","children":{}},"/playground/_radar/makefile":{"label":"Makefile","children":{}},"/playground/_radar/micro-frontend":{"label":"Micro Frontend","children":{}},"/playground/_radar/monorepo":{"label":"Monorepo","children":{}},"/playground/_radar/msw":{"label":"Msw","children":{}},"/playground/_radar/n6n":{"label":"N6n","children":{}},"/playground/_radar/nestjs":{"label":"Nestjs","children":{}},"/playground/_radar/netlify":{"label":"Netlify","children":{}},"/playground/_radar/newrelic":{"label":"Newrelic","children":{}},"/playground/_radar/nextjs":{"label":"Nextjs","children":{}},"/playground/_radar/nodejs":{"label":"Nodejs","children":{}},"/playground/_radar/nostrum":{"label":"Nostrum","children":{}},"/playground/_radar/nx":{"label":"Nx","children":{}},"/playground/_radar/orval":{"label":"Orval","children":{}},"/playground/_radar/page-object-model":{"label":"Page Object Model","children":{}},"/playground/_radar/partytown":{"label":"Partytown","children":{}},"/playground/_radar/phaser":{"label":"Phaser","children":{}},"/playground/_radar/phoenix":{"label":"Phoenix","children":{}},"/playground/_radar/playwright":{"label":"Playwright","children":{}},"/playground/_radar/pnpm":{"label":"Pnpm","children":{}},"/playground/_radar/progressive-delivery":{"label":"Progressive Delivery","children":{}},"/playground/_radar/prometheus":{"label":"Prometheus","children":{}},"/playground/_radar/prompt-engineering":{"label":"Prompt Engineering","children":{}},"/playground/_radar/qwik":{"label":"Qwik","children":{}},"/playground/_radar/radix-ui":{"label":"Radix Ui","children":{}},"/playground/_radar/react-hook-form":{"label":"React Hook Form","children":{}},"/playground/_radar/react-llm":{"label":"React Llm","children":{}},"/playground/_radar/react-native":{"label":"React Native","children":{}},"/playground/_radar/react-query":{"label":"React Query","children":{}},"/playground/_radar/react-server-component":{"label":"React Server Component","children":{}},"/playground/_radar/react-testing-library":{"label":"React Testing Library","children":{}},"/playground/_radar/react":{"label":"React","children":{}},"/playground/_radar/reinforcement-learning-from-human-feedback":{"label":"Reinforcement Learning From Human Feedback","children":{}},"/playground/_radar/remix":{"label":"Remix","children":{}},"/playground/_radar/replayio":{"label":"Replayio","children":{}},"/playground/_radar/reverse-engineering":{"label":"Reverse Engineering","children":{}},"/playground/_radar/rust":{"label":"Rust","children":{}},"/playground/_radar/selenium":{"label":"Selenium","children":{}},"/playground/_radar/semantic-release-auto-release":{"label":"Semantic Release Auto Release","children":{}},"/playground/_radar/sentry":{"label":"Sentry","children":{}},"/playground/_radar/serverlessq":{"label":"Serverlessq","children":{}},"/playground/_radar/solidity":{"label":"Solidity","children":{}},"/playground/_radar/solidjs":{"label":"Solidjs","children":{}},"/playground/_radar/stern":{"label":"Stern","children":{}},"/playground/_radar/svelte":{"label":"Svelte","children":{}},"/playground/_radar/swagger":{"label":"Swagger","children":{}},"/playground/_radar/swift-ui":{"label":"Swift Ui","children":{}},"/playground/_radar/swift":{"label":"Swift","children":{}},"/playground/_radar/swr":{"label":"Swr","children":{}},"/playground/_radar/tailwindcss":{"label":"Tailwindcss","children":{}},"/playground/_radar/tauri":{"label":"Tauri","children":{}},"/playground/_radar/team-topologies":{"label":"Team Topologies","children":{}},"/playground/_radar/timeline":{"label":"Timeline","children":{"/playground/_radar/timeline/create-working-devcontainer-for-nextjs-boilerplate":{"label":"Create Working Devcontainer For Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/open-source-devpod-paperspace-provider":{"label":"Open Source Devpod Paperspace Provider","children":{}},"/playground/_radar/timeline/create-working-devcontainer-for-go-api":{"label":"Create Working Devcontainer For Go Api","children":{}},"/playground/_radar/timeline/fe-23-training-type-safe-client-server":{"label":"Fe 23 Training Type Safe Client Server","children":{}},"/playground/_radar/timeline/first-introduced-use-of-duckdb-in-consolelabs-logconsoleso":{"label":"First Introduced Use Of Duckdb In Consolelabs Logconsoleso","children":{}},"/playground/_radar/timeline/add-type-safe-client-server-support-for-next-boilerplate":{"label":"Add Type Safe Client Server Support For Next Boilerplate","children":{}},"/playground/_radar/timeline/building-reliable-apps-sentry-and-distributed-tracing-for-effective-monitoring":{"label":"Building Reliable Apps Sentry And Distributed Tracing For Effective Monitoring","children":{}},"/playground/_radar/timeline/an-engineering-story-map-for-llms":{"label":"An Engineering Story Map For Llms","children":{}},"/playground/_radar/timeline/exploring-resumable-server-side-rendering-with-qwik":{"label":"Exploring Resumable Server Side Rendering With Qwik","children":{}},"/playground/_radar/timeline/challenge-faced-when-researching-rlhf-with-open-assistant":{"label":"Challenge Faced When Researching Rlhf With Open Assistant","children":{}},"/playground/_radar/timeline/embracing-go-1210s-slog-a-unified-logging-interface-with-benchmarks-against-zerolog-and-zap":{"label":"Embracing Go 1210s Slog A Unified Logging Interface With Benchmarks Against Zerolog And Zap","children":{}},"/playground/_radar/timeline/adoption-of-pnpm":{"label":"Adoption Of Pnpm","children":{}},"/playground/_radar/timeline/diagnosing-and-resolving-performance-issues-with-pprof-and-trace-in-go":{"label":"Diagnosing And Resolving Performance Issues With Pprof And Trace In Go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-fortress":{"label":"Migrate Yarn To Pnpm In Fortress","children":{}},"/playground/_radar/timeline/level-up-your-testing-game-harnessing-gomock-for-unbeatable-unit-testing-in-go":{"label":"Level Up Your Testing Game Harnessing Gomock For Unbeatable Unit Testing In Go","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nghe-nhan-droppii":{"label":"Migrate Yarn To Pnpm In Nghe Nhan Droppii","children":{}},"/playground/_radar/timeline/common-design-patterns-in-golang-part-1":{"label":"Common Design Patterns In Golang Part 1","children":{}},"/playground/_radar/timeline/go-training-2023-from-basic-to-advanced":{"label":"Go Training 2023 From Basic To Advanced","children":{}},"/playground/_radar/timeline/llms-accuracy-self-refinement":{"label":"Llms Accuracy Self Refinement","children":{}},"/playground/_radar/timeline/adversarial-prompting":{"label":"Adversarial Prompting","children":{}},"/playground/_radar/timeline/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"Chunking Strategies To Overcome Context Limitation In Llm","children":{}},"/playground/_radar/timeline/dealing-with-long-term-memory-of-chatbot":{"label":"Dealing With Long Term Memory Of Chatbot","children":{}},"/playground/_radar/timeline/error-handling-and-failure-management-in-a-go-system":{"label":"Error Handling And Failure Management In A Go System","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-nextjs-boilerplate":{"label":"Migrate Yarn To Pnpm In Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/lessons-learned-building-an-llm-chatbot-a-case-study":{"label":"Lessons Learned Building An Llm Chatbot A Case Study","children":{}},"/playground/_radar/timeline/foundation-model":{"label":"Foundation Model","children":{}},"/playground/_radar/timeline/integrate-zod-to-nextjs-boilerplate":{"label":"Integrate Zod To Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/llm-query-caching":{"label":"Llm Query Caching","children":{}},"/playground/_radar/timeline/build-your-chatbot-with-open-source-large-language-models":{"label":"Build Your Chatbot With Open Source Large Language Models","children":{}},"/playground/_radar/timeline/integrate-playwright-x-codecept-with-discord":{"label":"Integrate Playwright X Codecept With Discord","children":{}},"/playground/_radar/timeline/overcoming-distributed-system-challenges-using-golang":{"label":"Overcoming Distributed System Challenges Using Golang","children":{}},"/playground/_radar/timeline/easy-prompt-engineering-for-business-use-and-mitigating-risks-in-llms":{"label":"Easy Prompt Engineering For Business Use And Mitigating Risks In Llms","children":{}},"/playground/_radar/timeline/migrate-headlessui-to-radixui":{"label":"Migrate Headlessui To Radixui","children":{}},"/playground/_radar/timeline/llm-101-enhance-developer-productivity":{"label":"Llm 101 Enhance Developer Productivity","children":{}},"/playground/_radar/timeline/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{}},"/playground/_radar/timeline/lessons-learned-from-being-a-part-of-corporate-microfrontend-implementation":{"label":"Lessons Learned From Being A Part Of Corporate Microfrontend Implementation","children":{}},"/playground/_radar/timeline/migrate-yarn-to-pnpm-in-react-toolkit":{"label":"Migrate Yarn To Pnpm In React Toolkit","children":{}},"/playground/_radar/timeline/lessons-learned-from-concurrency-practices-in-blockchain-projects":{"label":"Lessons Learned From Concurrency Practices In Blockchain Projects","children":{}},"/playground/_radar/timeline/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"Applying Mock Service Worker Msw For Seamless Web Development","children":{}},"/playground/_radar/timeline/integrate-playwright-to-run-e2e-test-with-fortress":{"label":"Integrate Playwright To Run E2e Test With Fortress","children":{}},"/playground/_radar/timeline/from-multi-repo-to-monorepo-a-case-study-with-nghenhan":{"label":"From Multi Repo To Monorepo A Case Study With Nghenhan","children":{}},"/playground/_radar/timeline/case-study-how-blue-green-deployment-help-mochi":{"label":"Case Study How Blue Green Deployment Help Mochi","children":{}},"/playground/_radar/timeline/develop-codecept-to-integrate-with-fortress":{"label":"Develop Codecept To Integrate With Fortress","children":{}},"/playground/_radar/timeline/case-study-from-multiple-repo-to-monorepo-at-nghe-nhan":{"label":"Case Study From Multiple Repo To Monorepo At Nghe Nhan","children":{}},"/playground/_radar/timeline/apply-blue-green-deployment-to-mochi":{"label":"Apply Blue Green Deployment To Mochi","children":{}},"/playground/_radar/timeline/memo-blue-green-deployment":{"label":"Memo Blue Green Deployment","children":{}},"/playground/_radar/timeline/brainery-blue-green-deployment":{"label":"Brainery Blue Green Deployment","children":{}},"/playground/_radar/timeline/brainery-validation-with-zod":{"label":"Brainery Validation With Zod","children":{}},"/playground/_radar/timeline/brainery-progressive-delivery":{"label":"Brainery Progressive Delivery","children":{}},"/playground/_radar/timeline/memo-react-native-new-architecture":{"label":"Memo React Native New Architecture","children":{}},"/playground/_radar/timeline/backend-for-call-requests-to-binance-and-get-data-from-multiple-platforms":{"label":"Backend For Call Requests To Binance And Get Data From Multiple Platforms","children":{}},"/playground/_radar/timeline/create-backend-monorepo-to-share-code-and-manage-multiple-services-in-one-repo":{"label":"Create Backend Monorepo To Share Code And Manage Multiple Services In One Repo","children":{}},"/playground/_radar/timeline/nextjs-boilerplate":{"label":"Nextjs Boilerplate","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-wego":{"label":"Apply Page Object Model Structure To Wego","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-aharooms":{"label":"Apply Page Object Model Structure To Aharooms","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-artzy":{"label":"Apply Page Object Model Structure To Artzy","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-sci":{"label":"Apply Page Object Model Structure To Sci","children":{}},"/playground/_radar/timeline/build-automation-for-sci":{"label":"Build Automation For Sci","children":{}},"/playground/_radar/timeline/apply-page-object-model-structure-to-basehq":{"label":"Apply Page Object Model Structure To Basehq","children":{}},"/playground/_radar/timeline/mdx-document-for":{"label":"Mdx Document For","children":{}},"/playground/_radar/timeline/develop":{"label":"Develop","children":{}},"/playground/_radar/timeline/apply-monorepos-to-repit-to-resolve-the-problem-of-consistency":{"label":"Apply Monorepos To Repit To Resolve The Problem Of Consistency","children":{}},"/playground/_radar/timeline/learn-typescript-as-a-mandatory-to-develop-reapit-foundation":{"label":"Learn Typescript As A Mandatory To Develop Reapit Foundation","children":{}},"/playground/_radar/timeline/develop-sdk-integration-demo-for-sajari":{"label":"Develop Sdk Integration Demo For Sajari","children":{}},"/playground/_radar/timeline/live-view":{"label":"Live View","children":{}},"/playground/_radar/timeline/migrate-aharooms-pms-to-typescript":{"label":"Migrate Aharooms Pms To Typescript","children":{}},"/playground/_radar/timeline/create-api-service-for-urbox-to-sync-orders-from-3rd-parties-and-manage-shipment":{"label":"Create Api Service For Urbox To Sync Orders From 3rd Parties And Manage Shipment","children":{}},"/playground/_radar/timeline/nghenhan-microservices":{"label":"Nghenhan Microservices","children":{}},"/playground/_radar/timeline/radio-talk-65-fullstack-type-safe-with-trpc":{"label":"Radio Talk 65 Fullstack Type Safe With Trpc","children":{}},"/playground/_radar/timeline/understanding-test-doubles-an-in-depth-look":{"label":"Understanding Test Doubles An In Depth Look","children":{}},"/playground/_radar/timeline/radio-talk-64-coding-best-practice-that-optimizing-go-compiler":{"label":"Radio Talk 64 Coding Best Practice That Optimizing Go Compiler","children":{}},"/playground/_radar/timeline/reward-model":{"label":"Reward Model","children":{}},"/playground/_radar/timeline/q-learning":{"label":"Q Learning","children":{}},"/playground/_radar/timeline/sum-command":{"label":"Sum Command","children":{}},"/playground/_radar/timeline/reinforcement-learning":{"label":"Reinforcement Learning","children":{}},"/playground/_radar/timeline/react-server-component":{"label":"React Server Component","children":{}},"/playground/_radar/timeline/select-vector-database-for-llm":{"label":"Select Vector Database For Llm","children":{}},"/playground/_radar/timeline/workaround-with-openais-token-limit-with-langchain":{"label":"Workaround With Openais Token Limit With Langchain","children":{}},"/playground/_radar/timeline/working-with-langchain-document-loaders":{"label":"Working With Langchain Document Loaders","children":{}},"/playground/_radar/timeline/the-cost-of-react-native":{"label":"The Cost Of React Native","children":{}},"/playground/_radar/timeline/state-of-frontend-2023-react-vs-angular-vs-vue":{"label":"State Of Frontend 2023 React Vs Angular Vs Vue","children":{}},"/playground/_radar/timeline/unit-testing-best-practices-in-golang":{"label":"Unit Testing Best Practices In Golang","children":{}},"/playground/_radar/timeline/what-is-pnpm":{"label":"What Is Pnpm","children":{}},"/playground/_radar/timeline/tackling-server-state-complexity-in-frontend-development":{"label":"Tackling Server State Complexity In Frontend Development","children":{}},"/playground/_radar/timeline/why-we-chose-our-tech-stack":{"label":"Why We Chose Our Tech Stack","children":{}},"/playground/_radar/timeline/why-micro-frontend":{"label":"Why Micro Frontend","children":{}},"/playground/_radar/timeline/radio-talk-monorepo":{"label":"Radio Talk Monorepo","children":{}},"/playground/_radar/timeline/radio-talk-blue-green-deployment":{"label":"Radio Talk Blue Green Deployment","children":{}},"/playground/_radar/timeline/radio-talk-a-demo-of-query-engine-postgresql-vs-apache-spark":{"label":"Radio Talk A Demo Of Query Engine Postgresql Vs Apache Spark","children":{}},"/playground/_radar/timeline/rnd-team-mentioned-apache-spark-as-a-solution-to-handle-query-big-data":{"label":"Rnd Team Mentioned Apache Spark As A Solution To Handle Query Big Data","children":{}},"/playground/_radar/timeline/radio-talk-engineering-health-metrics":{"label":"Radio Talk Engineering Health Metrics","children":{}},"/playground/_radar/timeline/radio-talk-nextjs-13":{"label":"Radio Talk Nextjs 13","children":{}},"/playground/_radar/timeline/radio-talk-using-nextjs-as-a-fullstack-framework":{"label":"Radio Talk Using Nextjs As A Fullstack Framework","children":{}},"/playground/_radar/timeline/use-yup-to-validate-form-values-in-droppii":{"label":"Use Yup To Validate Form Values In Droppii","children":{}},"/playground/_radar/timeline/vitejs-native-modules":{"label":"Vitejs Native Modules","children":{}},"/playground/_radar/timeline/radio-talk-introduction-to-apache-spark":{"label":"Radio Talk Introduction To Apache Spark","children":{}},"/playground/_radar/timeline/vercel-switching-their-packages-from-yarn-to-pnpm-caught-our-attention":{"label":"Vercel Switching Their Packages From Yarn To Pnpm Caught Our Attention","children":{}},"/playground/_radar/timeline/radio-talk-remix-vs-nextjs":{"label":"Radio Talk Remix Vs Nextjs","children":{}},"/playground/_radar/timeline/radio-talk-turborepo":{"label":"Radio Talk Turborepo","children":{}},"/playground/_radar/timeline/react-toolkit-migrate-from-lerna-to-turporepo":{"label":"React Toolkit Migrate From Lerna To Turporepo","children":{}},"/playground/_radar/timeline/use-monorepos-to-build-v3-of-react-sdk-for-searchio":{"label":"Use Monorepos To Build V3 Of React Sdk For Searchio","children":{}},"/playground/_radar/timeline/react-toolkit":{"label":"React Toolkit","children":{}},"/playground/_radar/timeline/use-nx-for-managing-basehq-frontend-monorepos":{"label":"Use Nx For Managing Basehq Frontend Monorepos","children":{}},"/playground/_radar/timeline/practice-and-using-selenium-in-setel-project":{"label":"Practice And Using Selenium In Setel Project","children":{}},"/playground/_radar/timeline/urbox-backend-api":{"label":"Urbox Backend Api","children":{}},"/playground/_radar/timeline/using-k6-in-setel":{"label":"Using K6 In Setel","children":{}},"/playground/_radar/timeline/use-monorepos-to-resolve-the-problem-of-sharing-ui-components-in-aharoom":{"label":"Use Monorepos To Resolve The Problem Of Sharing Ui Components In Aharoom","children":{}},"/playground/_radar/timeline/a-case-study-interview-into-micro-frontends-building-design-system-for-e-commerce-platform":{"label":"A Case Study Interview Into Micro Frontends Building Design System For E Commerce Platform","children":{}},"/playground/_radar/timeline/accelerate-project-initiation-with-advanced-nextjs-boilerplate-react-toolkit":{"label":"Accelerate Project Initiation With Advanced Nextjs Boilerplate React Toolkit","children":{}},"/playground/_radar/timeline/adapt-cucumber-as-a-bdd-for-wego":{"label":"Adapt Cucumber As A Bdd For Wego","children":{}}}},"/playground/_radar/timescaledb":{"label":"Timescaledb","children":{}},"/playground/_radar/tla":{"label":"Tla","children":{}},"/playground/_radar/trunk-based-development":{"label":"Trunk Based Development","children":{}},"/playground/_radar/turborepo":{"label":"Turborepo","children":{}},"/playground/_radar/type-safe-client-server":{"label":"Type Safe Client Server","children":{}},"/playground/_radar/typescript":{"label":"Typescript","children":{}},"/playground/_radar/ui-documentation":{"label":"Ui Documentation","children":{}},"/playground/_radar/uno-css":{"label":"Uno Css","children":{}},"/playground/_radar/upptime":{"label":"Upptime","children":{}},"/playground/_radar/v-model":{"label":"V Model","children":{}},"/playground/_radar/vector-database":{"label":"Vector Database","children":{}},"/playground/_radar/vercel":{"label":"Vercel","children":{}},"/playground/_radar/vitejs":{"label":"Vitejs","children":{}},"/playground/_radar/volta":{"label":"Volta","children":{}},"/playground/_radar/wasm":{"label":"Wasm","children":{}},"/playground/_radar/webdriverio":{"label":"Webdriverio","children":{}},"/playground/_radar/webflow":{"label":"Webflow","children":{}},"/playground/_radar/yup":{"label":"Yup","children":{}},"/playground/_radar/zod":{"label":"Zod","children":{}},"/playground/_radar/zustand":{"label":"Zustand","children":{}}}},"/playground/blockchain":{"label":"Blockchain","children":{"/playground/blockchain/build-custom-ai-agent-with-elizaos":{"label":"Build custom AI Agent with ElizaOS","children":{}},"/playground/blockchain/web3-development-with-foundry":{"label":"Web3 Development with Foundry","children":{}},"/playground/blockchain/cross-chain-transfers-implementing-a-token-swap-from-base-chain-to-bitcoin":{"label":"Implement a Token Swap from the Base chain to Bitcoin for cross-chain transactions","children":{}},"/playground/blockchain/ton_core_concept":{"label":"Ton's base concepts","children":{}},"/playground/blockchain/ton_blockchain_of_blockchains":{"label":"Ton: Blockchain of blockchains","children":{}},"/playground/blockchain/introduce-to-solana-token-2022-new-standard-to-create-a-token-in-solana":{"label":"Introduce to Solana Token 2022 - new standard to create a token in solana","children":{}},"/playground/blockchain/solana-core-concept":{"label":"Solana core concepts","children":{}},"/playground/blockchain/metaplex-nft-compression":{"label":"Metaplex NFT Compression","children":{}},"/playground/blockchain/plonky2":{"label":"Plonky2","children":{}},"/playground/blockchain/polygon-zkevm-architecture":{"label":"Polygon zkEVM architecture","children":{}},"/playground/blockchain/starknet-architecture":{"label":"StarkNet architecture","children":{}},"/playground/blockchain/zk-snarks":{"label":"zk-SNARKs","children":{}},"/playground/blockchain/layer-2":{"label":"Layer 2: Scaling Solutions for Ethereum","children":{}},"/playground/blockchain/solana-account":{"label":"Solana Account","children":{}},"/playground/blockchain/foundational-topics":{"label":"Foundational Topics","children":{"/playground/blockchain/foundational-topics/zero-knowledge-proofs":{"label":"Zero-knowledge Proofs","children":{}},"/playground/blockchain/foundational-topics/blocks":{"label":"Blocks","children":{}},"/playground/blockchain/foundational-topics/distributed-systems":{"label":"Distributed systems","children":{}},"/playground/blockchain/foundational-topics/pos":{"label":"PoS","children":{}},"/playground/blockchain/foundational-topics/smart-contract":{"label":"Smart Contract","children":{}},"/playground/blockchain/foundational-topics/topics":{"label":"Topics","children":{}}}},"/playground/blockchain/multisign-wallet":{"label":"Multisign wallet","children":{}},"/playground/blockchain/anchor-framework":{"label":"Anchor framework","children":{}},"/playground/blockchain/blockchain-bridge":{"label":"Blockchain Bridge","children":{}},"/playground/blockchain/nft-fractionalization":{"label":"NFT Fractionalization","children":{}},"/playground/blockchain/how-tokens-work-on-solana":{"label":"How Tokens Work on Solana","children":{}},"/playground/blockchain/liquidity-pool":{"label":"Liquidity pool","children":{}}}},"/playground/use-cases":{"label":"Use Cases","children":{"/playground/use-cases/create-slides-with-overleaf":{"label":"Create slides with Overleaf and ChatGPT","children":{}},"/playground/use-cases/optimize-init-load-time-for-trading-platform":{"label":"Optimizing initial load time for a Trading Platform","children":{}},"/playground/use-cases/ai-interview-platform-mvp":{"label":"Building MVP for AI-driven interview platform","children":{}},"/playground/use-cases/optimizing-ui-for-effective-investment-experience":{"label":"Hedge Foundation - Optimizing UI for effective investment experience","children":{}},"/playground/use-cases/implement-binance-future-pnl-analysis-page":{"label":"Implement Binance Futures PNL analysis page by Phoenix LiveView","children":{}},"/playground/use-cases/migrate-normal-table-to-timescale-table":{"label":"Migrate regular tables into TimescaleDB hypertables to improve query performance","children":{}},"/playground/use-cases/bitcoin-alt-performance-tracking":{"label":"Tracking Bitcoin-Altcoin Performance Indicators in BTC Hedging Strategy","children":{}},"/playground/use-cases/database-hardening-for-trading-platform":{"label":"Database hardening for a trading platform","children":{}},"/playground/use-cases/data-archive-and-recovery":{"label":"Building a data archive and recovery strategy for high-volume trading system","children":{}},"/playground/use-cases/persist-history-using-data-snapshot-pattern":{"label":"Implementing data snapshot pattern to persist historical data","children":{}},"/playground/use-cases/ai-ruby-travel-assistant-chatbot":{"label":"AI-powered Ruby travel assistant","children":{}},"/playground/use-cases/building-chatbot-agent-for-project-management-tool":{"label":"Building chatbot agent to streamline project management","children":{}},"/playground/use-cases/building-data-pipeline-ogif-transcriber":{"label":"Building data pipeline for OGIF transcriber","children":{}},"/playground/use-cases/centralized-monitoring-setup-for-trading-platform":{"label":"Setup centralized monitoring system for Hedge Foundation trading platform","children":{}},"/playground/use-cases/binance-transfer-matching":{"label":"Building better Binance transfer tracking","children":{}},"/playground/use-cases/crypto-market-outperform-chart-rendering":{"label":"Visualizing crypto market performance: BTC-Alt dynamic indicators in Golang","children":{}},"/playground/use-cases/enhancing-cryptocurrency-transfer-logger":{"label":"Transfer mapping: enhancing loggers for better transparency","children":{}},"/playground/use-cases/reconstructing_trading_pnl_data_pipeline_approach":{"label":"Reconstructing historical trading PnL: a data pipeline approach","children":{}},"/playground/use-cases/ai-powered-monthly-project-reports":{"label":"Project reports system: a case study","children":{}}}},"/playground/frontend":{"label":"Frontend","children":{"/playground/frontend/report":{"label":"Report","children":{"/playground/frontend/report/frontend-report-march-2025":{"label":"Frontend Report March 2025","children":{}},"/playground/frontend/report/frontend-report-february-2025":{"label":"February 2025","children":{}},"/playground/frontend/report/frontend-report-january-2025":{"label":"January 2025","children":{}},"/playground/frontend/report/frontend-report-second-half-of-november-2024":{"label":"Nov 2024 (Second Half)","children":{}},"/playground/frontend/report/frontend-report-first-half-of-november-2024":{"label":"Nov 2024 (First Half)","children":{}},"/playground/frontend/report/frontend-report-october-2024":{"label":"October 2024","children":{}},"/playground/frontend/report/frontend-report-september-2024":{"label":"September 2024","children":{}},"/playground/frontend/report/frontend-report-august-2024":{"label":"August 2024","children":{}},"/playground/frontend/report/frontend-report-july-2024":{"label":"July 2024","children":{}}}},"/playground/frontend/react":{"label":"React","children":{"/playground/frontend/react/code-splitting":{"label":"Code splitting","children":{}},"/playground/frontend/react/component-composition-patterns":{"label":"Component composition patterns","children":{}},"/playground/frontend/react/design-system-integration":{"label":"Design system integration","children":{}},"/playground/frontend/react/hook-architecture":{"label":"Hook architecture","children":{}},"/playground/frontend/react/rendering-strategies":{"label":"Rendering strategies","children":{}},"/playground/frontend/react/state-management-strategy":{"label":"State management strategy","children":{}},"/playground/frontend/react/testing-strategies":{"label":"Testing strategies","children":{}}}},"/playground/frontend/websockets":{"label":"WebSockets","children":{}},"/playground/frontend/from-markup-to-pixels-a-look-inside-the-dom-cssom-and-render-tree":{"label":"From Markup to Pixels - A look inside the DOM, CSSOM, and Render Tree","children":{}},"/playground/frontend/window-and-iframe-communication":{"label":"Window and iframe communication","children":{}},"/playground/frontend/applying-mock-service-worker-msw-for-seamless-web-development":{"label":"Applying Mock Service Worker (MSW) for Seamless Web Development","children":{}},"/playground/frontend/render-optimization-in-data-fetching-libraries":{"label":"Render optimization in data-fetching libraries","children":{}},"/playground/frontend/a-fragment-colocation-pattern-with-react-apollo-graphql":{"label":"A Fragment Colocation Pattern with React & Apollo GraphQL","children":{}},"/playground/frontend/scroll-driven-animations":{"label":"Scroll-driven animations","children":{}},"/playground/frontend/react-server-component":{"label":"React Server Components, NextJs Route and Data Fetching","children":{}},"/playground/frontend/url-formats-for-sharing-via-social-networks":{"label":"URL formats for sharing via social networks","children":{}},"/playground/frontend/shadow-dom":{"label":"Shadow DOM","children":{}},"/playground/frontend/retain-scroll-position-in-infinite-scroll":{"label":"Retain scroll position in infinite scroll","children":{}},"/playground/frontend/continuous-translation":{"label":"Continuous Translation","children":{}},"/playground/frontend/what-is-pnpm-compare-to-npmyarn":{"label":"What is PNPM Compare To NPM/Yarn","children":{}},"/playground/frontend/why-micro-frontend":{"label":"Why Micro Frontend","children":{}},"/playground/frontend/why-we-chose-our-tech-stack-accelerating-development-with-a-robust-frontend-solution":{"label":"Why We Chose Our Tech Stack Accelerating Development With A Robust Frontend Solution","children":{}},"/playground/frontend/tackling-server-state-complexity-in-frontend-development":{"label":"Tackling Server State complexity in Frontend Development","children":{}},"/playground/frontend/variable-fonts":{"label":"Variable Fonts","children":{}},"/playground/frontend/when-should-we-use-usereducer-instead-of-usestate":{"label":"When should we use useReducer instead of useState?","children":{}},"/playground/frontend/preserving-and-resetting-state-in-react":{"label":"Preserving and Resetting state in React","children":{}},"/playground/frontend/mixpanel":{"label":"Mixpanel","children":{}},"/playground/frontend/validation-with-zod":{"label":"Validation with Zod","children":{}},"/playground/frontend/parse-dont-validate-in-typescript":{"label":"Parse, don't validate in TypeScript","children":{}},"/playground/frontend/webassembly":{"label":"Webassembly","children":{}},"/playground/frontend/singleton-design-pattern-in-javascript":{"label":"Singleton Design Pattern in Javascript","children":{}},"/playground/frontend/an-introduction-to-atomic-css":{"label":"An Introduction to Atomic CSS","children":{}},"/playground/frontend/intro-to-indexeddb":{"label":"Intro to IndexedDB","children":{}},"/playground/frontend/the-fundamental-of-web-performance":{"label":"The fundamental of web performance","children":{}},"/playground/frontend/wai-aria":{"label":"WAI-ARIA","children":{}},"/playground/frontend/build-polymorphic-react-components-with-typescript":{"label":"Build polymorphic React components with Typescript","children":{}},"/playground/frontend/threejs":{"label":"Threejs","children":{"/playground/frontend/threejs/cameras-in-threejs":{"label":"Cameras in ThreeJS","children":{}}}},"/playground/frontend/prevent-layout-thrashing":{"label":"Prevent Layout Thrashing","children":{}},"/playground/frontend/pure-css-parallax":{"label":"Pure CSS Parallax","children":{}},"/playground/frontend/css-container-queries":{"label":"CSS Container Queries","children":{}},"/playground/frontend/hsl-color":{"label":"HSL Color","children":{}},"/playground/frontend/mitigate-blocking-the-main-thread":{"label":"Mitigate blocking the main thread","children":{}},"/playground/frontend/css-in-js":{"label":"CSS in JS","children":{}},"/playground/frontend/dark-mode-flickers-a-white-background-for-a-fraction-of-a-second":{"label":"Dark mode flickers a white background for a fraction of a second","children":{}},"/playground/frontend/why-dom-manipulation-is-slow":{"label":"Why DOM manipulation is slow?","children":{}},"/playground/frontend/why-virtual-dom-is-fast":{"label":"Why Virtual DOM is fast?","children":{}},"/playground/frontend/vitejs-native-modules":{"label":"ViteJS native modules","children":{}},"/playground/frontend/javascript-modules":{"label":"JavaScript modules","children":{}},"/playground/frontend/atomic-design-pattern":{"label":"Atomic Design Pattern","children":{}},"/playground/frontend/focus-trap":{"label":"Focus trap","children":{}},"/playground/frontend/html-inert":{"label":"HTML inert","children":{}},"/playground/frontend/useeffect-double-calls-in-react-18":{"label":"useEffect double calls in React 18","children":{}},"/playground/frontend/react-18":{"label":"React 18","children":{}},"/playground/frontend/remix-versus-nextjs":{"label":"Remix Versus Nextjs","children":{}},"/playground/frontend/zaplib-post-mortem":{"label":"Zaplib post-mortem","children":{}},"/playground/frontend/parallelism-in-javascript":{"label":"Parallelism in JavaScript","children":{}},"/playground/frontend/mpa-spa-and-partial-hydration":{"label":"MPA, SPA and Partial Hydration","children":{}},"/playground/frontend/micro-frontends-microservices-for-frontend-development":{"label":"Micro Frontends Microservices For Frontend Development","children":{}},"/playground/frontend/using-correct-html-element-to-increase-website-accessibility":{"label":"Using Correct Html Element To Increase Website Accessibility","children":{}},"/playground/frontend/remove-unused-css-styles-from-bootstrap-using-purgecss":{"label":"Remove Unused CSS Styles From Bootstrap Using Purgecss","children":{}}}},"/playground/ai":{"label":"AI","children":{"/playground/ai/securing-your-remote-mcp-servers":{"label":"Securing your remote MCP servers","children":{}},"/playground/ai/tool-level-security-for-remote-mcp-servers":{"label":"Tool-Level Security for Remote MCP Servers","children":{}},"/playground/ai/model-context-protocol":{"label":"Intro to Model Context Protocol","children":{}},"/playground/ai/building-llm-system":{"label":"Building LLM System","children":{"/playground/ai/building-llm-system/quantization-in-llm":{"label":"Quantization for large language models","children":{}},"/playground/ai/building-llm-system/graphrag":{"label":"GraphRAG - Building a knowledge graph for RAG system","children":{}},"/playground/ai/building-llm-system/guardrails-in-llm":{"label":"Guardrails in llm","children":{}},"/playground/ai/building-llm-system/react-in-llm":{"label":"ReAct(Reason + Act) in LLM","children":{}},"/playground/ai/building-llm-system/rewoo-in-llm":{"label":"ReWOO: Reasoning without observation - A deeper look","children":{}},"/playground/ai/building-llm-system/model-selection":{"label":"Model selection","children":{}},"/playground/ai/building-llm-system/logs-pillar":{"label":"Logging","children":{}},"/playground/ai/building-llm-system/metric-pillar":{"label":"Metrics","children":{}},"/playground/ai/building-llm-system/observability-in-ai-platforms":{"label":"Observability in AI platforms","children":{}},"/playground/ai/building-llm-system/trace-pillar":{"label":"Tracing","children":{}},"/playground/ai/building-llm-system/intent-classification-by-llm":{"label":"Intent classification by LLM","children":{}},"/playground/ai/building-llm-system/llm-as-a-judge":{"label":"LLM as a judge","children":{}},"/playground/ai/building-llm-system/use-cases-for-llm-applications":{"label":"Use cases for LLM applications","children":{}},"/playground/ai/building-llm-system/the-rise-of-ai-applications-with-llm":{"label":"The rise of AI applications with LLM","children":{}},"/playground/ai/building-llm-system/evaluation-guideline-for-llm-application":{"label":"Evaluation guidelines for LLM applications","children":{}},"/playground/ai/building-llm-system/prevent-prompt-injection":{"label":"Prevent prompt injection","children":{}},"/playground/ai/building-llm-system/building-llm-system":{"label":"Â§ Building LLM system","children":{}},"/playground/ai/building-llm-system/multi-agent-collaboration-for-task-completion":{"label":"Multi-agent collaboration for task completion","children":{}},"/playground/ai/building-llm-system/multimodal-in-rag":{"label":"Multimodal in rag","children":{}}}},"/playground/ai/digest":{"label":"Digest","children":{"/playground/ai/digest/ai-digest-02":{"label":"AI digest #2 New command Aider, OpenHands, Qwen2.5 Coder 32B, Predicted Output","children":{}},"/playground/ai/digest/ai-digest-01":{"label":"AI digest #1 Aider reasoning, OpenAI Realtime API, Cline - pre Claude-dev ","children":{}}}},"/playground/ai/copilots":{"label":"Copilots","children":{"/playground/ai/copilots/projects-operations":{"label":"Project Operations Copilots","children":{}},"/playground/ai/copilots/team-copilots":{"label":"Team Copilots","children":{}}}},"/playground/ai/text-to-mongodb":{"label":"Natural Language to Database Queries: Text-to-MongoDB","children":{}},"/playground/ai/use-cases":{"label":"Use Cases","children":{"/playground/ai/use-cases/salesforce":{"label":"Salesforce use cases","children":{}},"/playground/ai/use-cases/yelp":{"label":"Yelp use cases","children":{}}}},"/playground/ai/evaluate-chatbot-agent-by-simulated-user":{"label":"Evaluate Chatbot Agent by User Simulation","children":{}},"/playground/ai/journey-of-thought-prompting":{"label":"Journey of Thought Prompting: Harnessing AI to Craft Better Prompts","children":{}},"/playground/ai/llm-tracing-in-ai-system":{"label":"LLM tracing in AI system","children":{}},"/playground/ai/caching-with-rag-system":{"label":"Evaluating caching in RAG systems","children":{}},"/playground/ai/generative-ui":{"label":"What is Generative UI?","children":{}},"/playground/ai/re-ranking-in-rag":{"label":"Re-ranking in RAG","children":{}},"/playground/ai/function-calling":{"label":"Function calling in AI agents","children":{}},"/playground/ai/building-llm-powered-tools-with-dify":{"label":"Streamlining Internal Tool Development with Managed LLMOps: A Dify Case Study","children":{}},"/playground/ai/thumbs-up-and-thumbs-down-pattern":{"label":"Thumbs up and Thumbs down pattern","children":{}},"/playground/ai/supervisor-ai-agents":{"label":"Building Agent Supervisors to Generate Insights","children":{}},"/playground/ai/raptor-llm-retrieval":{"label":"RAPTOR: Tree-based Retrieval for Language Models","children":{}},"/playground/ai/proximal-policy-optimization":{"label":"Proximal Policy Optimization","children":{}},"/playground/ai/a-grand-unified-theory-of-the-ai-hype-cycle":{"label":"A Grand Unified Theory of the AI Hype Cycle","children":{}},"/playground/ai/developing-rapidly-with-generative-ai":{"label":"Developing rapidly with Generative AI","children":{}},"/playground/ai/rlhf-with-open-assistant":{"label":"RLHF with Open Assistant","children":{}},"/playground/ai/story-map-for-llms":{"label":"Story map for LLMs","children":{}},"/playground/ai/adversarial-prompting":{"label":"Adversarial Prompting in Prompt Engineering","children":{}},"/playground/ai/chunking-strategies-to-overcome-context-limitation-in-llm":{"label":"Chunking strategies to overcome context limitation in LLM","children":{}},"/playground/ai/llms-accuracy-self-refinement":{"label":"LLM's Accuracy - Self Refinement","children":{}},"/playground/ai/llm-query-caching":{"label":"Query Caching for Large Language Models","children":{}},"/playground/ai/reinforcement-learning":{"label":"Introduction to Reinforcement Learning and Its Application with LLMs","children":{}},"/playground/ai/foundation-model":{"label":"Foundation Models: The Latest Advancement in AI","children":{}},"/playground/ai/select-vector-database-for-llm":{"label":"Select Vector Database for LLM","children":{}},"/playground/ai/build-your-chatbot-with-open-source-large-language-models":{"label":"Build your chatbot with open source Large Language Models","children":{}},"/playground/ai/workaround-with-openais-token-limit-with-langchain":{"label":"Workaround with OpenAI's token limit with Langchain","children":{}},"/playground/ai/working-with-langchain-document-loaders":{"label":"Working with langchain document loaders","children":{}}}},"/playground/go":{"label":"Go","children":{"/playground/go/weekly":{"label":"Weekly","children":{"/playground/go/weekly/dec-13":{"label":"#24 Go 1.24 testing/synctest experiment for time and concurrency testing","children":{}},"/playground/go/weekly/dec-06":{"label":"#23 Draft Release Notes for Go 1.24 and weak pointers in Go","children":{}},"/playground/go/weekly/nov-29":{"label":"#22 GoMLX: ML in Go without Python","children":{}},"/playground/go/weekly/nov-22":{"label":"#21 Go sync.Once is Simple","children":{}},"/playground/go/weekly/nov-15":{"label":"#20 Go Turns 15","children":{}},"/playground/go/weekly/nov-08":{"label":"#19 Writing secure Go code","children":{}},"/playground/go/weekly/nov-01":{"label":"#18 Fuzz Testing Go HTTP Services","children":{}},"/playground/go/weekly/oct-25":{"label":"#17 Leveraging benchstat Projects in Go benchmark and Go Plan9 memo on 450% speeding up calculations","children":{}},"/playground/go/weekly/oct-18":{"label":"#16 Understand sync.Map","children":{}},"/playground/go/weekly/oct-11":{"label":"#15 Go embed and Reflect","children":{}},"/playground/go/weekly/oct-04":{"label":"#14 Compile-time eval & SQLite with wazero","children":{}},"/playground/go/weekly/sep-27":{"label":"#13 Compiler Quests and Vector Vexations","children":{}},"/playground/go/weekly/sep-20":{"label":"#12 CLI Tools for K8s, REST, and Terminals","children":{}},"/playground/go/weekly/sep-13":{"label":"#11 Actors, Frameworks, and the Future of Go","children":{}},"/playground/go/weekly/sep-06":{"label":"#10 Script, Telemetry","children":{}},"/playground/go/weekly/aug-30":{"label":"#9 TinyGo, SQLite vector search, and Permify","children":{}},"/playground/go/weekly/aug-23":{"label":"#8 GoNB, kubetrim, and GopherCon UK 2024","children":{}},"/playground/go/weekly/aug-16":{"label":"#7 Go 1.23, Websockets, and Structs","children":{}},"/playground/go/weekly/aug-09":{"label":"#6 Cogent Core, Russ Cox stepping down","children":{}},"/playground/go/weekly/aug-02":{"label":"#5 Go 1.23 features, Memory, Minecraft, and More","children":{}},"/playground/go/weekly/jul-26":{"label":"#4 Ethical Hacking, HTTP Requests, Mac App Development","children":{}},"/playground/go/weekly/jul-12":{"label":"#3 Generic Collections, Generics Constraints, AI Bot","children":{}},"/playground/go/weekly/jul-05":{"label":"#2 Go 1.23 Iterators","children":{}},"/playground/go/weekly/june-27":{"label":"#1 eBPF and PGO Optimization Techniques","children":{}}}},"/playground/go/extension-interface-pattern":{"label":"Go extension interface pattern","children":{}},"/playground/go/go-import":{"label":"Go import design: using git repo path","children":{}},"/playground/go/go-package":{"label":"Package first design","children":{}},"/playground/go/go-generics-type-safety":{"label":"How does Go achieve type safety when it enables generics?","children":{}},"/playground/go/go-for-enterprise":{"label":"Go For Enterprise","children":{"/playground/go/go-for-enterprise/who-using-golang-in-enterprise":{"label":"Who is using Go in enterprise?","children":{}},"/playground/go/go-for-enterprise/enterprise-standard-language":{"label":"Go as an Enterprise Standard Language","children":{}},"/playground/go/go-for-enterprise/how-to-use-go-in-enterprise":{"label":"How to use Go in the Enterprise","children":{}},"/playground/go/go-for-enterprise/when-to-use-golang-in-enterprise":{"label":"When to use Go in the Enterprise","children":{}},"/playground/go/go-for-enterprise/why-enterprise-chose-java":{"label":"Why Enterprise Chose Java","children":{}},"/playground/go/go-for-enterprise/why-go":{"label":"Why Go?","children":{}}}},"/playground/go/compute-union-2-finite-automata":{"label":"Efficient Union of Finite Automata in Golang: A Practical Approach","children":{}},"/playground/go/approaches-to-manage-concurrent-workloads-like-worker-pools-and-pipelines":{"label":"Approaches To Manage Concurrent Workloads Like Worker Pools And Pipelines","children":{}},"/playground/go/message-queues-and-streaming-platforms-eg-kafka-nats-rabbitmq":{"label":"Message Queues And Streaming Platforms Eg Kafka Nats Rabbitmq","children":{}},"/playground/go/unit-testing-best-practices-in-golang":{"label":"Unit Testing Best Practices In Golang","children":{}},"/playground/go/profiling-in-go":{"label":"Profiling in Go","children":{}},"/playground/go/go-in-software-engineering":{"label":"Go In Software Engineering","children":{}},"/playground/go/go-concurrency":{"label":"Go Concurrency","children":{}},"/playground/go/slice-and-array-in-golang":{"label":"Slice And Array In Golang","children":{}},"/playground/go/use-go-selenium-to-crawl-data":{"label":"Use Go Selenium To Crawl Data","children":{}},"/playground/go/connecting-vim-with-golang":{"label":"Connecting Vim With Golang","children":{}}}},"/playground/market-report":{"label":"Market Report","children":{"/playground/market-report/2024-october":{"label":"October 2024","children":{}},"/playground/market-report/2024-september":{"label":"September 2024","children":{}},"/playground/market-report/2024-august":{"label":"August 2024","children":{}},"/playground/market-report/2024-july":{"label":"July 2024","children":{}},"/playground/market-report/2024-may":{"label":"May 2024","children":{}},"/playground/market-report/2024-april":{"label":"April 2024","children":{}},"/playground/market-report/2024-march":{"label":"March 2024","children":{}},"/playground/market-report/2024-february":{"label":"February 2024","children":{}},"/playground/market-report/2024-january":{"label":"January 2024","children":{}},"/playground/market-report/2023-december":{"label":"December 2023","children":{}}}},"/playground/devbox":{"label":"Devbox","children":{"/playground/devbox/devbox":{"label":"Â§ Devbox","children":{}},"/playground/devbox/story":{"label":"Story","children":{"/playground/devbox/story/devbox-production-success-story":{"label":"Devbox in Production: Our Success Story","children":{}},"/playground/devbox/story/devbox-local-development-env":{"label":"Using Devbox to setup local development environment","children":{}},"/playground/devbox/story/devbox-nix-and-our-devbox-adoption":{"label":"The overview into Nix & how we use Devbox @ Dwarves","children":{}},"/playground/devbox/story/devbox-docker-adoption-and-challenges":{"label":"Our Docker adoption and its challenges","children":{}},"/playground/devbox/story/devbox-a-world-before-docker":{"label":"The world before Docker","children":{}}}},"/playground/devbox/guide":{"label":"Guide","children":{"/playground/devbox/guide/containerless":{"label":"Ditch the Containers: Go Containerless with Devbox","children":{}},"/playground/devbox/guide/devboxjson":{"label":"Devbox.json: Your Project's DNA","children":{}},"/playground/devbox/guide/run-your-own-shell":{"label":"Devbox Shell: Your Dev Environment, Your Rules","children":{}}}},"/playground/devbox/introduction":{"label":"Introduction","children":{"/playground/devbox/introduction/the-reason-for-being":{"label":"The reason for being","children":{}},"/playground/devbox/introduction/why-devbox-but-not-nix":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}}}},"/playground/devbox/research":{"label":"Research","children":{"/playground/devbox/research/content-addressable-storage-in-docker":{"label":"Devbox vs Nix: Why We Chose Simplicity","children":{}},"/playground/devbox/research/fixed-output-derivation":{"label":"Fixed-output Derivation in Nix","children":{}},"/playground/devbox/research/nix-is-faster-than-docker-build":{"label":"Nix is Faster Than Docker Build","children":{}},"/playground/devbox/research/pinning-nixpkgs":{"label":"Pinning nixpkgs in Nix","children":{}},"/playground/devbox/research/shadow-copies":{"label":"Shadow Copies in Docker Builds","children":{}},"/playground/devbox/research/unstable-package-installation":{"label":"Unstable Package Installation in Docker","children":{}}}}}}}},"/careers":{"label":"Careers","children":{"/careers/archived":{"label":"Archived","children":{"/careers/archived/full-stack-engineer":{"label":"Full-Stack Engineer","children":{}},"/careers/archived/executive-assistant":{"label":"Executive Assistant","children":{}},"/careers/archived/technical-recruiter":{"label":"Technical Recruiter","children":{}},"/careers/archived/backend-engineer-go-elixir-rust":{"label":"Backend Engineer, Go/Elixir/Rust","children":{}},"/careers/archived/react-native-developer":{"label":"React Native Developer","children":{}},"/careers/archived/android-developer":{"label":"Mobile Engineer, Android","children":{}},"/careers/archived/community-executive":{"label":"Community Executive","children":{}},"/careers/archived/data-engineering":{"label":"Energy - Data Engineering","children":{}},"/careers/archived/devops":{"label":"DevOps Engineer - FinTech","children":{}},"/careers/archived/frontend-developer-junior":{"label":"Junior Frontend Developer","children":{}},"/careers/archived/frontend":{"label":"Frontend","children":{}},"/careers/archived/ios-developer":{"label":"iOS Developer - EnergyTech","children":{}},"/careers/archived/macos-developer":{"label":"Software Engineer, macOS","children":{}},"/careers/archived/product-designer-new-grad":{"label":"Product Designer, New Grad","children":{}},"/careers/archived/product-designer":{"label":"Product Designer","children":{}},"/careers/archived/qc-automation":{"label":"QC Engineer, Automation - Logistics","children":{}},"/careers/archived/qc-manual":{"label":"Fintech - QC Engineer, Manual","children":{}},"/careers/archived/reactjs-web-engineer":{"label":"Web Engineer, React.js","children":{}},"/careers/archived/visual-designer":{"label":"Visual Designer","children":{}},"/careers/archived/android":{"label":"Android","children":{}},"/careers/archived/golang":{"label":"Golang","children":{}},"/careers/archived/intern":{"label":"Intern","children":{}},"/careers/archived/ios":{"label":"iOS Developer","children":{}},"/careers/archived/qa":{"label":"QA Engineer","children":{}}}},"/careers/open-positions":{"label":"Open Positions","children":{"/careers/open-positions/business-development-manager":{"label":"Business Development","children":{}},"/careers/open-positions/growth":{"label":"Growth","children":{}}}},"/careers/life":{"label":"Life","children":{"/careers/life/dat-nguyen":{"label":"Dat Nguyen","children":{}},"/careers/life/software-design-group":{"label":"Software Design Group","children":{}},"/careers/life/hieu-vu":{"label":"Hieu Vu","children":{}},"/careers/life/nam-nguyen":{"label":"Nam Nguyen","children":{}},"/careers/life/an-tran":{"label":"An Tran","children":{}},"/careers/life/tom-nguyen":{"label":"Tom Nguyen","children":{}},"/careers/life/anh-tran":{"label":"Anh Tran","children":{}},"/careers/life/thanh-pham":{"label":"Thanh Pham","children":{}}}},"/careers/additional-info":{"label":"Additional Info","children":{"/careers/additional-info/culture-handbook":{"label":"Culture Handbook","children":{}},"/careers/additional-info/how-we-hire":{"label":"How we hire","children":{}},"/careers/additional-info/how-we-work":{"label":"How we work","children":{}},"/careers/additional-info/making-a-career":{"label":"Making a career","children":{}},"/careers/additional-info/the-manifesto":{"label":"The Manifesto","children":{}},"/careers/additional-info/what-we-stand-for":{"label":"What we stand for","children":{}},"/careers/additional-info/what-we-value":{"label":"What we value","children":{}},"/careers/additional-info/where-we-work":{"label":"Where we work","children":{}},"/careers/additional-info/life-at-dwarves":{"label":"Life at Dwarves","children":{}},"/careers/additional-info/benefits-and-perks":{"label":"Benefits And Perks","children":{}}}},"/careers/hiring":{"label":"Hiring","children":{"/careers/hiring/readme":{"label":"Careers","children":{}}}},"/careers/apprentice":{"label":"Apprentice","children":{"/careers/apprentice/batch-of-2022":{"label":"Batch of 2022","children":{}},"/careers/apprentice/2022-meet-ngoc-thanh-pham":{"label":"Meet the Mentors: Ngoc Thanh Pham","children":{}},"/careers/apprentice/2022-meet-tuan-dao":{"label":"Meet the Mentors: Tuan Dao","children":{}},"/careers/apprentice/apprentice":{"label":"Apprentice Program","children":{}}}}}},"/playbook":{"label":"Playbook","children":{"/playbook/operations":{"label":"Operations","children":{"/playbook/operations/checklists":{"label":"Checklists","children":{"/playbook/operations/checklists/leave-and-request-checklist":{"label":"Leave Request","children":{}},"/playbook/operations/checklists/offboarding-checklist":{"label":"Offboarding","children":{}},"/playbook/operations/checklists/artifact-checklist":{"label":"Back up Artifact","children":{}},"/playbook/operations/checklists/project-archive":{"label":"Project Archive","children":{}},"/playbook/operations/checklists/project-case-study":{"label":"Project Case Study","children":{}},"/playbook/operations/checklists/project-communication":{"label":"Project Communication","children":{}},"/playbook/operations/checklists/project-handover":{"label":"Project Handover","children":{}},"/playbook/operations/checklists/project-initialization":{"label":"Project Initialization","children":{}},"/playbook/operations/checklists/assets-checklist":{"label":"Assets","children":{}},"/playbook/operations/checklists/billing-checklist":{"label":"Billing","children":{}},"/playbook/operations/checklists/candidate-checklist":{"label":"Candidate","children":{}},"/playbook/operations/checklists/consulting-contract-checklist":{"label":"Consulting Contract","children":{}},"/playbook/operations/checklists/hiring-checklist":{"label":"Hiring","children":{}},"/playbook/operations/checklists/onboarding-checklist":{"label":"Onboarding","children":{}},"/playbook/operations/checklists/unemployment-social-health-insurance":{"label":"Unemployment, Social, Health Insurance","children":{}},"/playbook/operations/checklists/vietnam-invoice-checklist":{"label":"Vietnam Invoice","children":{}}}},"/playbook/operations/project-schedule-delivery-guidelines":{"label":"Project Delivery Schedule and Guidelines","children":{}},"/playbook/operations/ogif":{"label":"OGIF - Oh God It's Friday","children":{}},"/playbook/operations/red-flags":{"label":"Red Flags","children":{}},"/playbook/operations/focus-on-software-delivery":{"label":"Focus On Software Delivery","children":{}},"/playbook/operations/are-you-helping":{"label":"Are You Helping","children":{}},"/playbook/operations/the-inner-circle":{"label":"The Inner Circle","children":{}},"/playbook/operations/mbti-type-intj":{"label":"MBTI Type INTJ","children":{}},"/playbook/operations/mbti-type-istp":{"label":"MBTI Type ISTP","children":{}},"/playbook/operations/mbti-type-estj":{"label":"MBTI Type ESTJ","children":{}},"/playbook/operations/mbti-type-istj":{"label":"MBTI Type ISTJ","children":{}},"/playbook/operations/applying-myersbriggs-type-indicator-in-hr":{"label":"Applying Myersbriggs Type Indicator In Hiring","children":{}},"/playbook/operations/the-four-preferences":{"label":"The Four Preferences","children":{}},"/playbook/operations/making-decision-as-a-team-member":{"label":"Making Decision As A Team Member","children":{}},"/playbook/operations/adjust-the-way-we-work-in-basecamp-style":{"label":"Adjust The Way We Work In Basecamp Style","children":{}},"/playbook/operations/beyond-the-title":{"label":"Beyond The Title","children":{}},"/playbook/operations/go-the-extra-mile":{"label":"Go The Extra Mile","children":{}},"/playbook/operations/the-dwarves-runs-by-ideas":{"label":"The Dwarves Runs By Ideas","children":{}},"/playbook/operations/a-tips-of-hiring-dont":{"label":"A Tips Of Hiring - Do & Don't","children":{}},"/playbook/operations/the-dwarves-culture-handbook":{"label":"The Dwarves Culture Handbook","children":{}},"/playbook/operations/delegation-and-believe-it-will-work":{"label":"Delegation And Believe It Will Work","children":{}},"/playbook/operations/constructive-feedback":{"label":"Constructive Feedback","children":{}},"/playbook/operations/transparency":{"label":"Transparency","children":{}},"/playbook/operations/bric-a-brac":{"label":"Bric A Brac","children":{}},"/playbook/operations/account":{"label":"Account","children":{}},"/playbook/operations/avoid-burn-out":{"label":"Avoid Burn Out","children":{}},"/playbook/operations/writing-management-objectives-in-smart":{"label":"Writing Management Objectives In Smart","children":{}},"/playbook/operations/building-a-solid-high-performing-team":{"label":"Building A Solid High Performing Team","children":{}},"/playbook/operations/hiring-for-operations-team":{"label":"Hiring For Operations Team","children":{}},"/playbook/operations/annual-bonus-for-sales":{"label":"Annual bonus for sales","children":{}},"/playbook/operations/bunk-license-check":{"label":"Bunk license check","children":{}},"/playbook/operations/collaboration-guidelines":{"label":"Collaboration Guidelines","children":{}},"/playbook/operations/compliance-check-process":{"label":"Compliance Check Process","children":{}},"/playbook/operations/email-template":{"label":"Email Template","children":{"/playbook/operations/email-template/assignment-invitation-2":{"label":"Assignment Inviation (Skip pre-assessment)","children":{}},"/playbook/operations/email-template/assignment-invitation":{"label":"Assignment Inviation","children":{}},"/playbook/operations/email-template/confirm-resume-date":{"label":"Confirm Employee's Resume Date Day","children":{}},"/playbook/operations/email-template/farewell":{"label":"Farewell Letter","children":{}},"/playbook/operations/email-template/follow-up-onboarding-items":{"label":"Follow-up Onboarding Items","children":{}},"/playbook/operations/email-template/hung-king-commemoration-day":{"label":"Hung King Commemoration Day","children":{}},"/playbook/operations/email-template/information-about-resource-change":{"label":"Inform about resource change","children":{}},"/playbook/operations/email-template/international-labour-day":{"label":"International Labour Day","children":{}},"/playbook/operations/email-template/interview-invitation":{"label":"Interview Invitation","children":{}},"/playbook/operations/email-template/milestone-sign-off":{"label":"Milestone sign-off","children":{}},"/playbook/operations/email-template/national-day":{"label":"National Day","children":{}},"/playbook/operations/email-template/new-year-day":{"label":"New Year Day","children":{}},"/playbook/operations/email-template/offer-letter":{"label":"Offer Letter","children":{}},"/playbook/operations/email-template/referral-bonus-confirmation-note":{"label":"Referral Bonus Confirmation Note","children":{}},"/playbook/operations/email-template/rejection-email":{"label":"Rejection","children":{}},"/playbook/operations/email-template/salary-increment":{"label":"Salary Increment Announcement","children":{}},"/playbook/operations/email-template/tet-holiday":{"label":"Tet Holiday","children":{}},"/playbook/operations/email-template/thank-you-letter":{"label":"Thank you letter","children":{}},"/playbook/operations/email-template/welcome-onboard":{"label":"Welcome Onboard","children":{}},"/playbook/operations/email-template/welcome-to-dwarves-update":{"label":"Welcome to Dwarves Updates","children":{}}}},"/playbook/operations/naming-convention":{"label":"Naming convention","children":{}},"/playbook/operations/delegate-work-not-responsibility":{"label":"Delegate Work Not Responsibility","children":{}},"/playbook/operations/types-of-employees":{"label":"Types Of Employees","children":{}},"/playbook/operations/hiring-approach":{"label":"Hiring Approach","children":{}},"/playbook/operations/the-okr":{"label":"The OKR","children":{}},"/playbook/operations/our-metrics-for-performance-review":{"label":"Our Metrics For Performance Review","children":{}},"/playbook/operations/make-remote-working-works":{"label":"Make Remote Working Works","children":{}},"/playbook/operations/blocking-distraction":{"label":"Blocking Distraction","children":{}},"/playbook/operations/effective-meeting":{"label":"Effective Meeting","children":{}},"/playbook/operations/our-policy-for-remote-working":{"label":"Our Policy For Remote Working","children":{}}}},"/playbook/business":{"label":"Business","children":{"/playbook/business/pricing-model-bill-by-hours":{"label":"Pricing model: Bill by hours","children":{}},"/playbook/business/invoice":{"label":"Invoice","children":{}},"/playbook/business/nda":{"label":"NDA","children":{}},"/playbook/business/collaboration-guideline":{"label":"Collaboration Guideline","children":{}},"/playbook/business/df-workflow":{"label":"Dwarves Workflow","children":{}},"/playbook/business/fbsc":{"label":"FBSC","children":{}},"/playbook/business/how-to-work-with-clients":{"label":"How to work with clients","children":{}},"/playbook/business/service-feedbacks":{"label":"Service Feedbacks","children":{}},"/playbook/business/setting-the-budget":{"label":"Setting The Budget","children":{}},"/playbook/business/fixed-budget-scope-controlled":{"label":"Fixed Budget Scope Controlled","children":{}},"/playbook/business/the-adjacent-possible":{"label":"The Adjacent Possible","children":{}}}},"/playbook/engineering":{"label":"Engineering","children":{"/playbook/engineering/estimation-guidelines":{"label":"Estimation Guidelines","children":{}},"/playbook/engineering/presentation":{"label":"monitoring","children":{}},"/playbook/engineering/repo-icon":{"label":"release","children":{}}}},"/playbook/design":{"label":"Design","children":{"/playbook/design/design-system":{"label":"lean-canvas","children":{}},"/playbook/design/ia":{"label":"nda","children":{}},"/playbook/design/ix":{"label":"IA","children":{}},"/playbook/design/aarrr":{"label":"aarrr","children":{}},"/playbook/design/design-sprint":{"label":"Design Sprint","children":{}},"/playbook/design/lean-canvas":{"label":"Lean Canvas","children":{}},"/playbook/design/prototype":{"label":"Low-fidelity prototype: UI Design","children":{}},"/playbook/design/ui":{"label":"UI","children":{}},"/playbook/design/ux":{"label":"UX","children":{}},"/playbook/design/wireframe":{"label":"wireframe","children":{}}}}}},"/earn":{"label":"Earn","children":{"/earn/readme":{"label":"Open Bounties","children":{}}}},"/updates":{"label":"Updates","children":{"/updates/ogif":{"label":"OGIF","children":{"/updates/ogif/41-20250314":{"label":"#41 ICY-BTC, GitHub Bot, MCP-DB, Pocket Turing","children":{}},"/updates/ogif/28-20241018":{"label":"#28 Go sync.Map, AI UX, Yelp AI, LLM Patterns, Git Analysis","children":{}},"/updates/ogif/27-20241011":{"label":"#27 Go weekly, Frontend, AI UX, Finite Automata","children":{}},"/updates/ogif/26-20241004":{"label":"#26 Design insights, Go tools, Trading app, Chatbots, Essays","children":{}},"/updates/ogif/25-20240927":{"label":"#25 Team updates, Hybrid work, AI insights, Go weekly","children":{}},"/updates/ogif/24-20240920":{"label":"#24 Go weekly, AI workflows, Team AI demo, Figma-UI with Claude","children":{}},"/updates/ogif/23-20240913":{"label":"#23 Go weekly, FE report, Hybrid work, AI agents","children":{}},"/updates/ogif/22-20240906":{"label":"#22 Hybrid work, Tech report, Go weekly, AI demo","children":{}},"/updates/ogif/21-20240830":{"label":"#21 Community engagement, Go weekly, Journey of thought for prompt engineering","children":{}},"/updates/ogif/20-20240823":{"label":"#20 Go weekly, Dynamic objects, Devbox, LLM tracing, Cursor AI","children":{}},"/updates/ogif/19-20240821":{"label":"#19 Go weekly, UI design, File sharing, Dify AI","children":{}},"/updates/ogif/18-20240809":{"label":"#18 Go weekly, RAG, UI, FE updates","children":{}},"/updates/ogif/17-20240802":{"label":"#17 Community Call July, C4 Model, Interview Life in the US","children":{}},"/updates/ogif/16-20240726":{"label":"#16 Go weekly, Dune query, AI voice clone, RAG re-ranking","children":{}},"/updates/ogif/15-20240719":{"label":"#15 AI Supervisors, Local-first Software, Code Completion, Bot Commands","children":{}},"/updates/ogif/14-20240712":{"label":"#14 Generic Collections, Pricing Models, and OGIF Summarizer","children":{}},"/updates/ogif/13-20240705":{"label":"#13 Go Weekly updates, Radix Sort, Human Feedback Mechanism, and effective ChatGPT usage","children":{}},"/updates/ogif/12-20240628":{"label":"#12 June updates, Go Performance, eBPF, PGO, Multimodal RAG","children":{}},"/updates/ogif/11-20240621":{"label":"#11 Design patterns: template method & visitor, Radix sort, and weekly tech commentary","children":{}},"/updates/ogif/10-20240614":{"label":"#10 Behavioral Patterns and Map Content Organization","children":{}},"/updates/ogif/9-20240607":{"label":"#9 What's next for June and Behavior Design Patterns","children":{}},"/updates/ogif/7-20240517":{"label":"#7 Echelon EXPO, Programming patterns, and Moonlighting","children":{}},"/updates/ogif/6-20240510":{"label":"#6 Factory Pattern, Erlang State Machines, and Trading Process","children":{}},"/updates/ogif/5-20240503":{"label":"#5 Singapore Market Report, C4 Modelling, Memo's Nested Sidebar","children":{}},"/updates/ogif/4-20240426":{"label":"#4 DCA, Devbox","children":{}},"/updates/ogif/3-20240419":{"label":"#3 Generative AI, Tokenomics, and Finance Talks","children":{}},"/updates/ogif/2-20240412":{"label":"#2 Devbox as the new Docker, Security Standards, and Understanding Liquidity","children":{}},"/updates/ogif/1-20240405":{"label":"#1 Markdown Presentations, Research Pipeline, Screenshots How-to","children":{}},"/updates/ogif/readme":{"label":"OGIF - Oh God It's Friday","children":{}}}},"/updates/changelog":{"label":"Changelog","children":{"/updates/changelog/2025-whats-new-february":{"label":"What's New in February 2025","children":{}},"/updates/changelog/2024-in-review":{"label":"2024 In Review","children":{}},"/updates/changelog/2024-whats-new-december":{"label":"What's New in December 2024","children":{}},"/updates/changelog/2024-summit-building-bonds-our-way":{"label":"Summit 2024: Building bonds our way","children":{}},"/updates/changelog/2024-whats-new-november":{"label":"What's New in November 2024","children":{}},"/updates/changelog/2024-whats-new-oct":{"label":"What's New in October 2024","children":{}},"/updates/changelog/2024-whats-new-september":{"label":"What's New in September 2024","children":{}},"/updates/changelog/2024-navigating-changes":{"label":"Navigating changes","children":{}},"/updates/changelog/2024-whats-new-august":{"label":"What's New in August 2024","children":{}},"/updates/changelog/2024-whats-new-july":{"label":"What's New in July 2024","children":{}},"/updates/changelog/2024-semi-annual-review":{"label":"State of Dwarves: 2024 Semi-annual Review","children":{}},"/updates/changelog/2024-whats-new-june":{"label":"What's New in June 2024","children":{}},"/updates/changelog/2024-whats-new-may":{"label":"What's New in May 2024","children":{}},"/updates/changelog/2024-community-meet-up":{"label":"Dwarvesâ€™ 2nd community offline meet-up","children":{}},"/updates/changelog/2024-whats-new-april":{"label":"What's New in April 2024","children":{}},"/updates/changelog/2024-whats-new-march":{"label":"What's New in March 2024","children":{}},"/updates/changelog/2024-whats-new-february":{"label":"What's New in February 2024","children":{}},"/updates/changelog/2024-whats-new-january":{"label":"What's New in January 2024","children":{}},"/updates/changelog/2023-whats-new-december":{"label":"What's New in December 2023","children":{}},"/updates/changelog/readme":{"label":"Changelog","children":{}},"/updates/changelog/2023-whats-new-november":{"label":"What's New in November 2023","children":{}},"/updates/changelog/2023-whats-new-october":{"label":"What's New in October 2023","children":{}},"/updates/changelog/2023-happy":{"label":"Happy 2023","children":{}},"/updates/changelog/2022-dwarves-of-the-year":{"label":"Dwarves Of The Year 2022","children":{}},"/updates/changelog/2022-in-review":{"label":"2022 In Review","children":{}},"/updates/changelog/2022-summit-engineering-a-good-time":{"label":"Summit 2022: Engineering A Good Time","children":{}},"/updates/changelog/road-to-100":{"label":"Road To 100","children":{}},"/updates/changelog/2022-whats-new-may":{"label":"What's New in May 2022","children":{}},"/updates/changelog/2022-whats-new-january":{"label":"What's New in January 2022","children":{}},"/updates/changelog/2021-whats-new-december":{"label":"What's New in December 2021","children":{}},"/updates/changelog/2021-dwarves-of-the-year":{"label":"Dwarves Of The Year 2021","children":{}},"/updates/changelog/2021-whats-new-july":{"label":"What's New in July 2021","children":{}},"/updates/changelog/2020-in-review":{"label":"2020 In Review","children":{}},"/updates/changelog/2021-in-review":{"label":"2021 In Review","children":{}},"/updates/changelog/2019-in-review":{"label":"2019 In Review","children":{}},"/updates/changelog/2018-in-review":{"label":"2018 In Review","children":{}}}},"/updates/forward-engineering":{"label":"Forward Engineering","children":{"/updates/forward-engineering/2024-2025":{"label":20242025,"children":{}},"/updates/forward-engineering/2024-quarter-3":{"label":"Quarter 3 2024","children":{}},"/updates/forward-engineering/2023-november":{"label":"November 2023","children":{}},"/updates/forward-engineering/2023-october":{"label":"October 2023","children":{}},"/updates/forward-engineering/2023-august":{"label":"August 2023","children":{}},"/updates/forward-engineering/2023-june":{"label":"June 2023","children":{}},"/updates/forward-engineering/2023-may":{"label":"May 2023","children":{}},"/updates/forward-engineering/2023-march":{"label":"March 2023","children":{}},"/updates/forward-engineering/2023-december":{"label":"December 2023","children":{}},"/updates/forward-engineering/2022":{"label":2022,"children":{}},"/updates/forward-engineering/tech-radar-volume-03":{"label":"Tech Radar Volume 03","children":{}},"/updates/forward-engineering/tech-radar-volume-02":{"label":"Tech Radar Volume 02","children":{}},"/updates/forward-engineering/tech-radar-volume-01":{"label":"Tech Radar Volume 01","children":{}},"/updates/forward-engineering/tech-radar-the-introduction":{"label":"Tech Radar Introduction","children":{}}}},"/updates/digest":{"label":"Digest","children":{"/updates/digest/15-new-year-gathering":{"label":"#15 New year gathering","children":{}},"/updates/digest/14-back-to-the-office":{"label":"#14 Hybrid work harmony","children":{}},"/updates/digest/13-more-than-lines-of-code":{"label":"#13 More than lines of code","children":{}},"/updates/digest/12-summer-moments":{"label":"#12 Summer moments","children":{}},"/updates/digest/11-come-grow-with-us":{"label":"#11 Come grow with us","children":{}},"/updates/digest/10-from-lean-to-learner":{"label":"#10 From lean to learner","children":{}},"/updates/digest/9-a-little-more-speed-for-summer":{"label":"#9 A little more speed for summer","children":{}},"/updates/digest/8-then-came-the-last-days-of-may":{"label":"#8 Then came the last days of May","children":{}},"/updates/digest/7-a-journey-through-time":{"label":"#7 A journey through time","children":{}},"/updates/digest/6-stay-for-the-culture":{"label":"#6 Come for the conversation, stay for the culture","children":{}},"/updates/digest/5-delay-the-gratification":{"label":"#5 Endure the hardship, delay the gratification","children":{}},"/updates/digest/4-finding-your-authentic-tribe":{"label":"#4 Finding your authentic tribe","children":{}},"/updates/digest/3-we-all-start-somewhere":{"label":"#3 We all start somewhere","children":{}},"/updates/digest/2-walk-around-learn-around":{"label":"#2 Walk around learn around","children":{}},"/updates/digest/1-what-do-you-stand-for":{"label":"#1 What do you stand for?","children":{}},"/updates/digest/readme":{"label":"Digest","children":{}}}},"/updates/newsletter":{"label":"Newsletter","children":{"/updates/newsletter/knowledge-base":{"label":"Build your knowledge base","children":{}},"/updates/newsletter/dwarve-updates-ai-llm":{"label":"The Stage of AI and LLM at Dwarves","children":{}},"/updates/newsletter/readme":{"label":"_base","children":{}},"/updates/newsletter/growth-stages":{"label":"The Stage of Growth at Dwarves","children":{}},"/updates/newsletter/the-next-leading-chairs":{"label":"The Next Leading Chairs","children":{}},"/updates/newsletter/blockchain-and-data":{"label":"The future is blockchain and data","children":{}},"/updates/newsletter/hiring-stages":{"label":"The stages of hiring at Dwarves","children":{}},"/updates/newsletter/2021-in-review":{"label":"It's a wrap: 2021 in Review","children":{}},"/updates/newsletter/engineering-org-structure":{"label":"Engineering Organizational Structure","children":{}},"/updates/newsletter/path-to-growth":{"label":"The Path To Growth at Dwarves","children":{}},"/updates/newsletter/engineer-performance-review":{"label":"Engineer Performance Review","children":{}},"/updates/newsletter/project-compliance":{"label":"Project Compliance","children":{}},"/updates/newsletter/dalat-office":{"label":"Da Lat Office","children":{}},"/updates/newsletter/dwarves-updates":{"label":"Dwarves Updates","children":{}}}},"/updates/culture-test":{"label":"Culture Test","children":{}},"/updates/fund":{"label":"Fund","children":{"/updates/fund/dwarves-ventures-fund-1":{"label":"Dwarves Ventures Fund 1","children":{}},"/updates/fund/dwarves-ventures-fund-0":{"label":"Dwarves Ventures Fund 0","children":{}}}}}}}},"/tags":{"label":"Popular Tags","children":{"/tags/ai":{"label":"#ai","children":{},"count":58},"/tags/hiring":{"label":"#hiring","children":{},"count":61},"/tags/case-study":{"label":"#case-study","children":{},"count":29},"/tags/handbook":{"label":"#handbook","children":{},"count":43},"/tags/business":{"label":"#business","children":{},"count":10},"/tags/growth":{"label":"#growth","children":{},"count":2},"/tags/consulting":{"label":"#consulting","children":{},"count":24},"/tags/market-report":{"label":"#market-report","children":{},"count":34},"/tags/tech-report":{"label":"#tech-report","children":{},"count":15},"/tags/software-development":{"label":"#software-development","children":{},"count":1},"/tags/database-management":{"label":"#database-management","children":{},"count":1},"/tags/icy":{"label":"#icy","children":{},"count":10},"/tags/career":{"label":"#career","children":{},"count":44},"/tags/full-stack":{"label":"#full-stack","children":{},"count":1},"/tags/engineer":{"label":"#engineer","children":{},"count":3},"/tags/ux-ui":{"label":"#ux-ui","children":{},"count":13},"/tags/product-design":{"label":"#product-design","children":{},"count":7},"/tags/report":{"label":"#report","children":{},"count":8},"/tags/checklist":{"label":"#checklist","children":{},"count":17},"/tags/presentation":{"label":"#presentation","children":{},"count":1},"/tags/business-development":{"label":"#business-development","children":{},"count":1},"/tags/database":{"label":"#database","children":{},"count":8},"/tags/sql":{"label":"#sql","children":{},"count":4},"/tags/data-modeling":{"label":"#data-modeling","children":{},"count":1},"/tags/data-engineering":{"label":"#data-engineering","children":{},"count":4},"/tags/system-design":{"label":"#system-design","children":{},"count":2},"/tags/architecture":{"label":"#architecture","children":{},"count":4},"/tags/etl":{"label":"#etl","children":{},"count":3},"/tags/automata":{"label":"#automata","children":{},"count":1},"/tags/fintech":{"label":"#fintech","children":{},"count":16},"/tags/mobile":{"label":"#mobile","children":{},"count":1},"/tags/wala":{"label":"#wala","children":{},"count":3},"/tags/fnb":{"label":"#fnb","children":{},"count":2},"/tags/film":{"label":"#film","children":{},"count":1},"/tags/go":{"label":"#go","children":{},"count":5},"/tags/error":{"label":"#error","children":{},"count":1},"/tags/startup":{"label":"#startup","children":{},"count":9},"/tags/shares":{"label":"#shares","children":{},"count":1},"/tags/founder":{"label":"#founder","children":{},"count":1},"/tags/entertainment":{"label":"#entertainment","children":{},"count":1},"/tags/life-at-dwarves":{"label":"#life-at-dwarves","children":{},"count":8},"/tags/hybrid-working":{"label":"#hybrid-working","children":{},"count":3},"/tags/guide":{"label":"#guide","children":{},"count":10},"/tags/security":{"label":"#security","children":{},"count":9},"/tags/reward":{"label":"#reward","children":{},"count":3},"/tags/team":{"label":"#team","children":{},"count":47},"/tags/community":{"label":"#community","children":{},"count":39},"/tags/design":{"label":"#design","children":{},"count":31},"/tags/ux":{"label":"#ux","children":{},"count":2},"/tags/directory-structure":{"label":"#directory-structure","children":{},"count":2},"/tags/file-management":{"label":"#file-management","children":{},"count":2},"/tags/file-system":{"label":"#file-system","children":{},"count":2},"/tags/permissions":{"label":"#permissions","children":{},"count":1},"/tags/database-modelling":{"label":"#database-modelling","children":{},"count":1},"/tags/people":{"label":"#people","children":{},"count":25},"/tags/operations":{"label":"#operations","children":{},"count":75},"/tags/llm":{"label":"#llm","children":{},"count":76},"/tags/rag":{"label":"#rag","children":{},"count":5},"/tags/search":{"label":"#search","children":{},"count":1},"/tags/evaluation":{"label":"#evaluation","children":{},"count":3},"/tags/project":{"label":"#project","children":{},"count":16},"/tags/billbyhours":{"label":"#billbyhours","children":{},"count":1},"/tags/careers":{"label":"#careers","children":{},"count":2},"/tags/engineering":{"label":"#engineering","children":{},"count":64},"/tags/delivery":{"label":"#delivery","children":{},"count":2},"/tags/subscription":{"label":"#subscription","children":{},"count":1},"/tags/pricing":{"label":"#pricing","children":{},"count":1},"/tags/product":{"label":"#product","children":{},"count":1},"/tags/blockchain":{"label":"#blockchain","children":{},"count":50},"/tags/evm":{"label":"#evm","children":{},"count":5},"/tags/foundry":{"label":"#foundry","children":{},"count":2},"/tags/search-engine":{"label":"#search-engine","children":{},"count":1},"/tags/duckdb":{"label":"#duckdb","children":{},"count":3},"/tags/transformers.js":{"label":"#transformers.js","children":{},"count":1},"/tags/hybrid-search":{"label":"#hybrid-search","children":{},"count":1},"/tags/erlang":{"label":"#erlang","children":{},"count":1},"/tags/elixir":{"label":"#elixir","children":{},"count":5},"/tags/fsm":{"label":"#fsm","children":{},"count":1},"/tags/design-pattern":{"label":"#design-pattern","children":{},"count":9},"/tags/gang-of-four":{"label":"#gang-of-four","children":{},"count":9},"/tags/observer-pattern":{"label":"#observer-pattern","children":{},"count":1},"/tags/behavior-pattern":{"label":"#behavior-pattern","children":{},"count":2},"/tags/visitor-design-pattern":{"label":"#visitor-design-pattern","children":{},"count":1},"/tags/strategy-design-pattern":{"label":"#strategy-design-pattern","children":{},"count":1},"/tags/ogif":{"label":"#ogif","children":{},"count":29},"/tags/guidelines":{"label":"#guidelines","children":{},"count":3},"/tags/feedback":{"label":"#feedback","children":{},"count":2},"/tags/mechanism":{"label":"#mechanism","children":{},"count":1},"/tags/local-first":{"label":"#local-first","children":{},"count":1},"/tags/crdt":{"label":"#crdt","children":{},"count":2},"/tags/data-synchronization":{"label":"#data-synchronization","children":{},"count":1},"/tags/data-ownership":{"label":"#data-ownership","children":{},"count":1},"/tags/real-time-collaboration":{"label":"#real-time-collaboration","children":{},"count":1},"/tags/rust":{"label":"#rust","children":{},"count":10},"/tags/trait":{"label":"#trait","children":{},"count":1},"/tags/error-handling":{"label":"#error-handling","children":{},"count":1},"/tags/data-structure":{"label":"#data-structure","children":{},"count":1},"/tags/bloom-filter":{"label":"#bloom-filter","children":{},"count":1},"/tags/big-o":{"label":"#big-o","children":{},"count":1},"/tags/behavioral-pattern":{"label":"#behavioral-pattern","children":{},"count":1},"/tags/golang":{"label":"#golang","children":{},"count":44},"/tags/behavior-patterns":{"label":"#behavior-patterns","children":{},"count":2},"/tags/algorithms":{"label":"#algorithms","children":{},"count":1},"/tags/sorting":{"label":"#sorting","children":{},"count":1},"/tags/network":{"label":"#network","children":{},"count":2},"/tags/machine-learning":{"label":"#machine-learning","children":{},"count":2},"/tags/zettelkasten":{"label":"#zettelkasten","children":{},"count":1},"/tags/prompt":{"label":"#prompt","children":{},"count":1},"/tags/chatgpt":{"label":"#chatgpt","children":{},"count":1},"/tags/solana":{"label":"#solana","children":{},"count":7},"/tags/amm":{"label":"#amm","children":{},"count":1},"/tags/memo":{"label":"#memo","children":{},"count":15},"/tags/instructions":{"label":"#instructions","children":{},"count":10},"/tags/guideline":{"label":"#guideline","children":{},"count":15},"/tags/ops":{"label":"#ops","children":{},"count":2},"/tags/nft":{"label":"#nft","children":{},"count":3},"/tags/workflow":{"label":"#workflow","children":{},"count":4},"/tags/recording":{"label":"#recording","children":{},"count":1},"/tags/history":{"label":"#history","children":{},"count":1},"/tags/creational-design-pattern":{"label":"#creational-design-pattern","children":{},"count":1},"/tags/moc":{"label":"#moc","children":{},"count":3},"/tags/software-design":{"label":"#software-design","children":{},"count":2},"/tags/software-architecture":{"label":"#software-architecture","children":{},"count":3},"/tags/graphical-notation":{"label":"#graphical-notation","children":{},"count":2},"/tags/energy":{"label":"#energy","children":{},"count":1},"/tags/techecosystem":{"label":"#techecosystem","children":{},"count":1},"/tags/summit":{"label":"#summit","children":{},"count":4},"/tags/crypto":{"label":"#crypto","children":{},"count":1},"/tags/content":{"label":"#content","children":{},"count":6},"/tags/investment":{"label":"#investment","children":{},"count":1},"/tags/personal-finance":{"label":"#personal-finance","children":{},"count":1},"/tags/dfg":{"label":"#dfg","children":{},"count":2},"/tags/tutorial":{"label":"#tutorial","children":{},"count":5},"/tags/standardization":{"label":"#standardization","children":{},"count":1},"/tags/work-adoption":{"label":"#work-adoption","children":{},"count":1},"/tags/research":{"label":"#research","children":{},"count":3},"/tags/field-notes":{"label":"#field-notes","children":{},"count":1},"/tags/innovation":{"label":"#innovation","children":{},"count":2},"/tags/radar":{"label":"#radar","children":{},"count":10},"/tags/bounty":{"label":"#bounty","children":{},"count":4},"/tags/communications":{"label":"#communications","children":{},"count":3},"/tags/token":{"label":"#token","children":{},"count":2},"/tags/brain":{"label":"#brain","children":{},"count":1},"/tags/knowledge-base":{"label":"#knowledge-base","children":{},"count":1},"/tags/engineering/data":{"label":"#engineering/data","children":{},"count":5},"/tags/data-pipeline":{"label":"#data-pipeline","children":{},"count":1},"/tags/vector-database":{"label":"#vector-database","children":{},"count":4},"/tags/partners":{"label":"#partners","children":{},"count":1},"/tags/brainery":{"label":"#brainery","children":{},"count":2},"/tags/devops":{"label":"#devops","children":{},"count":5},"/tags/google-cloud":{"label":"#google-cloud","children":{},"count":1},"/tags/google-data-studio":{"label":"#google-data-studio","children":{},"count":1},"/tags/google-data-fusion":{"label":"#google-data-fusion","children":{},"count":1},"/tags/reliability":{"label":"#reliability","children":{},"count":2},"/tags/cdap":{"label":"#cdap","children":{},"count":1},"/tags/data":{"label":"#data","children":{},"count":14},"/tags/google-dataproc":{"label":"#google-dataproc","children":{},"count":1},"/tags/hadoop":{"label":"#hadoop","children":{},"count":2},"/tags/streaming":{"label":"#streaming","children":{},"count":1},"/tags/earn":{"label":"#earn","children":{},"count":2},"/tags/ecommerce":{"label":"#ecommerce","children":{},"count":2},"/tags/dropshipping":{"label":"#dropshipping","children":{},"count":1},"/tags/dwarves":{"label":"#dwarves","children":{},"count":23},"/tags/work":{"label":"#work","children":{},"count":18},"/tags/internal":{"label":"#internal","children":{},"count":10},"/tags/discussion":{"label":"#discussion","children":{},"count":6},"/tags/event":{"label":"#event","children":{},"count":7},"/tags/labs":{"label":"#labs","children":{},"count":28},"/tags/catchup":{"label":"#catchup","children":{},"count":5},"/tags/policies":{"label":"#policies","children":{},"count":1},"/tags/home":{"label":"#home","children":{},"count":2},"/tags/tauri":{"label":"#tauri","children":{},"count":1},"/tags/htmx":{"label":"#htmx","children":{},"count":2},"/tags/frontend":{"label":"#frontend","children":{},"count":68},"/tags/performance":{"label":"#performance","children":{},"count":37},"/tags/culture":{"label":"#culture","children":{},"count":10},"/tags/emplpoyee":{"label":"#emplpoyee","children":{},"count":1},"/tags/estimation":{"label":"#estimation","children":{},"count":1},"/tags/code-generation":{"label":"#code-generation","children":{},"count":1},"/tags/typesafe":{"label":"#typesafe","children":{},"count":1},"/tags/fullstack":{"label":"#fullstack","children":{},"count":2},"/tags/lifeatdwarves":{"label":"#lifeatdwarves","children":{},"count":1},"/tags/craftsmanship":{"label":"#craftsmanship","children":{},"count":1},"/tags/workshop":{"label":"#workshop","children":{},"count":1},"/tags/demo":{"label":"#demo","children":{},"count":1},"/tags/performance-review":{"label":"#performance-review","children":{},"count":2},"/tags/assessment":{"label":"#assessment","children":{},"count":1},"/tags/knowledge":{"label":"#knowledge","children":{},"count":2},"/tags/tech-radar":{"label":"#tech-radar","children":{},"count":1},"/tags/evaluating-tech":{"label":"#evaluating-tech","children":{},"count":1},"/tags/process":{"label":"#process","children":{},"count":9},"/tags/updates":{"label":"#updates","children":{},"count":39},"/tags/distributed-system":{"label":"#distributed-system","children":{},"count":1},"/tags/data-types":{"label":"#data-types","children":{},"count":1},"/tags/data-structures":{"label":"#data-structures","children":{},"count":2},"/tags/client":{"label":"#client","children":{},"count":6},"/tags/guidline":{"label":"#guidline","children":{},"count":1},"/tags/playbook":{"label":"#playbook","children":{},"count":3},"/tags/software":{"label":"#software","children":{},"count":11},"/tags/framework":{"label":"#framework","children":{},"count":6},"/tags/productivity":{"label":"#productivity","children":{},"count":7},"/tags/learning":{"label":"#learning","children":{},"count":4},"/tags/system design":{"label":"#system design","children":{},"count":1},"/tags/enterprise":{"label":"#enterprise","children":{},"count":10},"/tags/australia":{"label":"#australia","children":{},"count":1},"/tags/sargable-queries":{"label":"#sargable-queries","children":{},"count":1},"/tags/zookeeper":{"label":"#zookeeper","children":{},"count":1},"/tags/kafka":{"label":"#kafka","children":{},"count":1},"/tags/sequential-reads":{"label":"#sequential-reads","children":{},"count":1},"/tags/sequential-writes":{"label":"#sequential-writes","children":{},"count":1},"/tags/random-reads":{"label":"#random-reads","children":{},"count":1},"/tags/random-writes":{"label":"#random-writes","children":{},"count":1},"/tags/url-redirect":{"label":"#url-redirect","children":{},"count":1},"/tags/url-rewrite":{"label":"#url-rewrite","children":{},"count":1},"/tags/http":{"label":"#http","children":{},"count":1},"/tags/seo":{"label":"#seo","children":{},"count":1},"/tags/dx":{"label":"#dx","children":{},"count":1},"/tags/machine learning":{"label":"#machine learning","children":{},"count":1},"/tags/r&d":{"label":"#r&d","children":{},"count":1},"/tags/web":{"label":"#web","children":{},"count":9},"/tags/micro-frontend":{"label":"#micro-frontend","children":{},"count":3},"/tags/backend":{"label":"#backend","children":{},"count":4},"/tags/tool":{"label":"#tool","children":{},"count":3},"/tags/technique":{"label":"#technique","children":{},"count":9},"/tags/vietnam":{"label":"#vietnam","children":{},"count":1},"/tags/write-heavy":{"label":"#write-heavy","children":{},"count":1},"/tags/inventory-platform":{"label":"#inventory-platform","children":{},"count":1},"/tags/scalability":{"label":"#scalability","children":{},"count":1},"/tags/doordash":{"label":"#doordash","children":{},"count":1},"/tags/low-latency":{"label":"#low-latency","children":{},"count":1},"/tags/observability":{"label":"#observability","children":{},"count":5},"/tags/teamwork":{"label":"#teamwork","children":{},"count":2},"/tags/leadership":{"label":"#leadership","children":{},"count":4},"/tags/multi-column-index":{"label":"#multi-column-index","children":{},"count":1},"/tags/index":{"label":"#index","children":{},"count":1},"/tags/composite-index":{"label":"#composite-index","children":{},"count":1},"/tags/react":{"label":"#react","children":{},"count":15},"/tags/hooks":{"label":"#hooks","children":{},"count":2},"/tags/components":{"label":"#components","children":{},"count":1},"/tags/scrum":{"label":"#scrum","children":{},"count":2},"/tags/technicaldebt":{"label":"#technicaldebt","children":{},"count":1},"/tags/projectmanagement":{"label":"#projectmanagement","children":{},"count":1},"/tags/email":{"label":"#email","children":{},"count":22},"/tags/decoder":{"label":"#decoder","children":{},"count":1},"/tags/json":{"label":"#json","children":{},"count":1},"/tags/materialized-view":{"label":"#materialized-view","children":{},"count":1},"/tags/data-warehouse":{"label":"#data-warehouse","children":{},"count":1},"/tags/mapreduce":{"label":"#mapreduce","children":{},"count":1},"/tags/distributed":{"label":"#distributed","children":{},"count":3},"/tags/form":{"label":"#form","children":{},"count":1},"/tags/uilibraries":{"label":"#uilibraries","children":{},"count":1},"/tags/migrations":{"label":"#migrations","children":{},"count":1},"/tags/agile":{"label":"#agile","children":{},"count":6},"/tags/behavior-driven-development":{"label":"#behavior-driven-development","children":{},"count":1},"/tags/testing":{"label":"#testing","children":{},"count":4},"/tags/ubiquitous-language":{"label":"#ubiquitous-language","children":{},"count":1},"/tags/forward-proxy":{"label":"#forward-proxy","children":{},"count":1},"/tags/payment":{"label":"#payment","children":{},"count":1},"/tags/apprenticeship":{"label":"#apprenticeship","children":{},"count":4},"/tags/remote":{"label":"#remote","children":{},"count":12},"/tags/showcase":{"label":"#showcase","children":{},"count":1},"/tags/practice":{"label":"#practice","children":{},"count":6},"/tags/senior":{"label":"#senior","children":{},"count":1},"/tags/internship":{"label":"#internship","children":{},"count":4},"/tags/swap":{"label":"#swap","children":{},"count":2},"/tags/quant":{"label":"#quant","children":{},"count":1},"/tags/radio":{"label":"#radio","children":{},"count":3},"/tags/writing":{"label":"#writing","children":{},"count":1},"/tags/english":{"label":"#english","children":{},"count":1},"/tags/apprentice":{"label":"#apprentice","children":{},"count":1},"/tags/designer":{"label":"#designer","children":{},"count":1},"/tags/meeting":{"label":"#meeting","children":{},"count":4},"/tags/us":{"label":"#us","children":{},"count":4},"/tags/mbti":{"label":"#mbti","children":{},"count":6},"/tags/intj":{"label":"#intj","children":{},"count":1},"/tags/istp":{"label":"#istp","children":{},"count":1},"/tags/estj":{"label":"#estj","children":{},"count":1},"/tags/istj":{"label":"#istj","children":{},"count":1},"/tags/personalities":{"label":"#personalities","children":{},"count":1},"/tags/management":{"label":"#management","children":{},"count":4},"/tags/early-stage":{"label":"#early-stage","children":{},"count":3},"/tags/design-thinking":{"label":"#design-thinking","children":{},"count":2},"/tags/healthcare":{"label":"#healthcare","children":{},"count":1},"/tags/browser-extension":{"label":"#browser-extension","children":{},"count":2},"/tags/git":{"label":"#git","children":{},"count":2},"/tags/marketplace":{"label":"#marketplace","children":{},"count":2},"/tags/tips":{"label":"#tips","children":{},"count":10},"/tags/real-estate":{"label":"#real-estate","children":{},"count":1},"/tags/nocode":{"label":"#nocode","children":{},"count":1},"/tags/hospitality":{"label":"#hospitality","children":{},"count":1},"/tags/ride-hailing":{"label":"#ride-hailing","children":{},"count":1},"/tags/iot":{"label":"#iot","children":{},"count":1},"/tags/macos":{"label":"#macos","children":{},"count":3},"/tags/swift":{"label":"#swift","children":{},"count":7},"/tags/partnership":{"label":"#partnership","children":{},"count":1},"/tags/pm":{"label":"#pm","children":{},"count":4},"/tags/travel":{"label":"#travel","children":{},"count":1},"/tags/operation":{"label":"#operation","children":{},"count":7},"/tags/idea":{"label":"#idea","children":{},"count":1},"/tags/ventures":{"label":"#ventures","children":{},"count":3},"/tags/purpose":{"label":"#purpose","children":{},"count":2},"/tags/wasm":{"label":"#wasm","children":{},"count":2},"/tags/transparency":{"label":"#transparency","children":{},"count":1},"/tags/event-sourcing":{"label":"#event-sourcing","children":{},"count":1},"/tags/sdlc":{"label":"#sdlc","children":{},"count":1},"/tags/modeling":{"label":"#modeling","children":{},"count":2},"/tags/goal":{"label":"#goal","children":{},"count":2},"/tags/license":{"label":"#license","children":{},"count":1},"/tags/template":{"label":"#template","children":{},"count":20},"/tags/k8s":{"label":"#k8s","children":{},"count":1},"/tags/js":{"label":"#js","children":{},"count":2},"/tags/clojure":{"label":"#clojure","children":{},"count":1},"/tags/react.js":{"label":"#react.js","children":{},"count":2},"/tags/employee":{"label":"#employee","children":{},"count":2},"/tags/onboarding":{"label":"#onboarding","children":{},"count":1},"/tags/company":{"label":"#company","children":{},"count":1},"/tags/tooling":{"label":"#tooling","children":{},"count":9},"/tags/human-resource":{"label":"#human-resource","children":{},"count":1},"/tags/dcos":{"label":"#dcos","children":{},"count":5},"/tags/docker":{"label":"#docker","children":{},"count":11},"/tags/okr":{"label":"#okr","children":{},"count":1},"/tags/oss":{"label":"#oss","children":{},"count":1},"/tags/overleaf":{"label":"#overleaf","children":{},"count":1},"/tags/slide":{"label":"#slide","children":{},"count":1},"/tags/web3":{"label":"#web3","children":{},"count":4},"/tags/mcp":{"label":"#mcp","children":{},"count":3},"/tags/office-hours":{"label":"#office-hours","children":{},"count":28},"/tags/discord":{"label":"#discord","children":{},"count":35},"/tags/btc":{"label":"#btc","children":{},"count":1},"/tags/newsletter":{"label":"#newsletter","children":{},"count":44},"/tags/forward-engineering":{"label":"#forward-engineering","children":{},"count":14},"/tags/tech-community":{"label":"#tech-community","children":{},"count":1},"/tags/weekly-digest":{"label":"#weekly-digest","children":{},"count":15},"/tags/wrap-up":{"label":"#wrap-up","children":{},"count":7},"/tags/real-time":{"label":"#real-time","children":{},"count":1},"/tags/phoenix-live-view":{"label":"#phoenix-live-view","children":{},"count":1},"/tags/timescaledb":{"label":"#timescaledb","children":{},"count":1},"/tags/go-weekly":{"label":"#go-weekly","children":{},"count":24},"/tags/finance":{"label":"#finance","children":{},"count":1},"/tags/protocol":{"label":"#protocol","children":{},"count":1},"/tags/agents":{"label":"#agents","children":{},"count":4},"/tags/monitoring":{"label":"#monitoring","children":{},"count":1},"/tags/defi":{"label":"#defi","children":{},"count":2},"/tags/aider":{"label":"#aider","children":{},"count":2},"/tags/qwen2.5":{"label":"#qwen2.5","children":{},"count":1},"/tags/openhand":{"label":"#openhand","children":{},"count":1},"/tags/predicted output":{"label":"#predicted output","children":{},"count":1},"/tags/project-management":{"label":"#project-management","children":{},"count":1},"/tags/copilots":{"label":"#copilots","children":{},"count":2},"/tags/team-management":{"label":"#team-management","children":{},"count":1},"/tags/mongodb":{"label":"#mongodb","children":{},"count":1},"/tags/salesforce":{"label":"#salesforce","children":{},"count":1},"/tags/use cases":{"label":"#use cases","children":{},"count":2},"/tags/design-system":{"label":"#design-system","children":{},"count":1},"/tags/storybook":{"label":"#storybook","children":{},"count":1},"/tags/hook":{"label":"#hook","children":{},"count":1},"/tags/cline":{"label":"#cline","children":{},"count":1},"/tags/realtime api":{"label":"#realtime api","children":{},"count":1},"/tags/interface":{"label":"#interface","children":{},"count":1},"/tags/import":{"label":"#import","children":{},"count":1},"/tags/package":{"label":"#package","children":{},"count":1},"/tags/yelp":{"label":"#yelp","children":{},"count":1},"/tags/generics":{"label":"#generics","children":{},"count":2},"/tags/log":{"label":"#log","children":{},"count":1},"/tags/pillar":{"label":"#pillar","children":{},"count":3},"/tags/metric":{"label":"#metric","children":{},"count":1},"/tags/tracing":{"label":"#tracing","children":{},"count":1},"/tags/intent-classification":{"label":"#intent-classification","children":{},"count":1},"/tags/prompting":{"label":"#prompting","children":{},"count":1},"/tags/changelog":{"label":"#changelog","children":{},"count":1},"/tags/test":{"label":"#test","children":{},"count":1},"/tags/language":{"label":"#language","children":{},"count":5},"/tags/ai-agents":{"label":"#ai-agents","children":{},"count":2},"/tags/ai-evaluation":{"label":"#ai-evaluation","children":{},"count":1},"/tags/prompt-engineering":{"label":"#prompt-engineering","children":{},"count":4},"/tags/ai-integration":{"label":"#ai-integration","children":{},"count":1},"/tags/networking":{"label":"#networking","children":{},"count":7},"/tags/finite-automata":{"label":"#finite-automata","children":{},"count":1},"/tags/pattern-matching":{"label":"#pattern-matching","children":{},"count":1},"/tags/state-machines":{"label":"#state-machines","children":{},"count":1},"/tags/java":{"label":"#java","children":{},"count":1},"/tags/programming":{"label":"#programming","children":{},"count":1},"/tags/caching":{"label":"#caching","children":{},"count":1},"/tags/devbox":{"label":"#devbox","children":{},"count":17},"/tags/nix":{"label":"#nix","children":{},"count":9},"/tags/generative-ui":{"label":"#generative-ui","children":{},"count":1},"/tags/function-calling":{"label":"#function-calling","children":{},"count":1},"/tags/ton":{"label":"#ton","children":{},"count":2},"/tags/ai-powered":{"label":"#ai-powered","children":{},"count":1},"/tags/pattern":{"label":"#pattern","children":{},"count":1},"/tags/supervisor-architecture":{"label":"#supervisor-architecture","children":{},"count":1},"/tags/document-processing":{"label":"#document-processing","children":{},"count":1},"/tags/information-retrieval":{"label":"#information-retrieval","children":{},"count":1},"/tags/iterators":{"label":"#iterators","children":{},"count":1},"/tags/reinforcement-learning":{"label":"#reinforcement-learning","children":{},"count":3},"/tags/kernel-programing":{"label":"#kernel-programing","children":{},"count":1},"/tags/anchor":{"label":"#anchor","children":{},"count":2},"/tags/containerization":{"label":"#containerization","children":{},"count":4},"/tags/virtualization":{"label":"#virtualization","children":{},"count":4},"/tags/meet-up":{"label":"#meet-up","children":{},"count":4},"/tags/meetup":{"label":"#meetup","children":{},"count":2},"/tags/motivation":{"label":"#motivation","children":{},"count":1},"/tags/cybersecurity":{"label":"#cybersecurity","children":{},"count":2},"/tags/serverless":{"label":"#serverless","children":{},"count":1},"/tags/doty":{"label":"#doty","children":{},"count":5},"/tags/websocket":{"label":"#websocket","children":{},"count":1},"/tags/protocols":{"label":"#protocols","children":{},"count":1},"/tags/nextjs":{"label":"#nextjs","children":{},"count":2},"/tags/open-source":{"label":"#open-source","children":{},"count":2},"/tags/rendering":{"label":"#rendering","children":{},"count":1},"/tags/dom":{"label":"#dom","children":{},"count":3},"/tags/cssom":{"label":"#cssom","children":{},"count":1},"/tags/render-tree":{"label":"#render-tree","children":{},"count":1},"/tags/iframe":{"label":"#iframe","children":{},"count":1},"/tags/postmessage":{"label":"#postmessage","children":{},"count":1},"/tags/mock-service-worker":{"label":"#mock-service-worker","children":{},"count":1},"/tags/api-mocking":{"label":"#api-mocking","children":{},"count":1},"/tags/web-development-tool":{"label":"#web-development-tool","children":{},"count":1},"/tags/data-fetching":{"label":"#data-fetching","children":{},"count":1},"/tags/frontend,":{"label":"#frontend,","children":{},"count":1},"/tags/graphql":{"label":"#graphql","children":{},"count":1},"/tags/reactjs":{"label":"#reactjs","children":{},"count":2},"/tags/scroll-driven-animations":{"label":"#scroll-driven-animations","children":{},"count":1},"/tags/animations":{"label":"#animations","children":{},"count":1},"/tags/intersection-observer":{"label":"#intersection-observer","children":{},"count":1},"/tags/server-component":{"label":"#server-component","children":{},"count":1},"/tags/caching-data":{"label":"#caching-data","children":{},"count":1},"/tags/social-networks":{"label":"#social-networks","children":{},"count":1},"/tags/foundation-model":{"label":"#foundation-model","children":{},"count":1},"/tags/fine-tuning":{"label":"#fine-tuning","children":{},"count":1},"/tags/vector database":{"label":"#vector database","children":{},"count":1},"/tags/shadow-dom":{"label":"#shadow-dom","children":{},"count":1},"/tags/web-api":{"label":"#web-api","children":{},"count":1},"/tags/swr-infinite":{"label":"#swr-infinite","children":{},"count":1},"/tags/web-design":{"label":"#web-design","children":{},"count":1},"/tags/tuning-llm":{"label":"#tuning-llm","children":{},"count":2},"/tags/langchain":{"label":"#langchain","children":{},"count":1},"/tags/translation":{"label":"#translation","children":{},"count":1},"/tags/profiling":{"label":"#profiling","children":{},"count":1},"/tags/state-mangement":{"label":"#state-mangement","children":{},"count":1},"/tags/global-state-management":{"label":"#global-state-management","children":{},"count":1},"/tags/css":{"label":"#css","children":{},"count":5},"/tags/fonts":{"label":"#fonts","children":{},"count":1},"/tags/variable-fonts":{"label":"#variable-fonts","children":{},"count":1},"/tags/state-management":{"label":"#state-management","children":{},"count":2},"/tags/component":{"label":"#component","children":{},"count":1},"/tags/proof-of-knowledge":{"label":"#proof-of-knowledge","children":{},"count":1},"/tags/fronten":{"label":"#fronten","children":{},"count":1},"/tags/typescript":{"label":"#typescript","children":{},"count":4},"/tags/analytics-tools":{"label":"#analytics-tools","children":{},"count":1},"/tags/analytics-platform":{"label":"#analytics-platform","children":{},"count":1},"/tags/software engineer":{"label":"#software engineer","children":{},"count":1},"/tags/parsing":{"label":"#parsing","children":{},"count":1},"/tags/technology":{"label":"#technology","children":{},"count":5},"/tags/validation":{"label":"#validation","children":{},"count":1},"/tags/webassembly":{"label":"#webassembly","children":{},"count":1},"/tags/sandbox":{"label":"#sandbox","children":{},"count":1},"/tags/zk-rollup":{"label":"#zk-rollup","children":{},"count":2},"/tags/polygon":{"label":"#polygon","children":{},"count":1},"/tags/starknet":{"label":"#starknet","children":{},"count":1},"/tags/ethereum":{"label":"#ethereum","children":{},"count":2},"/tags/zero-knowledge":{"label":"#zero-knowledge","children":{},"count":1},"/tags/atomic-css":{"label":"#atomic-css","children":{},"count":1},"/tags/client-side":{"label":"#client-side","children":{},"count":1},"/tags/storage":{"label":"#storage","children":{},"count":1},"/tags/frontend/performance":{"label":"#frontend/performance","children":{},"count":2},"/tags/wai-aria":{"label":"#wai-aria","children":{},"count":1},"/tags/accessibility":{"label":"#accessibility","children":{},"count":4},"/tags/polymorphic-component":{"label":"#polymorphic-component","children":{},"count":1},"/tags/threejs":{"label":"#threejs","children":{},"count":1},"/tags/web-performance":{"label":"#web-performance","children":{},"count":2},"/tags/html":{"label":"#html","children":{},"count":4},"/tags/animation":{"label":"#animation","children":{},"count":1},"/tags/zk-proof":{"label":"#zk-proof","children":{},"count":1},"/tags/guides":{"label":"#guides","children":{},"count":1},"/tags/responsive-design":{"label":"#responsive-design","children":{},"count":1},"/tags/hsl":{"label":"#hsl","children":{},"count":1},"/tags/javascript":{"label":"#javascript","children":{},"count":4},"/tags/css-in-js":{"label":"#css-in-js","children":{},"count":1},"/tags/tip":{"label":"#tip","children":{},"count":1},"/tags/dark-mode":{"label":"#dark-mode","children":{},"count":1},"/tags/multisign-wallet":{"label":"#multisign-wallet","children":{},"count":1},"/tags/virtual-dom":{"label":"#virtual-dom","children":{},"count":1},"/tags/native-modules":{"label":"#native-modules","children":{},"count":1},"/tags/vitejs":{"label":"#vitejs","children":{},"count":1},"/tags/esm":{"label":"#esm","children":{},"count":1},"/tags/modules":{"label":"#modules","children":{},"count":1},"/tags/blockchain-bridge":{"label":"#blockchain-bridge","children":{},"count":1},"/tags/foundational-topics":{"label":"#foundational-topics","children":{},"count":5},"/tags/distributed-systems":{"label":"#distributed-systems","children":{},"count":1},"/tags/pos":{"label":"#pos","children":{},"count":1},"/tags/smart-contract":{"label":"#smart-contract","children":{},"count":1},"/tags/atomic-design":{"label":"#atomic-design","children":{},"count":1},"/tags/a11y":{"label":"#a11y","children":{},"count":1},"/tags/useeffect":{"label":"#useeffect","children":{},"count":1},"/tags/concurrency":{"label":"#concurrency","children":{},"count":2},"/tags/parallelism":{"label":"#parallelism","children":{},"count":1},"/tags/liquidity":{"label":"#liquidity","children":{},"count":1},"/tags/engineering/frontend":{"label":"#engineering/frontend","children":{},"count":1},"/tags/funding":{"label":"#funding","children":{},"count":2},"/tags/wfh":{"label":"#wfh","children":{},"count":1},"/tags/tech radar":{"label":"#tech radar","children":{},"count":1},"/tags/policy":{"label":"#policy","children":{},"count":1},"/tags/vim":{"label":"#vim","children":{},"count":1}}}},"slug":["playground","use-cases"],"childMemos":[{"content":"\nOur engineering team collaborated with a confidential HRTech client to create an **MVP** for AI-based, real-time voice interviews. Built in just **two weeks** to validate the concept, the solution leverages advanced **AI** and **AI voice processing** to conduct sales-specific interviews. Despite the short development timeline, the initial results have been very promising.\n\n### Challenges\n\n1. **Time-consuming screening**: Traditional processes took weeks to screen and evaluate candidates.\n2. **Inconsistent assessment**: Evaluations often varied due to human bias and fluctuating criteria.\n3. **Manual processes**: Recruiters spent excessive time on repetitive screening tasks, detracting from strategic decision-making.\n\n### Our MVP solution\n\nWe developed an automated interview system using real-time voice processing to evaluate candidates. By integrating with [ElevenLabs](https://elevenlabs.io/)â€™ voice technology, the platform simulates realistic sales conversations and measures performance through consistent AI-driven scoring. Key benefits include:\n\n- **Faster hiring**: Automated screening accelerates the time-to-hire.\n- **Objective evaluations**: AI-driven scoring removes bias and maintains uniform standards.\n- **Scalable & efficient**: Repetitive tasks are automated, freeing recruiters for strategic activities.\n\n### Architecture highlights\n\n1. **Serverless backend**: A Node.js (Next.js) backend processes interviews and orchestrates AI-based evaluations.\n2. **Real-time voice processing**: Integrations with ElevenLabsâ€™ API handle immediate speech input and feedback.\n3. **Data storage**: TimescaleDB provides scalable, high-performance data management for sessions and analytics.\n4. **Analytics & frontend**: [Retool](https://retool.com/) dashboards offer real-time insights, while a Next.js interface ensures a smooth user experience\n\n![](assets/ai-interview-architecture.webp)\n\n<p style=\"text-align: center; margin-top: 0\">Architecture diagram</p>\n\n![](assets/ai-interview-flow.webp)\n\n<p style=\"text-align: center; margin-top: 0\">Main flow</p>\n\n### Screenshots\n\n![](assets/ai-interview-screenshot-1.webp)\n\n![](assets/ai-interview-screenshot-2.webp)\n\n![](assets/ai-interview-screenshot-3.webp)\n\n### Key lessons & future enhancements\n\n**Edge cases with silent or uncooperative participants**\n\nSome candidates tested the system by remaining silent, causing the interview to stall. While adjusting ElevenLabsâ€™ settings (e.g., time bounds, prompts) can mitigate this, a more robust solution would involve an independent agent monitoring the interview and making decisions if participants remain idle.\n\n**Video processing requirements**\n\nFor a more comprehensive evaluation, video data (e.g., posture, facial expressions) should be captured. However, ElevenLabs currently supports voice mode only, so a separate data pipeline would be necessary for video-based assessments.\n\n**Role-based agent assignment**\n\nFuture versions could include a dynamic, â€œagenticâ€ system that tailors the interview agent to each candidateâ€™s CV and desired role. This would enhance relevance and improve the quality of feedback.\n\n### Outcomes\n\n- **Promising early results**: The MVP drastically reduced screening time and demonstrated the potential for unbiased, real-time AI assessments.\n- **Scalable foundation**: A serverless architecture, combined with voice-based AI, positions the platform for future growth and enhancements.\n\n### Conclusion\n\nWithin just two weeks, we delivered a functioning MVP that effectively tests the concept of **AI-driven, real-time voice interviews** for sales candidates. Despite its rapid development, the solution has already shown considerable potential in reducing hiring bottlenecks, providing objective evaluations, and opening doors for more advanced features like video analysis and role-based interview agents. This success confirms our commitment to leveraging cutting-edge AI technologies to drive efficient, unbiased hiring solutions.\n","title":"Building MVP for AI-driven interview platform","short_title":"","description":"Discover how our two-week MVP harnesses AI-driven, real-time voice processing to streamline interviews, reduce bias, and accelerate hiring success","tags":["ai","llm"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh"],"date":"2025-03-04","filePath":"playground/use-cases/ai-interview-platform-mvp.md","slugArray":["playground","use-cases","ai-interview-platform-mvp"]},{"content":"\nAt Dwarves, we've developed a Monthly Project Reports system that transforms communication data into actionable intelligence. This lean system orchestrates multiple data streams into comprehensive project insights while maintaining enterprise-grade security and cost efficiency.\n\n## The need for orchestrated intelligence\n\nOur engineering teams exchange thousands of Discord messages daily across projects, capturing critical technical discussions, architectural decisions, and implementation details. However, while Discord excels at real-time communication, valuable insights often remain buried in chat histories, making it difficult to:\n\n1. Track project progress against client requirements.\n2. Align ongoing discussions with formal documentation.\n3. Extract actionable insights from technical conversations.\n\nThis challenge led us to develop the Project Reports system - an intelligent orchestration layer that transforms scattered communication data into structured project intelligence. Our system processes multiple data streams, extracting key insights and patterns to generate comprehensive project visibility.\n\n## The foundation: Data architecture\n\nOur architecture follows a simple yet powerful approach to data management, emphasizing efficiency and practicality over complexity. We've built our system on three core principles:\n\n1. **Lean storage**: S3 serves as our primary data lake and warehouse, using Parquet and CSV files to optimize for both cost and performance\n2. **Efficient processing**: DuckDB and Polars provide high-performance querying without the overhead of traditional data warehouses\n3. **Secure access**: Modal orchestrates our serverless functions, ensuring secure and efficient data processing\n\n### Data flow overview\n\n```mermaid\ngraph TB\n    subgraph Data Sources [\"Data Sources (Raw)\"]\n        D1[Discord Messages]\n        D2[Git Activity]\n        D3[JIRA Tickets]\n        D4[Google Docs]\n        D5[Notion Pages]\n        D6[...]\n    end\n\n    subgraph Data Engineering\n        L1[Landing Zone - S3]\n        G1[Gold Zone - S3]\n        DQ[Data Quality Checks]\n\n        D1 & D2 & D3 & D4 & D5 & D6 --> L1\n        L1 --> DQ\n        DQ --> G1\n    end\n\n    subgraph Platform Engineering\n        API[REST API]\n        SEC[Security Layer]\n        MON[Monitoring]\n        ORCH[Modal Orchestration]\n\n        G1 --> API\n        API --> SEC\n        SEC --> MON\n        MON --> ORCH\n    end\n\n    subgraph AI Engineering\n        LLM[LLM Processing]\n        AGG[Aggregations]\n        SUM[Summarization]\n\n        ORCH --> LLM\n        LLM --> AGG\n        AGG --> SUM\n    end\n\n    subgraph Operations Usage\n        R1[Monthly Reports]\n        R2[Progress Tracking]\n        R3[Resource Planning]\n\n        SUM --> R1 & R2 & R3\n    end\n\n    classDef data fill:#d4ebf2,stroke:#1b70a6,color:#000\n    classDef platform fill:#fdf1d5,stroke:#d4a017,color:#000\n    classDef ai fill:#e8f5e8,stroke:#2d862d,color:#000\n    classDef ops fill:#ffe6e6,stroke:#cc0000,color:#000\n\n    class D1,D2,D3,D4,D5,D6 data\n    class L1,G1,DQ platform\n    class API,SEC,MON,ORCH platform\n    class LLM,AGG,SUM,VEC ai\n    class R1,R2,R3 ops\n```\n\nThe system begins with raw data collection from various sources, primarily Discord at present, with planned expansion to Git, JIRA, Google Docs, and Notion. This data moves through our S3-based landing and gold zones, where it undergoes quality checks and transformations before feeding into our platform and AI engineering layers.\n\n### Detailed processing pipeline\n\n```mermaid\ngraph LR\n    subgraph Data Collection\n        DC1[Discord Collector]\n        DC2[Git Collector]\n        DC3[JIRA Collector]\n        SCHEDULE[Weekly Schedule]\n\n        SCHEDULE --> DC1 & DC2 & DC3\n    end\n\n    subgraph Processing Pipeline\n        B1[Message Buffer]\n        B2[Git Buffer]\n        B3[Ticket Buffer]\n\n        P1[PII Scrubber]\n        P2[Data Validator]\n        P3[Schema Enforcer]\n\n        DC1 --> B1\n        DC2 --> B2\n        DC3 --> B3\n\n        B1 & B2 & B3 --> P1\n        P1 --> P2\n        P2 --> P3\n    end\n\n    subgraph Storage Layer\n        S1[S3 - Parquet Files]\n        S2[S3 - CSV Files]\n\n        P3 --> S1\n        P3 --> S2\n    end\n\n    subgraph Query Layer\n        Q1[DuckDB Engine]\n        Q2[Polars Engine]\n        Q3[Report Generator]\n\n        S1 --> Q1\n        S2 --> Q2\n        Q1 & Q2 --> Q3\n    end\n\n    style DC1 fill:#d4ebf2,stroke:#1b70a6,color:#000\n    style DC2 fill:#d4ebf2,stroke:#1b70a6,color:#000\n    style DC3 fill:#d4ebf2,stroke:#1b70a6,color:#000\n\n    style P1 fill:#fdf1d5,stroke:#d4a017,color:#000\n    style P2 fill:#fdf1d5,stroke:#d4a017,color:#000\n    style P3 fill:#fdf1d5,stroke:#d4a017,color:#000\n\n    style Q1 fill:#e8f5e8,stroke:#2d862d,color:#000\n    style Q2 fill:#e8f5e8,stroke:#2d862d,color:#000\n    style Q3 fill:#e8f5e8,stroke:#2d862d,color:#000\n```\n\nOur processing pipeline emphasizes efficiency and security:\n\n1. **Collection layer**: Weekly scheduled collectors gather data from various sources\n2. **Processing pipeline**: Data undergoes PII scrubbing, validation, and schema enforcement\n3. **Storage layer**: Processed data is stored in S3 using Parquet and CSV formats\n4. **Query layer**: DuckDB and Polars engines provide fast, efficient data analysis\n\n## Dify - Operational intelligence through low-code workflows\n\nWe use Dify to transform our raw data streams into intelligent insights through low-code workflows. This process bridges the gap between our data collection pipeline and the operational insights needed by our team.\n\n![](assets/project-report-use-case-dify.png)\n\n```mermaid\ngraph LR\n    subgraph \"Input Collection\"\n        START[Start] --> |channel_id/dates| PE1[Parameter Extractor 1]\n        START --> |git_token| PE2[Parameter Extractor 2]\n        START --> |condition check| IE{IF/ELSE}\n    end\n\n    subgraph \"Data Extraction\"\n        PE1 --> |Map| LE[Links Extraction]\n        PE2 --> |Map| GE[Git Extraction]\n        IE --> |dialogue_count â‰¤ 1| DM[Discord Messages]\n    end\n\n    subgraph \"Parallel Processing\"\n        LE --> |Iterate| IT[Link Iterator]\n        IT --> |Map| FSP[Fetch Single Page]\n        GE --> |Map| GT[Git Traverser]\n        DM --> |Map| VA[Variable Aggregator]\n    end\n\n    subgraph \"Reduction & Output\"\n        FSP --> |Reduce| RED[Template Transform]\n        GT --> |Reduce| RED\n        VA --> |Reduce| RED\n        RED --> LLM[Monthly Reporter LLM]\n        LLM --> ANS[Answer]\n    end\n\n    style START fill:#f9f,stroke:#333,color:#000\n    style IT fill:#bbf,stroke:#333,color:#000\n    style RED fill:#bfb,stroke:#333,color:#000\n    style ANS fill:#fbf,stroke:#333,color:#000\n\n```\n\nOur Dify implementation provides a few key advantages:\n\n- **Rapid Iteration** The low-code nature of Dify allows us to quickly adjust workflows based on operational feedback. When our operations team needs new types of insights, we can modify templates and processing logic without extensive development cycles.\n- **Flexible Integration** The workflow system easily integrates with our existing data pipeline, pulling from our S3 storage and utilizing DuckDB/Polars for efficient data processing before applying intelligence templates.\n- **Maintainable Intelligence** Templates and workflows are version-controlled and documented, making it easy for team members to understand and modify the intelligence generation process. This ensures our reporting system can evolve with our organizational needs.\n\n## Operational impact\n\nThe Project Reports system serves as the foundation for our Operations team's project oversight. It provides:\n\n- **Real-time Project Visibility**: Operations can track progress across multiple projects through consolidated communication data, enabling early identification of potential issues or bottlenecks.\n- **Data-Driven Decision Making**: By analyzing communication patterns and project discussions, we can make informed decisions about resource allocation and project timelines.\n- **Automated Reporting**: The system generates comprehensive monthly reports, reducing manual effort and ensuring consistent project tracking across the organization.\n\n## Technical implementation\n\n### Secure data collection\n\nThe cornerstone of our system is a robust collection pipeline built on Modal. Our collection process runs weekly, automatically processing Discord messages through a sophisticated filtering system that preserves critical technical discussions while ensuring security and privacy.\n\n```python\n@app.function(\n    schedule=modal.Cron(\"0 1 * * 1\"),  # Weekly Monday collection\n    secrets=[secrets],\n)\ndef weekly_discord_collection():\n    category_id = get_category_id.local()\n    channels = get_category_channels.remote(category_id)\n    channel_args = [(channel, year, month) for channel in channels]\n    saved_files = process_channel_monthly_data.starmap(channel_args)\n\n```\n\nThrough Modal's serverless architecture, we've implemented separate landing zones for different project data, ensuring granular access control and comprehensive audit trails. Each message undergoes content filtering and PII scrubbing before being transformed into optimized Parquet format, providing both storage efficiency and query performance.\n\n### Query interface\n\nThe system provides a flexible API for accessing processed data:\n\n```python\n@app.function(\n    volumes={MOUNT_PATH: modal.CloudBucketMount(\"dwarvesf-discord\", secret=secrets)},\n    secrets=[secrets],\n)\n@modal.web_endpoint(method=\"POST\")\ndef query_messages(item: QueryRequest, token: str = Depends(verify_token)) -> Dict:\n    parquet_files = get_relevant_files.remote(\n        channel_id=item.channel_id,\n        category_id=item.category_id,\n        start_date=item.start_date,\n        end_date=item.end_date,\n    )\n\n```\n\n## Measured impact\n\nThe implementation of Project Reports has fundamentally transformed our project management approach. Our operations team now have greater visibility into project progress, with tracking and early issue identification becoming the norm rather than the exception. The automated documentation of key decisions has significantly reduced meeting overhead, while the correlation between discussions and deliverables ensures nothing falls through the cracks.\n\n## Future development\n\nWe're expanding the system's capabilities in several key areas:\n\n- **Additional Data Sources**: Integration with Git metrics, JIRA tickets, and documentation platforms will provide a more comprehensive view of project health.\n- **Enhanced Analytics**: Implementation of advanced pattern recognition and trend analysis will improve our predictive capabilities.\n- **Automated Insights**: Deeper AI integration will enable more sophisticated report generation and context understanding.\n\nWe also donâ€™t plan to be vendor-locked using entirely Modal. The foundations weâ€™ve laid out to create our landing zones and data lake make it very easy to swap in-and-out query and API architectures.\n\n## Conclusion\n\nAt Dwarves, our Project Reports system demonstrates the power of thoughtful data engineering in transforming raw communication into strategic project intelligence. By combining secure data collection, efficient processing, and AI-powered analysis, we've created a system that doesn't just track progress â€“ it actively contributes to project success.\n\nThe system continues to coordinate our project data streams with precision and purpose, ensuring that every piece of information contributes to a clear picture of project health. Through this systematic approach, we're setting new standards for data-driven project management in software development, one report at a time.\n","title":"Project reports system: a case study","short_title":"","description":"An in-depth look at Dwarves' monthly Project Reports system - a lean, efficient system that transforms communication data into actionable intelligence for Operations teams. This case study explores how we orchestrate multiple data streams into comprehensive project insights while maintaining enterprise-grade security and cost efficiency.","tags":["data","agents","ai"],"pinned":false,"draft":false,"hiring":false,"authors":["monotykamary"],"date":"2024-11-14","filePath":"playground/use-cases/ai-powered-monthly-project-reports.md","slugArray":["playground","use-cases","ai-powered-monthly-project-reports"]},{"content":"\n> \"When the easy path isn't the right path, true engineering shines in adapting the right tools for the job.\"\n\n## The challenge: Beyond the obvious choice\n\nIn AI development, Python is often the default choice. A leading financial services provider approached us to build an AI-powered travel assistant, their initial technical specification centered around Python's rich AI ecosystem. However, reality proved more complex.\n\nThe client's infrastructure wasn't just built on Rubyâ€”it was deeply entrenched in it. Their security systems, authentication layers, and core services were all Ruby-based. Their choice of AWS Bedrock as the AI provider was intrinsically tied to their security requirements. The seemingly simple option of using Python would have meant creating unnecessary complexity, additional security layers, and potential points of failure.\n\n## The vision: More than just a chatbot\n\nThe client's vision extended beyond a simple query-response system. They wanted an intelligent travel assistant that could not only answer questions but actively engage users with relevant suggestions based on their context, location, and historical interactions. This dual approach of reactive responses and proactive suggestions would set their platform apart from conventional travel booking systems.\n\n```mermaid\ngraph LR\n    A[User] --> B[Messages API]\n    A -.-> C[Suggestion Prompts API]\n\n    B --> D\n    C --> D\n\n    subgraph Travel Assistant\n    D[Agent Creation]\n    D --> E[Message History]\n    E --> F[LLM Processing]\n    F --> G[Response Handling]\n    end\n\n    G --> H[Output to User]\n\n    I[User Location] --> J[Suggestion System]\n    J --> C\n\n    K[(Database)] <--> E\n    L[(Cache)] <--> J\n\n```\n\n## Building an engaging experience\n\nThe system architecture reflected this dual nature. The core Travel Assistant handled direct interactions through the Messages API, providing detailed responses to user queries. Alongside this, we implemented a Suggestion System that continuously generated contextual prompts to spark user engagement.\n\nThe Suggestion System was particularly innovative. It took into account the user's location and previous interactions to generate relevant conversation starters. For instance, a user in Singapore might receive suggestions about weekend getaways to Bali, while another in Tokyo might see prompts about cherry blossom season travel deals.\n\nHere's how we implemented the suggestion generation:\n\n```ruby\nmodule LLM\n  module TravelAssistant\n    class SuggestionService\n      def generate_suggestions(user_location:, user_context:)\n        cached_suggestions = fetch_cached_suggestions(user_location)\n        return cached_suggestions if cached_suggestions.present?\n\n        location_based_prompts = generate_location_prompts(user_location)\n        context_based_prompts = generate_context_prompts(user_context)\n\n        merge_and_rank_suggestions(location_based_prompts, context_based_prompts)\n      end\n    end\n  end\nend\n```\n\n## The technical implementation\n\nInstead of forcing Python into an environment optimized for Ruby, we enhanced Ruby's AI capabilities to meet our needs. Working with LangchainRB alongside their team, we evolved it from a promising library into an enterprise-grade solution.\n\nThe core assistant implementation showcased Ruby's elegant syntax while handling complex operations:\n\n```ruby\ndef create_an_assistant(messages, context)\n  Langchain::Assistant.new(\n    llm: LLM::Bedrock.client,\n    instructions: format_instructions(context),\n    messages:,\n    tools: create_tools\n  )\nend\n```\n\n## Intelligent caching strategy\n\nOne key innovation was implementing a caching system that served both the main assistant and the suggestion system. We used Redis here, as some of their previous architecture had integrated that as their main KV database. Location-based suggestions were cached with appropriate time-to-live values, ensuring quick response times while maintaining relevance:\n\n```ruby\ndef fetch_cached_suggestions(location)\n  cache_key = generate_location_cache_key(location)\n  Rails.cache.fetch(cache_key, expires_in: 6.hours) do\n    generate_fresh_suggestions(location)\n  end\nend\n```\n\n## Security and integration\n\nBy staying within the Ruby ecosystem, we leveraged the client's existing security infrastructure seamlessly. Both the main assistant and the suggestion system inherited all the security benefits of the main application, from authentication to audit logging.\n\n```mermaid\ngraph TD\n    A[User Request] --> B[Ruby Security Layer]\n    B --> C[Travel Assistant]\n    C --> D[AWS Bedrock]\n    C --> E[Suggestion System]\n\n    subgraph Ruby Environment\n    B\n    C\n    E\n    end\n\n```\n\n## The results\n\nThe dual approach of reactive assistance and proactive suggestions proved highly effective. We found that:\n\nThe suggestion system successfully prompted users to explore new travel possibilities they hadn't considered. The contextual relevance of suggestions led to higher conversion rates compared to static prompts. The caching strategy ensured that both systems maintained quick response times even under heavy load, albeit costing a bit to cache combinatronically every variation.\n\n## Impact on the Ruby community\n\nBoth the client and our work have had lasting effects beyond this project. The improvements both of us contributed to LangchainRB have made enterprise-grade AI more accessible to the Ruby community. We've shown that organizations heavily invested in Ruby don't need to pivot to Python for AI capabilitiesâ€”they can build sophisticated AI applications while leveraging their existing expertise and infrastructure.\n\n## Key learnings\n\nThis project taught us valuable lessons about choosing the right tool for the context rather than the most obvious tool for the task. The decision to enhance Ruby's capabilities rather than introduce Python complexity paid dividends in terms of system reliability, security, and maintainability.\n\nThe suggestion system proved that AI assistants can do more than just respondâ€”they can proactively enhance user experience through contextual awareness. The careful balance between fresh and cached suggestions showed how performance optimization doesn't have to come at the cost of relevance.\n\n## Conclusion\n\nThis project stands as a testament to the power of working with, rather than against, existing infrastructure. By choosing to enhance Ruby's AI capabilities rather than force-fitting Python into a Ruby environment, we delivered a solution that ultimately was more engaging for end users.\n\nThe success of this implementation proves that with the right engineering approach, Ruby can be a formidable platform for enterprise AI applications. More importantly, it demonstrates that sometimes the best path forward isn't the most obvious oneâ€”it's the one that best fits the entire ecosystem while pushing the boundaries of what's possible.\n","title":"AI-powered Ruby travel assistant","short_title":"","description":"A case study exploring how we built an AI-powered travel assistant using Ruby and AWS Bedrock, demonstrating how choosing the right tools over popular choices led to a more robust and maintainable solution. This study examines our approach to integrating AI capabilities within existing Ruby infrastructure while maintaining enterprise security standards.","tags":["agents","ai"],"pinned":false,"draft":false,"hiring":false,"authors":["monotykamary"],"date":"2024-11-21","filePath":"playground/use-cases/ai-ruby-travel-assistant-chatbot.md","slugArray":["playground","use-cases","ai-ruby-travel-assistant-chatbot"]},{"content":"\nBinance is one of the most popular Decentralized Exchanges worldwide, so the demand for building Binance-integrated applications is growing daily. My team is also onboarding. We have a deal that requires us to build a Binance trading application with the ability to trade on multiple accounts simultaneously. In this way, our clients can optimize their trading progress as much as possible.\n\nEverything worked well at the beginning, motivating the clients to increase the amount of trading accounts and assets. The nightmare came at this moment. The funds began transferring between accounts to balance the strategies, making the client hard to control the fund and its flow. They must log in to each Binance account to track the transfer history manually. This behavior looks bad.\n\nThis emergency lets us begin record every transfers between accounts in the system, then notify to the clients continuously.\n\n## Limitations of Binance income history\n\nTo record every transfers, we need the help of Binance APIs, specifically is [Get Income History (USER_DATA)](https://developers.binance.com/docs/derivatives/usds-margined-futures/account/rest-api/Get-Income-History). Once calling to this endpoint with proper parameters, we can retrieve the following `JSON` response.\n\n```JSON\n[\n\t{\n    \t\"symbol\": \"\",\t\t\t\t\t// trade symbol, if existing\n    \t\"incomeType\": \"TRANSFER\",\t// income type\n    \t\"income\": \"-0.37500000\",  // income amount\n    \t\"asset\": \"USDT\",\t\t\t\t// income asset\n    \t\"info\":\"TRANSFER\",\t\t\t// extra information\n    \t\"time\": 1570608000000,\n    \t\"tranId\":9689322392,\t\t// transaction id\n    \t\"tradeId\":\"\"\t\t\t\t\t// trade id, if existing\n\t},\n\t{\n   \t\t\"symbol\": \"BTCUSDT\",\n    \t\"incomeType\": \"COMMISSION\",\n    \t\"income\": \"-0.01000000\",\n    \t\"asset\": \"USDT\",\n    \t\"info\":\"COMMISSION\",\n    \t\"time\": 1570636800000,\n    \t\"tranId\":9689322392,\n    \t\"tradeId\":\"2059192\"\n\t}\n]\n```\n\n_Code 1: JSON response of Binance API Get Income History (USER_DATA)_\n\nOur job is just passing `TRANSFER` as `incomeType` to filter out other types of Binance transactions. Then we can store these records for use later. But when looking at this response, can you imagine the limitations that I mentioned in the title of this part? Yes! you actually can't know where the fund comes from or move to? Just only can detect whether it is a deposit or withdrawal by using the sign, which is not enough in our system where every account is under our control. If it is hard for you to understand, the result of the transfer notification is look sus as below screenshot.\n\n![Sporadic and confusing transfer logs](assets/nghenhan-bad-logging.png) _Figure 1: Sporadic and confusing transfer logs that lack clear relationships between transactions_\n\nTo me, it looks bad. Ignore the wrong destination balance because of another issue with the data, this logging is too sporadic, hard to understand, and confusing. We can't understand how the fund is transferred. In my expectation, at least, it should like following.\n\n![Clear and connected transfer logs showing fund flow between accounts](assets/nghenhan-better-logging.png) _Figure 2: Clear and connected transfer logs that show the complete flow of funds between accounts_\n\nIf you pay attention to the `JSON` response of Binance API, an idea can be raised in your mind that \"_Hmm, it looks easy to get the better version of logging by just only matching the transaction ID aka tranId field value_\". Yes, it is the first thing that popped into my mind. Unfortunately, once the transfer happens between two accounts, different transaction IDs are produced on each account side.\n\n## Our approach to transfer history mapping\n\n### Current implementation\n\nIt can make you a bit of your time at the beginning when looking at the response of Binance API and ask yourself \"Why does Binance give us a bad API response?\". Bit it is not a dilemma. And Binance API is not as bad as when I mentioned it. This API serves things enough for its demand in the Biance. And more general means can serve more use cases at all.\n\nEnough to explain, now, we get to the important part: matching transfers to make the transfer history logging becomes more robust. I think we have more than two ways to do it. But because this issue comes from a data aspect, we will use a database solution to make it better.\n\nOf course, we need to know the current query first. But it is inconvenient when sharing the source code here. So I will use a flow chart to replace it. This chart can also help us easy to imagine what's happening. It is easy, but the real query is not just to get from transfer history and show everything directly. To know the balance change, one needs to do some additional steps.\n\n```mermaid\nflowchart LR\n    subgraph Input\n        FI[Future Incomes]\n        ACBS[Balance Snapshots]\n    end\n\n    subgraph Processing\n        TD[Transfer Data]\n        TT[Transfer Time]\n        BB[Before Balance]\n        AB[After Balance]\n    end\n\n    subgraph Output\n        FR[Final Record]\n    end\n\n    %% Data collection\n    FI --> TD\n    TD --> TT\n\n    %% Balance processing\n    TT --> ACBS\n    ACBS --> BB\n    ACBS --> AB\n\n    %% Final calculations\n    BB --> FR\n    AB --> FR\n    TD --> FR\n```\n\n_Figure 3: Current flow to build transfer history_\n\nThe flow chart above shows how the current system produced transfer tracking logging.\n\n- From `Future Incomes`, we simply query transfer information such as amount, time, and its sign.\n- Using the time of transfer, query `Balance snapshots` to detect balance before and after it is changed by the transfer.\n\n### How to make it better?\n\nTo do it better, we need to match the transfers together to know the source and destination of the fund. To match the transfers together, we need to specify what is the transfer before and after it (**with the assumption that transfers of the same fund on the send and receive side happen in a small gap of time, and two transfers can't happen in the same time**). We are lucky that Postgresql provides us with two convenient window functions, LEAD and LAG. LEAD is used to access a row following the current row at a specific physical offset. On the other hand, LAG helps with previous row access. With simple syntax and better performance, it is our choice to do transfer paring.\n\n```sql\nWITH matched_transfers AS (\n    SELECT\n        ...,\n        LEAD(...) OVER (ORDER BY fi.time) AS next_...,\n        LAG(...) OVER (ORDER BY fi.time) AS prev_...,\n```\n\n_Code 2: SQL query to match transfer by using LEAD and LAG_\n\nOnce we match each transfer with its previous and follows, we can easily detect type of each transfer by following script.\n\n```sql\nCASE\n    WHEN amount < 0\n        AND next_amount > 0\n        AND (amount + next_amount = 0)\n        AND (next_time - time < interval '5 seconds')\n    THEN 'INTERNAL_TRANSFER'\n```\n\n_Code 3: SQL query to detect type of each transfer depend on it transaction before and after it_\n\nIt is not enough, we can list the following types, and each type has a separate way of detecting:\n\n- Internal transfers (between accounts)\n- External transfers out (withdrawals)\n- External transfers in (deposits)\n\nEverything is fine, from the two above queries, we can produce the record of the transfer with sender and receiver information. But don't miss the balance change. To do it, we need to select proper before and after balances depending on the time of transfer. Imagine we have 100 transfers, and the total amount of records of balance snapshot reaches million or more, it is a real nightmare.\n\nThere is a more subtle way. We can group close transactions of the same account together into a group, then just only need to query the balance of the account at the beginning of the group and calculate other balances by accumulating the amount.\n\n```sql\nSUM(CASE\n    WHEN sender_time_gap > interval '20 seconds' THEN 1\n    ELSE 0\nEND) OVER (\n    PARTITION BY from_account\n    ORDER BY time\n) AS sender_group\n```\n\n_Code 4: SQL to group transfers of the same account and order by time_\n\n```sql\nFIRST_VALUE(\n    COALESCE(\n        (SELECT current_balance\n         FROM account_current_balance_snapshots bs\n         WHERE bs.account_id = from_account\n             AND bs.created_at <= time\n         ORDER BY bs.created_at DESC\n         LIMIT 1),\n        0\n    )\n) OVER (...)\n```\n\n_Code 5: SQL to find balance for the first record of each transfer group that is the result of Code 4_\n\nNow, we have transfer history, in this, each record has its type, and information of the records before and after it. These records are also grouped together, and the leader of each group has its balanced information. Everything readies for querying the final result. Before going to the result, we may be missing a important step that is calculate balance for each transfer in the transfer group. To do it, Postgresql provides us some other interesting window functions. Tale a look following code.\n\n```sql\nGREATEST(0, (\n    sender_initial_balance +\n    SUM(signed_amount) OVER (\n        PARTITION BY from_account, sender_group\n        ORDER BY time\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    )\n))\n```\n\n_Code 6: SQL to calculate balance for each transfer in the transfer group by using window functions ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW_\n\nLet's break down the window frame `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`:\n\n- `UNBOUNDED PRECEDING` means \"start from the very first row in the partition\". In our case, it starts from the first transfer in the group\n- `CURRENT ROW` specifies \"up to the transfer we're currently calculating\"\n- Together, they create a sliding window that grows as we move through the transfers, always starting from the first transfer and including all transfers up to the current one\n\nThis black magic save us from the danger from self join and `RECURSIVE` when calculating the accumulated total.\n\nAfter all, every we are building can be wrapped in the below chart.\n\n```mermaid\nflowchart TD\n    subgraph Input[\"Data Sources\"]\n        FI[Future Incomes]\n        ACBS[Balance Snapshots]\n    end\n\n    subgraph Processing[\"Enhanced Processing\"]\n        RT[\"Transfers Pairing<br/>(LEAD/LAG Analysis)\"]\n        TWT[\"Type Detection<br/>(Internal/External Classification)\"]\n        TWG[\"Transfer Grouping<br/>(Time-Based Clustering)\"]\n        GFB[\"Find Initial Balance<br/>(Starting States)\"]\n        TWB[\"Calculate Balances<br/>(Running Totals)\"]\n    end\n\n    subgraph Output[\"Enhanced Output\"]\n        FR[\"Final Record:<br/>- Paired Transfers<br/>- Balance Changes<br/>- Transfer Types<br/>- Time Relationships\"]\n    end\n\n    %% Data flow\n    FI --> RT\n    RT --> TWT\n    TWT --> TWG\n    TWG --> GFB\n    ACBS --> GFB\n    GFB --> TWB\n    TWB --> FR\n```\n\n_Figure 4: Upgraded process to build transfer history_\n\n## Conclusions\n\nFrom the problem to the idea and finally is the implementation, nothing is too difficult. Every normal software developer can do it even better. But to do the huge thing, we first should begin from the smaller and make it done subtly and carefully. From this small problem, I learned some things:\n\n- **The answer may lie in the question itself.** Instead of blaming Binance API for being so bad, we can take a sympathetic look at it, and see if there is anything we can get out of it.\n- **One small change can make everything better.** When comparing the original transfer tracking log, and the version after upgrading with some small changes in the DB query, there is a huge difference when seeing the new one. This reminds uss that impactful solutions don't always require complex architectures â€“ sometimes they just need careful refinement of existing approaches.\n- **Data challenges are often best addressed through data-driven solutions**. Rather than seeking fixes elsewhere, the key is to leverage the inherent patterns and structure within the data itself.\n","title":"Building better Binance transfer tracking","short_title":"","description":"A deep dive into building a robust transfer tracking syste m for Binance accounts, transforming disconnected transaction logs into meaningful fund flow narratives through SQL and data analysis","tags":["data","fintech","defi"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Mon Nov 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/binance-transfer-matching.md","slugArray":["playground","use-cases","binance-transfer-matching"]},{"content":"\nIn recent years, alongside the growth of blockchain technology, the cryptocurrency market has attracted significant attention from investors and traders. These market participants develop strategies based on their experience and knowledge, primarily demonstrated through their ability to analyze market indicators. One valuable indicator is the BTC-Altcoins outperform metric. This article examines the importance of tracking Bitcoin-Altcoin performance indicators in a trading strategy known as **Hedge**, and explains how to visualize this data effectively.\n\n### What is \"Hedge\"?\n\nIn financial markets, tradeable assets are categorized as **financial instruments**. These instruments primarily fall into two categories: **cash instruments** and **derivatives instruments**, distinguished by how their value is determined.\n\nCash instruments, including stocks, bonds, or currencies, derive their value directly from the underlying asset through market supply and demand. While these values fluctuate, investors maintain ownership of the actual asset, providing inherent value.\n\n> Example: When you own 100 shares of Apple stock, your investment value changes based on Apple's market price. Even during price declines, you retain ownership of these shares, including rights to potential dividends and future value appreciation. Similarly, bondholders receive ongoing interest payments and principal at maturity, regardless of market price fluctuations.\n\nDerivatives, however, base their value on the performance of another asset without conferring direct ownership. This characteristic means their value can diminish completely if the underlying asset moves unfavorably.\n\n> Consider an agreement to purchase Tesla stock at \\$500 in the future. If Tesla's stock price falls below this amount, the agreement loses its practical value since the same stock could be purchased at a lower market price. This type of agreement, known as a call option, could become worthless, resulting in the loss of the initial investment. Unlike owning Tesla stock directly, this agreement holds no residual value under unfavorable conditions.\n\nIn volatile markets like cryptocurrency, derivative risks increase substantially. This volatility necessitates risk management strategies, leading investors to seek hedging opportunities. **Hedge** refers to a strategy where investors minimize potential losses by investing in assets that typically move in opposite directions to their primary investment.\n\n### How \"Hedge\" works?\n\n![alt text](assets/hedge.png) _Figure 1: How Hedge works in general_\n\nLetâ€™s dive into the hedging in cryptocurrency markets to understand it clearly through a practical example.\n\nImagine youâ€™re an investor excited about **Bitcoin (BTC)**. The current price is \\$50,000, and you believe it will go higher. To profit from this, you decide to take a position in BTC by purchasing a derivative contract. This contract represents 1 BTC and allows you to earn if Bitcoin's price goes up. However, if the price drops below \\$50,000, the contract loses value, and your funds could even be liquidated. Essentially, your success depends on BTCâ€™s price staying above \\$50,000. The risk is clear: if BTCâ€™s value falls, your investment takes a hit.\n\nTo protect against potential losses, you implement a hedge by investing \\$50,000 in various alternative cryptocurrencies (altcoins). This decision stems from the observation that altcoins often exhibit different price movements from Bitcoin. This diversification creates a secondary position that can offset potential losses in your primary Bitcoin position.\n\nThe hedging mechanism functions through balanced exposure:\n\n- When Bitcoin's price decreases, negatively affecting your derivative contract, your altcoin investments may appreciate, providing compensatory gains.\n- Conversely, during Bitcoin price increases, profits from your BTC position can offset potential decreases in altcoin values.\n\nThis balanced approach reduces exposure to Bitcoin's price volatility. Portfolio diversification across altcoins serves as protection against significant losses in Bitcoin positions. **Hedging** functions as a risk management tool, allowing continued market participation while maintaining a safety margin against adverse price movements.\n\n### The nature of price volatility\n\nAfter exploring the example, you might wonder _Why do we chose Bitcoin and Altcoins?_, and _Why do their price movements behave this way in the scenario?_ To answer this, letâ€™s look at the nature of price volatility and the dynamics between Bitcoin and Altcoins.\n\n![alt text](assets/btc-dominance.png) _Figure 2: The dominance of Bitcoin (BTC) in the cryptocurrency market_\n\nBitcoin functions as the cryptocurrency market's cornerstone. According to CoinMarketCap:\n\n> Bitcoin dominance is a metric used to measure the relative market share or dominance of Bitcoin in the overall cryptocurrency market. It represents the percentage of Bitcoin's total market capitalization compared to the total market capitalization of all cryptocurrencies combined. Since Bitcoin was the first asset, it has remained the largest by market cap, which is why its dominance in the market is a number that many people follow. We describe the assets tracked in this chart as cryptoassets because it includes tokens and stablecoins, not just cryptocurrencies.\n\nIn simpler terms, Bitcoinâ€™s dominance reflects its central role in the market. When BTCâ€™s price moves, it often triggers a ripple effect across other cryptocurrencies (altcoins). However, the exact behavior of BTC and altcoins can vary depending on market scenarios. Here are some common scenarios that explain the interplay between Bitcoin and Altcoins:\n\n**Money Flow Scenarios:**\n\n- BTC to Altcoins: Fund transfers from Bitcoin to altcoins typically result in decreased Bitcoin prices alongside altcoin appreciation.\n- Altcoins to BTC: Capital movement back to Bitcoin often leads to Bitcoin appreciation while altcoin values decline.\n\n**Market Entry Scenarios:**\n\n- BTC-Focused: New capital entering primarily through Bitcoin can increase BTC prices while altcoin values remain stable.\n- Altcoin-Focused: Fresh investments targeting altcoins may drive their prices upward without significantly affecting Bitcoin.\n\n**Market Exit Scenarios:**\n\n- BTC Withdrawal: Bitcoin liquidation for external markets can decrease BTC prices.\n- Altcoin Withdrawal: Altcoin selloffs may reduce their values while Bitcoin maintains stability.\n\nUnderstanding these market scenarios proves crucial for effective hedging. The strategy works optimally when price movements follow predictable patterns. For instance, during fund flows between Bitcoin and altcoins, losses in one position often balance with gains in the other. However, certain market conditions can challenge hedging effectiveness.\n\n### The downside of hedging\n\nHedging strategies present an inherent trade-off between risk reduction and profit potential. While protecting against losses, hedging inherently limits maximum gains from favorable market movements. The more comprehensive the hedge protection, the more constrained the profit potential becomes.\n\nFurthermore, market-wide withdrawals can negate hedging benefits when both Bitcoin and altcoins experience simultaneous value declines. Additionally, unexpected market events causing concurrent sharp movements in both assets can transform hedging positions into permanent losses. While advanced hedging techniques can address some of these risks, they require additional expertise and careful strategy implementation.\n\nThese limitations emphasize the importance of reliable market indicators for guiding hedging decisions. Performance metrics, particularly the outperforming chart comparing Bitcoin and altcoin movements, provide essential data for strategy refinement. These tools enable traders to navigate market volatility while optimizing their risk-reward balance.\n\n### Why use BTC-Altcoin outperformance?\n\nThe significance of performance indicators in derivative market hedging stems from the direct relationship between contract values and underlying asset performance. This connection makes performance analysis fundamental to precise hedging strategy execution.\n\nThe BTC-Altcoin outperformance metric is comprised of two key components: Bitcoin performance (BTCp) and altcoin performance (ALTp). These components are calculated separately and then compared to determine the relative strength of the two assets.\n\nBitcoin performance (BTCp) measures the performance of Bitcoin relative to its historical average. It is calculated as a percentage change from the historical mean. A positive BTCp indicates that Bitcoin is outperforming its historical average, while a negative BTCp suggests that it is underperforming.\n\nAltcoin performance (ALTp) measures the performance of altcoins relative to their historical average. It is also calculated as a percentage change from the historical mean. A positive ALTp indicates that altcoins are outperforming their historical average, while a negative ALTp suggests that they are underperforming.\n\n$$BTC/Altcoin_{outperformance} = \\frac{BTC_{profit}}{BTC_{init}}-\\frac{Altcoin_{profit}}{Altcoin_{init}}$$ _Formula 1: BTC-Altcoin outperformance calculation_\n\nThe BTCp - ALTp calculation provides clear insight into Bitcoin's relative market position, enabling strategic hedge adjustments. For example, a positive BTCp - ALTp indicates Bitcoin outperformance, suggesting capital flow from altcoins to Bitcoin. This scenario requires adjusting hedge positions to favor Bitcoin exposure. A negative BTCp - ALTp suggests altcoin outperformance, indicating the need to protect altcoin positions more actively.\n\nThis relative performance measurement enhances hedging precision. Rather than maintaining static position balances, traders can dynamically adjust allocations based on market strength indicators. This approach improves hedge efficiency and reduces exposure to unexpected market movements.\n\n### How can we find and render this metric to a chart?\n\nTo visualize the BTC-Altcoin outperformance metric, we need to systematically track and calculate the relative performance of Bitcoin and altcoins. Hereâ€™s a step-by-step guide to obtaining and rendering this metric on a chart:\n\n**1. Create an Account on a Cryptocurrency Marketplace:** Start by creating an account on a platform that supports derivatives trading for Bitcoin and a wide variety of altcoins. Ensure the platform allows both long and short positions, as this is critical for collecting performance data.\n\n**2. Open a Position on Bitcoin (BTC):** Take a long or short position on Bitcoin with a specific amount, say 1 BTC. This position will serve as the benchmark for tracking Bitcoinâ€™s performance.\n\n**3. Open Positions on All Altcoins:** Simultaneously, take the opposite position on all available altcoins. The total value of these altcoin positions should equal the Bitcoin position, but divide this equally among the altcoins for fair comparison.\n\n**4. Track the Profit/Loss on Positions Intervally:** At regular intervals (e.g., every minute), collect the profit or loss data for the Bitcoin position and for the combined altcoin positions. This data will form the basis of the performance calculation.\n\n**5. Calculate Bitcoinâ€™s Performance (BTC Performance):** The profit or loss on the Bitcoin position relative to the starting value represents Bitcoinâ€™s performance over the tracked interval.\n\n**6. Calculate Altcoinsâ€™ Performance (Altcoin Performance):** Add up the profit or loss from all the altcoin positions to calculate the total altcoin performance for the same interval.\n\n**7. Convert to Percentage Gains or Losses:** For both BTC and altcoins, calculate the percentage profit or loss relative to the initial position value. This standardizes the performance metrics and makes them directly comparable.\n\n**8. Compute BTC-Altcoin Outperformance:** Subtract the percentage performance of altcoins from the percentage performance of Bitcoin.\n\n**9. Render the Data as a Chart:** Plot this BTC-Altcoin outperformance metric on a line chart over time. This chart will visually represent whether Bitcoin is outperforming or underperforming relative to altcoins at each interval.\n\n![alt text](assets/perf-chart.png) _Figure 3: BTC-Altcoin outperformance chart_\n\n### Conclusions\n\nIn this post, we discussed the importance of tracking Bitcoin-Altcoin outperformance and how it can be used to inform trading decisions. We walked through the process of collecting the necessary data, calculating the performance metrics and rendering the data as a chart. This process is applicable to various platforms and can be adapted to the specific requirements of each. By tracking the outperformance of Bitcoin relative to altcoins, traders can gain valuable insights into market trends and make more informed decisions about their investments.\n\nThe techniques and knowledge presented in this post are valuable for:\n\n- Traders and investors who are interested in creating a trading strategy based on the outperformance of Bitcoin relative to altcoins.\n- Developers who are tasked with building a trading platform that provides this information to its users.\n- Data analysts who are looking to gain insights into the performance of Bitcoin relative to altcoins.\n\nThe post is designed to be accessible to readers with a basic understanding of trading and investing. No technical knowledge is required to follow the discussion.\n","title":"Tracking Bitcoin-Altcoin Performance Indicators in BTC Hedging Strategy","short_title":"","description":"This article provides an overview of the importance of tracking Bitcoin-Altcoin performance indicators in a trading strategy known as Hedge, and explains how to visualize this data effectively. It also demonstrates how to render a chart for this strategy using Matplotlib and Seaborn","tags":["data","fintech","blockchain"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Thu Jan 02 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/bitcoin-alt-performance-tracking.md","slugArray":["playground","use-cases","bitcoin-alt-performance-tracking"]},{"content":"\nUmbrella is a project management platform tailored for athletes, musicians, creatives, and businesses alike, bringing everything from team collaboration to secure document sharing under one roof. As our user base grew to a substantial number of active users managing a significant volume of projects and tasks, we identified an opportunity to leverage generative AI to enhance our platform's capabilities and streamline project management workflows.\n\nThe challenge was to natively integrate a generative AI chatbot that could assist users in brainstorming ideas, generating project proposals, and performing tasks directly within the chat interface. By enabling users to seamlessly switch between research, ideation, and execution, we aimed to boost productivity and simplify project management.\n\nImplementing the chatbot agent involved key technical domains such as developing an interface to communicate with external AI platforms like OpenAI, creating an agentic system to interpret and execute user requests, and setting up usage monitoring to control AI token consumption and track chatbot performance.\n\n## System requirements\n\n### Business requirements\n\n- Chatbot should be able to answer general questions about project management, such as writing project proposals or epic planning.\n- Chatbot should assist users in managing tasks, events, and projects by intelligently clarifying user questions, performing tasks accurately, and providing helpful suggestions when needed.\n- Chatbot should generate project proposals, provide task recommendations, and assist with event planning.\n\n### Technical requirements\n\n**Scalability**\n\n- The chatbot functionality should scale with the increase in system functions, following a supervisor-worker design pattern to modularize chatbot capabilities.\n- The system should ensure the response time under 4-6 seconds.\n- Implement multiple API keys combined with a load balancer to distribute API usage and prevent reaching rate limits.\n\n**Reliability**\n\n- Integrate LangSmith for logging and monitoring to track model performance metrics and analyze conversation success rates.\n- Implement A/B testing for prompts to optimize chatbot performance.\n\n**Security**\n\n- Implement user authentication and authorization to prevent unauthorized access and prompt injection attacks.\n- Add guardrails to restrict chatbot usage to activities within the system's scope.\n- Implement data encryption and access control measures to protect user data.\n\n**Integration**\n\n- Utilize the GPT-4o model for general logic and reasoning tasks, considering cost-benefit analysis.\n- Leverage the LangChain framework for coding LLM agents due to its large community and comprehensive documentation.\n\n## Architecture overview\n\n### System components\n\n![](assets/umbrella-chat-bot.webp)\n\n**Supervisor**\n\n- Acts as the central coordinator and decision-maker, breaking down complex tasks into smaller subtasks and assigning them to appropriate workers.\n- Monitors and evaluates worker outputs to ensure accuracy and coherence.\n\n**Worker agents**\n\n- **Task agent**: Handles tasks related to the Task module, such as creating tasks, validating tasks, and providing task recommendations.\n- **Event agent**: Manages events within the Event module, including creating events, validating events, and assisting with event planning.\n- **Project agent**: Deals with project-related functions in the Project module, such as listing projects, creating projects, querying project attributes, and checking project members.\n- **General agent**: Handles general Q&A within the scope of the system, providing informative responses and guidance.\n\n**Load balancer**\n\n- Distributes LLM API keys using the Least Frequency Used (LFU) technique to optimize API usage.\n- Acts as a gateway to check user token limits and prevent excessive usage.\n\n**LLM providers**\n\n- Utilizes the GPT-4o model from OpenAI for its strong performance in general logic and reasoning tasks.\n\n**Monitoring & logging**\n\n- Integrates LangSmith for tracing input and output of AI systems, monitoring system metrics, and evaluating overall performance.\n\n**Database**\n\n- Uses MongoDB to store data, including chat history and token usage, enabling efficient retrieval and analysis.\n\nThe data flows from the user to the Supervisor, which routes the request to the appropriate worker agent. The worker agent processes the request, interacting with the necessary tools and the database, and generates a response. The response is then returned to the Supervisor and finally to the user.\n\n## Technical implementation\n\n### Core workflows\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Supervisor\n    participant Agent\n    participant Tool\n    participant MongoDB\n\n    User ->> Supervisor: Send query\n    Supervisor ->> Supervisor: Select appropriate agent based on user query\n    Supervisor ->> Agent: Send user query with system prompt to matched agent\n    Agent ->> Agent: Determine if tool usage is needed based on input\n    alt Need to call tool\n        Agent ->> Tool: Call tool to process user query\n        Tool ->> MongoDB: Set/Get data\n        MongoDB ->> Tool: Response\n        Tool ->> Agent: Return processed data\n        Agent ->> Agent: Generate response based on tool output\n    else No need to call tool\n        Agent ->> Agent: Generate response directly\n    end\n    Agent ->> Supervisor: Return user response\n    Supervisor ->> User: Return user response\n\n```\n\nThe workflow diagram illustrates the core interaction between the user, Supervisor, worker agents, tools, and the database. The Supervisor analyzes the user's query and routes it to the appropriate worker agent. The worker agent determines if tool usage is necessary and generates a response based on the processed data or directly, depending on the query. The response is then returned to the user via the Supervisor.\n\n### Technical challenges & solutions\n\n**Managing long conversation threads**\n\nTo address the challenge of endless conversations reaching the LLM model's context limit due to the UI design not splitting conversations into separate threads, we implemented a cronjob that runs every minute to close threads where the last message is more than 10 minutes old and limited the history context to the last 25 messages. This solution successfully prevented context limit errors and maintained conversation manageability\n\n**Maintaining chatbot accuracy/performance while adding functions**\n\nAs the number of function modules increased, the chatbot's scope expanded, leading to longer system prompts, increased hallucination, reduced accuracy, and difficult codebase maintenance. To overcome this challenge, we implemented a supervisor-worker pattern using LangGraph, a library of LangChain, to build a multi-agent AI system. By dividing the AI workload among multiple agents and using a supervisor to orchestrate and route tasks, we successfully reduced hallucination, maintained stable accuracy, and improved codebase maintainability even with the addition of new chatbot functions.\n\n**Widget-based display**\n\nTo address the need for displaying custom UI elements instead of text-only responses in chatbot conversations, we configured the chatbot to respond with HTML widget strings, allowing the frontend to render custom UI elements within the chat. For example, when a user requests to create a task, the chatbot generates an HTML widget string, based on which the frontend can render a polished UI card containing all the relevant task information and a link to the task detail. This solution enhanced chatbot responses with visually appealing and informative custom UI blocks, improving user experience and comprehension.\n\n## Technology stack\n\n- Core Technologies:\n  - **TypeScript**: Primary programming language for development.\n  - **Node.js**: Backend runtime environment.\n  - **React**: Frontend library for building user interfaces.\n- Key Frameworks/Libraries:\n  - **LangChain**: Framework for developing LLM-based agents, providing a structured approach to building conversational AI systems.\n  - **LangGraph**: Library within LangChain used to build LLM systems based on graph structures, enabling multi-agent architectures.\n  - **LangSmith**: Platform developed by LangChain for debugging, testing, evaluating, and monitoring LLM applications, ensuring robustness and reliability.\n- Infrastructure Components:\n  - **Next.js**: Framework for building server-rendered React applications, providing seamless integration between frontend and backend.\n  - **MongoDB**: NoSQL database for storing chat history, token usage, and other relevant data, offering flexibility and scalability.\n\n## Lessons learned\n\n### What worked well\n\n1. Implementing the supervisor-worker pattern using LangGraph allowed us to build a scalable and extensible multi-agent AI system that could handle increasing functionalities without compromising performance.\n2. Leveraging popular AI frameworks like LangChain and platforms like LangSmith accelerated development and provided robust tools for debugging, testing, and monitoring the chatbot agent.\n3. Structuring the chatbot's responses as HTML widgets significantly enhanced the user experience by enabling visually appealing and informative custom UI elements within the chat interface.\n\n### Areas for improvement\n\n1. Managing long conversation threads remains a challenge due to the UI design limitations. In the future, we plan to explore text summarization techniques and implement a more user-friendly thread management system.\n2. While the current implementation handles scalability well, there is room for optimization in terms of resource utilization and load balancing. We aim to investigate advanced load balancing techniques and fine-tune the system architecture.\n\n### Future considerations\n\n1. Building a Retrieval-Augmented Generation (RAG) system to enable the chatbot to access real-time knowledge and provide more up-to-date and contextually relevant responses.\n2. Implementing a feedback system to gather user input and continuously improve the chatbot's accuracy and performance based on real-world interactions and user preferences.\n\n## Conclusion\n\nThe implementation of the chatbot agent has significantly streamlined project management workflows within the Umbrella platform. By leveraging generative AI and a multi-agent architecture, we have enabled users to seamlessly brainstorm ideas, generate project proposals, and perform tasks directly within the chat interface.\n\nThe scalable and extensible architecture, built using the supervisor-worker pattern and powered by LangChain and LangGraph, allows for future enhancements and the addition of new functionalities without compromising performance. The integration of LangSmith ensures robust debugging, testing, and monitoring capabilities, maintaining the chatbot's accuracy and reliability.\n\nThe successful adoption of the chatbot agent has resulted in increased productivity, improved user satisfaction, and reduced cognitive load for project managers and team members alike. As we continue to iterate and improve upon the chatbot agent, we remain committed to delivering a seamless and intelligent project management experience for our users.\n","title":"Building chatbot agent to streamline project management","short_title":"","description":"A technical case study detailing the implementation of an AI chatbot agent in a project management platform. Learn how the team leveraged LangChain, LangGraph, and GPT-4 to build a multi-agent system using the supervisor-worker pattern. ","tags":["agents","ai"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh","hoangnnh"],"date":"2024-11-21","filePath":"playground/use-cases/building-chatbot-agent-for-project-management-tool.md","slugArray":["playground","use-cases","building-chatbot-agent-for-project-management-tool"]},{"content":"\nAt Dwarves, we faced the challenge of efficiently transcribing and summarizing our weekly OGIF event recordings for our Brainery knowledge hub. This required developing a scalable data pipeline capable of processing YouTube videos, extracting audio, and leveraging AI models for transcription.\n\nOur solution needed to handle diverse video formats, support concurrent processing, and integrate seamlessly with existing infrastructure. The goal: democratize access to OGIF content while enabling powerful search and analytics capabilities.\n\n## Data pipeline design\n\nThe transcription data pipeline is part of a larger system that includes a REST API for job management, a task queue for asynchronous processing, and a web interface for user interaction. The pipeline architecture can be divided into three main stages: Data Extraction, Data Transformation, and Data Loading and Analysis.\n\n```mermaid\ngraph LR\n\n    %% Extract Stage\n    subgraph \"Data Extraction\"\n        A[YouTube API] -->  B[Video Metadata]\n        A --> C[Audio Data]\n    end\n\n    %% Transform Stage\n    subgraph \"Data Transformation\"\n        B --> D[Process Audio]\n        C --> D\n        D --> E[Transcription Text]\n        E --> F[Cleaned Text]\n    end\n\n    %% Load Stage\n    subgraph \"Data Loading and Analysis\"\n        F --> |Store Transcription| G[(Database)]\n        G --> I[Analyze Transcription Data]\n        I --> J[Insights]\n    end\n\n    %% Style Definitions for Consistency\n    style A fill:#f9f,stroke:#333,color:#000\n    style D fill:#bfb,stroke:#333,color:#000\n    style I fill:#bbf,stroke:#333,color:#000\n    style J fill:#fbf,stroke:#333,color:#000\n\n```\n\n### Data extraction\n\nThe data extraction stage involves fetching video metadata and audio data from the YouTube API. The Video Downloader component interacts with the YouTube API to retrieve the necessary information.\n\n1. **Video metadata**: The Video Downloader fetches the video from YouTube using the provided URL. It downloads the video file and extracts the video duration. The downloaded video is then passed to the next stage of the pipeline for audio extraction.\n2. **Audio data**: The Video Downloader extracts the audio data from the YouTube video. It retrieves the audio stream in a suitable format for further processing.\n\n### Data transformation\n\nThe data transformation stage takes the extracted audio data and performs various processing steps to prepare it for transcription and analysis.\n\n1. **Audio processing**: The Audio Preprocessor component compresses the extracted audio data to minimize its size. This compression step optimizes the audio for efficient transmission to Groq's transcription service while maintaining the necessary audio quality for accurate transcription.\n2. **Transcription**: The Transcription Engine takes the preprocessed audio data and converts it into text using AI-based transcription models. It may utilize services like [Groq AI](https://groq.com/) to perform accurate speech-to-text conversion.\n3. **Text cleaning**: The transcribed text undergoes a cleaning process to remove any artifacts, formatting issues, or inconsistencies. The cleaned text is then ready for storage and analysis.\n\n### Data loading and analysis\n\nThe data loading and analysis stage involves storing the transcribed text in a database and performing various analyses to derive insights from the transcription data.\n\n1. **Database storage**: The cleaned transcription text is stored in a PostgreSQL database along with relevant metadata. This allows for efficient retrieval and querying of the transcription data.\n2. **Text analysis platform**: The stored transcription data is made available to a text analysis platform, which performs various analytics tasks to extract insights and generate meaningful results.\n3. **Insights generation**: The text analysis platform applies techniques such as sentiment analysis, topic modeling, and keyword extraction to derive valuable insights from the transcription data. These insights can be used for content summarization, trend analysis, and data-driven decision-making.\n\n## Core workflow\n\nThe transcription workflow involves the User, API, Database, Downloader, S3Storage, Transcriber, GroqAI, and OpenAI.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API\n    participant Database\n    participant Downloader\n    participant S3Storage\n    participant Transcriber\n    participant GroqAI\n    participant OpenAI\n\n    User->>API: Submit YouTube URL\n    API->>Database: Create Job (Pending Status)\n    Database-->>API: Job ID Returned\n    API-->>User: Job Submission Confirmation\n\n    loop Job Processing\n        Downloader->>Database: Fetch Pending Download Jobs\n        Downloader->>Database: Lock Job\n        Downloader->>YouTube: Download Video\n        Downloader->>Downloader: Convert to Audio\n        Downloader->>S3Storage: Upload Compressed Audio\n        Downloader->>Database: Update Job Status (Uploaded)\n    end\n\n    loop Transcription\n        Transcriber->>Database: Fetch Uploaded Jobs\n        Transcriber->>Database: Lock Job\n        Transcriber->>S3Storage: Download Audio\n        Transcriber->>GroqAI: Transcribe Audio\n        GroqAI-->>Transcriber: Transcription Result\n\n        alt Optional Typo Correction\n            Transcriber->>OpenAI: Refine Transcription\n            OpenAI-->>Transcriber: Cleaned Transcription\n        end\n\n        Transcriber->>Database: Store Transcription\n        Transcriber->>Database: Update Job Status (Completed)\n        Transcriber->>S3Storage: Delete Temporary Audio\n    end\n\n    User->>API: Query Job Status\n    API->>Database: Retrieve Job Details\n    Database-->>API: Job Status/Transcription\n    API-->>User: Return Results\n\n```\n\nThe main steps are:\n\n**Job submission:**\n\n- User submits YouTube URL to API\n- API creates a new \"Pending\" job in Database\n- API confirms job submission to User with Job ID\n\n**Job processing - Downloader:**\n\n- Downloader fetches pending jobs from Database\n- Downloader locks job, downloads video, converts to audio\n- Compressed audio uploaded to S3Storage\n- Job status updated to \"Uploaded\"\n\n**Transcription - Transcriber:**\n\n- Transcriber fetches uploaded jobs from Database\n- Transcriber locks job, downloads audio from S3 Storage\n- Audio sent to GroqAI for transcription\n- Transcription refined by OpenAI\n- Final transcription stored in Database\n- Job status updated to \"Completed\"\n- Temporary audio file deleted from S3 Storage\n\n**Result retrieval**:\n\n- User queries job status through API\n- API retrieves job details and transcription from Database\n- API sends transcription results to User\n\nThe API acts as the intermediary between User and backend components. The Database stores job information and transcriptions. The Downloader handles video downloading and audio conversion, while the Transcriber manages transcription, interacting with GroqAI and OpenAI. S3Storage is used for temporary audio storage.\n\nThe workflow ensures efficient job processing through asynchronous processing and job locking. The separation of Downloader and Transcriber allows for parallel processing and scalability. The optional typo correction step with OpenAI enhances transcription quality.\n\n## Performance benchmarks\n\nThe system is designed to handle the following benchmarks:\n\n- Process 100 simultaneous transcription jobs\n- Handle videos from 5 minutes to 2 hours\n- Complete processing within 5 minutes per video\n- Maintain 90%+ transcription accuracy\n- Ensure sub-500ms API response times\n- Complete jobs within 15 minutes\n\nTo ensure the pipeline could handle the expected scale and provide timely results, several optimizations were implemented:\n\n1. **Parallel Processing**: The Video Downloader, Audio Preprocessor, and Transcription Engine were designed to process multiple jobs concurrently. The number of parallel workers can be dynamically adjusted based on load.\n2. **Asynchronous API Calls**: The interactions with external services (YouTube, Groq, OpenAI) were made asynchronous to avoid blocking the main pipeline flow.\n3. **Intelligent Chunking**: Dynamic chunking of audio files allowed optimizing for the input constraints of the AI transcription services while minimizing total API calls.\n4. **Temporary File Management**: Audio files were stored in S3 only for the duration of processing and deleted afterwards to minimize storage costs.\n5. **Database Connection Pooling**: A pool of reusable database connections was used to avoid the overhead of establishing new connections for each operation.\n\nRobust error handling and monitoring were critical to ensure pipeline reliability and maintainability:\n\n1. **Retry Policies**: Each stage of the pipeline was configured with appropriate retry policies to handle transient failures from external services or temporary resource constraints.\n2. **Dead-Letter Queues**: Jobs that repeatedly failed even after retries were moved to a dead-letter queue for manual inspection and intervention.\n\n## Technology stack\n\nThe transcription service leverages the following technologies and tools:\n\n**Core platform**\n\n- Python 3.9+\n- Flask/FastAPI for RESTful APIs\n- Celery + Redis for task queue\n- Gunicorn for WSGI server\n\n**AI/ML services**\n\n- Groq AI for transcription\n- OpenAI GPT-4 for text refinement\n- Custom rate limiting and retry logic\n\n**Data & storage**\n\n- PostgreSQL for persistent storage\n- Redis for caching/queues\n- AWS S3 for file storage\n- `Boto3` for AWS operations\n\n**Media processing**\n\n- `yt-dlp` for video downloading\n- `FFmpeg` for video manipulation\n- `pydub` for audio processing\n\n**Infrastructure & devOps**\n\n- Docker + Docker Compose\n- GitHub Actions for CI/CD\n- Prometheus + Grafana monitoring\n- Nginx reverse proxy\n\n**Security & documentation**\n\n- JWT authentication\n- SSL/TLS encryption\n- OpenAPI/Swagger documentation\n\n## Lessons learned\n\nKey successes of the project include:\n\n1. Modular decoupling of downloader, transcriber and API logic improved scalability and maintainability. Issues could be identified and fixed quickly in each module.\n2. Optimized resource allocation by isolating components and catering to their specific needs. This led to efficient performance without over or under-provisioning.\n3. Asynchronous architecture allowed non-blocking processing of long-running jobs. Users receive immediate job ID and can poll for status without holding up resources.\n\nAn area for improvement is the current video chunking implementation, which needs refinement to ensure perfectly synced transcription. The team is exploring alternatives to split videos more intelligently based on breaks and allow finer-grained processing.\n\n## Conclusion\n\nBuilding the YouTube transcription data pipeline required carefully orchestrating the interaction between several system components and external services. The ETL pattern provided a clear structure to reason about the data flow and transformation steps.\n\nThe asynchronous, event-driven architecture allowed each stage of the pipeline to scale independently and handle failures gracefully. Techniques like parallel processing, intelligent chunking, and connection pooling helped achieve the required performance and throughput.\n\nThe end result was a robust and efficient data pipeline that could reliably transcribe a high volume of YouTube videos and make the content searchable and accessible across the organization. The pipeline unlocked new ways to distill insights from previously opaque video content.\n","title":"Building data pipeline for OGIF transcriber","short_title":"","description":"A technical case study of creating an automated system that downloads videos, processes audio, and generates transcripts using AI services like Groq and OpenAI.","tags":["ai","agents","data"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh","quang"],"date":"2024-11-21","filePath":"playground/use-cases/building-data-pipeline-ogif-transcriber.md","slugArray":["playground","use-cases","building-data-pipeline-ogif-transcriber"]},{"content":"\nHedge Foundtion, a private trading platform serving select traders, required a robust centralized monitoring system to ensure platform reliability and prevent financial losses. Given the high-stakes nature of trading operations, the system needed to provide real-time alerts, maintain data integrity, and optimize resource allocation to protect traders from potential monetary losses.\n\n## Understanding the unique challenges\n\nAs a privately-owned platform with a limited user base, Nghenhan faces unique challenges:\n\n1. **High-stakes trading**: Each user represents a significant portion of the platform's trading volume. Any system failure or data loss can lead to substantial financial losses for these traders.\n2. **Reputational risk**: With a smaller user base, any issues with the platform can quickly erode trust and lead to user attrition. Maintaining a stellar reputation is essential for Nghenhan's long-term success.\n3. **Resource allocation**: While the user base is limited, the platform must still be equipped to handle peak usage times and spikes. Efficient resource allocation is critical to ensure reliable performance without overspending on infrastructure.\n\n## Mitigating financial losses through proactive monitoring\n\nTo address these challenges, Nghenhan must implement a proactive monitoring strategy that focuses on:\n\n1. **Real-time alerts**: The monitoring system must provide instant notifications for any anomalies or threshold breaches. This allows the team to react swiftly and minimize the duration and impact of any issues.\n2. **Data integrity**: Ensuring the accuracy and synchronization of trading data is paramount. Any data loss or discrepancies can trigger false alarms or missed opportunities, leading to financial losses for traders.\n3. **Resource optimization**: Monitoring resource utilization helps Nghenhan allocate resources effectively during peak times while avoiding over-provisioning during normal usage.\n\n## Implementing Grafana and Prometheus for robust monitoring\n\nIntegrating Grafana and Prometheus provides Nghenhan with a powerful centralized monitoring solution. Let's dive deeper into how these tools work together and examine the system diagram:\n\n![](assets/nghenhan-monitoring-system-diagram.webp)\n\n- **Backend services**: The backend services of Nghenhan's trading platform expose metrics through an HTTP endpoint, which Prometheus scrapes at regular intervals.\n- **Prometheus server**: The Prometheus server scrapes the metrics from the backend services and stores them as time series data. It also handles the querying and alerting functionality.\n- **Alert manager**: The Alert Manager is a component of Prometheus that handles the routing and management of alerts. It receives alerts from Prometheus and sends notifications to the configured receivers, such as the admin or notification channels.\n- **Grafana**: Grafana fetches data from Prometheus to create visualizations and dashboards. It also allows users to set up alerts and explore historical data.\n- **Notification receivers**: The notification receivers are the endpoints or channels where alerts are sent, such as email, Discord, or custom webhooks. The admin can also receive notifications and take appropriate actions based on the alerts.\n\n### Prometheus as Data collector\n\nPrometheus serves as the primary data collection and monitoring tool, scraping metrics from various services and recording health and performance information. The setup involves configuring Prometheus to gather data on key metrics, including:\n\n**CPU and Memory usage**\n\nMaking sure that system resources are not reaching critical thresholds.\n\n![](assets/nghenhan-cpu-usage.webp)\n\n**Error rates**\n\nTracking the number of errors in real time to quickly detect discrepancies.\n\n![](assets/nghenhan-error-rate.webp)\n\n**Data synchronization status**\n\nMonitoring the synchronization of data from Binance to ensure its latest version without any data loss.\n\n![](assets/data-sync-status.webp)\n\n**Binance rate limit monitoring**\n\nImplementing a rate limit monitoring system to ensure that requests to Binance are still compliant with the rate limits. This will prevent data loss during periods of high network traffic.\n\n![](assets/nghenhan-binance-rate-limit.webp)\n\n**Service back-off restarting**\n\ndue to multiple issues, such as resource limits, configuration errors, or dependency failures.\n\n![](assets/nghenhan-service-back-off-restarting.webp)\n\n### Grafana as Data Visualizer for insightful observations\n\nGrafana complements Prometheus by providing robust data visualization capabilities. With Grafana, Nghenhan can create dynamic dashboards that display real-time data on service performance. These dashboards include:\n\n**Real-time alerts**\n\nConfigured alerts notify our team of any anomalies, such as sudden increases in CPU usage or error rates, etc. that exceed established thresholds.\n\nHereâ€™s an example of how we configured conditions on Grafana to trigger an alert using the Alert Manager\n\n![](assets/nghenhan-real-time-alert.webp)\n\nThe setup above will trigger an alert if data exceeds the threshold, and the Alert Manager will send it to Discord by webhook.\n\n![](assets/nghenhan-discord-alert.webp)\n\n**Interactive graphs**\n\nWe utilize visual representations of data that help us easily identify trends during peak trading times and spikes.\n\n![](assets/nghenhan-interactive-graph.webp)\n\n**Historical data analysis**\n\nGrafanaâ€™s capabilities allow us to analyze historical data to understand system behavior and improve resource allocation strategies.\n\n![](assets/nghenhan-historical-analytics.webp)\n\n## Conclusion\n\nTo sum up, Nghenhan's decision to adopt a centralized monitoring system powered by Grafana and Prometheus is a testament to its dedication to providing a reliable and efficient trading platform. By focusing on real-time monitoring and ensuring data synchronization, Nghenhan can proactively identify and resolve potential issues, minimizing downtime and financial losses for its users. This monitoring system not only bolsters Nghenhan's operational capabilities but also serves as a foundation for future growth.\n","title":"Setup centralized monitoring system for Hedge Foundation trading platform","short_title":"","description":"A technical case study for implementing centralized monitoring for a trading platform using Grafana and Prometheus, focusing on real-time alerts, data integrity, and resource optimization to prevent financial losses.","tags":["monitoring","fintech"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh","quang"],"date":"2024-11-21","filePath":"playground/use-cases/centralized-monitoring-setup-for-trading-platform.md","slugArray":["playground","use-cases","centralized-monitoring-setup-for-trading-platform"]},{"content":"\n\n\n\n\n## My workflow with Overleaf and ChatGPT\n\nA few weeks ago, I was asked to make a slide deck for a team meeting. Normally, I usually use [Google Slides](https://workspace.google.com/products/slides/) or [Markdown via Marp](https://marp.app/) to make a simple slide for presentation. But this meeting is more serious, so I needed to make a professional, high-standard slide deck. This requirement *made* me think of using [Overleaf](https://www.overleaf.com/), a tool that helps create slides in a professional format. It worked so well that I want to share my experience. This is my story, walking through the problems I faced, the solution I found, and some tips that might help you too.\n\n### Consistent and polished slides\n\nWhile tools like [Google Slides](https://workspace.google.com/products/slides/) are great for quick presentations, ensuring a consistently professional and polished look for a more serious meeting can present its own set of challenges. Even seemingly simple tasks, like maintaining uniform fonts, precise spacing, and a cohesive design across all slides, can become surprisingly time-consuming and require meticulous attention to detail. This can detract from the core task of crafting compelling content.\n\nI experimented with markdown via [Marp](https://marp.app/), hoping for a more efficient way to create slides. I found the writing process faster, but I struggled to achieve the level of visual refinement needed for a professional presentation. The output, while functional, lacked the polished aesthetic that would convey the seriousness of the meeting. This experience underscored the need for a tool that could not only streamline the creation process but also inherently produce a high-quality, professional visual output. That's why I decided to explore Overleaf. I knew it was designed to create professional documents and slides with built-in themes that ensure consistency and a polished appearance with minimal effort. Furthermore, its features like online collaboration, and debugging tools made it an even more attractive option for ensuring a smooth and efficient workflow.\n\n![Overleaf](assets/overleaf.png)\n\nBut to make it work, normally, we need to know how to use LaTeX. Not everyone is familiar with it, and it can be a barrier to entry for some. But this is the age of AI, and we can use it to make it easier. Let's see how.\n\n### A two-step solution\n\nI came up with a simple way to make things easier, after some trial and error late at night. I decided to use ChatGPT to write the content and Overleaf to handle the formatting. It felt like having one helper for ideas and another for design. Hereâ€™s how it worked. First, I asked ChatGPT to help me. Iâ€™d give it a request like: \"Write a 3-slide presentation in a format I can use, about implementing a data snapshot pattern to persist historical data, with 3 bullet points per slide.\" It quickly gave me a draft with titles, points, and a structure I could use. It wasnâ€™t perfect, but it was a great starting point. Then, Iâ€™d take that draft and put it into Overleaf. Iâ€™d pick one of its predefined themes, hit â€œRecompile,â€ and get clean slides fast. Overleafâ€™s live preview let me make small changes as I went, and its online setup made it easy for my team to join in. No more confusion over file versions. It all came together smoothly.\n\nThis method worked well for a few reasons. ChatGPT saved me time on writing, turning hours into minutes. It gave me a clear structure, so I didnâ€™t have to plan everything myself. Overleaf made the slides look good with its exporter and themes, without me needing to do much. And it made teamwork simple, keeping us all on the same page. It turned a slow task into something quick and manageable. I was really happy with how it turned out.\n\n### A real example\n\nLet me share one time I used this method, preparing slides on persisting historical data for a tech talk. Iâ€™d been working on a project about using the data snapshot pattern to store historical data, like in a cryptocurrency trading system, and I wanted to explain it. I asked ChatGPT: \"Write a 3-slide presentation in a format I can use, on implementing a data snapshot pattern to persist historical data, 3-4 points per slide.\" It gave me something I could work with, like this:\n\n```latex\n\\begin{frame}{Slide 2: Why Use Snapshots?}\n    \\begin{itemize}\n        \\item Captures data at a specific time\n        \\item Prevents recalculation errors\n        \\item Speeds up report generation\n    \\end{itemize}\n\\end{frame}\n```\n\nI copied it into Overleaf, picked a nice predefined theme, and watched it turn into proper slides. I made a few small changes and added a point about how snapshots help with long-term trend analysis. In about 15 minutes, I had a finished deck. I exported it as a PDF, presented it at the tech talk, and the audience found it clear and useful. It didnâ€™t feel like a chore. It felt like a small win, and I left the talk feeling good.\n\n### Turning this article into slides\n\nHereâ€™s where it gets interesting. I used this very article to test my workflow again, turning it into a slide deck with Overleaf. I wanted to see if it could handle something Iâ€™d already written, and it did. I asked ChatGPT: \"Take this article and make a 4-slide presentation in a format I can use, with 3 points per slide, summarizing my workflow.\" It gave me a good starting point, like this:\n\n```latex\n\\begin{frame}{Slide 1: The Struggle with Slides}\n    \\begin{itemize}\n        \\item Writing content takes too long\n        \\item Organizing ideas is hard\n        \\item Teamwork gets messy\n    \\end{itemize}\n\\end{frame}\n```\n\nI pasted it into Overleaf, chose a clean predefined theme, and added a slide for each section: the problem, the solution, an example, and tips. I adjusted the wording a bit and recompiled it. In under 20 minutes, I had a deck ready to share with you. It shows that this method works, even on itself. You could do the same with this article. Try it, and youâ€™ll see how fast it comes together.\n\n### Automating with Dify\n\n![Workflow](assets/workflow.png)\n\nIf you want to take this even further, Iâ€™ve streamlined the whole process using a tool called Dify. It automates the workflow, making it even easier for anyone to follow. The process starts with your content idea, which gets analyzed and optimized by a content tool. Then, itâ€™s turned into a format you can use, styled, and finalized. After that, itâ€™s uploaded to a gist for easy access, and you get the code as a result. If something goes wrong, it retries up to three times to ensure it works. This setup saves so much time, especially if youâ€™re not comfortable with the manual steps. If youâ€™re feeling lucky, you can try our Dify workflow directly [here](https://prompt.d.foundation/app/eb483740-3915-4aea-9fc4-5c50eb4700f5/workflow). Itâ€™s a great way to see the process in action without doing all the steps yourself. You can also check the example result [here](https://www.overleaf.com/read/jhywvqsdvwxk#8a280e), this is the result of the workflow when I typed \"introduce latex for presentation generation\".\n\n### Tips that helped\n\nAfter using this a few times, I found some ways to make it better. Be specific with ChatGPT. Tell it to keep things short or use lists, and itâ€™ll save you cleanup time. Play with Overleafâ€™s predefined themes to make slides look nicer without extra work. Iâ€™ve found ones like â€œCopenhagenâ€ or â€œBerlinâ€ work well. For my tech talk slides, I asked ChatGPT to summarize key points, which saved me from digging through details. And I kept ChatGPTâ€™s raw text in a separate file, just in case something went wrong in Overleaf. These small steps made the process smoother and more reliable.\n\n### Making it useful for you\n\nTo help you get the most out of this, Iâ€™ve added a few things. Youâ€™ll see real examples, like the snapshot slide or the article summary above. Try them out yourself and see how they work. Picture a simple flow: prompt goes to ChatGPT, then to a format you can use, then to Overleaf, and finally to slides. Thatâ€™s the process in a nutshell. If youâ€™re working on a topic like data persistence, change the prompt to fit your needs, like â€œsnapshot patterns for e-commerce.â€ One time, this saved me when I made a full deck in two hours instead of two days. Just check ChatGPTâ€™s work. Itâ€™s good, but it can miss details if you donâ€™t guide it. Youâ€™ll catch those quickly with a little practice.\n\n#### Wrapping it up\n\nThatâ€™s my story. Itâ€™s how I turned slide-making from a hassle into something simple with Overleaf and ChatGPT. Itâ€™s not complicated, just a practical fix that gets the job done. Next time youâ€™ve got a presentation coming up, maybe about data patterns or your own project, try this out. Itâ€™s helped me more than once, and I think it could help you too. What do you think? Give it a go, adjust it to fit you, and tell me how it works. See you at the next meeting. Your slides will be done, and the stress wonâ€™t be there.\n","title":"Create slides with Overleaf and ChatGPT","short_title":"","description":"This article shares a workflow for making slide decks with Overleaf and ChatGPT. It solves issues like slow content creation using ChatGPT and formats with Overleafâ€™s themes. It includes examples, tips, and a Dify automation for engineers.","tags":["overleaf","llm","slide"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh","tieubao"],"date":"Thu Mar 20 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/create-slides-with-overleaf.md","slugArray":["playground","use-cases","create-slides-with-overleaf"]},{"content":"\nCrypto trading is not just gambling. It has strategies. Once traders can take advantage of it effectively, they can earn. One of the most popular strategies in cryptocurrency is **Hedge**, which is often mentioned with the name **Hedge Bitcoin**.\n\nIn simple words, assuming that we are living in a world where we can only just buy/sell salt and pepper. As a merchant, you buy one for speculation. But the risk happens when the price of your holding asset decreases; you lose. With the **Hedge** strategy, you buy both of them with the belief that if one of them decreases, the other will increase. For example, if the price of salt increases while the price of pepper decreases, you buy more pepper and sell a part of your salt.\n\nThe real strategy is more complex than my example. But we can understand that **Hedge Bitcoin** means trading entirely other assets on the exchange in the inverse direction to decrease the risk of Bitcoin trading. **To execute this strategy effectively, traders must monitor how liquidity flows between Bitcoin and Altcoins across the exchange, revealing their relative performance patterns.**\n\n### How can we extract market-wide insights from trading data\n\nPerformance, in our context, represents the token's profit and loss. For example, if I say, _Performance of BTC is 10%,_ it means BTC has profited 10% since we placed the order. This number closely relates to the asset's liquidity in the market. If you can't imagine the relationship between them, try thinking about the price first. The price of an asset only increases when its total liquidity in the market rises. The change can come from scenarios such as the following:\n\n- Liquidity is moved from other assets to this asset\n- Liquidity from new buyers entering the market\n\n![alt text](assets/liq-price-pnl-relationship.png) _Figure 1: Relationship between asset liquidity, price, and trading PnL_\n\nPrice movements serve as indicators of liquidity flows between assets. When capital moves between Bitcoin and Altcoins, it triggers price changes that result in trading profits or losses. This chain reaction works in reverse too. Trading PnL patterns reveal price movements, which in turn expose the underlying liquidity shifts in the market.\n\nFinally, market performance can be calculated using the following formula:\n\n```math\nmarket_{perf} = BTC_{perf} - Alt_{perf} = \\frac{BTC_{pnl}}{BTC_{init}} - \\frac{\\sum {Alt_{pnl}}}{\\sum{Alt_{init}}}\n```\n\n_Formula 1: Formula to calculate market performance_\n\nThis formula serves as a powerful indicator, revealing liquidity movements across the market whether capital is flowing from Bitcoin to Altcoins, from Altcoins to Bitcoin, or entering/exiting the market entirely. Unlike evaluating price, which provides a general view of trends, performance allows us to quantitatively assess the market. For example:\n\n- When market performance is 5% and BTC performance is 2.5%, this means 2.5% of the funds are moved from Altcoins to BTC, and no new funds have entered the market.\n- When market performance is -10% and BTC performance remains unchanged, this means 10% of funds are poured into the market from outside.\n\nNow we have our framework to hedge. How can we obtain this data? Based on _Formula 1_, we need the trading PnL of BTC and all Altcoins. So our business is to prepare a Binance account; depositing money into it; opening a BTC long position with 50% of our funds, and splitting the rest 50% among short positions on all Altcoins. Finally, we wait for PnL changes at each interval, usually a minute; calculate the performances from the PnLs, and record them in our database for later use.\n\n### From data to dashboard: implementing a Go web interface to render performance charts\n\nAt the end of the previous part, we mentioned the data gathering. Let me show you how was it constructed.\n\n![alt text](assets/market-perf.png) _Figure 2: Market performance data_\n\nIn _Figure 2_, you can easily see that a large amount of liquidity has been pumped into the market. So, how easy is it to get an overview of the market when these figures are visualized? This is the reason why we are trying to do it here, until now.\n\nMarket performance alone is not enough. In addition to estimating future market trends for trading, we also need to evaluate the performance of past trades. To achieve this, I have integrated our trading history into the charts. These include:\n\n- **Trade round**: The time period from the beginning to the end of each trade.\n- **Trading account PnL history**: The PnL changes of a specific Binance account over time.\n\nBy visualizing both market trends and historical trading data, we gain a more comprehensive understanding of our trading efficiency and decision-making process. The first step is selecting the most appropriate type of chart. It must ensure that when multiple data sources are combined, the visualization retains clarity, readability, and meaning. A mixed chart is ideal for this purpose, combining lines to represent market performance and trading PnL changes over time with a double bar chart that juxtaposes BTC and Altcoin performance to highlight their variations. Finally, scoping each trade within a separate window allows us to analyze individual trading periods in detail.\n\n![alt text](assets/perf-chart.png) _Figure 3: The mixed chart that represent the relationship between market performance and trading effective_\n\n#### Aggregating lines from multiple sources\n\nWe will begin the implementation with aggregating and aligning data from different source. The performance data, round period and PnL records each come with their own structure and time frames. To ensure everything aligns properly on the time axis, the data is mapped using time-based Golang map as following.\n\n```go\nperformanceMap := make(map[string]Performance)\nfor _, perf := range performances {\n    performanceMap[perf.Time.Format(\"2006-01-02 15:04:05\")] = perf\n}\n\npnlMap := make(map[string]Pnl)\nfor _, pnl := range pnls {\n    pnlMap[pnl.Time.Format(\"2006-01-02 15:04:05\")] = pnl\n}\n```\n\n_Code 1: Simple mapping value of performance and pnl to the time axis_\n\n#### Construct the chart\n\n```go\nline := charts.NewLine()\nline.SetGlobalOptions(\n    charts.WithTitleOpts(opts.Title{Title: \"Performance and PnL\"}),\n    charts.WithXAxisOpts(opts.XAxis{Name: \"Time\"}),\n    charts.WithYAxisOpts(opts.YAxis{\n        Name: \"Performance (%)\",\n        Min: -maxPerf,\n        Max: maxPerf,\n    }),\n)\n\nline.ExtendYAxis(opts.YAxis{\n\tName: \"PnL ($)\",\n\tAxisLabel: &opts.AxisLabel{\n\t\tFormatter: \"{value} $\",\n\t},\n\tMin: -maxPnL,\n\tMax: maxPnL,\n\tAxisLine: &opts.AxisLine{\n\t\tShow:            &trueval,\n\t\tOnZeroAxisIndex: 1,\n\t},\n})\n\nline.AddSeries(\"Performance\", yAxisPerf)\nline.AddSeries(\"Unrealized PnL\", yAxisPnl, charts.WithLineChartOpts(opts.LineChart{YAxisIndex: 1}))\n\nbar := charts.NewBar()\nbar.SetXAxis(xAxis).\n    AddSeries(\"AltCoin Perf\", yAxisShortPerf).\n    AddSeries(\"BTC Perf\", yAxisLongPerf)\n\nline.Overlap(bar)\n```\n\n_Code 2: Code snippet demonstrates how to create a mixed chart using the **go-echarts** library_\n\nTo combine line and bar charts to visualize market performance, PnL, and BTC/Altcoin performance. The process begins with defining a line chart using the `charts.NewLine()` function. This line chart is configured with global options such as the title, X-axis for time, and a Y-axis labeled \"Performance (%)\", which ranges from `-maxPerf` to `maxPerf`. This setup ensures that the performance data is plotted on a dedicated axis, making it easy to interpret trends over time.\n\nNext, a secondary Y-axis is added to represent PnL, labeled \"PnL ($)\". This axis is configured with its own range (`-maxPnL` to `maxPnL`) and includes an axis line centered on zero for better visual balance. By extending the Y-axis with this configuration, the chart supports two distinct datasets on different scales, ensuring both performance and PnL are displayed clearly without visual clutter.\n\nThe `line.AddSeries` method is used to add the performance data and unrealized PnL data to the chart. Each dataset is represented as a separate line, with the PnL data assigned to the secondary Y-axis using `charts.WithLineChartOpts(opts.LineChart{YAxisIndex: 1})`. This approach ensures that performance and PnL are plotted on their respective axes, maintaining the clarity of the visualization.\n\nTo include BTC and Altcoin performance data, a bar chart is created using `charts.NewBar()`. This bar chart shares the same X-axis as the line chart and is populated with series for \"AltCoin Perf\" and \"BTC Perf\" using the `AddSeries` method. The bar chart highlights how the performance of these two asset classes changes over time, complementing the overall visualization.\n\nThe `line.Overlap(bar)` method combines the line and bar charts into a single cohesive visualization. This allows the user to analyze market trends, PnL changes, and asset performance simultaneously within one chart.\n\nAfter all, we may still missing something. Yes it is the area to represent the boundaries of trade rounds. It is a bit easy, **go-echart** provides us the option `charts.WithMarkAreaNameCoordItemOpts` to integrate the mark areas to the line chart. Our business is construct each area boundaries by specify its coordinates using trade round start, end time.\n\n```go\nopts.MarkAreaNameCoordItem{\n\t\t\t\tCoordinate0: []interface{}{trade.OpenedTime.Format(\"2006-01-02 15:04:05\"), -maxYAxis},\n\t\t\t\tCoordinate1: []interface{}{trade.ClosedTime.Format(\"2006-01-02 15:04:05\"), maxYAxis},\n\t\t\t\tItemStyle: &opts.ItemStyle{\n\t\t\t\t\tColor: \"rgba(255, 255, 255, 0.3)\", // White color with blur effect\n\t\t\t\t},\n\t\t\t},\n```\n\n_Code 3: Code snippet to construct MarkAreaNameCoordItem depend on trade period_\n\n### Conclusions\n\nThis project demonstrates how we can combine Golang and go-echarts to build powerful visualizations that provide deep insights into crypto trading performance. By integrating market trends, historical PnL data, and trading rounds into a single chart, we create a tool that allows traders to make informed decisions with clarity and precision.\n\nThe challenges of aligning multiple datasets, ensuring readability, and maintaining meaningfulness were overcome with careful design and thoughtful implementation. This visualization not only simplifies the analysis of complex trading data but also empowers users to refine their strategies and improve efficiency. As the crypto market continues to evolve, tools like these will be indispensable for staying ahead of the curve.\n","title":"Visualizing crypto market performance: BTC-Alt dynamic indicators in Golang","short_title":"","description":"Implementing a Golang-based visualization for crypto market performance indicators, focusing on Bitcoin vs Altcoin dynamics and trading strategy effectiveness through interactive charts and data analysis","tags":["data","blockchain","fintech"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Mon Nov 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/crypto-market-outperform-chart-rendering.md","slugArray":["playground","use-cases","crypto-market-outperform-chart-rendering"]},{"content":"\n## Data safeguarding strategies\n\nData is an important part of software development and one of the most valuable assets for any organization, especially in economics and finance. Along with the growth of business models, a large amount of data is generated diversely. Keeping data safe is critical for business. Data is lost or becomes wrong, which can cause irreversible loss. For example, in the banking system or stock marketplace, if one transaction record is missed, it can lead to a chain of consecutive wrong behaviors. It can even cause money losses for individual users as well as organizations.\n\nIn software development, we have many strategies to safeguard the data. Depending on each use case, some strategies can be listed here:\n\n- **Data Encryption** marks the original data by ciphertext, making it harder to access.\n- **Data security policy** defines rules to manage data access and role-based permissions.\n- **Data lifecycle management** defines the framework to manage data from creation to destruction.\n- **Data backup and recovery** regularly backs up key data to restore them once any critical issue happens.\n\nEach strategy aligns with specific stages of the data lifecycle and can be combined to maximize protective capabilities.\n\nThis overview provides some strategies to protect the data safely. We will continue delving into a more specific problem in the rest of this post, and explore the way we deal with it.\n\n## Problem with storing large amounts of transactional data that are not accessed frequently\n\n### What problem are we solving?\n\nImagine you are developing a financial application that produces tens of thousands of transactions per day by users because of their cryptocurrency trades. These transactions are firstly stored in the data lake as raw records. Once a trade round (normally 3 months of trading) is over, the raw transactions in this period will be used to produce the final reports and persist to the database. After this time, these records will not be used anymore in both the trading process and summary calculation except for data auditing or report recovery in the future.\n\nOnce the project continues running, the amount of data becomes bigger. It requires us to spend more money to expand the database. The amount of data grows quickly also leading to decreased performance of any operation that needs to interact with the database.\n\nThis situation lets us think about the data archive which is a strategy helping to offload unused data into long-term storage at minimal cost.\n\n### Data archive, why do we need it?\n\nWe first take a look at **data backup** which is the cyclic process of duplicating the entire or a part of data, wrapping it in a stable format then storing it in a secure place. This process is scheduled periodically to make sure we always have at least a copy of production data readies to restore at any time one issue happens.\n\n![alt text](assets/data-backup-and-restore.png) _Figure 1: Simple example in SQL to represent the data backup and recovery process_\n\nIn my point of view when writing this post, backup leans toward the action that captures the state of the database for rolling back to the specific point in the past. By using the backup data, we can do the \"disaster recovery\" in time when a critical problem needs to hotfix. It is often complex and expensive. Backup and recovery in this context can also impact the ongoing work on the production. Data can be lost or wrong if the strategy is not executed carefully.\n\nThe **data archive** may or may not be similar to **data backup**, depending on your definition for each of them. For me, they are similar but with some differences.\n\nWhile backup comes from production data hotfix problems, data archive focuses on long-term data-keeping. With the growth of production data, especially in transactional applications, a lot of data is not needed for normal execution. However, they are required to reproduce important metrics and auditing in the future. **The data archive is the progress of shelving data that has reached the end-of-life in an organized manner to be easy to use later.**\n\n![alt text](assets/data-backup-and-archive.png) _Figure 2: Visualization the differences between data backup and data archiving_\n\nBy implementing the **data archive** strategy, we can decrease the live database's pressure significantly and optimize storage costs while still ensuring long-term data availability.\n\n### Recovery using archived data\n\nArchived data is normally not used for production data hotfix or rollback application state. Instead, it is used to recover critical data such as data snapshots, market reports, or even legal matters like audits.\n\nThis progress is often executed manually. This means that the data is archived automatically each time it persists after an operational phase is completed. This period is determined depending on your application. It can be monthly, yearly, or each trading round in the trading application. However, once the recovery is required to execute, it should be run manually by the administrator.\n\nThis data recovery strategy mainly focuses on calculating instead of restoring. The calculated result usage depends on your use case. However, once it is used to recover the database directly, it must be ensured that it does not interfere with the normal operation of the application or alter any online information as when rolling back the system state using backup data. This process has another name that is called **forward recovery**.\n\nThis approach has some advantages:\n\n- Can regenerate data even without having direct backups\n- Provides data validation through reprocessing\n- Often results in cleaner data since it goes through current business rules\n- Can be useful for audit purposes\n\n## Implementing archive-based recovery strategy for trading application\n\nBack to the first example that was used at the second part to raise the problem. Assume we have a high-frequency cryptocurrency trading platforms that produce 50,000 transactions per day, which accumulates to approximately 4.5 million transactions in a single trading cycle of three months. At an average size of 2KB per transaction, this translates to nearly 9GB of raw data every cycle.\n\nTo deal with this situation, we can design a simple archive and recovery strategy as following:\n\n```mermaid\nflowchart TD\n    subgraph \"Production Environment\"\n        A[Trading System] -->|Real-time Transactions| B[Data Lake]\n        B -->|Raw Records| C[PostgresQL]\n    end\n\n    subgraph \"Archival Process\"\n        C -->|3-month cycle| D[Dedicated Compute Environment]\n        D -->|Encrypted Archives| E[Archive Storage]\n        D -->|Metadata| F[Lightweight Database]\n    end\n\n    subgraph \"Recovery Process\"\n        G[Admin Request] -->|Search| F\n        F -->|Archive Location| E\n        E -->|Retrieve Archive| H[Dedicated Compute Environment]\n        H -->|Processed Data| I[Analysis Instance]\n    end\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#blue,stroke:#333,stroke-width:2px\n    style H fill:#green,stroke:#333,stroke-width:2px\n```\n\n_Figure 3: diagram to visualize the workflow of a archive and recovery strategy implementation to resolve the problem with the data of high-frequency cryptocurrency trading platforms_\n\n**Archiving Workflow**:\n\n- After each trading cycle (e.g., 3 months), transactional records are processed and moved to cloud-based storage. These records are compressed and encrypted for security and cost optimization.\n- Metadata for these archived transactions is maintained in a lightweight database for quick lookup.\n\n**Recovery Workflow**:\n\n- When data is required, administrators search the metadata for the relevant archive.\n- The archived records are retrieved and reprocessed using a dedicated compute environment to generate the required reports or validate metrics.\n- If needed, the processed data can be restored to a separate database instance for further analysis without affecting the production environment.\n\n## Conclusion\n\nFrom this discussion, we have seen how archive and recovery strategy can address specific challenges such as efficiently handling large volumes of rarely accessed data. Implementing a robust archive and recovery system provides several benefits, including long-term data availability, cost-effective storage, and support for audits or legal requirements. This strategy is particularly valuable for industries like finance, healthcare, and e-commerce, where data integrity and accessibility are critical.\n\nThis knowledge is essential for system architects, database administrators, and developers who manage large-scale applications with growing data needs. Understanding and implementing this strategy equips teams to handle data growth effectively, ensuring their systems remain reliable, secure, and future-ready.\n","title":"Building a data archive and recovery strategy for high-volume trading system","short_title":"","description":"A guide to implementing data archival and recovery strategies for high-volume transactional application.","tags":["data","blockchain","finance"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Fri Dec 13 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/data-archive-and-recovery.md","slugArray":["playground","use-cases","data-archive-and-recovery"]},{"content":"\n## Introduction\n\nDatabase vulnerabilities are a silent threat in trading platforms. They lurk in unrestricted access controls, posing risks of data breaches, operational disruptions, and loss of client trust. This case study examines how we identified these risks and implemented a structured, practical approach to mitigate them. By integrating tools like Teleport, enforcing strict access controls, and embedding detailed logging mechanisms, we significantly enhanced our security posture and operational resilience.\n\n## Problem statement\n\nEvery trading platform depends on its database to handle sensitive operationsâ€”from storing client funds to managing trade records. Yet, our initial access controls had critical gaps:\n\n**Unrestricted access to sensitive data**\n\nDeveloper accounts could access client funding information, exposing the platform to intentional misuse or accidental exposure.\n\n**Data manipulation**\n\nDevelopers with write permissions could inadvertently or maliciously alter critical data, risking financial discrepancies.\n\n**Data loss**\n\nPermissions to execute destructive commands, such as table deletions, left the system vulnerable to catastrophic data loss.\n\n**Lack of auditability**\n\nWithout logging and audit trails, accountability gaps hindered issue resolution and increased operational risks.\n\n> Developer accounts refer to those belonging to engineers and DevOps personnel. These accounts, if compromised, could act as vectors for unauthorized access.\n\n### Operational needs vs. security risks\n\nWhile access should be minimized, it is recognized that developers occasionally need to:\n\n- **Manipulate data** to fulfill client requests (e.g., updating specific records).\n- **Query data** to trace production issues when source code analysis is insufficient.\n\nThese activities must be conducted under strict safeguards to prevent \"oops\" moments, where accidental actions result in catastrophic data loss or manipulation.\n\n### Risk assessment\n\n| **Type**                   | **Impact**                                                                     | **Cause**                                         |\n| -------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------- |\n| **Fund loss**              | Misuse of sensitive funding data for personal gain                             | Unrestricted developer access                     |\n| **Data loss**              | Irreversible deletion of critical data                                         | Developer accounts performing destructive actions |\n| **Information loss**       | Exposure of sensitive client data                                              | Unregulated read access                           |\n| **Operational disruption** | Downtime caused by accidental or malicious actions                             | Developer accounts with write permissions         |\n| **Operational cost**       | Increased expenses for data recovery, incident response, and breach mitigation | Lack of log trails and recovery mechanism         |\n\n## Proposed approach\n\nAddressing these risks required a phased approach. Each step introduced a new layer of security, designed to mitigate specific vulnerabilities.\n\n### Role-based access control\n\nUnrestricted developer access was the root cause of several risks. To address this:\n\n- Enforce least-privilege principles: Developers accessed only the data essential to their roles.\n- Differentiate access levels:\n  - **Read-only access**: For troubleshooting non-sensitive data.\n  - **Write permissions**: Granted only with explicit, time-limited approval.\n- Provide standby databases: Developers used a read-only copy of the production database for debugging.\n\n### Network isolation\n\nOpen access points created opportunities for unauthorized interactions with the database. To minimize exposure:\n\n- Restricted database access to approved endpoints or IP addresses.\n- Mandated VPN usage or secure proxy connections for all database interactions.\n\n### Multi-factor authentication\n\nInsufficient authentication measures left accounts vulnerable to compromise. Implementing MFA added an extra layer of security by requiring developers to verify their identities using multiple factors before accessing the database.\n\n### Data masking\n\nTo further protect sensitive data, even when accessed by authorized personnel, we implemented data masking:\n\n- **Selective masking**: Sensitive data like client Personally Identifiable Information (PII) or financial details were masked or obfuscated.\n- **Granular control**: Masking rules were applied based on user roles and specific data fields.\n- **Dynamic masking**: Data was masked in real-time during queries, ensuring that sensitive information was never exposed in its raw form.\n\n### Database observability and audit logging\n\nLack of visibility into database interactions hindered accountability. To address this, we:\n\n- **Implemented robust logging**: Tracked every database interaction, including queries, data changes, and administrative actions.\n- **Set up alerts**: Suspicious activities, such as bulk deletions or schema modifications, triggered instant notifications.\n- **Made logs tamper-proof**: Ensured secure storage to prevent alterations.\n\n### Break glass access\n\nIn emergencies, developers needed immediate access to resolve critical issues. However, such access carried risks if not carefully managed. We implemented a \"break-glass\" process:\n\n- **Multi-party approval**: Emergency access required sign-offs from multiple stakeholders.\n- **Time-limited access**: Permissions expired automatically after a set duration.\n- **Comprehensive logging**: Every action during emergency access was logged for accountability.\n\n## Technical implementation\n\n### System architecture\n\nWe used [**Teleport**](https://goteleport.com/) as the central platform for managing access controls and monitoring database interactions. The architecture featured:\n\n![](assets/nn-security-architecture.webp)\n\n- **Public network**: Developers authenticated via HTTPS or CLI (tsh) to obtain access certificates.\n\n- **Teleport proxy**: Served as the gateway, enforcing MFA, role-based permissions, and secure connections.\n\n- **Private network**: Hosted the database tier, segregated into read-only and write-only instances, and the logging infrastructure.\n\n- **Event aggregator**: Used Fluentd to process and route logs to tamper-proof storage and notification systems.\n\n- **Notification system**: Alerted administrators to suspicious activities and provided actionable insights.\n\n**Workflow**\n\n1. A developer authenticated via Teleport, receiving a temporary certificate.\n2. The Teleport proxy validated their permissions before granting access to the private network.\n3. Logs of all interactions were processed by the event aggregator and stored securely.\n4. Alerts were sent to the security team for any suspicious activities.\n\n### Masking data\n\nWe hide some sensitive information in our tables to keep data safe. Most of these fields stay hidden forever. However, a few can be accessed with special permissions when needed. Right now, we use [postgresql-anonymizer](https://postgresql-anonymizer.readthedocs.io/en/latest/) for data masking and follow this process:\n\n1. **Identify the table**: Find out which table you need access to.\n2. **Request the tight role**: Use the table name with `unmasked_` as the role name.\n\nFor example, if you need to see hidden fields in the `deposits` table, request the `unmasked_deposits` role.\n\n### Request a new role for extensive access\n\nIf there is a special request for an action beyond the permissions of the existing role, the requester must follow this protocol to perform the action:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Approval as Teleport\n    participant Database\n    participant Audit as Audit logs\n\n    User->>Approval: Submit emergency access request\n    activate Approval\n    Note over User,Approval: Includes: reason, duration, resources\n\n    loop Approval Process\n        Approval->>Approval: Notify approvers\n        Approval->>Approvers: Wait for the request to be approved\n    end\n\n    alt Request Approved\n        Approval->>Database: Grant temporary permission\n        activate Database\n        Database->>Approval: Acknowledge credential issuance\n        deactivate Approval\n\n        Approval->>User: Notify approval with the new permission grant\n\n        User->>Database: Connect with new permission\n        activate Database\n        Database->>Audit: Log connection attempt\n\n        Note over Database,Audit: Auto-cleanup after X hours\n\n        loop During Access Period\n            User->>Database: Execute queries\n            Database->>Audit: Log operations\n        end\n\n        alt Time Limit Reached\n            Database->>Database: Terminate session\n            Database->>Audit: Log session end\n        else Manual End\n            User->>Database: End session\n            Database->>Audit: Log manual end\n        end\n        deactivate Database\n\n    else Request Denied\n        Approval->>User: Notify rejection\n        Approval->>Audit: Log rejected request\n    end\n```\n\n**Workflow summary:**\n\n1. Developer initiates the request.\n2. Approvers evaluate and approve the request via Teleport.\n3. Developer performs required actions with temporary permissions.\n4. All activities are logged, and permissions are automatically revoked after expiration.\n\n## Results and benefits\n\nThe implementation delivered measurable benefits:\n\n- **Enhanced security**: Reduced risks of unauthorized access, data breaches, and misuse.\n- **Improved data integrity**: Maintained through RBAC and robust logging.\n- **Operational efficiency**: Developers performed essential tasks without compromising security.\n- **Accountability and traceability**: Comprehensive logs enabled rapid issue resolution.\n- **Increased client trust**: Demonstrated commitment to safeguarding sensitive data.\n\n## Conclusion\n\nThis case study highlights how robust access control measures can transform database security in a trading platform. By layering tools like Teleport, enforcing RBAC, and integrating detailed observability, we not only mitigated immediate risks but also established a secure foundation for future growth. These measures underscore the importance of proactive security in maintaining operational resilience and client trust.\n","title":"Database hardening for a trading platform","short_title":"","description":"Discover how a trading platform mitigated database access risks, enhanced security, and ensured data integrity through role-based access control, network isolation, MFA, and robust logging. Learn about the strategies and tools, like Teleport, that transformed operational efficiency and reinforced client trust.","tags":["blockchain","fintech","database","security"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh"],"date":"2025-01-02","filePath":"playground/use-cases/database-hardening-for-trading-platform.md","slugArray":["playground","use-cases","database-hardening-for-trading-platform"]},{"content":"\n## What is a logger, and why does it matter?\n\nA **logger** is a fundamental component of modern software systems, designed to record system events, user actions, and issues in real-time. Itâ€™s like the memory of an application, enabling both users and administrators to trace activities. Loggers serve two main purposes:\n\n> - **For users**: They provide notifications or updates about events like successful transfers, errors, or system changes.\n> - **For developers and support teams**: They offer a detailed record of system behavior, aiding in debugging and monitoring.\n\nWithout a logger, understanding the flow of actions or diagnosing issues would be like navigating a dark room without a flashlight.\n\n## What makes an effective logger?\n\nAn effective logger goes beyond simply storing data. It organizes and presents information in a way thatâ€™s **useful and easy to understand**. To be effective, a logger must have the following qualities:\n\n### Consistency in format\n\nLogs should maintain a uniform structure across the application, much like a well-organized manual where every chapter follows the same layout. This consistency makes it easier to identify patterns and quickly interpret information.\n\n### Clarity and self-documentation\n\nLogs should be _self-explanatory_, requiring little to no additional context to understand their meaning. For instance, a good log entry is like a well-written headline: concise, clear, and informative.\n\n### Purposefulness and informativeness\n\nEvery log entry should serve a purpose. For example, instead of simply stating, \"Transfer completed,\" a log should provide actionable insights, such as the accounts involved, the amount transferred, and timestamps.\n\n## Context: transfer logs in cryptocurrency applications\n\nImagine a **cryptocurrency trading application** that enables users to manage multiple accounts on one platform. One of its core features is handling **transfers**, which can be categorized as follows:\n\n- _Deposits_: Funds added to an account from an external source.\n- _Withdrawals_: Funds removed from an account to an external destination.\n- _Internal transfers_: Movement of funds between two accounts belonging to the same user.\n\nHereâ€™s an example scenario: A user transfers **$1,000 USDT** from their main account (**Account_A**) to their savings account (**Account_B**). The system generates two records in the database:\n\n1. A **withdrawal record** for Account_A.\n2. A **deposit record** for Account_B.\n\nHowever, the current logs fail to establish a clear relationship between these records. Consider the following example:\n\n```\nAccount_A | -1000 USDT | 2024-01-01 10:00:00\nAccount_B | +1000 USDT | 2024-01-01 10:00:01\n-- Are these movements related? No way to tell!\n```\n\nFrom this log, users cannot deduce that the two entries are part of the same transfer. This ambiguity can cause confusion, especially in financial applications where clarity and transparency are paramount.\n\n## Why is this problematic?\n\nThe lack of clear relationships between log entries creates the following issues:\n\n1. **User confusion**: Without context, users may struggle to understand the flow of their funds.\n2. **Reduced trust**: Ambiguous logs can erode user confidence, especially in financial systems.\n3. **Limited debugging capability**: Developers and support teams cannot efficiently diagnose issues or trace transactions without meaningful, connected data.\n\n## Why does this happen? A look at the current system\n\nThe existing system focuses on individual transactions, treating withdrawals and deposits as **isolated events**. The process is outlined below:\n\n```mermaid\nflowchart LR\n    subgraph Input\n        FI[Future Incomes]\n        ACBS[Balance Snapshots]\n    end\n\n    subgraph Processing\n        TD[Transfer Data]\n        BB[Before Balance]\n        AB[After Balance]\n        CT[Cumulative Totals]\n    end\n\n    subgraph Output\n        FR[Final Record]\n    end\n\n    %% Data collection\n    FI --> TD\n\n    %% Balance processing\n    ACBS --> BB\n    ACBS --> AB\n\n    %% Final calculations\n    BB --> FR\n    TD --> CT\n\n    %% Result compilation\n    CT --> FR\n    AB --> FR\n    TD --> FR\n```\n\nThis method records events but fails to link related transactions. For example, a withdrawal from one account and a deposit into another might appear as two separate, unrelated logs.\n\n## A solution: enhanced logging system\n\nTo resolve these limitations, we propose an **enhanced logging system** that links related transactions and provides a clear view of asset movement. The process is illustrated below:\n\n```mermaid\nflowchart TD\n    subgraph Input[\"Data Sources\"]\n        FI[Future Incomes]\n        ACBS[Balance Snapshots]\n    end\n\n    subgraph Processing[\"Enhanced Processing\"]\n        RT[\"Transfers Pairing<br/>(LEAD/LAG Analysis)\"]\n        TWT[\"Type Detection<br/>(Internal/External Classification)\"]\n        TWG[\"Transfer Grouping<br/>(Time-Based Clustering)\"]\n        GFB[\"Find Initial Balance<br/>(Starting States)\"]\n        TWB[\"Calculate Balances<br/>(Running Totals)\"]\n    end\n\n    subgraph Output[\"Enhanced Output\"]\n        FR[\"Final Record:<br/>- Paired Transfers<br/>- Balance Changes<br/>- Transfer Types<br/>- Time Relationships\"]\n    end\n\n    %% Data flow\n    FI --> RT\n    RT --> TWT\n    TWT --> TWG\n    TWG --> GFB\n    ACBS --> GFB\n    GFB --> TWB\n    TWB --> FR\n```\n\n## Key steps in the enhanced system\n\n1. **Data sources (input)** 2. **Future incomes**: Primary source of transfer records containing the raw transaction data including amounts, timestamps, and account IDs 3. **Balance snapshots**: Historical balance records at specific timestamps\n\n2. **Transfers pairing**\n\n```sql\nWITH ranked_transfers AS (\n    SELECT\n        ...,\n        LEAD(...) OVER (ORDER BY fi.time) AS next_...,\n        LAG(...) OVER (ORDER BY fi.time) AS prev_...,\n```\n\n- Uses SQL window functions (LEAD/LAG) to look at adjacent transfers\n- Connects incoming and outgoing transfers by analyzing their temporal relationships\n- Creates pairs of transactions by matching withdrawals with corresponding deposits\n- Examines transactions within a period window to identify related transfers\n\n3. **Type detection**\n\n```sql\nCASE\n    WHEN amount < 0\n        AND next_amount > 0\n        AND (amount + next_amount = 0)\n        AND (next_time - time < interval '5 seconds')\n    THEN 'INTERNAL_TRANSFER'\n```\n\n- Classifies transfers into three categories:\n  - Internal transfers (between accounts)\n  - External transfers out (withdrawals)\n  - External transfers in (deposits)\n- Uses some simple time-based matching logic combining amount matching and timing\n- Considers both previous and next transactions to ensure accurate classification\n\n4. **Transfer grouping**\n\n```sql\nSUM(CASE\n    WHEN sender_time_gap > interval '20 seconds' THEN 1\n    ELSE 0\nEND) OVER (\n    PARTITION BY from_account\n    ORDER BY time\n) AS sender_group\n```\n\n- Groups related transfers using a period window\n- Maintains separate groups for sender and receiver accounts\n- Helps handle high-frequency trading scenarios\n- Ensures accurate balance tracking during concurrent transfers\n\n5. **Find first balance of group**\n\n```sql\nFIRST_VALUE(\n    COALESCE(\n        (SELECT current_balance\n         FROM account_current_balance_snapshots bs\n         WHERE bs.account_id = from_account\n             AND bs.created_at <= time\n         ORDER BY bs.created_at DESC\n         LIMIT 1),\n        0\n    )\n) OVER (...)\n```\n\n- Identifies the starting balance for each transfer group\n- Uses the most recent balance snapshot before the transfer\n- Handles both sender and receiver balances independently\n- Ensures accurate balance baseline for calculations\n\n6. **Calculate balance for each transfer**\n\n```sql\nGREATEST(0, (\n    sender_initial_balance +\n    SUM(signed_amount) OVER (\n        PARTITION BY from_account, sender_group\n        ORDER BY time\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    )\n))\n```\n\n- Computes running balances for both sender and receiver\n- Maintains non-negative balance constraints\n- Calculates both before and after states for each transfer\n- Uses window functions for running totals within groups\n\n## Benefits of the enhanced logger\n\n1. **Enhanced clarity** 2. Logs clearly link related transactions. 3. Users see the complete flow of funds, from source to destination.\n\n2. **Improved accuracy** 2. Tracks balances with precision, even during concurrent transfers. 3. Uses historical snapshots to ensure reliable calculations.\n\n3. **Better user experience** 2. Provides actionable insights in a user-friendly format. 3. Differentiates between internal and external transactions.\n\n---\n\n### **Conclusion**\n\nThe proposed enhancements transform the logging system from disconnected entries into a **comprehensive, user-friendly narrative**. By applying advanced processing techniques, the enhanced system offers:\n\n- **Consistency**: Uniform formatting for all logs.\n- **Clarity**: Clear relationships between transactions.\n- **Purposefulness**: Meaningful, actionable data for users and developers.\n\nThis system not only addresses current logging limitations but also sets a solid foundation for future improvements in transaction tracking and user notifications.\n","title":"Transfer mapping: enhancing loggers for better transparency","short_title":"","description":"A comprehensive guide on improving cryptocurrency transfer logging systems to provide better transparency and traceability for users and developers.","tags":["data","blockchain","fintech"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Mon Nov 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/enhancing-cryptocurrency-transfer-logger.md","slugArray":["playground","use-cases","enhancing-cryptocurrency-transfer-logger"]},{"content":"\nAs Binance doesn't allow Master Account see MSA account Future PNL Analysis, so we decide to clone Binance Future PNL Analysis page with Phoenix Live View to show all Account Future PNL\n\n## Why we use Phoenix Live View for Binance Future PNL Analysis page\n\n### Real-Time Data Handling\n\n- Phoenix Live View has built-in Websocket management so we can update data realtime with price or position update\n- Efficient handling of continuous data streams from Binance\n- Automatic connection management and recovery\n\n### Server-Side State Management\n\n- Keeps sensitive trading data secure on the server\n- Ensures calculation accuracy for PNL computations\n- Prevents client-side manipulation of important data\n\n### Complex Calculations\n\n- Handles all PnL calculations server-side\n- Better precision for financial calculations\n- Centralized calculation logic\n\n### Development Efficiency\n\n- Single technology stack (Elixir)\n- No need for separate frontend framework\n- Simplified state management\n\n## How to optimize query with timescale\n\n### Data Source\n\nBase on [Binance Docs](https://www.binance.com/en/support/faq/how-are-pnl-calculated-on-binance-futures-and-options-pnl-analysis-dbb171c4db1e4626863ec8bc545be46a) we have compound data from 2 timescale tables: `ts_user_trades` and `ts_future_incomes`\n\n- ts_user_trades: to calculate realized pnl, commission, and trading volume\n- ts_future_incomes: to calculate funding fee and net inflow\n\n### Timescale table\n\n- ts_user_trades and ts_future_incomes are large data tables so if we use normal table with indexing it will be slower by time that why we use timescale to hyper chunks to prevent this issue\n\n- ts_user_trades is hyper by 1 day\n- ts_future_incomes is hyper by 30 days\n\n### Use timescale style query to get summary data in date range\n\nEcto query to calculate PnL data from `ts_user_trades` and `ts_future_incomes`\n\n```elixir\nfrom(t in TsUserTrades,\n  where: t.account_id in ^account_ids,\n  where: t.time >= ^start_time and t.time <= ^end_time,\n  group_by: [\n    t.account_id,\n    fragment(\"time_bucket('1 day', ?)::date\", t.time)\n  ],\n  select: %{\n    account_id: t.account_id,\n    date: fragment(\"time_bucket('1 day', ?)::date\", t.time),\n    commission: fragment(\"COALESCE(-1 * ABS(SUM(?)), 0)\", t.commission),\n    realized_pnl: coalesce(sum(t.realized_pnl), 0),\n    trade_volume:\n      fragment(\n        \"COALESCE(SUM(CASE WHEN ? IS NOT NULL THEN ? ELSE 0 END), 0)\",\n        t.quote_qty,\n        t.quote_qty\n      )\n  }\n)\n\nfrom(i in TsFutureIncomes,\n  where: i.account_id in ^account_ids,\n  where: i.time >= ^start_time and i.time <= ^end_time,\n  group_by: [\n    i.account_id,\n    fragment(\"time_bucket('1 day', ?)::date\", i.time)\n  ],\n  select: %{\n    account_id: i.account_id,\n    date: fragment(\"time_bucket('1 day', ?)::date\", i.time),\n    net_inflow:\n      sum(fragment(\"CASE WHEN ? = 'TRANSFER' THEN ? ELSE 0 END\", i.income_type, i.income)),\n    received_funding_fee:\n      sum(\n        fragment(\n          \"CASE WHEN ? = 'FUNDING_FEE' AND ? > 0 THEN ? ELSE 0 END\",\n          i.income_type,\n          i.income,\n          i.income\n        )\n      ),\n    paid_funding_fee:\n      sum(\n        fragment(\n          \"CASE WHEN ? = 'FUNDING_FEE' AND ? < 0 THEN ? ELSE 0 END\",\n          i.income_type,\n          i.income,\n          i.income\n        )\n      ),\n    insurance_clear:\n      sum(\n        fragment(\n          \"CASE WHEN ? = 'INSURANCE_CLEAR' THEN ? ELSE 0 END\",\n          i.income_type,\n          i.income\n        )\n      )\n  }\n)\n```\n\nBecause timescale table will be spitted into multiple chunks so if we use normal query it will have timeout issue if range too long. So we have to use `time_bucket` to let it join multiple chunks.\n\n### Create cronjob to fill ts_account_pnl_analysis\n\n- We implement account_pnl_analysis_cronjob to backfill from today until oldest date has ts_user_trades order ts_future_incomes to calculate daily pnl\n\n- For current day, we will run interval 1 hour to update data\n\n- With this cronjob it will help us no need to recalculate from 2 tables every request so it will be fast and don't make database pressure\n\n## User interface implementation\n\n![Overview](assets/analysis-page/overview.jpg) _Figure 1: Future PNL Analysis Overview Tab_\n\n![Detail](assets/analysis-page/detail.png) _Figure 2: Future PNL Detail Tab_\n\n![Symbol Analysis](assets/analysis-page/symbol-analysis.png) _Figure 3: Future PNL Analysis Symbol Tab_\n\n![Symbol Analysis](assets/analysis-page/funding-and-transaction.png) _Figure 4: Future PNL Funding and Transaction Tab_\n","title":"Implement Binance Futures PNL analysis page by Phoenix LiveView","short_title":"","description":"Implementing Binance Futures PNL Analysis page with Phoenix LiveView to optimize development efficiency. This approach reduces the need for separate frontend and backend resources while enabling faster real-time data updates through WebSocket connections and server-side rendering.","tags":["blockchain","fintech","real-time","phoenix-live-view"],"pinned":false,"draft":false,"hiring":false,"authors":["minhth"],"date":"Wed Jan 15 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/implement-binance-future-pnl-analysis-page.md","slugArray":["playground","use-cases","implement-binance-future-pnl-analysis-page"]},{"content":"\nDue to increasing trading volume, the `user_trades` and `incomes` tables have grown significantly, causing slower performance in our reporting queries. To address this, we propose migrating these tables to TimescaleDB, which will allow us to partition the data into time-based chunks. This partitioning strategy should optimize our report query performance.\n\n## We create new timescale tables\n\nBecause current tables have large data so we can't migrate it in one SQL Query so that why we have create new table with new timescale structure and this is new table structures\n\n### New ts_user_trades\n\n```sql\nCREATE TABLE \"public\".\"ts_user_trades\" (\n    \"id\" uuid NOT NULL DEFAULT uuid_generate_v4(),\n    \"buyer\" bool,\n    \"commission\" numeric,\n    \"commission_asset\" text,\n    \"trade_id\" int8 NOT NULL,\n    \"maker\" bool,\n    \"order_id\" int8,\n    \"price\" numeric,\n    \"qty\" numeric,\n    \"quote_qty\" numeric,\n    \"realized_pnl\" numeric,\n    \"side\" text,\n    \"position_side\" text,\n    \"symbol\" text,\n    \"time\" timestamptz NOT NULL,\n    \"time_unix\" int8,\n    \"account_id\" uuid,\n    \"is_locked_position\" bool DEFAULT false,\n    \"created_at\" timestamptz NOT NULL DEFAULT now(),\n    \"updated_at\" timestamptz NOT NULL DEFAULT now(),\n    CONSTRAINT \"ts_user_trades_account_id_fkey\" FOREIGN KEY (\"account_id\") REFERENCES \"public\".\"accounts\"(\"id\"),\n    PRIMARY KEY (\"id\",\"time\")\n);\n\n-- Indices\nCREATE UNIQUE INDEX ts_user_trades_account_id_symbol_trade_id_time_index ON public.ts_user_trades USING btree (account_id, symbol, trade_id, \"time\");\n\nCREATE INDEX ts_user_trades_account_id_time_index ON public.ts_user_trades USING btree (account_id, \"time\");\n\nCREATE INDEX ts_user_trades_account_id_symbol_time_index ON public.ts_user_trades USING btree (account_id, symbol, \"time\");\n\nSELECT create_hypertable('ts_user_trades', 'time',\n  chunk_time_interval => INTERVAL '1 day',\n  if_not_exists => TRUE\n);\n```\n\n#### ts_user_trades indices explain\n\n**1. Unique index**\n\n```SQL\nCREATE UNIQUE INDEX ts_user_trades_account_id_symbol_trade_id_time_index ON public.ts_user_trades USING btree (account_id, symbol, trade_id, \"time\");\n```\n\nThree columns `account_id`, `symbol`, `trade_id` can detect duplicate data but we need to add time to unique index for hypertable also\n\n**2. Index `account_id`, `time`**\n\n```sql\nCREATE INDEX ts_user_trades_account_id_time_index ON public.ts_user_trades USING btree (account_id, \"time\");\n```\n\nThis index for optimize query `WHERE account_id = {id} AND time BETWEEN {time1} and {time2}`\n\n**3. Index `account_id`, `time`, `symbol`**\n\n```sql\nCREATE INDEX ts_user_trades_account_id_symbol_time_index ON public.ts_user_trades USING btree (account_id, symbol, \"time\");\n```\n\nThis index for optimize query `WHERE account_id = {id} AND symbol in {symbols} AND time BETWEEN {time1} and {time2}`\n\n**4. Hypertable**\n\n```sql\nSELECT create_hypertable('ts_user_trades', 'time',\n  chunk_time_interval => INTERVAL '1 day',\n  if_not_exists => TRUE\n);\n```\n\nUser trading activity is sporadic, but when trades occur, they tend to cluster into periods of high volume within the same minute.\n\nWe choose interval 1 day to balance chunk number and size per chunk to make sure each chunk less than 300MB for optimized query\n\n### New ts_future_incomes\n\n```sql\nCREATE TABLE \"public\".\"ts_future_incomes\" (\n    \"id\" uuid NOT NULL DEFAULT uuid_generate_v4(),\n    \"account_id\" uuid NOT NULL,\n    \"symbol\" text,\n    \"income_type\" text NOT NULL,\n    \"income\" numeric,\n    \"asset\" text,\n    \"info\" text,\n    \"time\" timestamptz,\n    \"time_unix\" int8 NOT NULL,\n    \"trade_id\" text,\n    \"tran_id\" text,\n    \"is_locked_position\" bool DEFAULT false,\n    \"is_notify\" bool DEFAULT false,\n    \"created_at\" timestamptz NOT NULL DEFAULT now(),\n    \"updated_at\" timestamptz NOT NULL DEFAULT now(),\n    CONSTRAINT \"ts_future_incomes_account_id_fkey\" FOREIGN KEY (\"account_id\") REFERENCES \"public\".\"accounts\"(\"id\"),\n    PRIMARY KEY (\"id\",\"account_id\",\"income_type\",\"time_unix\")\n);\n\n-- Indices\nCREATE UNIQUE INDEX ts_future_incomes_account_id_income_type_tran_id_time_unix_inde ON public.ts_future_incomes USING btree (account_id, income_type, tran_id, time_unix);\n\n-- This index for optimize query WHERE account_id = {id} AND income_type = {income_type} AND time BETWEEN {time1} and {time2}\nCREATE INDEX ts_future_incomes_account_id_income_type_time_index ON public.ts_future_incomes USING btree (account_id, income_type, \"time\");\n\n-- This index for optimize query WHERE account_id = {id} AND income_type = {income_type} AND time BETWEEN {time1} and {time2} and symbol = {symbol}\nCREATE INDEX ts_future_incomes_account_id_symbol_income_type_time_index ON public.ts_future_incomes USING btree (account_id, symbol, income_type, \"time\");\n\nSELECT create_hypertable(\n  'ts_future_incomes',\n  'time_unix',\n  chunk_time_interval => 604800000, -- 7 days in milliseconds\n  create_default_indexes => false\n)\n```\n\n#### ts_user_trades indices explain\n\n**1. Unique index**\n\n```SQL\nCREATE UNIQUE INDEX ts_future_incomes_account_id_income_type_tran_id_time_unix_inde ON public.ts_future_incomes USING btree (account_id, income_type, tran_id, time_unix);\n```\n\nAvoid duplicate data\n\n**2. Index `account_id`, `income_type`, and `time`**\n\n```sql\nCREATE INDEX ts_future_incomes_account_id_income_type_time_index ON public.ts_future_incomes USING btree (account_id, income_type, \"time\");\n```\n\nThis index for optimize query `WHERE account_id = {id} AND income_type = {income_type} AND time BETWEEN {time1} and {time2}`\n\n**3. Index `account_id`, `symbol`, `income_type`, and `time`**\n\n```sql\nCREATE INDEX ts_future_incomes_account_id_symbol_income_type_time_index ON public.ts_future_incomes USING btree (account_id, symbol, income_type, \"time\");\n```\n\nThis index for optimize query `WHERE account_id = {id} AND symbol in {symbols} AND income_type = {income_type} AND time BETWEEN {time1} and {time2} AND `\n\n**4. Hypertable**\n\n```sql\nSELECT create_hypertable(\n  'ts_future_incomes',\n  'time_unix',\n  chunk_time_interval => 604800000, -- 7 days in milliseconds\n  create_default_indexes => false\n)\n```\n\nWe store two types of transactions in ts_future_incomes: FUNDING_FEE and TRANSFER.\n\nFor FUNDING_FEE transactions:\n\n- Each symbol generates 4-8 fees per account per day\n- With 320 positions per account across 200 accounts, this results in:\n  - Daily records: 320 Ã— 200 Ã— 8 = 512,000\n  - Weekly chunk size: 512,000 Ã— 7 = 3,584,000\n\nThe resulting chunk size is within acceptable limits.\n\n## Migration plan\n\n### Dual Write\n\nTo make sure new tables have new data same as old tables we insert both of tables to make sure we don't lost new data and can rollback to old table if we have problem\n\n### Backfilling\n\n#### Create migrator to import data from old table from new timescale table\n\n**How migrator work**\n\n1. Compare total record between old and new\n2. If not equal we will get min time from new ts_user_trades to continue backfill\n3. Get data from old table with query `WHERE time < {min_time} ORDER BY time DESC, trade_id DESC OFFSET {offset} LIMIT 1000`\n4. Increase min_time if latest record oldest than min_time 1 hours to reset offset back to 0 (The offset is bigger, the query time is longer)\n5. Backfill until no data in query\n\n```mermaid\nflowchart TD\n    A[Start] --> B[Compare total records<br>between old and new tables]\n    B --> C{Records equal?}\n    C -->|Yes| D[End]\n    C -->|No| E[Get min timestamp from<br>new ts_user_trades]\n    E --> F[Query old table with:<br>WHERE time < min_time<br>ORDER BY time DESC, trade_id DESC<br>OFFSET offset LIMIT 1000]\n    F --> G[Process batch]\n    G --> H{Latest record<br>> min_time - 1hr?}\n    H -->|Yes| I[Reset offset to 0<br>Update min_time to latest record time]\n    H -->|No| J[Increase offset by 1000]\n    I --> K{Query returned<br>data?}\n    J --> K\n    K -->|Yes| F\n    K -->|No| D\n```\n\n### Validate data\n\nWe need to replace new query to old query function by function to retest to make sure correct data and acceptable query time\n\n### Change primary table to new table\n\nAfter everything is work fine, we can replace primary table to new table then consider to remove old tables if needed to save data storage\n","title":"Migrate regular tables into TimescaleDB hypertables to improve query performance","short_title":"","description":"How do we migrate normal table to timescale table to optimized data storage","tags":["data","blockchain","fintech","timescaledb"],"pinned":false,"draft":false,"hiring":false,"authors":["minhth"],"date":"Wed Jan 15 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/migrate-normal-table-to-timescale-table.md","slugArray":["playground","use-cases","migrate-normal-table-to-timescale-table"]},{"content":"\nOur development team recently optimized the frontend performance of a trading platform designed for Binance traders. A key performance bottleneck was the long initial load time, which worsened as users managed more accounts. This sluggish start directly impacted the platform's responsiveness, unacceptable for real-time trading. This report outlines our solutions to this primary problem of lengthy initial load times, resulting in a much faster and more dependable user experience. Solving this required overcoming complex network and browser-side rendering limitations. Ultimately, we achieved a dramatic reduction in load times: initial content now appears in under a second, and full platform usability is reached in approximately 1.5 seconds, a significant improvement from the previous 2.5-3 seconds. The following sections explain our approach and its importance.\n\n## Why speed matters\n\nThis platform serves serious traders demanding high precision. They often handle many accounts, sometimes 50 or more, to swiftly place large orders and capitalize on market shifts. Importantly, the platform performs real-time calculations like balances and price updates directly in the user's browser, offering great power. However, this frontend focus means users need to frequently refresh the page to ensure all information is completely up-to-date, unintentionally worsening load times when managing numerous accounts and large datasets. Slow loading wasn't just an inconvenience; it became a major obstacle. Traders must react instantly to market changes, and our data revealed user frustration escalating with each loading delay. Given our dedicated user base, every delay chipped away at satisfaction and threatened platform use. This requires us to confront two key performance challenges: **slow network connections** and the **browser's rendering workload**.\n\n### Slow network connections: Too many requests\n\nGetting data from Binance was the first bottleneck. For each account, we were making several requests to Binance â€“ one to get account details (`/account`) and another to set up real-time updates (`/listenKey`). Each of these took some time, around 100â€“300 milliseconds. When a user had 50 accounts, this meant hundreds of requests. Web browsers can only send a few requests at the same time to one website. This meant most requests had to wait, adding up to long delays\n\n| **Endpoint** | **Purpose**              | **Response Time** | **Problem with many accounts?** |\n| ------------ | ------------------------ | ----------------- | ------------------------------- |\n| `/account`   | Get account info         | 100â€“300ms         | Yes                             |\n| `/listenKey` | Set up real-time updates | 50â€“100ms          | Yes                             |\n\n![](assets/nn-init-load-many-requests.webp)\n\n_A waterfall of requests to fetch account infos and listen keys_\n\n### Slow rendering workload: Rendering struggles\n\nOnce the data arrived, the browser had a lot to do. It had to process large amounts of code, apply styles to make the platform look good, and show information for 50 accounts. This made the browser take a long time to display everything. We were using Web Workers to handle some data processing in the background to try and keep the platform responsive. But there was a problem: it took 500 milliseconds for these background workers to even start.\n\nThis delay meant that even with our code optimizations, the initial display was still taking over 1 second â€“ longer than we wanted. We tried to reduce the amount of code and styles, which helped a little, but the main tasks for the browser â€“ processing code, running it, and displaying things â€“ still took a significant amount of time. We figured we could only save about 0.5 seconds this way, so fixing the network delays was more important.\n\nThe following diagram shows the sequence of tasks a browser must complete before displaying a full webpage:\n\n![](assets/nn-init-load-old-flow.webp)\n\n## How we solved the problem\n\nOur first approach was simple: get data, process it in the background, and show it on the screen. But the delay in starting the background processing showed us that just doing processing in the background wasn't enough for the initial load. To get the initial display under 1 second, we needed to drastically reduce network delays and change how we handled data from the start. Using cache became key, but we also needed to make sure traders still got up-to-date information. Hereâ€™s what we did.\n\n### Enhance backend API speed\n\nWe moved some of the work from the user's web browser to our backend system by improving our API:\n\n- **Caching data:** Account information is now saved in a backend cache. This means we don't have to ask Binance for the same data every time, which reduces external requests. The cache is updated smartly to ensure traders see almost real-time data without always fetching everything again. However, we know that in very active markets, data changes quickly, and the cached data might become slightly out of date compared to the live data on Binance.\n- **Request batching:** Instead of making 50+ separate requests for account data, we now make just one request to get all account data at once. This greatly reduces the number of round-trips and avoids the browser's request limits.\n- **Data compression:** We used Gzip to compress the data we send, making it smaller and faster to transfer without losing any information.\n- **Combined WebSocket setup:** We included the WebSocket setup information (`listenKey`) in the initial data response. This removed the need for a separate request, making setup faster.\n\nThese changes turned many network requests into a single, efficient process, making data quickly available for the platform to use.\n\n### Faster WebSocket initialization\n\nReal-time updates are essential, and delays in setting up these updates were not acceptable. By including the `listenKey` in the batched response, the real-time connections now start immediately. Traders get live data as soon as the platform loads â€“ which is very important. Even though we are using cached data initially, these WebSocket updates quickly bring in the very latest information.\n\n### Caching processed data\n\nCaching the raw data from Binance helped, but we still had to wait for the background workers to process it. Our key insight was to also cache the _processed_ data â€“ the data that is already prepared to be displayed. When the page loads, the platform quickly grabs this pre-processed data, completely skipping the background worker startup time for the initial display. While the very first view might show slightly older data, this is quickly updated with real-time WebSocket updates, so traders get fresh data very quickly. Because the market can change fast, especially in peak times, and there might be a slight delay between our backend cache and Binance's servers, we still need to re-verify the account data. To do this, after the initial data from the socket arrives and is displayed, we make a quick, non-blocking call to the `/account` API to double-check and update the data if needed. This ensures the data is as accurate as possible without slowing down the initial loading of the platform.\n\nHereâ€™s the new data flow:\n\n1. **Backend:** Gets account data in batches, caches it, processes key information, and saves the results.\n2. **Frontend:** Loads the cached, processed data instantly and displays the platform.\n3. **WebSocket:** Streams real-time updates to keep the displayed data in sync.\n4. **Revalidation:** After the initial load and socket data display, a non-blocking call to `/account` is made to revalidate data.\n\n![](assets/nn-load-init-change-flow.webp)\n\n## What we achieved\n\nThe impact was clear right away. Testing with 50 accounts, we achieved an initial display in under 1 second and a fully usable platform in about 1.5 seconds. This is much faster than the previous 2.5â€“4 seconds â€“ a significant improvement:\n\n![](assets/nn-init-load-comparison.gif)\n\nIn the old version, loading a full view sometimes took almost 4 seconds:\n\n![](assets/nn-init-load-before.gif)\n\nAfter the update, it takes less than 1 second:\n\n![](assets/nn-init-load-after.gif)\n\n## **Conclusion**\n\nIn the trading world, platform speed is essential. By directly addressing slow network connections and browser display issues, we transformed our platform's frontend from a problem into a strength. Caching, batching requests, and optimizing real-time updates were not complicated solutions, but they were effective. This shows that practical engineering solutions are often more valuable than complex, theoretical approaches.\n","title":"Optimizing initial load time for a Trading Platform","short_title":"","description":"Discover the technical strategies behind optimizing a Binance trading platform, reducing initial load times to under 1 second for enhanced trader productivity.","tags":["fintech","performance"],"pinned":false,"draft":false,"hiring":false,"authors":["thanh"],"date":"2025-03-12","filePath":"playground/use-cases/optimize-init-load-time-for-trading-platform.md","slugArray":["playground","use-cases","optimize-init-load-time-for-trading-platform"]},{"content":"\nDesigning the UI for a blockchain-based hedge fund platform like [**Hedge Foundation**](https://www.hedge.foundation/) is not just about creating a visually appealing interface. The real challenge lies in ensuring that complex financial data is **presented in an intuitive, understandable way that helps users make quick investment decisions**. Optimizing the UI plays a crucial role in allowing investors to access accurate data without feeling overwhelmed by excessive information.\n\n## UI design principles for data visualization\n\nBefore starting the design, we focused on understanding who our users are and what problems they face. We divided our users into three main groups:\n\n- **Individual investors:** Need simple tools to protect their assets.\n- **Institutional investors:** Need the best hedging strategies for large portfolios.\n- **Professional traders:** Want to use hedging tools to maximize profits.\n\nWhen looking at other hedge fund platforms, we noticed many had overly complex interfaces that made it hard for new users. For example, some platforms required too many steps to make a simple trade, causing users to leave. Also, many platforms didn't clearly show important information like BTC Dominance, Market Trends, and Volatility Index, forcing users to look for data from different sources. Based on these findings, we decided to make trading simpler and add easy-to-read charts right on the main screen to help investors make faster decisions.\n\nTo make this work well, we focused on these key principles:\n\n1. **Highlight key data ([Information Hierarchy](https://www.bridgewaterlearning.co.za/2013/04/16/design-principles-hierarchy-of-information/))**: We use dynamic charts and interactive dashboards to help users quickly grasp market trends.\n2. **Minimize information overload ([Cognitive Load Reduction](https://www.nngroup.com/articles/minimize-cognitive-load/))**: We keep the layout simple and only show necessary information to avoid overwhelming users.\n3. **Enhance data recognition ([Recognition Over Recall](https://www.nngroup.com/articles/recognition-and-recall/))**: Important indicators like BTC Dominance, Market Trends, and Volatility Price are always visible on the main screen, so investors don't have to search through multiple pages.\n\n## Three key UI design for Hedge Foundation\n\nTo improve data visualization and usability, we have implemented the following key UI design considerations:\n\n### 1. Integrate an interactive dashboard with dynamic charts\n\n**Problem**: Investors found it difficult to track market movements because key data was fragmented across multiple sections. This forced them to navigate between several pages, increasing the time required to make informed decisions. The lack of a centralized view also made it harder to identify trends and compare portfolio performance efficiently.\n\n**Solution**: We designed a centralized dashboard that consolidates all critical data on a single screen, allowing users to get an overview without navigating between multiple pages.\n\n**Charts Used**: Trend analysis charts are essential for helping individual and institutional investors monitor asset performance and assess risk management strategies. Key implementations include:\n\n- For **long-term patterns**, we use **Trend Lines** to show good times to buy or sell.\n- For **investment performance**, the **Portfolio Return** chart shows how much money you're making over time.\n- To check **risk levels**, the **Volatility Chart** and **Drawdown Analysis** help investors understand how risky their investments are.\n- To see **where money is moving**, the **Liquidity Flow** and **Heatmap Chart** show which areas have the most trading activity.\n\n![](assets/hedge-foundation-charts.png)\n\nðŸ’¡ Tip: We used [i Charts Generate](https://www.figma.com/community/plugin/1370606842652257742/i-charts-generate-line-chart-bar-chart-pie-cahrt-radar-chart-scatter-radial) in Figma to create charts quickly and easily, saving design time and keeping charts consistent throughout the project.\n\n### 2. Enhance readability through color & layout hierarchy\n\n**Problem**: Too much information on the screen can overwhelm users, causing difficulty in focusing on key insights. Without a structured layout, investors struggle to accurately compare values. Tracking performance metrics and identifying market trends also becomes more challenging.\n\n**Solution**:\n\n- Use **colors meaningfully** to help investors understand quickly:\n    - Green shows strong growth\n    - Blue shows slight growth\n    - Yellow shows steady markets\n    - Red shows decline\n    - Gray shows stable or predicted numbers\n    - Purple shows when a stock price hits its upper limit\n\nOn price charts, we use different shades of red to show how big a drop is.\n\n- Apply appropriate font sizes and contrast levels to **highlight** essential information:\n    - Using bigger text for total investment value\n        \n        ![](assets/hedge-foundation-price-charts.png)\n        \n    - Using smaller text for extra details like **Total Yield** and **Performance%**\n        \n        ![](assets/hedge-foundation-total-yeild.png)\n        \n- Arrange information in ways that are natural to read:\n    - **Z-pattern**: Perfect for comparing different investments side by side\n    - **F-pattern**: Makes it easy to scan long lists of numbers from left to right\n    \n    ![](assets/hedge-foundation-reading-patterns.png)\n    \n### 3. Select the right chart types for different data sets\n\n**Problem**: Different types of data require specific visualization methods. Choosing the wrong chart type can cause confusion, making it hard for investors to interpret trends, compare assets, and evaluate risks effectively. Poor data representation can lead to misinformed investment decisions and missed opportunities.\n\n**Solution**:\n\n- **Show one big number**: For example, showing **Assets Under Management** right on the dashboard helps investors quickly see how much money is being managed.\n- **Compare values within and between groups**: **Bar Charts** and **Stacked Bar Charts** allow investors to analyze and compare the performance of different portfolios or asset groups side by side, making it easier to spot trends and discrepancies.\n- **Show relative composition of data**: **Pie Charts** and **Treemaps** illustrate portfolio allocations by asset type or industry.\n- **Display change over time**: **Line Charts** and **Candlestick Charts** track price trends and portfolio fluctuations over time.\n- **Explain relationships between metrics**: **Scatter Plots** and **Correlation Matrices** help investors see how BTC Dominance relates to altcoin volatility.\n- **Plot geographical data**: For hedge funds with global asset distribution, **Map Charts** visualize investment flows across regions.\n- **Show detailed data for multiple assets**: **Tables** provide investors with a structured way to analyze asset details, compare performance metrics, and assess risk across different holdings efficiently.\n\n![](assets/hedge-foundation-data-sets.png)\n\n*Source: [Medium](https://medium.com/gooddata-developers/how-to-choose-the-best-chart-type-to-visualize-your-data-85c866ca13a1)*\n\n## AI-based UI evaluation tools\n\nAfter completing the design, we utilized various AI tools to predict user interactions and assess UI effectiveness. This allowed us to refine the interface for better usability. One example is **Attention Insight**, which provides heatmaps that highlight areas attracting the most user attention. Warmer colors indicate high engagement, while cooler colors suggest areas receiving less attention. Areas with no color indicate almost zero engagement.\n\nTo ensure an optimal user experience, we analyzed heatmaps generated by AI-based tools to identify **attention hotspots** and assess whether **key areas** were effectively capturing user focus. This data-driven approach allowed us to refine the **visual hierarchy**, ensuring that critical information was immediately noticeable. Additionally, we used tools like Predict by Neurons and 3M's Visual Attention System (VAS) to compare accuracy levels, and Heurix to conduct automated UX evaluations for the website.\n\n![](assets/hedge-foundation-heat-map.png)\n\n*The heatmap from Attention Insight shows 90-96% accuracy in predicting where visitors will look in the first 3-5 seconds of seeing the design.\nÂ© Attention Insight*\n\n## Key lessons from designing UI for Hedge Foundation\n\nFrom research and UI optimization, we have identified several important takeaways:\n\n1. **Good information layout speeds up decision-making**: Investors can understand data faster when charts and information are shown in a clear way.\n2. **Show less information**: By hiding less important data and only showing what's needed, we help users focus better.\n3. **Color and layout choices significantly impact UX**: Even small design adjustments can change how users interpret data and make investment decisions.\n4. **AI in design workflow**: Automates repetitive tasks to help designers focus on key improvements. It evaluates designs based on industry standards, detects usability issues, and generates predictive heatmaps to improve user engagement.\n\n## Conclusion\n\nBy using the right charts, showing less information, and making data easy to understand, Hedge Foundation makes things better for users. Better dashboards, clear information, and good charts make trading simpler, reduce mistakes, and help with investment results.\n\nLooking ahead, integrating AI and advanced data analytics will continue to refine UI/UX, enabling hedge funds to optimize their strategies and deliver greater value to investors.","title":"Hedge Foundation - Optimizing UI for effective investment experience","short_title":"","description":"Designing the UI for a blockchain-based hedge fund platform like Hedge Foundation is not just about creating a visually appealing interface. The real challenge lies in ensuring that complex financial data is presented in an intuitive, understandable way that helps users make quick investment decisions. Optimizing the UI plays a crucial role in allowing investors to access accurate data without feeling overwhelmed by excessive information.","tags":["ux-ui","fintech","blockchain"],"pinned":false,"draft":false,"hiring":false,"authors":["anna"],"date":"Tue Feb 25 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/optimizing-ui-for-effective-investment-experience.md","slugArray":["playground","use-cases","optimizing-ui-for-effective-investment-experience"]},{"content":"\nWith the growth of financial models, transactional data has become larger and increasingly diverse. It includes a wide range of fields:\n\n- Stock purchases and sales\n- E-commerce site transaction history\n- Cryptocurrency purchases on the blockchain marketplace\n- Manufacturer's materials orders\n\nThe common point is the requirement of reporting each time a phase is completed. For each specific period, the transactional data must be aggregated, computed, and analyzed to produce the final summary for the user. From this report, the user can gain insight, evaluate the cost in the previous period, estimate, and plan for the future.\n\nThe \"report\" and \"summary\" are the \"historical data\" that is mentioned in the title. This post is the strange yet familiar journey to remind the name of the techniques we use to \"persist historical data\" in our transactional system.\n\n### Data persistence, why do we need to persist our historical data?\n\nThe longevity of data after the application or process that created it is done or crashed is the data persistence. Once data is persisted, each time the application is opened again, the same data is retrieved from the data storage, giving a seamless experience to the user no matter how much time has passed.\n\n![alt text](assets/binance-order-history.png) _Figure 1: Order history on the Binance, a popular trading marketplace. Each transaction is affected by the asset's price. If it is not persisted, it may wrong in the next query_\n\nFor example, in a cryptocurrency marketplace, as a normal user, once I make an order, a transaction is created with an amount of fee. By aggregating entire transactions in each month, a monthly report can be produced easily. In this way, I can reproduce the report of a specific month in the past without storing any additional things such as monthly reports.\n\nBut the marketplace is more than just normal transactions. Imagine, I am not a normal user, but a gold badge V.I.P. With this badge, I am discounted 20% of the fee on each transaction. One month later, because my total trading value increased spectacularly, I was promoted to the next V.I.P level with 50% of the fee on each transaction. At the end of this month, we still easily produce the current month's report with new applied discount. But when I click to see the previous month's report, it is wrong because the discount percentage has changed. It is the same as when we think about \"How can we produce the quarterly or yearly summary?\". This is the reason why we need to convert transactional data to historical data and persist it cyclically.\n\nThe idea of long-time reports brings us to another question. Unlike a nascent market like cryptocurrencies, the stock market has hundreds of years of history. How much computing power is enough for us to analyze the market trend over 50 years? This question is another aspect that we will answer later.\n\n### Snapshot pattern, data snapshot\n\nA snapshot aka memento is normally mentioned as a complete copy of a dataset or system at a specific point in its life cycle. As a \"photograph\" of the data that represents its exact state at that moment. It is typically used in rolling back to the previous object's state once something goes wrong; running tests or analysis data in the production-like environment; or backup and preserving data for compliance or audit purposes. Data snapshot means the applying of snapshot pattern in the data processing. Depending on the use case, we choose proper strategy type of snapshot.\n\n![alt text](assets/aws-ebs-snapshot-function.png) _Figure 2: AWS EBS snapshot mechanism showing how complete data states are captured and stored in S3 for recovery purposes_\n\nThese are 3 popular types of data snapshots. Firstly, the **full snapshot** copies all data in the system at a specific time. Because of its data integrity characteristic, it is often used in backup and restore data before major upgrades, compliance audits that require the complete system state, or data warehouse periodic loads.\n\nIn another scenario, when we don't want to snapshot the entire database but just the data that has changed since the last snapshot, we use **incremental snapshot**. This type of snapshot is typically used in monitoring systems, data reporting/summary, project documentation versioning, or database recovery.\n\n![alt text](assets/snapshot-type.png) _Figure 3: Visualization of the way each data snapshot strategy works when it is implemented to backup database_\n\nThe final type is the **differential snapshot**. As its name, this strategy stores all changes in the database since the original baseline snapshot. Its purpose is nearly similar to incremental snapshots. But they have a bit of difference. Let me make it more transparent with an example.\n\nImagine our system has implemented all 3 types of snapshot strategies: full snapshot each Monday, daily incremental snapshots, and daily differential snapshot. In this way, on the Tuesday, we have the following snapshot data:\n\n- Full snapshot on Monday aka the weekly baseline snapshot\n- Two snapshots that contain data generated on Monday and Tuesday separately that are produced by the incremental snapshots\n- Two snapshots that are produced by the differential snapshots. First, keep the data for the Monday. Another keep data for both Monday and Tuesday.\n\nAfter Tuesday, a problem happens in the system that requires us to recover data. We can choose to use the incremental Tuesday snapshot to recover Tuesday's data or use the differential Tuesday snapshot to recover data for both Monday and Tuesday. It depends on the use case and your strategy.\n\n## Use snapshot pattern to persist historical data\n\nBack to the problem at the beginning of this post, we don't want to re-calculate our finance report which is affected by multiple factors over time, each time a user makes a new request. So we need to persist the summary to historical data. In simple words, it is the progress of collecting transactional data, aggregating and calculating the report for each period. Finally, we store these reports in the database as snapshot records.\n\nYou may think that this progress looks familiar to your experiences when developing applications in your career. And it does not relate to any strategy that is mentioned above. If you feel it, you are right but wrong. Firstly, it is actually a normal practice when developing this type of application. We implement this feature as a feasible part of our database. But rarely think about it seriously. Second, the next story is one of the cases in which we use **incremental snapshot** to persist our historical data. Lets go to our hypothetical problem.\n\n### Hypothetic: simple cryptocurrency trading system\n\nImagine we are required to implement a simple cryptocurrency trading application. In this application, a user can open BUY orders to purchase supported assets. Once the order is opened, it finds SELL orders that match prices together to execute the transaction continuously until the entire amount of BUY orders is served. It is typically the order-matching engine. We will not discuss it here. The main point I want to mention is that we have a lot of transaction records under the hood once a transaction happens on the application. Besides, the transactions that result from trading, the transfers between accounts and some other types of fees can also affect the trading process.\n\n### Problem: the profit report is inconsistent over time\n\nIt is not a problem until the trade is considered completed. And users in the trading group begin sharing the profit. In a few first trade periods, it is still simple and easy to share because every user is a normal user, each type of fee is fixed. The system just only needs to sum the profit or loss and share it to users equally.\n\nOver time, the new features are applied. Users are promoted to V.I.P with proper service discounts. And the amount of transactions becomes large. Each time a user requests to see the report, a large computing power is required. However, the result is not even true because the current user's V.I.P level is different from the time these transactions are executed.\n\n### Solution: persist the trading history each time a trade is done\n\nTo address the above challenge, as I mentioned at the beginning of this part, we use the **incremental snapshot** to persist trading history and prevent the result from recalculation. In this way, we can ensure that reports accurately reflect the conditions that were in place at the time of the trade.\n\nThis **incremental snapshot** does not just capture all additional data in the database from the previous trade period until now such as all orders, all fees, all transfers, and every profit and loss. It also aggregates all of these data into a few records and distributes them to proper snapshot tables in the database such as the `account_trade_sets` table that represents everything related to a completed trade like `opened_at`, `closed_at`, `profit_and_loss`, `total_fees`; or the `trade_set_sharing` table that contains profit sharing information to distribute proper profit to users in the same trading group.\n\nOnce this solution is implemented, our application can show the exact trading report of any trading period in the past without doing any calculations that lead to the wrong result.\n\n### Beyond just correct the reporting process\n\nYou can be aware that this solution does not just correct the trading report but also helps to improve user experience by decreasing the response time. It is possible because all values are pre-calculated. So we do not need to calculate them again. It is evident. But having one more thing we may missed that is mentioned in the question mentioned in the first part.\n\n> How much computing power is enough for us to analyze the market trend over 50 years?\n\nWhen applying the snapshot pattern to the application, we accidentally resolve another problem that looks similar to the above question. We can statement it as follows\n\n> \"How can we export trading reports for a long period?\"\n\nSince snapshots store pre-calculated summaries for each trading period, generating long-term reports involves retrieving and consolidating these snapshots without recalculating historical data.\n\n> \"How can we monitor the accumulated profit and loss on the proper time series trading chart with minimum computing power?\".\n\nInstead of recalculating the sum of profit and loss from a massive volume of transactions for each point on the chart, we leverage snapshots. By summing the pre-aggregated values from all snapshots up to the desired point and combining them with the relevant transactions between the last snapshot's end time and the target point, we efficiently calculate the accumulated profit and loss with minimal computational effort.\n\nWe can considered that we are using DB-lvl **memoization** incidentally. That is an optimization technique used in programming where the results of expensive function calls are stored in a cache. When the function is called again with the same inputs, the result is retrieved from the cache instead of recomputing it.\n\n### Conclusions\n\nThe snapshot pattern is deceptively simple. This is the reason why it is often overlooked during development. We are often focus on solving immediate problems, implementing features without considering the long-term implications of recalculations and data inconsistency. The simplicity of this approach masks its power to address complex issues like historical data accuracy and computational efficiency.\n\nIt ensures accurate and consistent data, reduces computational overhead, and improves user experience by delivering faster query responses. Moreover, by persisting historical data, businesses gain a robust foundation for long-term analytics, such as trend analysis and strategic decision-making. It highlights the importance of viewing data as a long-term asset, requiring strategies like snapshots to ensure its reliability and usability over time.\n\nWhether you are building financial applications, e-commerce platforms, or any system requiring accurate historical records, understanding and applying the snapshot pattern can elevate your application's performance and reliability.\n","title":"Implementing data snapshot pattern to persist historical data","short_title":"","description":"A technical exploration of implementing the data snapshot pattern for efficient historical data persistence","tags":["data","fintech","blockchain"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Wed Dec 11 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/persist-history-using-data-snapshot-pattern.md","slugArray":["playground","use-cases","persist-history-using-data-snapshot-pattern"]},{"content":"\n## Executive summary\n\nRecovering historical trading profit and loss (PnL) data is a critical challenge for finance and cryptocurrency platforms. When historical records are unavailable, users cannot validate past trading strategies, assess long-term performance, or reconcile discrepancies. This blog details how I tackled this problem by transforming a technically daunting challenge into a robust, maintainable data pipeline solution.\n\n## Background and context\n\n### What is trading PnL?\n\nIn trading, **Profit and loss (PnL)** represents financial outcomes:\n\n- **Realized PnL**: The actual profit or loss from completed trades.\n- **Unrealized PnL**: The potential gain or loss from open positions.\n\nFor instance, when you close a Bitcoin position at a higher price than you entered, your realized PnL reflects the profit after fees. If the position is still open, unrealized PnL tracks potential outcomes as prices fluctuate.\n\n### Why does historical PnL matter?\n\nHistorical PnL data provides traders with:\n\n1. **Performance insights**: Understanding which strategies worked and which didnâ€™t.\n2. **Compliance and reporting**: Regulatory or internal needs often require accurate historical data.\n3. **Strategy validation**: Testing new algorithms against past market conditions relies on accurate PnL records.\n\n### The problem at hand\n\nWhile designing a trading PnL chart for my platform, a significant gap emerged: historical PnL data for certain periods was missing. The existing system calculated PnL in real-time but didnâ€™t store intermediary data, making reconstruction impossible without extensive changes to the codebase.\n\n## The challenge\n\n> â€œHow can we reconstruct historical trading PnL data efficiently when the original records no longer exist?â€\n\nThis question encapsulated two core issues:\n\n1. **Data loss**: Real-time calculations discarded intermediary steps, leaving gaps in historical records.\n2. **System complexity**: The platformâ€™s codebase, written in Elixir, was intricate, tightly coupled, and unfamiliar to me.\n\nMoreover, the systemâ€™s reliance on multiple data sources (trades, market prices, fees) and the sheer volume of transactions compounded the problem.\n\n## Technical requirements\n\nTo reconstruct PnL, the following were essential:\n\n- **Historical trade data**: Information about each executed trade, including sizes, directions, and timestamps.\n- **Market prices**: Historical price data to calculate unrealized PnL.\n- **Fee details**: Trading commissions, funding rates, and other costs affecting PnL.\n- **Efficient processing**: Handling massive datasets without overloading system resources.\n\n## System analysis\n\n### From complex code to data flows\n\nInstead of delving into intricate application logic, I reimagined the system as a series of **data flows**, where data is ingested, transformed, and stored across multiple layers. Below is the existing flow:\n\n```mermaid\nflowchart LR\n    subgraph Sources\n        B[Binance]\n        R[REST API]\n    end\n\n    subgraph DataLake\n        ETS[Elixir ETS]\n    end\n\n    subgraph Processing\n        MV[Materialized Views]\n        DB[(PostgreSQL)]\n    end\n\n    subgraph Output\n        A[Analytics]\n        Rep[Reports]\n    end\n\n    B --> ETS\n    R --> ETS\n    ETS --> DB\n    DB --> MV\n    MV --> A\n    MV --> Rep\n```\n\n- **Data sources**: Trading data originates from Binance (market data, trades) and a REST API.\n- **Temporary storage**: Elixir ETS stores raw data temporarily before processing.\n- **Transformation**: Postgres stores normalized data, which is further refined using materialized views for specific use cases.\n- **Outputs**: Processed data powers analytics and reporting tools.\n\n### Reconstructing the flow\n\nFrom the above flow of data, we can easily determine which parts of the flow we should reproduce to find the old PnLs.\n\n- Firstly, data comes from Binance\n- Second, data passes through ETS before processing\n- Finally, data is transformed to missing PnL and stored in Postgresql DB\n\nOne more important thing is the formula to calculate PnL when transforming data in the 3rd step.\n\n- For the realized PnL that represents confirmed gains or losses from closed trades and affects your actual cash balance. The formula is:\n\n  ```go\n  Realized PNL = Î£(closed trade realized PNL)\n  \t\t\t   - Î£(commission, funding, insurance)\n  ```\n\n- For the unrealized PnL, it is the potential profit or loss from open trades. In the simple way, we can calculate it via the following formula\n  ```go\n  Unrealized PNL = Position Size * Direction of Order * (Mark Price - Entry Price)\n  ```\n\nThere are many things that must be reproduced. But we will not implement all of them completely. Some useful data stored in the Postgresql DB can be reused. Letâ€™s check!\n\n- Trading positions information that contain:\n  - Open trade history as `user_trades`\n    - Realized PNL of closed trades\n    - Commission fee for trades\n    - Historical data contains the price, and quantity of assets when open, and close trades. These data can be used to calculate the average entry price at the time proper trade is opened or closed.\n  - Transfer, profit, and fee as `future_incomes`\n    - Funding fee\n    - Locked positions commission fee\n    - Insurance fee (for the future)\n\nComparing to the available data to the above formulas, we can see that everything is enough to calculate the unrealized PnL without Binance. But Binance is needed in retrieving the old marking prices to calculate historically unrealized. So we can illustrate the new flow as follows.\n\n```mermaid\nflowchart LR\n    subgraph Input\n        B[Binance API]\n        DB[(Trading Info DB)]\n    end\n\n    subgraph Processing\n        P[Data Mapping]\n        C[Parallel Processing]\n    end\n\n    subgraph Storage\n        PNL[(PnL Storage)]\n    end\n\n    B -->|Price Data| P\n    DB -->|Trading History| P\n    P --> C\n    C -->|Results| PNL\n```\n\n## Implementation\n\nThe reconstruction process involves five major steps:\n\n1. **Data collection** Fetch necessary data from two sources:\n\n   - **Database**: Historical trading data, fees, commissions.\n   - **Binance API**: Historical Kline data for price points.\n\n2. **Mapping data** Group the data by trading pairs (tokens) for efficient processing.\n\n3. **Token-level calculation** For each token:\n\n   - Use minute-level Kline data to calculate fees, realized PnL, unrealized PnL, and entry prices.\n   - Apply cumulative calculations to ensure accuracy.\n\n4. **Aggregate results** Sum PnL across all positions for the userâ€™s account.\n\n5. **Storage and visualization** Save results back into the database and visualize them in the PnL chart.\n\n```mermaid\nflowchart TD\n    subgraph DataCollection\n        F[Fees Data]\n        T[Trade Data]\n        K[Kline Data]\n    end\n\n    subgraph Processing\n        M[Map by Token]\n        C[Calculate per Token]\n        A[Aggregate Results]\n    end\n\n    F --> M\n    T --> M\n    K --> M\n    M --> C\n    C --> A\n```\n\n## Outstanding challenges\n\n**Volume of data** Minute-level Kline data is essential for accuracy, but retrieving and processing it is resource-intensive:\n\n- One month of data requires **43,200 points per token**.\n- Accounts with **300+ open positions** significantly amplify the workload.\n- Binance API limits Kline data to **1,500 points per request**, introducing additional complexity.\n\n**PnL accuracy** PnL, specifically realized PnL, is stuck to the trade set to help us know the total PnL of this trade set by accumulating the closed trade PnL and fee over time. So if we retrieve the list of user trades randomly, it may produce the wrong PnL and let our report make nonsense.\n\n## Optimization strategies\n\n- **Time-series state reconstruction**: Our trading events naturally fall into their proper timeline. Each trade, fee, and price change finds its proper place in the chronological sequence. So our system can reconstruct a trading position's PnL at any moment.\n- **Map-reduce**: As mentioned above, an account needs a long time to process. So forcing all our data through a single filter is impossible. The real benchmark test takes me about 5 hours to recover 1 trade set. By creating mapping by trading pairs, the data can be processed in parallel and done in minutes.\n- **Parallel processing**: Awakening that each token has its own PnL let us think about trying to process them in parallel to reduce the time cost.\n- **Running total**: Basically, each trade set is considered done once all positions of the account are closed completely. It means from a specific time, we can track the cumulative sum of positions (both long and short) for each account. A trade cycle is complete when the cumulative quantity equals zero. So we can update the flow a bit to resolve the missing trade set problem.\n\n```mermaid\nflowchart TD\n    TS[Find Trade Sets]\n    P[Process Each Set]\n    C[Calculate PnL]\n\n    TS --> P\n    P --> C\n```\n\n---\n\n## Quality assurance\n\nTo validate the reconstruction process:\n\n- **Unit testing**: Test with small data samples, validating each step with detailed logs.\n- **Cross-validation**: Compare reconstructed PnL with existing records in the database.\n- **Visual analysis**: Render reconstructed data onto charts to ensure trends align with expected strategies.\n\n## Conclusion\n\nThis case study highlights the power of a **data-centric approach** in solving financial system problems. By treating the challenge as a structured data pipeline problem, we avoided risky codebase modifications and developed a robust, scalable solution. Techniques such as parallel processing, time-series reconstruction, and efficient data retrieval were key to solving the problem within system constraints.\n\nThis approach demonstrates that **data-driven solutions** can effectively address complex challenges while maintaining flexibility and performance for future needs.\n","title":"Reconstructing historical trading PnL: a data pipeline approach","short_title":"","description":"A detailed look at how we rebuilt historical trading PnL data through an efficient data pipeline approach, transforming a complex problem into a maintainable solution.","tags":["data","fintech","blockchain"],"pinned":false,"draft":false,"hiring":false,"authors":["bievh"],"date":"Mon Nov 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","filePath":"playground/use-cases/reconstructing_trading_pnl_data_pipeline_approach.md","slugArray":["playground","use-cases","reconstructing_trading_pnl_data_pipeline_approach"]}],"isListPage":true},"__N_SSG":true}